1:178	Incremental Feature Selection and lscript1 Regularization for Relaxed Maximum-Entropy Modeling Stefan Riezler and Alexander Vasserman Palo Alto Research Center 3333 Coyote Hill Road, Palo Alto, CA 94304 Abstract We present an approach to bounded constraintrelaxation for entropy maximization that corresponds to using a double-exponential prior or lscript1 regularizer in likelihood maximization for log-linear models.
2:178	We show that a combined incremental feature selection and regularization method can be established for maximum entropy modeling by a natural incorporation of the regularizer into gradientbased feature selection, following Perkins et al.3:178	(2003).
4:178	This provides an efficient alternative to standard lscript1 regularization on the full feature set, and a mathematical justification for thresholding techniques used in likelihood-based feature selection.
5:178	Also, we motivate an extension to n-best feature selection for linguistic features sets with moderate redundancy, and present experimental results showing its advantage over lscript0, 1-best lscript1, lscript2 regularization and over standard incremental feature selection for the task of maximum-entropy parsing.1 1 Introduction The maximum-entropy (ME) principle, which prescribes choosing the model that maximizes the entropy out of all models that satisfy given feature constraints, can be seen as a built-in regularization mechanism that avoids overfitting the training data.
6:178	However, it is only a weak regularizer that cannot avoid overfitting in situations where the number of training examples is significantly smaller than the number of features.
7:178	In such situations some features will occur zero times on the training set and receive negative infinity weights, causing the assignment of zero probabilities for inputs including those features.
8:178	Similar assignment of (negative) infinity weights happens to features that are pseudominimal (or pseudo-maximal) on the training set (see Johnson et al.9:178	(1999)), that is, features whose value on correct parses always is less (or greater) 1This research has been funded in part by contract MDA904-03-C-0404 of the Advanced Research and Development Activity, Novel Intelligence from Massive Data program.
10:178	than or equal to their value on all other parses.
11:178	Also, if large features sets are generated automatically from conjunctions of simple feature tests, many features will be redundant.
12:178	Besides overfitting, large feature sets also create the problem of increased time and space complexity.
13:178	Common techniques to deal with these problems are regularization and feature selection.
14:178	For ME models, the use of an lscript2 regularizer, corresponding to imposing a Gaussian prior on the parameter values, has been proposed by Johnson et al.15:178	(1999) and Chen and Rosenfeld (1999).
16:178	Feature selection for ME models has commonly used simple frequencybased cut-off, or likelihood-based feature induction as introduced by Della Pietra et al.17:178	(1997).
18:178	Whereas lscript2 regularization produces excellent generalization performance and effectively avoids numerical problems, parameter values almost never decrease to zero, leaving the problem of inefficient computation with the full feature set.
19:178	In contrast, feature selection methods effectively decrease computational complexity by selecting a fraction of the feature set for computation; however, generalization performance suffers from the ad-hoc character of hard thresholds on feature counts or likelihood gains.
20:178	Tibshirani (1996) proposed a technique based on lscript1 regularization that embeds feature selection into regularization such that both a precise assessment of the reliability of features and the decision about inclusion or deletion of features can be done in the same framework.
21:178	Feature sparsity is produced by the polyhedral structure of the lscript1 norm which exhibits a gradient discontinuity at zero that tends to force a subset of parameter values to be exactly zero at the optimum.
22:178	Since this discontinuity makes optimization a hard numerical problem, standard gradient-based techniques for estimation cannot be applied directly.
23:178	Tibshirani (1996) presents a specialized optimization algorithm for lscript1 regularization for linear least-squares regression called the Lasso algorithm.
24:178	Goodman (2003) and Kazama and Tsujii (2003) employ standard iterative scaling and conjugate gradient techniques, however, for regularization a simplified one-sided exponential prior is employed which is non-zero only for non-negative parameter values.
25:178	In these approaches the full feature space is considered in estimation, so savings in computational complexity are gained only in applications of the resulting sparse models.
26:178	Perkins et al.27:178	(2003) presented an approach that combines lscript1 based regularization with incremental feature selection.
28:178	Their basic idea is to start with a model in which almost all weights are zero, and iteratively decide, by comparing regularized feature gradients, which weight should be adjusted away from zero in order to decrease the regularized objective function by the maximum amount.
29:178	The lscript1 regularizer is thus used directly for incremental feature selection, which on the one hand makes feature selection fast, and on the other hand avoids numerical problems for zero-valued weights since only non-zero weights are included in the model.
30:178	Besides the experimental evidence presented in these papers, recently a theoretical account on the superior sample complexity of lscript1 over lscript2 regularization has been presented by Ng (2004), showing logarithmic versus linear growth in the number of irrelevant features for lscript1 versus lscript2 regularized logistic regression.
31:178	In this paper, we apply lscript1 regularization to loglinear models, and motivate our approach in terms of maximum entropy estimation subject to relaxed constraints.
32:178	We apply the gradient-based feature selection technique of Perkins et al.33:178	(2003) to our framework, and improve its computational complexity by an n-best feature inclusion technique.
34:178	This extension is tailored to linguistically motivated feature sets where the number of irrelevant features is moderate.
35:178	In experiments on real-world data from maximum-entropy parsing, we show the advantage of n-best lscript1 regularization over lscript2, lscript1, lscript0 regularization and standard incremental feature selection in terms of better computational complexity and improved generalization performance.
36:178	2 lscriptp Regularizers for Log-Linear Models Let p(x|y) = e summationtextn i=1 ifi(x,y)summationtext x e summationtextn i=1 ifi(x,y) be a conditional log-linear model defined by feature functions f and log-parameters .
37:178	For data {(xj,yj)}mj=1, the objective function to be minimized in lscriptp regularization of the negative log-likelihood L() is C() = L() + p() =  1m msummationdisplay j=1 lnp(xj|yj) + bardblbardblpp The regularizer family p() is defined by the Minkowski lscriptp norm of the parameter vector  raised to the pth power, i.e. bardblbardblpp = summationtextni=1|i|p. The essence of this regularizer family is to penalize overly large parameter values.
38:178	If p = 2, the regularizer corresponds to a zero-mean Gaussian prior distribution on the parameters with  corresponding to the inverse variance of the Gaussian.
39:178	If p = 0, the regularizer is equivalent to setting a limit on the maximum number of non-zero weights.
40:178	In our experiments we replace lscript0 regularization by the related technique of frequency-based feature cutoff.
41:178	lscript1 regularization is defined by the case where p = 1.
42:178	Here parameters are penalized in the sum of their absolute values, which corresponds to applying a zero-mean Laplacian or double exponential prior distribution of the form p(i) = 12e|i| with  = 1 being proportional to the inverse standard deviation 2.
43:178	In contrast to the Gaussian, the Laplacian prior puts more mass near zero (and in the tails), thus tightening the prior by decreasing the standard deviation  provides stronger regularization against overfitting and produces more zerovalued parameter estimates.
44:178	In terms of lscript1-norm regularization, feature sparsity can be explained by the following observation: Since every non-zero parameter weight incurs a regularizer penalty of |i|, its contribution to minimizing the negative loglikelihood has to outweigh this penalty.
45:178	Thus parameters values where the gradient at  = 0 is vextendsinglevextendsingle vextendsinglevextendsingleL() i vextendsinglevextendsingle vextendsinglevextendsingle  (1) can be kept zero without changing the optimality of the solution.
46:178	3 Bounded Constraint Relaxation for Maximum Entropy Estimation As shown by Lebanon and Lafferty (2001), in terms of convex duality, a regularization term for the dual problem corresponds to a potential on the constraint values in the primal problem.
47:178	For a dual problem of regularized likelihood estimation for log-linear models, the corresponding primal problem is a maximum entropy problem subject to relaxed constraints.
48:178	Let H(p) denote the entropy with respect to probability function p, and g : IRn  IR be a convex potential function, and p[] and p[] be expectations with respect to the empirical distribution p(x,y) = 1m summationtextmj=1 (xj,x)(yj,y) and the model distribution p(x|y)p(y).
49:178	The primal problem can then be stated as Maximize H(p)g(c) subject to p[fi] p[fi] = ci,i = 1,,,n Constraint relaxation is achieved in that equality of the feature expectations is not enforced, but a certain amount of overshooting or undershooting is allowed by a parameter vector c  IRn whose potential is determined by a convex function g(c) that is combined with the entropy term H(p).
50:178	In the case of lscript2 regularization, the potential function for the primal problem is a quadratic penalty of the form 12 summationtexti c2i for  = 12 i,i = 1,,,n (Lebanon and Lafferty, 2001).
51:178	In order to recover the specific form of the primal problem for our case, we have to start from the given dual problem.
52:178	Following Lebanon and Lafferty (2001), the dual function for regularized estimation can be expressed in terms of the dual function (p,) for the unregularized case and the convex conjugate g() of the potential function g(c).
53:178	In our case the negative of (p,) corresponds to the likelihood term L(), and the negative of the convex conjugate g() is the lscript1 regularizer.
54:178	Thus our dual problem can be stated as star = argmax  (p,)g() = argmin  L() + bardblbardbl11 Since for convex and closed functions, the conjugate of the conjugate is the original function, i.e. g = g (Boyd and Vandenberghe, 2004), the potential function g(c) for the primal problem can be recovered by calculating the conjugate g of the conjugate g() = bardblbardbl11.
55:178	In our case, we get g(c) = g(c) = braceleftbigg 0 bardblcbardbl     otherwise (2) where bardblcbardbl = max|c1|,,,|cn|}.
56:178	A proof for this proposition is given in the Appendix.
57:178	The resulting potential function g(c) is the indicator function on the interval [,].
58:178	That is, it restricts the allowable amount of constraint relaxation to at most .
59:178	From this perspective, increasing  means to allow for more slack in constraint satisfaction, which in turn allows to fit a more uniform, less overfitting distribution to the data.
60:178	For features that are included in the model, the parameter values have to be adjusted away from zero to meet the constraints |p[fi] p[fi]|  , i = 1,,,n (3) Initialization: Initialize selected features S to , and zero-weighted features Z to the full feature set, yielding the uniform distribution p(0),S(0).
61:178	n-best grafting: For steps t = 1,,,T, (1) for all features fi in Z(t1), calculate vextendsinglevextendsingle vextendsinglevextendsingle vextendsingle L((t1),S(t1)) i vextendsinglevextendsingle vextendsinglevextendsingle vextendsingle> , (2) S(t) := S(t1) N(t) and Z(t) := Z(t1) \ N(t) where N(t) is the set of n-best features passing the test in (1), (3) perform conjugate gradient optimization to find the optimal model pstar,S(t) where  is initialized at (t1), and (t) := star = argmax  C(,S(t)).
62:178	Stopping condition: Stop if for all fi in Z(t1): vextendsinglevextendsingle vextendsinglevextendsingle vextendsingle L((t1),S(t1)) i vextendsinglevextendsingle vextendsinglevextendsingle vextendsingle  Figure 1: n-best gradient feature testing For features that meet the constraints without parameter adjustment, parameter values can be kept at zero, effectively discarding the features.
63:178	Note that equality of equations 3 and 1 connects the maximum entropy problem to likelihood regularization.
64:178	4 Standardization Note that the p regularizer presented above penalizes the model parameters uniformly, corresponding to imposing a uniform variance onto all model parameters.
65:178	This motivates a normalization of input data to the same scale.
66:178	A standard technique to achieve this is to linearly rescale each feature count to zero mean and standard deviation of one over all training data.
67:178	The same rescaling has to be done for training and application of the model to unseen data.
68:178	As we will see in the experimental evaluation presented below, a standardization of input data can also dramatically improve convergence behavior in unregularized optimization.
69:178	Furthermore, parameter values estimated from standardized feature counts are directly interpretable to humans.
70:178	Combined with feature selection, interpretable parameter weights are particularly useful for error analysis of the models feature design.
71:178	5 Incremental n-best Feature Selection The basic idea of the grafting (for gradient feature testing) algorithm presented by (Perkins et al. , 2003) is to assume a tendency of lscript1 regularization to produce a large number of zero-valued parameters at the functions optimum, thus to start with all-zero weights, and incrementally add features to the model only if adjusting their parameter weights away from zero sufficiently decreases the optimization criterion.
72:178	This idea allows for efficient, incremental feature selection, and at the same time avoids numerical problems caused by the discontinuity of the gradient in lscript1 regularization.
73:178	Furthermore, the regularizer is incorporated directly into a criterion for feature selection, based on the observation made above: It only makes sense to add a feature to the model if the regularizer penalty is outweighed by the reduction in negative log-likelihood.
74:178	Thus features considered for selection have to pass the following test: vextendsinglevextendsingle vextendsinglevextendsingleL() i vextendsinglevextendsingle vextendsinglevextendsingle>  In the grafting procedure suggested by (Perkins et al. , 2003), this gradient test is applied to each feature, and at each step the feature passing the test with maximum magnitude is added to the model.
75:178	Adding one feature at a time effectively discards noisy and irrelevant features, however, the overhead introduced by grafting can outweigh the gain in efficiency if there is a moderate number of noisy and truly redundant features.
76:178	In such cases, it is beneficial to add a number of n > 1 features at each step, where n is adjusted by cross-validation or on a held-out data set.
77:178	In the experiments on maximumentropy parsing presented below, a feature set of linguistically motivated features is used that exhibits only a moderate amount of redundancy.
78:178	We will see that for such cases, n-best feature selection considerably improves computational complexity, and also achieves slightly better generalization performance.
79:178	After adding n  1 features to the model in a grafting step, the model is optimized with respect to all parameters corresponding to currently included features.
80:178	This optimization is done by calling a gradient-based general purpose optimization routine for the regularized objective function.
81:178	We use a conjugate gradient routine for this purpose (Minka, 2001; Malouf, 2002)2.
82:178	The gradient of our criterion with respect to a parameter i is: C() i = 1 m msummationdisplay k=1 L() i +  sign(i) 2Note that despite gradient feature testing, the parameters for some features can be driven to zero in conjugate gradient optimization of the lscript1-regularized objective function.
83:178	Care has to be taken to catch those features and prune them explicitly to avoid numerical instability.
84:178	The sign of i decides if  is added or subtracted from the gradient for feature fi.
85:178	For a feature that is newly added to the model and thus has weight i = 0, we use the feature gradient test to determine the sign.
86:178	If L()i > , we know that C()i > 0, thus we let sign(i) = 1 in order to decrease C. Following the same rationale, if L()i <  we set sign(i) = +1.
87:178	An outline of an n-best grafting algorithm is given in Fig.
88:178	1.
89:178	6 Experiments 6.1 Train and Test Data In the experiments presented in this paper, we evaluate lscript2, lscript1, and lscript0 regularization on the task of stochastic parsing with maximum-entropy models For our experiments, we used a stochastic parsing system for LFG that we trained on section 02-21 of the UPenn Wall Street Journal treebank (Marcus et al. , 1993) by discriminative estimation of a conditional maximum-entropy model from partially labeled data (see Riezler et al.90:178	(2002)).
91:178	For estimation and best-parse searching, efficient dynamicprogramming techniques over features forests are employed (see Kaplan et al.92:178	(2004)).
93:178	For the setup of discriminative estimation from partially labeled data, we found that a restriction of the training data to sentences with a relatively low ambiguity rate was possible at no loss in accuracy compared to training from all sentences.
94:178	Furthermore, data were restricted to sentences of which a discriminative learner can possibly take advantage, i.e. sentences where the set of parses assigned to the labeled string is a proper subset of the parses assigned to the unlabeled string.
95:178	Together with a restriction to examples that could be parsed by the full grammar and did not have to use a backoff mechanism of fragment parses, this resulted in a training set of 10,000 examples with at most 100 parses.
96:178	Evaluation was done on the PARC 700 dependency bank3, which is an LFG annotation of 700 examples randomly extracted from section 23 of the UPenn WSJ treebank.
97:178	To tune regularization parameters, we split the PARC 700 into a heldout and test set of equal size.
98:178	6.2 Feature Construction Table 1 shows the 11 feature templates that were used in our experiments to create 60,109 features.
99:178	On the around 300,000 parses for 10,000 sentences in our final training set, 10,986 features were active, resulting in a matrix of active features times parses that has 66 million non-zero entries.
100:178	The scale of this experiment is comparable to experiments where 3http://www2.parc.com/istl/groups/nltt/fsbank/ Table 1: Feature templates name parameters activation condition Local Templates cs label label constituent label is present in parse cs adj label parent label, constituent child label is child label child of constituent parent label cs right branch constituent has right child cs conj nonpar depth non-parallel conjuncts within depth levels fs attrs attrs f-structure attribute is one of attrs fs attr value attr, value attribute attr has value value fs attr subsets attr sum of cardinalities of subsets of attr lex subcat pred, args sets verb pred has one of args sets as arguments Non-Local (Top-Down) Templates cs embedded label, size chain of size constituents labeled label embedded into one another cs sub label ancestor label, constituent descendant label descendant label is descendant of ancestor label fs aunt subattr aunts, parents, one of descendants is descendant of one of descendants parents which is a sister of one of aunts much larger, but sparser feature sets are employed4.
101:178	The reason why the matrix of non-zeroes is less sparse in our case is that most of our feature templates are instantiated to linguistically motivated cases, and only a few feature templates encode all possible conjunctions of simple feature tests.
102:178	Redundant features are introduced mostly by the latter templates, whereas the former features are generalizations over possible combinations of grammar constants.
103:178	We conjecture that feature sets like this are typical for natural language applications.
104:178	Efficient feature detection is achieved by a combination of hashing and dynamic programming on the packed representation of cand f-structures (Maxwell and Kaplan, 1993).
105:178	Features can be described as local and non-local, depending on the size of the graph that has to be traversed in their computation.
106:178	For each local template one of the parameters is selected as a key for hashing.
107:178	Non-local features are treated as two (or more) local sub-features.
108:178	Packed structures are traversed depth-first, visiting each node only once.
109:178	Only the features keyed on the label of the current node are considered for matching.
110:178	For each non-local feature, the contexts of matching subfeatures are stored at the respective nodes, propagated upward in dynamic programing fashion, and conjoined with contexts of other subfeatures of the feature.
111:178	Fully matched features are associated with the corresponding contexts resulting in a feature-annotated and/or-forest.
112:178	This annotated 4For example, Malouf (2002) reports a matrix of non-zeroes that has 55 million entries for a shallow parsing experiment where 260,000 features were employed.
113:178	and/or forest is exploited for dynamic programming computation in estimation and best parse selection.
114:178	6.3 Experimental Results Table 2 shows the results of an evaluation of five different systems of the test split of the PARC 700 dependency bank.
115:178	The presented systems are unregularized maximum-likelihood estimation of a loglinear model including the full feature set (mle), standardized maximum-likelihood estimation as described in Sect.
116:178	4 (std), lscript0 regularization using frequency-based cutoff, lscript1 regularization using nbest grafting, and lscript2 regularization using a Gaussian prior.
117:178	All lscriptp regularization runs use a standardization of the feature space.
118:178	Special regularization parameters were adjusted on the heldout split, resulting in a cutoff threshold of 16, and penalization factors of 20 and 100 for lscript1 and lscript2 regularization respectively, with an optimal choice of 100 features to be added in each n-best grafting step.
119:178	Performance of these systems is evaluated firstly with respect to F-score on matching dependency relations.
120:178	Note that the F-score values on the PARC 700 dependency bank range between a lower bound of 68.0% for averaging over all parses and an upper bound of 83.6% for the parses producing the best possible matches.
121:178	Furthermore, compression of the full feature set by feature selection, number of conjugate gradient iterations, and computation time (in hours:minutes of elapsed time) are reported.5 5All experiments were run on one CPU of a dual processor AMD Opteron 244 with 1.8GHz clock speed and 4GB of main memory.
122:178	Table 2: F-score, compression, number of iterations, and elapsed time for unregularized and standardized maximum-likelihood estimation, and lscript0, lscript1, and lscript2 regularization on test split of PARC 700 dependency bank.
123:178	mle std lscript0 lscript2 lscript1 F-score 77.9 78.1 78.1 78.9 79.3 compr.
124:178	0 0 18.4 0 82.7 cg its.
125:178	761 371 372 34 226 time 129:12 66:41 60:47 6:19 5:25 Unregularized maximum-likelihood estimation using the full feature set exhibits severe overtraining problems, as the relation of F-score to the number of conjugate gradient iterations shows.
126:178	Standardization of input data can alleviate this problem by improving convergence behavior to half the number of conjugate gradient iterations.
127:178	lscript0 regularization achieves its maximum on the heldout data for a threshold of 16, which results in an estimation run that is slightly faster than standardized estimation using all features, due to a compression of the full feature set by 18%.
128:178	lscript2 regularization benefits from a very tight prior (standard deviation of 0.1 corresponding to penalty 100) that was chosen on the heldout set.
129:178	Despite the fact that no reduction of the full feature set is achieved, this estimation run increases the F-score to 78.9% and improves computation time by a factor of 20 compared to unregularized estimation using all features.
130:178	lscript1 regularization for n-best grafting, however, even improves upon this result by increasing the F-score to 79.3%, further decreasing computation time to 5:25 hours, at a compression of the full feature set of 83%.
131:178	77.5 78 78.5 79 79.5 1 10 100 1000 10000 10 100 1000 F-score NumCGIterations FeaturesAddedAtEachStep F-score a51 a51 a51 a51 a51 a51 a51NumCGIterations+ + + + + + + Figure 2: n-best grafting with n of features added at each step plotted against F-score on test set and conjugate gradient iterations.
132:178	As shown in Fig.
133:178	2, for feature selection from linguistically motivated feature sets with only a moderate amount of truly redundant features, it is crucial to choose the right number n of features to be added in each grafting step.
134:178	The number of conjugate gradient iterations decreases rapidly in the number of features added at each step, whereas F-score evaluated on the test set does not decrease (or increases slightly) until more than 100 features are added in each step.
135:178	100-best grafting thus reduces estimation time by a factor of 10 at no loss in F-score compared to 1-best grafting.
136:178	Further increasing n results in a significant drop in F-score, while smaller n is computationally expensive, and also shows slight overtraining effects.
137:178	Table 3: F-score, compression, number of iterations, and elapsed time for gradient-based incremental feature selection without regularization, and with lscript2, and lscript1 regularization on test split of PARC 700 dependency bank.
138:178	mle-ifs lscript2-ifs lscript1 F-score 78.8 79.1 79.3 compr.
139:178	88.1 81.7 82.7 cg its.
140:178	310 274 226 time 6:04 6:56 5:25 In another experiment we tried to assess the relative contribution of regularization and incremental feature selection to the lscript1-grafting technique.
141:178	Results of this experiments are shown in Table 3.
142:178	In this experiment we applied incremental feature selection using the gradient test described above to unregularized maximum-likelihood estimation (mleifs) and lscript2-regularized maximum-likelihood estimation (lscript2-ifs).
143:178	Threshold parameters  are adjusted on the heldout set, in addition to and independent of regularization parameters such as the variance of the Gaussian prior.
144:178	Results are compared to lscript1regularized grafting as presented above.
145:178	For all runs a number of 100 features to be added in each grafting step is chosen.
146:178	The best result for the mle-ifs run is achieved at a threshold of 25, yielding an F-score of 78.8%.
147:178	This shows that incremental feature selection is a powerful tool to avoid overfitting.
148:178	A further improvement in F-score to 79.1% is achieved by combining incremental feature selection with the lscript2 regularizer at a variance of 0.1 for the Gaussian prior and a threshold of 15.
149:178	Both runs provide excellent compression rates and convergence times.
150:178	However, they are still outperformed by the lscript1 run that achieves a slight improvement in F-score to 79.3% and a slightly better runtime.
151:178	Furthermore, by integrating regularization naturally into thresholding for feature selection, a separate thresholding parameter is avoided in lscript1-based incremental feature selection.
152:178	A theoretical account of the savings in computational complexity that can be achieved by nbest grafting can be given as follows.
153:178	Perkins et al.154:178	(2003) assess the computational complexity for standard gradient-based optimization with the full feature set by  cmp2, for a multiple c of p line minimizations for p derivatives over m data points, each of which has cost .
155:178	In contrast, for grafting, the cost is assessed by adding up the costs for feature testing and optimization for s grafting steps as  (msp+13cms3).
156:178	For n-best grafting as proposed in this paper, the number of steps can be decomposed into s = n  t for n features added at each of t steps.
157:178	This results in a cost of  mtp for feature testing, and  13cmn2t3 for optimization.
158:178	If we assume that t lessmuch n lessmuch s, this indicates considerable savings compared to both 1-best grafting and standard gradient-based optimization.
159:178	7 Discussion and Conclusion A related approach to lscript1 regularization and constraint-relaxation for maximum-entropy modeling has been presented by Kazama and Tsujii (2003).
160:178	In this approach, constraint relaxation is done by allowing two-sided inequality constraints Bi  p[fi]p[fi]  Ai, Ai,Bi > 0 in entropy maximization.
161:178	The dual function is the regularized likelihood function 1 m msummationdisplay j=1 p(xj|yj) nsummationdisplay i=1 iAi  nsummationdisplay i=1 iBi where the two parameter vectors  and  replace our parameter vector , and i,i  0.
162:178	This regularizer corresponds to a simplification of doublesided exponentials to a one-sided exponential distribution which is non-zero only for non-negative parameters.
163:178	The use of one-sided exponential priors for log-linear models has also been proposed by Goodman (2003), however, without a motivation in a maximum entropy framework.
164:178	The fact that Kazama and Tsujii (2003) allow for lower and upper bounds of different size requires the parameter space to be doubled in their approach.
165:178	Furthermore, similar to Goodman (2003), the requirement to work with a one-sided strictly positive exponential distribution makes it necessary to double the feature space to account for (dis)preferences in terms of strictly positive parameter values.
166:178	These are considerable computational and implementational disadvantages of these approaches.
167:178	More importantly, an integration of lscript1 regularization into incremental feature selection was not considered.
168:178	Incremental feature selection has been proposed firstly by Della Pietra et al.169:178	(1997) in a likelihoodbased framework.
170:178	In this approach, an approximate gain in likelihood for adding a feature to the model is used as feature selection criterion, and thresholds on this gain are used as stopping criterion.
171:178	Maximization of approximate likelihood gains and gradient feature testing both are greedy approximations to the true gain in the objective function grafting can be seen as applying one iteration of Newtons method, where the weight of the newly added feature is initialized at 0, to calculate the approximate likelihood gain.
172:178	Efficiency and accuracy of both approaches are comparable, however, the grafting framework provides a well-defined mathematical basis for feature selection and optimization by incorporating selection thresholds naturally as penalty factors of the regularizer.
173:178	The idea of adding n-best features in each selection step also has been investigated earlier in the likelihood-based framework (see for example McCallum (2003)).
174:178	However, the possible improvements in computational complexity and generalization performance due to n-best selection were not addressed explicitly.
175:178	Further improvements of efficiency of grafting are possible by applying Zhou et al.s (2003) technique of restricting feature selection in each step to the topranked features from previous stages.
176:178	In sum, we presented an application of lscript1 regularization to likelihood maximization for log-linear models that has a simple interpretation as bounded constraint relaxation in terms of maximum entropy estimation.
177:178	The presented n-best grafting method does not require specialized algorithms or simplifications of the prior, but allows for an efficient, mathematically well-defined combination of feature selection and regularization.
178:178	In an experimental evaluation, we showed n-best grafting to outperform lscript0, 1-best lscript1, lscript2 regularization and standard incremental feature selection in terms of computational efficiency and generalization performance.

