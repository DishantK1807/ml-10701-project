<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>K Ahmad</author>
<author>L Gillam</author>
<author>L Tostevin</author>
</authors>
<title>University of surrey participation in trec8: Weirdness indexing for logical document extrapolation and retrieval (wilder</title>
<date>1999</date>
<booktitle>In The Eighth Text REtrieval Conference (TREC-8</booktitle>
<marker>Ahmad, Gillam, Tostevin, 1999</marker>
<rawString>Ahmad, K.; Gillam, L.; and Tostevin, L. 1999. University of surrey participation in trec8: Weirdness indexing for logical document extrapolation and retrieval (wilder). In The Eighth Text REtrieval Conference (TREC-8).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ananiadou</author>
</authors>
<title>A methodology for automatic term recognition</title>
<date>1994</date>
<booktitle>In COLING 15th International Conference on Computational Linguistics</booktitle>
<pages>1034--1038</pages>
<contexts>
<context>semantic search (Bhagdev et al. 2007) and especially ontology engineering (Park, Byrd, &amp; Boguraev 2003; Brewster et al. 2007). There have been many studies into ATR. In the majority of these studies (Ananiadou 1994; Bourigault 1992; Fahmi, Bouma, &amp; van der Plas 2007; Frantzi &amp; Ananiadou 1999; Wermter &amp; Hahn 2005) linguistic processors (e.g. POS tagger, phrase chunker) are used to filter out stop words and restr</context>
</contexts>
<marker>Ananiadou, 1994</marker>
<rawString>Ananiadou, S. 1994. A methodology for automatic term recognition. In COLING 15th International Conference on Computational Linguistics, 1034–1038.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bhagdev</author>
<author>J Butters</author>
<author>A Chakravarthy</author>
<author>S Chapman</author>
<author>A-S Dadzie</author>
<author>M A Greenwood</author>
<author>J Iria</author>
<author>F Ciravegna</author>
</authors>
<title>Doris: Managing document-based knowledge in large organisations via semantic web technologies</title>
<date>2007</date>
<booktitle>In Proceedings of the 6th International Semantic Web Conference and the 2nd Asian Semantic Web Conference (Se-mantic Web Challenge Track</booktitle>
<contexts>
<context> important research area that deals with the extraction of technical terms from domain-specific language corpora. ATR is often a processing step preceding more complex tasks, such as semantic search (Bhagdev et al. 2007) and especially ontology engineering (Park, Byrd, &amp; Boguraev 2003; Brewster et al. 2007). There have been many studies into ATR. In the majority of these studies (Ananiadou 1994; Bourigault 1992; Fah</context>
</contexts>
<marker>Bhagdev, Butters, Chakravarthy, Chapman, Dadzie, Greenwood, Iria, Ciravegna, 2007</marker>
<rawString>Bhagdev, R.; Butters, J.; Chakravarthy, A.; Chapman, S.; Dadzie, A.-S.; Greenwood, M. A.; Iria, J.; and Ciravegna, F. 2007. Doris: Managing document-based knowledge in large organisations via semantic web technologies. In Proceedings of the 6th International Semantic Web Conference and the 2nd Asian Semantic Web Conference (Se-mantic Web Challenge Track).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bourigault</author>
</authors>
<title>Surface grammatical analysis for the extraction of terminological noun phrases</title>
<date>1992</date>
<booktitle>In 14th International Conference on Computational Linguistics COLING 92</booktitle>
<pages>977--98</pages>
<contexts>
<context>(Bhagdev et al. 2007) and especially ontology engineering (Park, Byrd, &amp; Boguraev 2003; Brewster et al. 2007). There have been many studies into ATR. In the majority of these studies (Ananiadou 1994; Bourigault 1992; Fahmi, Bouma, &amp; van der Plas 2007; Frantzi &amp; Ananiadou 1999; Wermter &amp; Hahn 2005) linguistic processors (e.g. POS tagger, phrase chunker) are used to filter out stop words and restrict candidate ter</context>
</contexts>
<marker>Bourigault, 1992</marker>
<rawString>Bourigault, D. 1992. Surface grammatical analysis for the extraction of terminological noun phrases. In 14th International Conference on Computational Linguistics COLING 92, 977–98 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Brewster</author>
<author>J Iria</author>
<author>Z Zhang</author>
<author>F Ciravegna</author>
<author>L Guthrie</author>
<author>Y Wilks</author>
</authors>
<title>Dynamic iterative ontology learning</title>
<date>2007</date>
<booktitle>In Recent Advances in Natural Language Processing (RANLP 07</booktitle>
<contexts>
<context>specific language corpora. ATR is often a processing step preceding more complex tasks, such as semantic search (Bhagdev et al. 2007) and especially ontology engineering (Park, Byrd, &amp; Boguraev 2003; Brewster et al. 2007). There have been many studies into ATR. In the majority of these studies (Ananiadou 1994; Bourigault 1992; Fahmi, Bouma, &amp; van der Plas 2007; Frantzi &amp; Ananiadou 1999; Wermter &amp; Hahn 2005) linguisti</context>
</contexts>
<marker>Brewster, Iria, Zhang, Ciravegna, Guthrie, Wilks, 2007</marker>
<rawString>Brewster, C.; Iria, J.; Zhang, Z.; Ciravegna, F.; Guthrie, L.; and Wilks, Y. 2007. Dynamic iterative ontology learning. In Recent Advances in Natural Language Processing (RANLP 07).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A Caraballo</author>
<author>E Charniak</author>
</authors>
<title>Determining the specificity of nouns from text</title>
<date>1999</date>
<booktitle>In Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora</booktitle>
<contexts>
<context>od’measures such as mutual information (Daille 1996), log likelihood (Cohen 1995), t-test (Fahmi, Bouma, &amp; van der Plas 2007; Wermter &amp; Hahn 2005), and the notion of ‘modifiability’ and its variants (Caraballo &amp; Charniak 1999; Deane 2005; Wermter &amp; Hahn 2005) are employed. In contrast, measures for ‘termhood’are circumscribed to frequencybased approaches and the use of reference corpora: the classic TFIDF used in (Evans &amp;</context>
<context>methods available, seldom is the full range of the problem dealt with by any one method. Firstly, most works rely on the simplifying assumption that the majority of terms consist of multi-word units (Caraballo &amp; Charniak 1999; Deane 2005; Fahmi, Bouma, &amp; van der Plas 2007; Wermter &amp; Hahn 2005). However, while (Nakagawa &amp; Mori 1998) claims that 85% of domain-specific terms are multi-word units, (Krauthammer &amp; Nenadic 2004)</context>
</contexts>
<marker>Caraballo, Charniak, 1999</marker>
<rawString>Caraballo, S. A., and Charniak, E. 1999. Determining the specificity of nouns from text. In Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Cohen</author>
</authors>
<title>Highlights: Languageand domainindependent automatic indexing terms for abstracting</title>
<date>1995</date>
<journal>Journal of the American Society for Information Science</journal>
<volume>46</volume>
<pages>162--174</pages>
<contexts>
<context>e a single term; and measures of ‘termhood’indicating the association strength of a term to domain concepts. For measuring ‘unithood’measures such as mutual information (Daille 1996), log likelihood (Cohen 1995), t-test (Fahmi, Bouma, &amp; van der Plas 2007; Wermter &amp; Hahn 2005), and the notion of ‘modifiability’ and its variants (Caraballo &amp; Charniak 1999; Deane 2005; Wermter &amp; Hahn 2005) are employed. In con</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>Cohen, J. D. 1995. Highlights: Languageand domainindependent automatic indexing terms for abstracting. Journal of the American Society for Information Science 46(3): 162–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Daille</author>
</authors>
<title>Study and implementation of combined techniques for automatic extraction of terminology</title>
<date>1996</date>
<booktitle>The Balancing Act: Combining Symbolic and Statistical Approaches to Language</booktitle>
<pages>49--66</pages>
<editor>In Klavans, J., and Resnik, P., eds</editor>
<publisher>The MIT Press</publisher>
<location>Cambridge, Massachusetts</location>
<contexts>
<context>strength of units that comprise a single term; and measures of ‘termhood’indicating the association strength of a term to domain concepts. For measuring ‘unithood’measures such as mutual information (Daille 1996), log likelihood (Cohen 1995), t-test (Fahmi, Bouma, &amp; van der Plas 2007; Wermter &amp; Hahn 2005), and the notion of ‘modifiability’ and its variants (Caraballo &amp; Charniak 1999; Deane 2005; Wermter &amp; Ha</context>
</contexts>
<marker>Daille, 1996</marker>
<rawString>Daille, B. 1996. Study and implementation of combined techniques for automatic extraction of terminology. In Klavans, J., and Resnik, P., eds., The Balancing Act: Combining Symbolic and Statistical Approaches to Language. Cambridge, Massachusetts: The MIT Press. 49–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Deane</author>
</authors>
<title>A nonparametric method for extraction of candidate phrasal terms</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference 43rd Annual Meeting of the Association</booktitle>
<institution>for Computational Linguistics. University of Michigan, USA: The Association for Computer Linguistics</institution>
<contexts>
<context>essors (e.g. POS tagger, phrase chunker) are used to filter out stop words and restrict candidate terms to nouns or noun phrases, while in others any n-gram sequences are selected as candidate terms (Deane 2005). Statistical measures are then used to rank the candidate terms. These measures can be categorised into two kinds: measures of ‘unithood’indicating the collocation strength of units that comprise a </context>
<context>information (Daille 1996), log likelihood (Cohen 1995), t-test (Fahmi, Bouma, &amp; van der Plas 2007; Wermter &amp; Hahn 2005), and the notion of ‘modifiability’ and its variants (Caraballo &amp; Charniak 1999; Deane 2005; Wermter &amp; Hahn 2005) are employed. In contrast, measures for ‘termhood’are circumscribed to frequencybased approaches and the use of reference corpora: the classic TFIDF used in (Evans &amp; Lefferts 19</context>
<context>s the full range of the problem dealt with by any one method. Firstly, most works rely on the simplifying assumption that the majority of terms consist of multi-word units (Caraballo &amp; Charniak 1999; Deane 2005; Fahmi, Bouma, &amp; van der Plas 2007; Wermter &amp; Hahn 2005). However, while (Nakagawa &amp; Mori 1998) claims that 85% of domain-specific terms are multi-word units, (Krauthammer &amp; Nenadic 2004) claims that</context>
<context>ll percentage of gene names are multi-word units. Hence, for some domains such an assumption leads to very low recall, which, in turn, can hamper tasks built on top of ATR. Secondly, some approaches (Deane 2005; Frantzi &amp; Ananiadou 1999; Wermter &amp; Hahn 2005) apply frequency thresholds to reduce the algorithm’s search space by filtering out low frequency candidate terms. This, however, does not take into acc</context>
<context>ssessing a selected section of the output (Frantzi &amp; Ananiadou 1999; Park, Byrd, &amp; Boguraev 2002; Sclano &amp; Velardi 2007), to unsupervised matching of parts of the output against dictionary resources (Deane 2005; Fahmi, Bouma, &amp; van der Plas 2007; Wermter &amp; Hahn 2005). Nevertheless, evaluations have two things in common: first, the majority only measure precision but not recall; second, they evaluate only a </context>
</contexts>
<marker>Deane, 2005</marker>
<rawString>Deane, P. 2005. A nonparametric method for extraction of candidate phrasal terms. In Proceedings of the Conference 43rd Annual Meeting of the Association for Computational Linguistics. University of Michigan, USA: The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Evans</author>
<author>R G Lefferts</author>
</authors>
<title>Clarit-trec experiments</title>
<date>1995</date>
<journal>Information Processing and Management</journal>
<volume>3</volume>
<pages>1--3</pages>
<contexts>
<context>ak 1999; Deane 2005; Wermter &amp; Hahn 2005) are employed. In contrast, measures for ‘termhood’are circumscribed to frequencybased approaches and the use of reference corpora: the classic TFIDF used in (Evans &amp; Lefferts 1995; Medelyan &amp; Witten 2006); the notion of ‘weirdness’as introduced in (Ahmad, Gillam, &amp; Tostevin 1999), which compares the term frequency in the corpus with its frequency in a reference corpus from a d</context>
</contexts>
<marker>Evans, Lefferts, 1995</marker>
<rawString>Evans, D. A., and Lefferts, R. G. 1995. Clarit-trec experiments. Information Processing and Management 3 1(3):385–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Fahmi</author>
<author>G Bouma</author>
<author>L van der Plas</author>
</authors>
<title>Improving statistical method using known terms for automatic term extraction</title>
<date>2007</date>
<booktitle>In Computational Linguistics in the Netherlands CLIN 17</booktitle>
<marker>Fahmi, Bouma, van der Plas, 2007</marker>
<rawString>Fahmi, I.; Bouma, G.; and van der Plas, L. 2007. Improving statistical method using known terms for automatic term extraction. In Computational Linguistics in the Netherlands CLIN 17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T</author>
<author>S Ananiadou</author>
</authors>
<title>The c/nc value domain independent method for multi-word term extraction</title>
<date>1999</date>
<journal>Journal of Natural Language Processing</journal>
<volume>6</volume>
<marker>T, Ananiadou, 1999</marker>
<rawString>Frantzi, K . T., and Ananiadou, S. 1999. The c/nc value domain independent method for multi-word term extraction. Journal of Natural Language Processing 6(3):145–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-D Kim</author>
<author>T Ohta</author>
<author>Y Tateisi</author>
<author>J Tsujii</author>
</authors>
<title>Genia corpus–semantically annotated corpus for bio-textmining</title>
<date>2003</date>
<journal>Bioinformatics</journal>
<volume>19</volume>
<pages>180--182</pages>
<contexts>
<context>in. For that reason, we chose two domains for our experiment, i.e., biological domain and a domain ‘closer’ to common knowledge in order to compare the methods. We firstly selected the GENIA corpus3 (Kim et al. 2003) which contains 2000 abstracts totaling 420,000 words selected from National Library of Medicine’s MEDLINE database for the biological domain; for the other domain we manually created a corpus of rou</context>
</contexts>
<marker>Kim, Ohta, Tateisi, Tsujii, 2003</marker>
<rawString>Kim, J.-D.; Ohta, T.; Tateisi, Y.; and Tsujii, J. 2003. Genia corpus–semantically annotated corpus for bio-textmining. Bioinformatics 19 Suppl 1 :i180–i182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>K Toutanova</author>
<author>H T Ilhan</author>
<author>S D Kamvar</author>
<author>C D Manning</author>
</authors>
<title>Combining heterogeneous classifiers for word-sense disambiguation</title>
<date>2002</date>
<booktitle>In Word Sense Disambiguation Workshop: Recent Successes and Future Directions (held in conjunction with ACL conference</booktitle>
<contexts>
<context>Extractor has been implemented as a web application and is publicly available2. 2.2 The Voting Algorithm Voting systems are often used as an improvement strategy in word sense disambiguation systems (Klein et al. 2002; Sinha &amp; Mihalcea 2007; Su´arez &amp; Palomar 2002), in which a voting algorithm is applied to the outputs from multiple classification algorithms to select the most appropriate word sense. In (Klein et </context>
</contexts>
<marker>Klein, Toutanova, Ilhan, Kamvar, Manning, 2002</marker>
<rawString>Klein, D.; Toutanova, K.; Ilhan, H. T.; Kamvar, S. D.; and Manning, C. D. 2002. Combining heterogeneous classifiers for word-sense disambiguation. In Word Sense Disambiguation Workshop: Recent Successes and Future Directions (held in conjunction with ACL conference).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Kozakov</author>
<author>Y Park</author>
<author>T-H Fin</author>
<author>Y Drissi</author>
<author>Y N Doganata</author>
<author>T Cofino</author>
</authors>
<title>Glossary extraction and utilization in the information search and delivery system for ibm technical support</title>
<date>2004</date>
<journal>IBM Systems Journal</journal>
<volume>43</volume>
<contexts>
<context>pares the term frequency in the corpus with its frequency in a reference corpus from a different domain; and measures such as ‘domain pertinence’in (Sclano &amp; Velardi 2007) and ‘domain specificity’in (Kozakov et al. 2004; Park, Byrd, &amp; Boguraev 2002), which extend and revise ‘weirdness.’ The trend in recent research is to use hybrid approaches, in which ‘unithood’and ‘termhood’are combined to produce an unified indic</context>
<context>rom text, prevents the successful application of ATR to a wide range of domains and corpora. To our best knowledge, the methods presented by (Ahmad, Gillam, &amp; Tostevin 1999; Frantzi &amp; Ananiadou 1999; Kozakov et al. 2004; Park, Byrd, &amp; Boguraev 2002; Sclano &amp; Velardi 2007) are capable of recognising both singleand multi-word terms and do not apply frequency thresholds. Thus, we selected these methods, together with</context>
<context>erimental Setting 2.1 Methods Selected We implemented and compared five algorithms: TF-IDF (as a baseline), ‘weirdness’(Ahmad, Gillam, &amp; Tostevin 1999), ‘C-value’(Frantzi &amp; Ananiadou 1999), ‘Glossex’(Kozakov et al. 2004; Park, Byrd, &amp; Boguraev 2002) and ‘TermExtractor’(Termex) (Sclano &amp; Velardi 2007). These algorithms were selected because they were capable of recognising both singleand multi-word terms and did no</context>
</contexts>
<marker>Kozakov, Park, Fin, Drissi, Doganata, Cofino, 2004</marker>
<rawString>Kozakov, L.; Park, Y.; Fin, T.-H.; Drissi, Y.; Doganata, Y. N.; and Cofino, T. 2004. Glossary extraction and utilization in the information search and delivery system for ibm technical support. IBM Systems Journal 43(3):546–563.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Krauthammer</author>
<author>G Nenadic</author>
</authors>
<title>Term identification in the biomedical literature</title>
<date>2004</date>
<journal>J Biomed Inform</journal>
<volume>37</volume>
<contexts>
<context> (Caraballo &amp; Charniak 1999; Deane 2005; Fahmi, Bouma, &amp; van der Plas 2007; Wermter &amp; Hahn 2005). However, while (Nakagawa &amp; Mori 1998) claims that 85% of domain-specific terms are multi-word units, (Krauthammer &amp; Nenadic 2004) claims that only a small percentage of gene names are multi-word units. Hence, for some domains such an assumption leads to very low recall, which, in turn, can hamper tasks built on top of ATR. Sec</context>
</contexts>
<marker>Krauthammer, Nenadic, 2004</marker>
<rawString>Krauthammer, M., and Nenadic, G. 2004. Term identification in the biomedical literature. J Biomed Inform 37(6):512–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Medelyan</author>
<author>I H Witten</author>
</authors>
<title>Thesaurus based automatic keyphrase indexing</title>
<date>2006</date>
<booktitle>Porceedings of the ACM/IEEE Joint Conference on Digital Libraries, JCDL 2006,, 296–297. Chapel Hill</booktitle>
<editor>In Marchionini, G.; Nelson, M. L.; and Marshall, C. C., eds</editor>
<publisher>ACM</publisher>
<location>NC, USA</location>
<contexts>
<context>rmter &amp; Hahn 2005) are employed. In contrast, measures for ‘termhood’are circumscribed to frequencybased approaches and the use of reference corpora: the classic TFIDF used in (Evans &amp; Lefferts 1995; Medelyan &amp; Witten 2006); the notion of ‘weirdness’as introduced in (Ahmad, Gillam, &amp; Tostevin 1999), which compares the term frequency in the corpus with its frequency in a reference corpus from a different domain; and mea</context>
</contexts>
<marker>Medelyan, Witten, 2006</marker>
<rawString>Medelyan, O., and Witten, I. H. 2006. Thesaurus based automatic keyphrase indexing. In Marchionini, G.; Nelson, M. L.; and Marshall, C. C., eds., Porceedings of the ACM/IEEE Joint Conference on Digital Libraries, JCDL 2006,, 296–297. Chapel Hill, NC, USA: ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Nakagawa</author>
<author>T Mori</author>
</authors>
<title>Nested collocation and compound noun for term extraction</title>
<date>1998</date>
<booktitle>In Proceedings of the First Workshop on Comutational Terminology(COMPUTERM’98</booktitle>
<pages>64--70</pages>
<contexts>
<context>y on the simplifying assumption that the majority of terms consist of multi-word units (Caraballo &amp; Charniak 1999; Deane 2005; Fahmi, Bouma, &amp; van der Plas 2007; Wermter &amp; Hahn 2005). However, while (Nakagawa &amp; Mori 1998) claims that 85% of domain-specific terms are multi-word units, (Krauthammer &amp; Nenadic 2004) claims that only a small percentage of gene names are multi-word units. Hence, for some domains such an as</context>
</contexts>
<marker>Nakagawa, Mori, 1998</marker>
<rawString>Nakagawa, H., and Mori, T. 1998. Nested collocation and compound noun for term extraction. In Proceedings of the First Workshop on Comutational Terminology(COMPUTERM’98), 64–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Park</author>
<author>R J Byrd</author>
<author>B Boguraev</author>
</authors>
<title>Automatic glossary extraction: Beyond terminology identification</title>
<date>2002</date>
<booktitle>In 19th International Conference on Computational Linguistics COLING 02</booktitle>
<location>Taipei, Taiwan: Howard</location>
<marker>Park, Byrd, Boguraev, 2002</marker>
<rawString>Park, Y.; Byrd, R. J.; and Boguraev, B. 2002. Automatic glossary extraction: Beyond terminology identification. In 19th International Conference on Computational Linguistics COLING 02. Taipei, Taiwan: Howard International House and Academia Sinica.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Park</author>
<author>R J Byrd</author>
<author>B K Boguraev</author>
</authors>
<title>Towards ontologies on demand</title>
<date>2003</date>
<booktitle>Proceedings of the Workshop on Semantic Web Technologies for Searching and Retrieving Scientific Data. Colocated with the Second International Semantic Web Conference (ISWC-03</booktitle>
<editor>In Ashish, N., and Goble, C., eds</editor>
<marker>Park, Byrd, Boguraev, 2003</marker>
<rawString>Park, Y.; Byrd, R. J.; and Boguraev, B. K. 2003. Towards ontologies on demand. In Ashish, N., and Goble, C., eds., Proceedings of the Workshop on Semantic Web Technologies for Searching and Retrieving Scientific Data. Colocated with the Second International Semantic Web Conference (ISWC-03).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Schone</author>
<author>D Jurafsky</author>
</authors>
<title>Is knowledge-free induction of multiword unit dictionary headwords a solved problem</title>
<date>2001</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing</booktitle>
<contexts>
<context>ologies for evaluation. Next we applied two evaluation metrics: ‘classic’ precision measuring the proportion of correct terms to all terms considered; and the Un-interpolated Average Precision (UAP) (Schone &amp; Jurafsky 2001) which averages precision at the ith correct term out of the total K correct terms in the ranked output considered (cf. Eq. 3). In Eq. 3, K is the set of K correct terms in the output, and Pi = i/Hi </context>
</contexts>
<marker>Schone, Jurafsky, 2001</marker>
<rawString>Schone, P., and Jurafsky, D. 2001. Is knowledge-free induction of multiword unit dictionary headwords a solved problem? In Proceedings of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sclano</author>
<author>P Velardi</author>
</authors>
<title>Termextractor: a web application to learn the shared terminology of emergent web communities</title>
<date>2007</date>
<booktitle>In Proceedings of the 3rd International Conference on Interoperability for Enterprise Software andApplications (I-ESA</booktitle>
<contexts>
<context>uced in (Ahmad, Gillam, &amp; Tostevin 1999), which compares the term frequency in the corpus with its frequency in a reference corpus from a different domain; and measures such as ‘domain pertinence’in (Sclano &amp; Velardi 2007) and ‘domain specificity’in (Kozakov et al. 2004; Park, Byrd, &amp; Boguraev 2002), which extend and revise ‘weirdness.’ The trend in recent research is to use hybrid approaches, in which ‘unithood’and ‘</context>
<context>rmhood’are combined to produce an unified indicator, such as ‘C-value’(Frantzi &amp; Ananiadou 1999), and many others (Fahmi, Bouma, &amp; van der Plas 2007; Kozakov et al. 2004; Park, Byrd, &amp; Boguraev 2002; Sclano &amp; Velardi 2007). Despite the plethora of methods available, seldom is the full range of the problem dealt with by any one method. Firstly, most works rely on the simplifying assumption that the majority of terms co</context>
<context>TR to a wide range of domains and corpora. To our best knowledge, the methods presented by (Ahmad, Gillam, &amp; Tostevin 1999; Frantzi &amp; Ananiadou 1999; Kozakov et al. 2004; Park, Byrd, &amp; Boguraev 2002; Sclano &amp; Velardi 2007) are capable of recognising both singleand multi-word terms and do not apply frequency thresholds. Thus, we selected these methods, together with a corpus and an evaluation methodology so as to mak</context>
<context>hms: TF-IDF (as a baseline), ‘weirdness’(Ahmad, Gillam, &amp; Tostevin 1999), ‘C-value’(Frantzi &amp; Ananiadou 1999), ‘Glossex’(Kozakov et al. 2004; Park, Byrd, &amp; Boguraev 2002) and ‘TermExtractor’(Termex) (Sclano &amp; Velardi 2007). These algorithms were selected because they were capable of recognising both singleand multi-word terms and did not throw away candidate terms on the basis of frequency only. TF-IDF and weirdness</context>
<context>y The way methods are evaluated in the ATR literature is diverse, ranging from human judges manually assessing a selected section of the output (Frantzi &amp; Ananiadou 1999; Park, Byrd, &amp; Boguraev 2002; Sclano &amp; Velardi 2007), to unsupervised matching of parts of the output against dictionary resources (Deane 2005; Fahmi, Bouma, &amp; van der Plas 2007; Wermter &amp; Hahn 2005). Nevertheless, evaluations have two things in commo</context>
</contexts>
<marker>Sclano, Velardi, 2007</marker>
<rawString>Sclano, F., and Velardi, P. 2007. Termextractor: a web application to learn the shared terminology of emergent web communities. In Proceedings of the 3rd International Conference on Interoperability for Enterprise Software andApplications (I-ESA 2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sinha</author>
<author>R Mihalcea</author>
</authors>
<title>Unsupervised graphbasedword sense disambiguation using measures of word semantic similarity</title>
<date>2007</date>
<booktitle>In ICSC ’07: Proceedings of the Inter-national Conference on Semantic Computing (ICSC</booktitle>
<pages>363--369</pages>
<publisher>IEEE Computer Society</publisher>
<location>Washington, DC, USA</location>
<contexts>
<context>implemented as a web application and is publicly available2. 2.2 The Voting Algorithm Voting systems are often used as an improvement strategy in word sense disambiguation systems (Klein et al. 2002; Sinha &amp; Mihalcea 2007; Su´arez &amp; Palomar 2002), in which a voting algorithm is applied to the outputs from multiple classification algorithms to select the most appropriate word sense. In (Klein et al. 2002) two types of </context>
</contexts>
<marker>Sinha, Mihalcea, 2007</marker>
<rawString>Sinha, R., and Mihalcea, R. 2007. Unsupervised graphbasedword sense disambiguation using measures of word semantic similarity. In ICSC ’07: Proceedings of the Inter-national Conference on Semantic Computing (ICSC 2007), 363–369. Washington, DC, USA: IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Su´arez</author>
<author>M Palomar</author>
</authors>
<title>A maximum entropybased word sense disambiguation system</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th international conference on Computational linguistics (COLING 02), 1–7</booktitle>
<location>Taipei, Taiwan</location>
<marker>Su´arez, Palomar, 2002</marker>
<rawString>Su´arez, A., and Palomar, M. 2002. A maximum entropybased word sense disambiguation system. In Proceedings of the 19th international conference on Computational linguistics (COLING 02), 1–7. Taipei, Taiwan: Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>

