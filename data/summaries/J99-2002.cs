As in decision tree induction, feature selection is also performed as a result of model search (Pedersen, Bruce, and Wiebe 1997).
A97-1056
4 Pedersen, Bruce, and Wiebe (1997) present the results of experiments covarying these measures and the direction of search.
A97-1056
Thus, we can estimate the parameters from the data without the need for an iterative fitting procedure (as used in NLP maximum entropy modeling \[Berger, Della Pietra, and Della Pietra 1996\]).
J96-1002
However, by generating models of 5 The material in this section was originally published in Bruce, Wiebe, and Pedersen (1996).
W96-0210
In maximum entropy modeling as applied to NLP (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1997), feature selection and model search are typically combined, but the procedure differs from that described here.
J96-1002 W97-0301
In model switching (Kayaalp, Pedersen, and Bruce 1997) and the naive mix (Pedersen and Bruce 1997), more than one of the models generated during search is used to perform classification.
A97-1056 W97-1005
Bayes has been shown to be competitive with state-of-the-art classifiers, and has proven remarkably successful on many AI and NLP applications (see, for example, Leacock, Towell, and Voorhees \[1993\]; Friedman, Geiger, and Goldszmidt \[1997\]; Mooney \[1996\]; Langley, Iba, and Thompson \[1992\]).
H93-1051 W96-0208
Alternatively, all the models generated during search can be considered, and the one with the highest accuracy on a held-out portion of the training data can be selected as the final model (Kayaalp, Pedersen, and Bruce 1997; Wiebe, Bruce, and Duan 1997).
A97-1056 W97-1005
framework was originally introduced into NLP in Bruce and Wiebe (1994).
P94-1020
Naive Bayes classifiers have been found to be remarkably successful in many applications, including word sense disambiguation (Mooney 1996).
W96-0208
