Turner and Charniak (Turner and Charniak, 2005) added some special rules and applied this method to unsupervised learning to overcome the lack of training data.
P05-1022 P05-1036
Entropy Model The maximum entropy model (Berger et al., 1996) estimates a probability distribution from training data.
J96-1002
Turner and Charniak (Turner and Charniak, 2005) revised and improved Knight and Marcu’s algorithm; however, their algorithm also uses only mother and daughter relations and has the same problem.
P05-1022 P05-1036
ROUGE-L and ROUGE-1 are supposed to be appropriate for the headline gener853 ation task (Lin, 2004).
W04-1013
Charniak. 2005.
P05-1022
To parse these corpora, we used Charniak and Johnson’s parser (Charniak and Johnson, 2005).
P05-1022
McDonald (McDonald, 2006) independently proposed a new machine learning approach.
E06-1038
4.1 Evaluation Method We evaluated each sentence compression method using word F-measures, bigram F-measures, and BLEU scores (Papineni et al., 2002).
P02-1040
Turner and Charniak (Turner and Charniak, 2005) solve this problem by appending special rules that are applied when a mother node and its daughter node have the same label.
P05-1022 P05-1036
ROUGE (Lin, 2004) is a set of recall-based criteria that is mainly used for evaluating summarization tasks.
W04-1013
While several methods have been proposed for sentence compression (Witbrock and Mittal, 1999; Jing and McKeown, 1999; Vandeghinste and Pan, 2004), this paper focuses on Knight and Marcu’s noisy-channel model (Knight and Marcu, 2000) and presents an extension of their method.
W04-1015
