Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 141–146,
Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
A Corpus of Scope-disambiguated English Text 
 
 
Mehdi Manshadi,  James Allen,  Mary Swift 
Department of Computer Science, University of Rochester 
Rochester, NY, 14627, USA 
{mehdih,james,swift}@cs.rochester.edu 
 
 
 
 
 
 
Abstract 
Previous work on quantifier scope anotation 
focuses on scoping sentences with only two 
quantified noun phrases (NPs), where the quan-
tifiers are restricted to a predefined list. It also 
ignores negation, modal/logical operators, and 
other sentential adverbials. We present a com-
prehensive scope anotation scheme. We ano-
tate the scope interaction betwen al scopal 
terms in the sentence from quantifiers to scopal 
adverbials, without puting any restriction on 
the number of scopal terms in a sentence. In ad-
dition, al NPs, explicitly quantified or not, with 
no restriction on the type of quantification, are 
investigated for posible scope interactions. 
1 Introduction

Since the early days of natural language under-
standing (NLU), quantifier scope disambiguation 
has ben an extremely hard task. Therefore, early 
NLU systems either devised some mechanism for 
leaving the semantic representation underspecified 
(Wods 1978, Hobs and Shieber 1987), or tried to 
asign scoping to sentences based on heuristics 
(VanLehn 1978, Moran 198, Alshawi 192). 
There has ben a lot of work since then on devel-
oping frameworks for scope-underspecified seman-
tic representations (Alshawi and Crouch 192, Bos 
196, Copestake et al., 201, Eg et al., 201). The 
motivation of most recent formalisms is to develop 
a constraint-based framework where you can in-
crementaly ad constraints to filter out unwanted 
scopings. However, almost al of these formalisms 
are based on hard constraints, which have to be 
satisfied in every reading of the sentence. It sems 
that the story is diferent in practice. Most of the 
constraints one can hope for (imposed by dis-
course, pragmatics, word knowledge, etc.) are soft 
constraints, that is they define a preference over 
the posible readings of a sentence. As a result, 
statistical methods sem to be wel suited for scope 
disambiguation. 
Surprisingly enough, after two decades of ex-
tensive work on statistical techniques in natural 
language procesing, there has not ben much 
work on scope disambiguation (se section 6 for a 
review). In adition, as discused later, this work is 
very restricted. It considers sentences with only 
two quantifiers, where the quantifiers are picked 
from a predefined list. For example, it ignores de-
finites, bare singulars/plurals, and proper nouns, as 
wel as negations and other scopal operators. 
A major reason for the lack of work on statisti-
cal scope disambiguation is the lack of a 
comprehensive scope-disambiguated corpus. In 
fact, there is not even a standard test set for 
evaluation purposes. The reason behind this later 
fact is simple. Scope disambiguation is very hard 
even for humans. In fact, our own early efort to 
anotate part of the Pen Trebank with ful scope 
information son proved to be to ambitious. 
Instead, we have picked a domain that covers 
many chalenging phenomena in scope disam-
biguation, while keping the scope disambiguation 
fairly intuitive. This helps us to build the first 
moderately sized corpus of natural language text 
with ful scope information. By fuly scoping a 
sentence, we mean to label the scope interaction 
betwen every two scopal elements in that sen-
141
tence. We scope al scope-bearing NPs (quantified 
or not), negations, logical/modal operators, and 
other sentential adverbials. We also anotate plu-
rals with their distributive vs. colective readings. 
In adition, we label sentences with coreference 
relations because they afect the scope interaction 
betwen NPs. 
2 Domain

The domain is the description of tasks about edit-
ing plain text files; in other words, a natural lan-
guage interface for text editors such as Linux SED, 
AWK, or EMACS programs. Figure (1) gives 
some sentences from the corpus. This domain has 
several properties that make it a great choice for a 
first efort to build a comprehensive scope-
disambiguated corpus. 
First, it caries a lot of scope interactions. As 
shown in the examples, the domain caries many 
quantified NPs. Also, scopal operators such as ne-
gation, and logical operators ocur prety often in 
the domain. Second, scope disambiguation is criti-
cal for dep understanding in this domain. Third, 
scoping is fairly intuitive, because a conscious 
knowledge of scoping is required in order to be 
able to acomplish the explained task. This is ex-
actly the key property of this domain that makes 
building a comprehensive scope-disambiguated 
corpus feasible. 
3 Corpus

3.1 The
core corpus 
The core part of the corpus has ben gathered from 
thre diferent resources, each making up roughly 
one third of the core corpus. 
• One liners: These are help documents found on 
the web for Linux comand-line text editors 
such as SED and AWK, giving a description of a 
task plus one line of code performing the task. 
•  Online tutorials: Many other online tutorials on 
using comand-line editors and regular expres-
sions exist. Sentences were manualy extracted 
from examples and exercises in these tutorials. 
• Computer science graduate students: These are 
the sentences provided by CS graduate students 
describing some of the routine text editing tasks 
they often do. The sentences have ben provided 
by both native and non-native English speakers. 
3.2 Expanding
corpus with crowd sourcing 
The core corpus was used to get more sentences 
using crowd sourcing. We provided input/output 
(I/O) examples for each task in the core corpus, 
and asked the workers on Mechanical Turk to pro-
vide the description of the task based on the I/O 
example(s). Figure (2) shows an example of two 
I/O pairs given to the workers in order to get the 
description of a single task. The reason for using 
two I/O pairs (instead of only one) is that there is 
almost always a trivial description for a single I/O 
pair. Even with two I/O pairs, we sometimes get 
the description of a diferent task, which hapens 
to work for the both pairs. For example the original 
description for the task given in figure (2) is: 
1. Sort al the lines by their second field. 
The folowing descriptions are provided by thre 
workers based on the given input/output texts: 
2. Sort the lines alphabeticaly by the values in the 2nd 
column. 
3. Sort the lines by the first group of leters. 
4. Alphabetize each line using the first leter of each 
word in the second column. 
(3) gives the description of a diferent task, but it 
works for the given I/O pairs. This is not a problem 
for us, but actualy a case that we would prefer to 
hapen, because this way, we not only get a variety 
of sentences defining the same task, but also obtain 
descriptions of new tasks. We can ad these new 
tasks to the core corpus, label them with new I/O 
1. Find an ocurence of the word "TBA" in every 
line and remove it from the line. 
2. Print a list of the lines that do not start with a 
digit or end with a leter. 
3. Replace every string "anti" posibly folowed by a 
hyphen with "not". 
Figure 1. Some examples from the core corpus 
INPUT OUTPUT 
100 NY April 
300 HU August 
400 OR May 
400 AL June 
400 AL June 
300 HU August 
100 NY April 
400 OR May 
c josh  21 
a adams 23 
d sam  26 
b john  25 
a adams 23 
b john  25 
c josh  21 
d sam  26 
 Figure 2. Two I/O pairs given for a single task 
142
pairs and hence expand the corpus in a botstrap-
ping fashion. 
The data acquired from Mechanical Turk is of-
ten quite noisy, therefore al sentences are re-
viewed manualy and taged with diferent 
categories (e.g. paraphrase of the original descrip-
tion, wrong but coherent description, etc.). 
3.3 Pre-procesing the corpus 
The corpus is tokenized and parsed using the Stan-
ford PCFG parser (Klein and Maning 203). We 
guide the parser by giving sugestions on part-of-
spech (POS) tags based on the gold standard POS 
tags provided for some clases of words such as 
verbs. Shalow NP chunks and negations are auto-
maticaly extracted from the parse tres and in-
dexed. The resulting NP-chunked sentences are 
then reviewed manualy, first to fix the chunking 
erors, hence providing gold standard chunks, and 
second, to ad chunks for other scopal operators 
such as sentential adverbials since the above auto-
mated aproach wil not extract those. Figure (3) 
shows the examples in figure (1) after chunking. 
As shown in these examples, NP chunks are in-
dexed by numbers, negation by the leter ‘N’ fol-
lowed by a number and al other scopal operators 
by the leter ‘O’ folowed by a number. 
4 Scope
annotation 
The chunked sentences are given to the anotators 
for scope anotation. Given a pair of chunks i and 
j, thre kinds of relation could hold betwen them. 
• Outscoping constraints: represented as (i>j), 
which means chunk i outscopes (i.e. has a wider 
scope over) chunk j.   
• Coreference relations: represented as (i=j). This 
could be betwen a pronoun and its antecedent or 
betwen two nouns.
1
 
• No scope interaction: If a pair is left unscoped, it 
means that either there is no scope interaction 
betwen the chunks, or switching the order of the 
chunks results in a logicaly equivalent formula. 
The overal scoping is represented as a list of 
semicolon-separated constraints. The anotators 
                                                             
1
  Bridging anaphora relations are simply represented as out-
scoping relations, because often there is not a clear distinction 
betwen the two. However for theoretical purposes, an out-
scoping constraint (i>j), where i is not acesible to j, is being 
understod as a bridging anaphora relation. 
are alowed to cascade constraints to form a more 
concise representation (se Figure 3). 
4.1 Logical
equivalence vs. intuitive scoping 
Our early experiments showed that a main source 
of inter-anotator disagrement are pairs of chunks 
for which, both orderings are logicaly equivalent 
(e.g. two existentials or two universals), but an an-
notator may label them with outscoping constraints 
based on his/her intuition. It turns out that the an-
notators’ intuitions are not consistent in these 
cases. Even a single anotator does not remain 
consistent throughout the data in such cases. Al-
though it does not make any diference in logic, 
this shows up as inter-anotator disagrement. In 
order to prevent this, anotators were asked to rec-
ognize these cases and leave them unscoped. 
4.2 Plurals

Plurals, in general, introduce a major source of 
complexity both in formal and computational se-
mantics (Link 197). From a scope–
disambiguation point of view, the main isue with 
plurals come from the fact that they cary two pos-
sible kinds of readings: colective vs. distributive. 
We treat plurals as a set of individuals and asume 
that the index of a plural NP refers to the set (col-
lective reading). However, we also asume that 
every plural potentialy caries an implicit univer-
sal quantifier ranging over al elements in the set. 
We represent this implicit universal with id (‘d’ for 
distributive) where i is the index of the plural NP. 
It is important to notice that while most theoretical 
papers talk about the colectivity vs. distributivity 
distinction at the sentence level, for us the right 
treatment is to make this distinction at the con-
straint level. That is, a plural may have a colective 
reading in one constraint but a distributive reading 
in another, as shown in example 2 in figure (3). 
1. Find [1/ an instance] of [2/ the word "TBA"] in [3/ 
every line] and remove [4/ it] from [5/ the line]. 
 (3>1 ; 3=5 ; 1=4) / concise form: (5=3>1=4) 
2. Print [1/ a list] of [2/ the lines] that do [N1/ not] 
start with [3/ a digit] [O1/ or] end with [4/ a leter]. 
(2>1 ; 2d>N1>3,4 ; N1>O1) // (i>j,k) ≡ (i>j; i>k) 
3. Replace [1/ every string "anti"] [O1/ posibly] fol-
lowed by [2/ a hyphen] with [3/ "not"]. 
(1>O1>2 ; 1>3) 
Figure 3. Chunked sentences labeled with scopings 
143
4.3 Other
chalenges of scope anotation 
In spite of chosing a specific domain with fairly 
intuitive quantifier scoping, the scope anotation 
has ben a very chalenging job. There are several 
major sources of dificulty in scope anotation. 
First, there has not ben much work on corpus-
based study of quantifier scoping. Most work on 
quantifier scoping focuses on scoping phenomena, 
which may be interesting from theoretical perspec-
tive, but do not ocur very often in practice. There-
fore many chalenging practical phenomena remain 
unexplored. During anotation of the corpus, we 
encountered a lot of these phenomena, which we 
have tried to generalize and find a reasonable 
treatment for. Second, other sources of ambiguity 
are likely to show up as scope disagrement. Fi-
naly, very often the disagrement in scoping does 
not result from the diferent interpretations of the 
sentence, but the diferent representations of the 
same interpretation. In writing the anotation 
scheme, extreme care has ben taken to prevent 
these spurious disagrements. Technical details of 
the anotation scheme are beyond the scope of this 
paper. We leave those for a longer paper. 
5 Statistics

The curent corpus contains around 50 sentences 
in the core level and 200 sentences acquired from 
crowd sourcing. The number of scopal terms per 
sentence is 3.9, out of which 95% are NPs and the 
rest are scopal operators. Table (1) shows the per-
centage of diferent types of NP in the corpus. 
The core corpus has already ben anotated, 
out of which a hundred sentences have ben ano-
tated by thre anotators in order to measure the 
inter-anotator agrement (IA). Two of the ano-
tators are native English speakers and the third is a 
non-native speaker who is fluent in English. Al 
thre have some background in linguistics. 
5.1 Inter-anotator agrement 
Although coreference relations were labeled in the 
corpus, we do not incorporate them in calculating 
IA. This is because, anotating coreference rela-
tions is much easier than scope disambiguation, so 
incorporating them favors toward higher IAs, 
which may be deceiving. Furthermore previous 
work only considers scope relations and hence we 
do the same in order to have a fair comparison. 
We represent each scoping using a directed graph 
over the chunk indices. For every outscoping rela-
tion i>j, node i is conected to node j by the di-
rected edge (i,j). For example, figure (4a) 
represents the scoping in (5). 
5. Delete [1/ the first character] of [2/ every word] 
and [3/ the first word] of [4/ every line] in [5/ 
the file]. 
(5>2>1 ; 5>4>3) 
Note that the directed graph must be a DAG (di-
rected acyclic graph), otherwise the scoping is not 
valid. In order to be able to measure the similarity 
of two DAGs coresponding to two diferent scop-
ings of a single sentence, we borow the notion of 
transitive closure from graph theory. The transitive 
closure (TC) of a directed graph G=(V,E) is the 
graph G
+
=(V,E
+
), where E
+
 is defined as folows: 
6. E
+
={(i,j) | i,j ∈V and i reaches j using a non-
nul directed path in G} 
Given the TC graph of a scoping, every pair (i,j), 
where i precedes j in the sentence, has one of the 
folowing thre labels: 
• WS (i outscopes j): (i,j) ∈ E
+
 
• NS (j outscopes i): (j,i) ∈ E
+
 
• NI (no interaction): (i,j) ∉ E
+
 ∧ (j,i) ∉ E
+
   
A pair is considered a match betwen two scop-
ings, if it has the same label in both. We define the 
metrics at two levels, constraint level and sentence 
level. At constraint level, every pair of chunks in 
every sentence is considered one instance. At sen-
tence level, every sentence is treated as an in-
Type of NP chunk Percentage 
NPs with explicit quantifiers 
(including indefinite A) 
35% 
Definites 27% 
Bare singulars/plurals 25% 
Pronouns 7% 
Proper names (files, variables, etc.) 6% 
Table 1. Corpus statistics 
 
                 
(a)   (b) 
Figure 4. DAG of scoping in (5) and its TC 
 
144
stance. A sentence counts as a match if and only if 
every pair of chunks in the sentence has the same 
label in both scopings. Unlike previous work (sec-
tion 6) where there is a strong skew in label distri-
bution, in our corpus the labels are almost evenly 
distributed, each consisting around 3% of the in-
stances. We use Cohen’s kapa score for multiple 
anotators (Davies & Fleis 1982) to measure IA. 
Table (2) reports the kapa score. 
The IA defined above serves wel for theo-
retical purposes, but an easier metric could be de-
fined which works fine for most practical purposes. 
For example, if the target language is first order 
logic with generalized quantifiers, the relative 
scope of the chunks labeled NI does not afect the 
interpretation.
2
 Therefore, we define a new version 
of observed agrement in which we consider a pair 
a match if it is labeled NI in one scoping or as-
signed the same label in both scopings. Table (2) 
reports the IA based on the later similarity 
measure, caled κ-EZ. 
6 Related
work 
To the best of our knowledge, there have ben 
thre major eforts on building a scope-
disambiguated corpus for statistical scope disam-
biguation, among which Higins and Sadock 
(203) is the most comprehensive. Their corpus 
consists of 890 sentences from the Wal Stret 
journal section of the Pen Trebank. They pick 
sentences containing exactly two quantifiers from a 
predefined list. This list does not include definites, 
indefinites, or bare singulars/plurals. Every sen-
tence is labeled with one of the thre labels 
coresponding to the first quantifier having wide-
scope, the second quantifier having wide scope, or 
no scope interaction betwen the two. They 
achieve an IA of 52% on this task. The majority 
of sentences in their corpus (more than 60%) have 
ben labeled with no scope interaction.  
Galen and McCartney (204) is another efort 
to provide scope-disambiguated data. They pick a 
set of sentences from LSAT and GRE logic games, 
which again contain only two quantifiers from a 
limited list of quantifiers. Their corpus consists of 
305 sentences. In around 70% of these sentences, 
                                                             
2
 Note that any pair left unscoped is labeled NI. Most of these 
pairs are those whose both orderings are logicaly equivalent 
(section 4.1). Besides, we asume al the scopings are valid 
that is there is at least one interpretation satisfying them. 
the first quantifier has wide scope. A major prob-
lem with this data is that the sentences are artifi-
cialy constructed for the LSAT and GRE tests. 
In a recent work Srinivasan and Yates (209) 
study the usage of pragmatic knowledge in finding 
the intended scoping of a sentence. Their labeled 
data set consists of 46 sentences, extracted from 
Web1Tgram (from Gogle, Inc) and hence is open-
domain. The corpus consists of short sentences 
with two specific quantifiers: Every and A. Al sen-
tences share the same syntactic structure, an active 
voice English sentence of the form (S (NP (V (NP | 
P)). In fact, they try to isolate the efect of 
pragmatic knowledge on scope disambiguation. 
7 Summary
and future work 
We have constructed a comprehensive scope–
disambiguated corpus of English text within the 
domain of editing plain text files. The domain car-
ries many scope interactions. Our work does not 
put any restriction on the type or the number of 
scope-bearing elements in the sentence. We 
achieve the IA of 75% on this task. Previous 
work focuses on anotating the relative scope of 
two NPs per sentence, while ignoring the complex 
scope-bearing NPs such as definites and indefi-
nites, and achieves the IA of 52%.  
The curent corpus contains 250 sentences, 
out of which 50 sentences have already ben an-
notated. Our goal is to expand the corpus up to 
twice in size. 20% of the corpus wil be anotated 
and the rest wil be left for the purpose of semi-
supervised learning. Since world knowledge plays 
a major role in scope disambiguation, we believe 
that leveraging unlabeled domain specific data in 
order to extract lexical information is a promising 
aproach for scope disambiguation. We hope that 
availability of this corpus motivates more research 
on statistical scope disambiguation. 
Acknowledgments 
This work was suported in part by grants from the 
National Science Foundation (IS-101205) and 
The Ofice of Naval Research (N0014110417). 
 Constraint-level Sentence-level 
κ 
75.0% 66% 
κ-EZ 92.3% 89% 
Table 2. Inter-anotator agrement 
 
145
References  
Alshawi, H.  (ed.)  (192) The core language Engine. 
Cambridge, MA, MIT Pres.  
Alshawi, H. and Crouch, R. (192) Monotonic semantic 
interpretation. In Proc. 30th ACL, pages 32–39. 
Bos, J. (196) Predicate logic unpluged. In Proc. 10th 
Amsterdam Coloquium, pages 13–143. 
Copestake, A., Lascarides, A. and Flickinger, D. (201) 
An Algebra for Semantic Construction in Constraint-
Based Gramars. ACL-01. Toulouse, France. 
Davies, M. and Fleis, J. (1982) Measuring Agrement 
for Multinomial Data. Biometrics, 38:1047–1051, 
Eg M., Koler A., and Niehren J. (201) The constraint 
language for lambda structures. Journal of Logic, 
Language, and Information, 10:457–485. 
Galen, A. and MacCartney, B. (2004). Statistical resolu-
tion of scope ambiguity in Natural language. 
htp:/nlp.stanford.edu/nlkr/scoper.pdf. 
Higins, D. and Sadock, J. (2003). A machine learning 
ap-proach to modeling scope preferences. Computa-
tional Linguistics, 29(1). 
Hobs, J. and Shieber, S. M. (1987) An Algorithm for 
Generating Quantifier Scopings. Computational Lin-
guistics 13, p. 47–63. 
Klein, D. and Maning, C. D. (203). Acurate Unlexi-
calized Parsing. Procedings of the 41st Meting of 
the Asociation for Computational Linguistics, p. 
423-430. 
Link, G. (198) Ten Years of Research on Plurals 
Where Do We Stand? Plurality and quantification By 
Fritz Ham, Erhard W. Hinrichs, 198  Kluwer 
Academic Publishers. 
Moran, D. B. (198). Quantifier scoping in the SRI core 
language engine. In Procedings of the 26th Anual 
Meting of the Asociation for Computational Lin-
guistics. 
Srinivasan, P., and Yates, A. (2009). Quantifier scope 
disambiguation using extracted pragmatic knowl-
edge: Preliminary results. In Procedings of the Con-
ference on Empirical Methods in Natural Language 
Procesing. 
VanLehn, K. (198) Determining the scope of English 
quantifiers, TR AI-TR-483, AI Lab, MIT. 
Wods, W. A.  (1978) Semantics and quantification in 
natural language question answering, Advances in. 
Computers, vol. 17, p 1-87. 
146

