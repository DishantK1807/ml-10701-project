LEFT-CORNER PARSING AND PSYCHOLOGICAL PLAUSIBILITY Philip l~esnik Department of ColImuter and information Science University of Pennsylvania, Philadelphia, PA 19104, USA resnik©linc.cis.upenn.edu Abstract It is well known that even extremely limited centerembedding causes people to have difficulty ill comprehension, but that leftand right-branching constractions produce no such effect.
If the difficulty in comprehension is taken to be a result of processing load, as is widely assumed, then measuring the processing load induced by a parsing strategy on these constructions may help determine its plausibility as a psychological model.
On this basis, it has been ~rgued \[A J91, JL83\] that by identifying processing load with space utilization, we can rule out both top-down and bottom-up parsing as viable candidates for the human sentence processing mechanism, attd that left-corner parsing represents a plausible Mternative.
Examining their arguments in detail, we find difficulties with each presentation.
In this paper we revise the argument and validate its central claim.
In so doing, we discover that the key distinction between the parsing methods is not the form of prediction (top-down vs.
bottom-up vs.
leftcorner), but rather the ability to iastantiate the operation of composition.
1 Introduction
One of our most robust observations about language -dating back at least to the seminal work of Miller and Chomsky \[MC63\] -is that rightand left-branching constructions such as (la) and (lb) seem to cause no particular difficulty in processing, but that multiply center-embedded constructions such as (lc) are difficult to understand.
a. \[\[\[John's\] brother's\] eat\] despises rats.
b. This is \[the dog that chased \[the cat that bit \[the rat that ate tbe cheese\]\]\].
c. #\[The rat that \[the cat that lille dog\] chased\] bit\] ate the cheese.
The standard explanation for this distinction is a tight bound on space in the human sentence processing mechanism: center-embedded constructions require that the head noun phrase of each subject be stored until the processing of the embedded clause is complete and the corresponding verb is finally encountered) Alternative accounts have been proposed, most sharing the premise that the parser's capacity for recursion is limited by bounds on storage.
(See, for exmnpie, \[Kim73\] and \[MI64\]; for opposing views and other pointers to the literature see \[DJK+82\]).
The distinction between center-embedding and left/right-branching has important implications for those who wish to construct psychologically plausible models of parsing.
Johnson-Laird \[JL83\] observes that neither the top-down nor the bottom-up methods of constructing a parse tree fit the facts of (1), arid proposes instead the lesS-well-known alternative of leftcorner parsing.
Abney mid Johnson \[AJgl\] discuss a somewhat more general version of Johnson-Laird's argument, introducing the abstract notion of a parsing sf~ntegy in order to characterize what is meant by bottom-up, top-down, and left-corner parsing.
In this paper, we examine the argument as presented by Abney and Johnson and by Johnson-Laird, and point out a central problem with each variation.
We then present the argument in a form that remedies those difficulties, and, in so doing, we identify a previously underrated aspect of the discussion that turns out to be of central importance.
In particular, we show that the psychological plausibility argument hinges on the operation of composition and not left-corner prediction per se.
2 Comparing
Strategies 2.1 Summary of the Argument For expository purposes, we begin with tile discussion in \[AJ91\].
Abney and Johnson sesame, as we shall, that the hunmn sentence processing mechanism construets a parse tree, consisting of labelled nodes and arcs, incrementally over the course of interpreting an utterance, though tile global parse tree need never "exist ill its entirety at any point".
They define a parsing IThis oh~rvatlon is by tto rne~n~ lanttnaage specific, though in SOV langttages it is embedding on objects, not subjectl, that causes ditllctdty.
ACRES DE COLING-92, NANTES, 23-28 Aour 1992 1 9 1 PROC.
oF COLING-92, NANTES, AUG.
23-28, 1992 A Figure 1: A parse tree strategy to be "a way of enumerating the nodes and arcs of parse trees".
This is, in fact, a generalization of the concept of a traversal \[ASU86\].
A top-down strategy is one in which each node is enumerated before any of its descendants are; a bottomup strategy is one in which all the descendants of a node are enumerated before it is.
So, for example, a topdown strategy would enumerate the nodes of the tree in Figure 1 in the order ABCDEFGHI, and a bottom-up strategy would enumerate them in the order CEFDBHIGA.
In a left-corner strategy, for each node ~1, the leftmost child of T/is enumerated before r/, and the siblings of the leftmcet child are enumerated after r/.
The strategy takes its name from the fact that the first item on the right-hand side of a context-free rule (its left corner) is used to predict the parent node.
For example, having recognized constituent C in Figure 1, the parser predicts an invocation of rule B --4 C D and introduces node B.
The complete left-corner enumeration of the tree is CBEDFAHGI.
Thus far, we have discussed only the order of enumeration of nodes, and not ares.
Abney and Johnson define as arc-eager any strategy that enumerates the arc between two nodes as soon as both nodes are present.
An are-standard strategy is one that enumerates the connecting arc once either none or all of the subtree dominated by the child has been enumerated.
For example, the arc-eager left-corner enumeration of the tree in Figure 1 would introduce arc (B,D) just after node D was enumerated, while the arc-standard version of the left-corner strategy would first completely enumerate the subtree containing E, D, and F, and then enumerate arc (B, D).
In order to characterize the space requirements of a parsing strategy, two more definitions are required.
A node is said to be incomplete either if its parent has not yet been enumerated (in which case the parser must store it until it can be attached to the parent node), or if some child has not yet been enumerated (in which case the parser must store tire node until its child can be attached).
The space requirement of a parsing strategy, given a grammar, is the maximum number of incomplete nodes at any point during tbe enumeration of any parse tree of the grammar.
Having established this set of definitions, the goal is to decide which parsing strategies are psychologically plausible, given the facts about the human parsx / e | c °/-,,, '¢ z ~ n / \ v z left-branching center-embedded right-branching Figure 2: Branching slr~e~ares ing mechanism as exemplified by (1).
The central claim is summarized in the following table: Strategy " Spaxze required ' \] No#, • A~c, L~/t ce.t~ I m~hLl Top-d..... ither o(,) O(u) I o(1) I Bottom-upeither 0(1) O(n) \]JO(n)" Left ........ tandard 0(1) O(n) O(n) Left ......... get, 0(1) O(a) \]Q(1) J What people do ..
O(1) O().
I o(1) I The table can be explained with reference to Figure 2.
A top-down enumeration of the left-branching tree clearly requires storage proportional to n, tile height of the tree: at the point when Z is enumerated, each of A, B,..., X remains inemnplete because its rightmost child has not yet been encounteredfl The same holds true for the center-embedded structure: using a top-down enumeration, each of A, C, D, ..., X remains incomplete until the subtree it dominates has been entirely enumerated.
In contrast, the top-down strategy requires only constant space for tim rightbranching structure: each of A, C ....., X becomes coruplete as soon as its rightmost child is enumerated, so the number of incomplete nodes at any time is at most two.
We conclude that if the human sentence processing strategy were top-down, people would find increasing difficulty with both multiply left-branching and multiply center-embedded constructions, but not with right-branching constructions.
The evidence exemplified by (1) suggests that this is not the case.
A similar analysis holds for the bottom-up strategy.
The left-branching structure requires only constant space, since each of X,..., B,A becomes complete as soon as both children have been enumerated.
In contrust, enumerations of the right-branching and centerembedded constructions require linear space, since every leftmest child remains incomplete until the subtree dominated by its right sibling has been entirely enumerated.
The left-corner strategy with arc-standard enumeration behaves similarly to the bottom-up strategy, since every parent node remains incomplete until the subtree dominated by its right sibling has been 2Abney and Johngoa di*cuss space complexity with r¢apect to the length of the input string, not the height of the ptmm tree, but if we t~sttme the grammar in finitely ambigltotm this distinction is of no hnportaxtce.
AcI~ DE COLING-92.
NANTEs, 23-28 AOI\]T 1992 1 9 2 PROC.
OF COLING-92, NANTES, AUO.
23-28. 1992 entirely enumerated.
If increased memory load is responsible for increased processing difficulty, as we have been assuming, then both the bottom-up strategy and the arc-standard left-corner strategy predict that people have more difficulty with right-branching than with left-branching structures.
Our conclusion is the same as for the top-down strategy: the asymmetry of the prediction is not supported by the evidence.
On the other hand, arc-eager enumeration makes a critical difference to the left-corner strategy when applied to the right-branching structure.
Recall that the left-corner enumeration of nodes for this structure is BADC ....
Notice that after node (7 has been enumerated, arc (A,C) is introduced immediately, and as a result, node A is no longer incomplete.
In general, the arc-eager left-corner strategy will enumerate the right-branching structure with at most three nodes incomplete at any point.
~ktrthermore, as was the case for the bottom-up strategy, the left-branching structure requires constant space.
We see that only tile centerembedded structure requires increased storage as the depth of embedding increases.
Thus of the four strategies, the arc-eager version of the left-corner strategy is the only one that makes predictions consistent with observed behavior.
2.2 Two
Problems Under the assumptions made by Abney and Johnson, the discussion sketched out above does make a case for a left-corner strategy being more psychologically plaitsible than top-down or bottom-up strategies.
However, there are two difficulties with the argument as it is presented.
First, by abstracting away from parsing algorithms and placing the focus on parsing strategies, Abney anti Johnson make it difficult to fairly compare space requirements across different methods of parsing.
Without a formal characterization of the algorithms themselves, it is not clear that their abstract notion of space utilization means the same thing in each case.
3 ~br example, consider the augmented transition network (ATN) in Figure 3, where the actions on tile arcs are as follows: II: npl ~ * I2: result ~ (S (npl *)) 13: dell ~ * 14: result ~ (NP (dell *)) I5: result ~ a I6: result ~ the Uppercase are labels represent PUSH operations, and lowercase labels represent terminal symbols.
In the pseudolanguage used here for are actions, npl, dell, 3\] am grateful to Stuart Shleber for this observation.
a(t5) tl~ (16) Figure 3: Fragment of an ATN and resull are registers, the leftward arrow (+--) indicates an assigmnent statement, the pop arc transmits control (aud tile contents of the ~esalt register) to the calling subuetwork, and the asterisk (*) represents the value so transmitted (cf.
\[WooT0\]). So, for instance, action I4 constructs an NP dominating the structure in the ddl register on tile left, and, on the right, tile noun structure received on retnrn froln a push to tile N subnetwork.
Now~ tile ATN is perhaps one of the mo~t common examples of a parser operating in a top-down fashion.
Yet according to the definitions proposed by Abney and Jolmson, the enumeration performed by the ATN parser given above would seem to make it, an instance of a bottom-up strategy.
For example, in parsing the noun phrase the man, the ATN above wonld recognize tile determiner the, then the nonn man, and finally it would build and return the structure \[,vthe man\] from the NP subnetwork.
The source of difficulty lies in the decoupling of the parser's hypotheses from the structures that it builds.
When the determiner the is encountered, no parse-tree structures have been built, but the mechanism controlling the ATN's computation has stored the hypotheses that we were parsing an S, that we had entered the NP subnetwork, and that we had subsequently entered the DET subnetwork.
These correspond precisely to the nodes we expect to see enumerated during the course of a top-down strategy.
One could, of course, choose in this case to identify the space utilization of this parser with the hypotheses rather than the structures built.
Itowever, that leaves the status of the structures themselves in question.
More to the point, re-characterizing tile storage requirements of a particular algorithm is exactly the sort of manipulation that the abstract notion of parsing strategies should help us avoid.
Tile second difficulty with Abney and Johnson's discussion concerns the distinction between arc-eager and arc-standard strategies.
As they point out, for both top-down and bottom-up strategies, the two forms of AcrEs i~ COL1NG-92, NANYES, 23-28 AOflr 1992 1 9 3 PROC.
OF COLING-92, NANTES, AUO.
23-28, 1992 arc enumeration are indistinguishable.
In addition, left-corner parsing with arc-standard enumeration is, at least for the purposes of this discussion, virtually identical to bottom-up parsing, having no distinguishable effects either with respect to space utilization or even with respect to the hypotheses that are proposed.4 So it seems somewhat odd to introduce a distinction between "eager" versus "standard" when it turns out to distinguish only one of six possible combinations (topdown/eager, top-down/standard, etc.).
The question of exactly what "eager enumeration" does would seem to merit further attention.
We shall give it that attention shortly, in Section 4.
3 Comparing
Automata Abney and Johnson's argument is largely an independent account quite similar to one made earlier in \[JL83\].
Here we present a brief summary of the argument as presented there.
Johnson-Laird's presentation, though it encounters a difficulty of its own, turns out to complement Abney and Johnson's and to make clear how to solve the difficulties in both.
Following the standard description in the compilers literature (see, e.g., \[ASU86\]), Johnson-Laird adopts the definition of a top-down parser as one that operates by recursive descent: it begins with the start symbol of the grammar and successively rewrites tile leftmost nonterminal until it reaches a terminal symbol or symbols that can be matched against the input.
Parsing in this fashion, the parse tree is constructed top down and from left to right.
A bottom-up parser builds the tree by working upward from the terminal symbols in the input string, constructing each parent node after all its children have been recognized.
A left.corner parser recognizes the left-corner of a context-free rule bottom-up, and predicts the remaining symbols on the right-hand-side of the rule top-down.
Johnson-Laird examines the psychological plausibility of parsers, not parsing strategies, but otherwise his argument is very much the same as the discussion in the previous section.
He concludes that the symmetry of human performance on leftand right-branching structures counts against the top-down mid bottom-up parsers, and that the left-corner p~trser is a viable alternative that appears to be consistent with the evidence.
He then provides a more formal characterizatiml of the various parsers by expressing each as a push-down automaton (PDA).
Such a characterization immediately *Although top-down filtering can be added (see, e.g., \[PS87, p.
182D, Schabea (personal commttrdcation) points out that leftcorner parsing with top-down ffltethtg iS e~entially the same a.s LR parsing.
Top-down filtering restricts the non-determinlstic choices made by the parser, bat does not affect the bottom-up construction of the parse tree along a single computation path.
remedies the first difficulty we found in \[AJ91\]: the formal specification of each parsing algorithm permits us to express space utilization uniformly in terms of the automaton's stack.
The top-down and bottom-up automata behave exactly as we would expect.
The stack of the bottomup automaton never grows beyond a constant size for left-branching constructions, but is potentially unbounded for center-embedded and right-branching constructions.
The top-down automaton displays the opposite behavior, the size of its stack size being bounded only for right-branching constructions, s Of particular interest is Johnson-Laird's construetion of a PDA for left-corner parsiug, which we consider in more detail.
The stack alphabet for the left-corner PDA includes not only terminal and non-terminal symbols from thc grau~nar, but also special symbols of tile form \[X Y\], where X mad Y are nonterminals.
The first symbol in such a pair represents tile top-down prediction of a node, and tile second a node that has been encountered bottom-up.
The use of these pairs permits a straightforward combination of left-corner prediction, which is bottom-up, and top-down prediction and matching against the input in the style of a topdown automaton.
tiere we consider an extremely simple left-corner automaton, constructed from a grammar having the following productions: (1) S ~ NP VP (2) NP ~ John \] Mary (3) VP ~ V NP (4) v -~ nke~ The rules of tlle automaton are as follows: \[ . I 'Inpn't I Stac.k " I New top of staclf..I 1 John ...
2 Mary
...
3 likes 4 iynored X John 5 ignored ...
X Mary' 6 ignored ...
X likes '7 ignored ...
\[X NP\]' -~ ic, .... a ...
\[x v\] 9' ignored :..
IX X\] ...
John ...
Mary ...
likes,..
\[×.NP 1 ...
\[x NPJ ...
Ix v3 ...
\[x s) vv ...\[XVB\] SP The top of the stack is at right, and rules 4-9 are actually schemata for a set of rules in which X can be replaced by each of tile nonterminals (S, NP, VP, and V).
Tile parser begins with S on top of the stack, and a string has been successfully recognized if the stack is empty and the input exhausted.
5The a~mlysis bein~ stralghtforward, we omit the details here; for n complete discussion of the construction of PDAs for topdown and bottom-up pm~ing, see ~LP81, §3.6\].
ACRES DE COLING-92, NANTES, 23-28 AO~T 1992 1 9 4 PROc.
oF COLING-92, NANTES, AUO.
23-28, 1992 /s~ i I ivel V (NP i I \ ~/ likes "Jr ......
Figure 4: Distinguishing the top-down view of a node b'om the bottom-up view Rules 1-3 simply introduce texical items onto the stack as they are scanned.
Rules 4--6 represent bottomup reductions according to the lexical productions of the grammar (productions (2) and (4)); for example, rule 4 states that if a constituent X has been predicted top-down, and the word John is scanned, we continue seeking X top-down with the knowledge that we have identified an NP bottom-up.
Rules 7 and 8 implement left-corner prediction: if the left-corner node of a rule has been recognized bottom-up, then we hypothesize the parent node in bottom-up fashion and also predict the right siblings top-down.
For example, rule 8 states that if a V has been recognized bottom-ul), we should hypothesize that a VP is being recognized and also predict the remainder of the VP, namely an NP, top-down.
Finally, rule 9 pops a symbol off the top of the stack if we have predicted a constituent X top-down and then succeeded in finding it bottom-up.
In examining the behavior of this automaton for the sentence John likes Mary, a problem immediately becomes apparent.
The contents of the stack at each step during the parse are as follows: ! ! .....
=1 Joha VP VP {VP V\] vp VP\] •, S ~ IS NP\] IS Sl IS Sl IS S I (Ij ) (a) (4) {s| (~) "r ! \] ! lit ....
NP INP NP • . \[vt r' vP\] \[VP VP\] \[Vp VP) Is sl IS sl s s I s s (6) (9) o (~) As the sentence --a right-branching structure --is recognized, we find that the stack is accumulating symbols of the form \[X X\].
It is clear that as the depth of right-branching increases, the number of stacked-up symbols of this form will also increase, without upper bmmd.
Why is this happening?
Let us distinguish between the top-down "view" of a node and the bottom-up (left-corner) "view" of that node.
Figure 4 makes this distinction explicit: the VP predicted top-down by the rule S --* NP VP is distinct from the VP predicted in left-corner fashion using VP --~ V NP.
These are, in fact, precisely the two VPs in the symbol \[VP VP\].
Now, enumerating the arc between VP and S in the final parse tree is equivalent to identifying these two views (dotted ellipses in the figure).
As long as we have not identified the two views of VP as the same node, the arc is not enumerated --and the parent S remains incomplete in the sense defined by Abney and Johnson, It is rule 9 in the automaton that effects this identification: popping \[VP VP\] amounts to recognizing that the top-down view and the bottom-up view match.
Since the operation of the automaton prevents the symbol from being popped until the bottoIn-up view has been completed, it is clear that this automaton implements an arc-standard strategy rather than an arc-eager one.
Itence it is not surprisiug that the antomaton fails to support JohusonLaird's argument: far from being bounded, the stack of such automaton can grow without bound as the depth of right-brmlching increases.
4 Arc-eager Enumeration as Composition 4.1 An Easy Fix...
To summarize thus far, \[AJ91\] and \[JL83\] present two forms of the same argument, but each presentation suffers from a central shortcoming.
Abney and Johnson, discussing parsing strategies rather than parsers, fail to characterize top-down, bottom-up, and left-corner parsing in a way that permits a fair comparison of space utilization.
Johnson-Laird, ibrmalizing parsers as push-down automata, provides a characterization that clearly defines the terms of the comparison, but his leftcorner automaton lacks the properties needed to make the argument succeed.
Modifying the left-corner automaton so that it performs arc-eager enumeration is straightforward.
As discussed toward the end of the previous section, "attachment" of a node X to its pareut occurs when the symbol IX X\], representing the top-down and bottom-up views of that node, is removed from the stack.
In order to attach the node (i.e., enumerate the arc) eagerly, we should pop the symbol as soon as it is introduced.
For the automaton in the previous section, this amounts to augmenting rule schema 8 with the rule I \[ Input I St~k I New top otst~k I \[8'1 ign°redl.,.\[vPV\] I,,.NP \[ and, in general, augmenting the rules of left-corner prediction so that symbols of the form \[X X\] are not introduced obligatorily.
It is easy to show that the automaton, modified in this fashion, requires only a finite stack for arbitrarily ACTES DE COLING-92, NANTES, 23-28 Aotrr 1992 1 9 $ Pgoc.
OF COLING-92, NAN'I~S, AUG.
23-28. 1992 (A'--~B1 ...B~ . ) (A~ • BI...B~: 1 (Bt--.
3'~)...(B~. rk) Figure 5: Inferenre-ru&,liar,ncter~:alion of bottomup reduction step (left) ~d t,,p.down prediction step (,~ght).
deep leftand right-l,ranehmg constructions, but requires increasing stack ~t,,trq" fi,r c,'nter-embedded constructions as the depth -f,-mb~-dding increases.
Thus we have succeeded in pr,-~enl|ng a complete version of the argument in \[AJ91\] and \[JL83\] in the sense that 1.
top-down, bottom-up, and left-corner parsing are characterized in a formally precise way, 2.
the chaxacterizations are abstract, in the sense that the logic of the algorithms (in the form of nondeterministic push-down automata) is separated from their control (namely the control of how the automata's nondeterministic choices are made), 3.
the notion of space utilization (namely stack size) is the same for each case, permitting us to make a fair comparison, and 4.
the conclusion, as expected, is that top-down and bottom-up parsing both make incorrect predictions, but a form of left-corner parsing is consistent with the apparent behavior of the human sentence processing mechanism.
4.2 ...and its Implications The import of the "fix" in the previous section is not simply that the automaton can be made to display the appropriate behavior.
It is that the "arc-eager" enumeration strategy is a different (and perhaps misleading) description of a purser's ability to perform composition on the structures that it is building.
If we describe the parsers as sets of inference rules rather than automata, s the inference permitting arceager enumeration in the left-corner parser turns out to be a rule of composition: A ~ c~ • B and B ~/3 . 7 can be composed to form the dotted item A ~/3 . 3'.
For instance, the effect of rule 8' is to predict VP V • NP from V, and then immediately compose this new item with S --, NP . VP.
Equivalently, the rule first predicts the VP structure in Figure 4 from the V (giving us \[VP VP\], corresponding to the two VP nodes the figure), and then immediately identifies the lower VP node with the upper one (which removes \[VP VP\]), leaving just an S structure that lacks an NP.
STwo descriptior~ that are formally equivalent.
In contrast, even if one were to add a rule of composition to the inferential description of top-down and bottom-up parsers, it would have no effect.
Neither the top-down nor the bottom-up parser ever introduces a configuration in which the A constituent and B constituent are both only partially completed (and thus can be composed).
Instead, these parsers rewrite the entire right-hand side of a rule at once (see Figure 5).
In order for a rule of composition to be relevant, it is necessary that the parser introduce both the top-down view of a constituent (e.g.
B in A ---* ~ • B) and the bottom-up view of that constituent (e.g.
B in B ~ ft.3`) so that they may later be identified.
Unlike top-down and bottom-up parsers, a left-corner parser meets this criterion.
By presenting a complete version of the argument in \[AJ91\] and \[JL83\], we have essentially re-discovered proposals made by Puhnan \[Pu185, Pul86\] and Thompson et al.\[TDL91\]. Both propose parsers with leftcorner prediction and a composition operation added.
Pulman motivates his purser's design on grounds of psychological plausibility, though he does not present a complete version of the argument discussed here.
Thompson et al.are motivated by issues in parallel parsing.
In addition, we should note that JohnsonLaird introduces a parser with a composition-like operation later in his discussion, though outside the context of a formal comparison among parsing methods.
Abney (personal communication) points out that, though psychologically plausible in terms of the space utilization argument we have discussed, the automaton presented here may nonetheless fail to be plausible because of its behavior with regard to local ambiguity.
If we opt to compose whenever possible (e.g., always preferring rule 8' to rule 8 when X = VP), which seems natural, then left-recursive structures will lead to counterintuitive results -for example, in processing (2), the automaton will prefer to attach the NP the cat as the object of the verb, rather than waiting for the full NP the cat's dinner.
2 John
prepared \[\[tlm eat\]'s dinner\].
More generally, as Abney and Johnson discuss, there is a tradeoff between storage, which is conserved by strategies that perform attachment "eagerly," and ambiguity, which is avoided by deferring attachment until more information is present to resolve it.
On the basis of the observations we have made here, it appears that this tradeoff is expressed most naturally not in terms of a comparison between different parsing strategies, but rather in terms of the criteria for when to invoke a composition operation that is available to the parser.
ACTES DE COLING-92, NANTES, 23-28 Ao~Yr 1992 1 9 6 PROC.
OF COLING -92, NANTES, AUO.
23 -28, 1992 5 Conclusions In this paper, we have considered a space-utilization argument concerning the psychological plausibility of different parsing methods.
Both \[AJ91\] and \[JL83\] make the same basic claim, namely that top-down and bottom-up parsing lead to incorrect predictions of asymmetry in human processing -predictions that can be avoided by utilizing a left-corner strategy.
We have demonstrated difficulties with both of their formulations and presented a more precise account.
In so doing, we have found that composition, rather than left-corner prediction per se, plays the central role in distinguishing parsing methods.
In making the argument, we were forced to abandon the abstract characterization of parsing methods in terms of strategies, and return to defining parsers in terms of their realizations as automata.
This has the unfortunate consequence of tying the argument to context-free gramnrars, losing tire attractive fornralismindependent quality evoked in \[AJ91\].
Since context:free grammars are no longer generally considered likely models for natural language in the general case \[Shi85\], one wonders how the discussion here might be extended to parsing within more powerful grannnatical frameworks.
It is interesting to note the relationship between the style of left-corner parsing described here and one such framework, combinatory categorial grammar (CCG) \[Ste90\].
Composition is an integral part of CCG, as is the notion of type-raising, which resembles left-corner prediction.
7 The
operation of a left-corner parser with composition can fairly be described as being in the style of CCG, but retaining the context-free base.
Since one attractive feature of CCG is its inherent left-to-right, word-by-word incrementality, it is perhaps not surprising to find that parsers of CCG tend naturally to meet the criteria for psychological plausibility discussed bere.
CCG is one instance of a general class known as the mildly context-sensitive grammar formalisms \[JVSW88\].
We are currently investigating a generalization of the argument presented here to other formalisms within that class.
Acknowledgements This research was supported by tile following grants: ARO DAAL 03-89-C-0031, DARPA N00014-9O-J-1863, NSF IRl 90-16592, and Ben Franklin 91S.3078C-1.
I would like to thank Steve Abney, Mark Johnson, Aravind Joshi, Yves Schabes, Stuart Shieber, and members of tile CLIFF group at Penn for their helpfifl discussion and criticism, rFor example, NP can be type-ralsed to S/(S\NP), whid~ roughly corresponds to S ~ NP . VP.
\[I(im73\] \[LP81\] \[MC63\] \[MI84\] \[ps871 \[Pu185\] \[Pul86\] \[Shi85\] \[Ste9O\] \[TI)L911 \[WooT0\] ACTES DE COLING-92, NAN'NS, 23-28 Aotrr 1992 1 9 7 PROC.
OF COLING-92, NAI~t'ES, AUO.
23-28, 1992 References Steven Abney and Mark Johnson.
Memory requirements and local ambiguities for parttiag strategies.
Journal of Psycholinguistic Research, 20(3):233--250, 1991.
Alfred Aho, Ravi Sethi, and Jeffrey Ullmu.
Compilers: Principles, Techniques, and Toe,t.
Addison Wesley, 1986.
A. DeRoeck, R.
Johnson, M.
King, M.
net, G.
Sampson, and N.
Varile. A myth about ceutre-embedding.
Lingua, 58:327-340, 1981.
Philip N.
Johnson-Laird. Mental Models.
Harvard University Press, 1983.
A. K.
Joshi, K.
Vijay-Shanker, and D.
J, Weir.
The convergence of mildly contextsensitive grammatical formalisms.
In P.
Selht and T.
Wasow, editors, Processing of Lingutsl,c Structure.
MIT Press, Cambridge, MA, 1988.
J. Kimball.
Seven principles of surface-structure parsing in natural language.
Cognition, 2:15-47, 1973.
Itarry Lewis and Christos Papadimitrion.
Ele. ments of the Theory of Computation.
PrenticeItall, 1981.
George Miller and Noam Chomsky.
Finitary models of language users.
In R.
Luce, R.
Bush, and E.
Galanter, editors, Handbook of Math.
ematical Psycholcfy, Volume 2.
John Wiley, 1963.
G. A.
Miller and S.
Isard. Free recall of selfembedded English sentences.
Information and Control, 7:292--303, 1964.
l"ernando C.
N. Pereira and Stuart M.
Shieber. Pralog and Natural Language Analysis.
Cen-. ter for the Study of Language and Information, 1987.
Stephen Pulman.
A parser that doesn't.
In Proceedings of the 2nd European ACL, pages 128135, 1985.
Stephen Pulman.
Grammars, parsers, and memory limitations.
Language and Cognitive Pro.
cesses, 1(3):197-225, 1986.
S. M.
Shieber. Evidence against the contextfreeness of naturM language.
Linguistics and Philosophy, 8:333-343, 1985.
Mark Steedman.
Gapping as constituent coordination.
Linguistics and Philosophy, 13:207-263, April 1990.
H. Thompson, M.
Dixon, and J.
Lumping. Compose-reduce parsing.
In Proceedings o.f the 29th Atmual Meetit,y of the ACL, pages 87-97, June 1991.
William A.
Woods. Transition network grammars for natural language analysis.
Commu. nications of the ACM, 13(10):591-606, October 1970 .

