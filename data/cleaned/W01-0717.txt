BusTUC A natural language bus route oracle Tore A m b l e Dept.
of computer and information science University of Trondheim Norway, N-7491 amble@idi, nt nu.
no Abstract The paper describes a natural language based expert system route advisor for the public bus transport in Trondheim, Norway.
The system is available on the Internet,and has been intstalled at the bus company's web server since the beginning of 1999.
The system is bilingual, relying on an internal language independent logic representation.
In between the question and the answer is a process of lexical analysis, syntax analysis, semantic analysis, pragmatic reasoning and database query processing.
One could argue that the information content could be solved by an interrogation, whereby the customer is asked to produce 4 items: s t a t i o n of departure, station of arrival, earliest departure timeand/or latest arrival time.
It Introduction A natural language interface to a computer database provides users with the capability of obtaining information stored in the database by querying the system in a natural language (NL).
With a natural language as a means of communication with a computer system, the users can make a question or a statement in the way they normally think about the information being discussed, freeing them from having to know how the computer stores or processes the information.
The present implementation represents a a major effort in bringing natural language into practical use.
A system is developed that can answer queries about bus routes, stated as natural language texts, and made public through the Internet World Wide Web is a myth that natural language is a better way of communication because it is "natural language".
The challenge is to prove by demonstration that an NL system can be made that will be preferred to the interrogative mode.
To do that, the system has to be correct, user friendly and almost complete within the actual domain.
P r e v i o u s Efforts, C H A T 8 0, P R A T 8 9 and HSQL Trondheim is a small city with a university and 140000 inhabitants.
Its central bus systems has 42 bus lines, serving 590 stations, with 1900 departures per day (in average).
T h a t gives approximately 60000 scheduled bus station passings per day, which is somehow represented in the route data base.
The starting point is to automate the function of a route information agent.
The following example of a system response is using an actual request over telephone to the local route information company: Hi, I live in Nidarvoll and tonight i must reach a train to Oslo at 6 oclock.
The system, called BusTUC is built upon the classical system CHAT-80 (Warren and Pereira, 1982).
CHAT-80 was a state of the art natural language system that was impressive on its own merits, but also established Prolog as a viable and competitive language for Artificial Intelligence in general.
The system was a brilliant masterpiece of software, efficient and sophisticated.
The natural language system was connected to a small query system for international geography.
The following query could be analysed and answered in a split second: Which country bordering the Mediterranean borders a country that is bordered by a country whose population exceeds the population of India?
(The answer 'Turkey' has become incorrect as time has passed.
The irony is that Geography was chosen as a domain without time.) and a typical answer would follow quickly: Bus number 54 passes by Nidarvoll skole at 1710 and arrives at Trondheim Railway Station at 1725.
The abi!ity to answer ridiculously long queries is of course not the main goal.
The main lesson is that complex sentences are analysed with a proper understanding without sacrificing efficiency.
Any superfificial pattern matching technique would prove futile sooner or later.
Making a N o r w e g i a n CHAT-80, PRAT-89 At the University of Trondheim (NTNU), two students made a Norwegian version of CHAT-80,called PRAT-89 (Teigen and Vetland, 1988),(Teigen and Vetland, 1989).
(Also, a similar Swedish project SNACK-85 was reported).
The dictionary was changed from English to Norwegian together with new rules for morphological analysis.
The change of grammar from English to Norwegian proved to be amazingly easy.
It showed that the langauges were more similar than one would believe, given that the languages are incomprehensible to each other's communities.
After changing the dictionary and graramar, the following Norwegian query about the same domain could be answered correctly in a few seconds.
Hvilke afrikanske land som hat en befolkning stoerre enn 3 millioner og mindre enn 50 millioner og er nord for Botswana og oest for Libya hat en hovedstad som hat en befolkning stoerre enn 100 tusen.
Coupling the s y s t e m to an SQL database.
After the remodelling, the system could answer queries in "Scandinavian" to an internal hospital database as well as CHAT-80 could answer Geography questions.
HSQL produced a Prolog-like code FOL (First Order Logic) for execution.
A mapping from FOL to the data base Schema was defined, and a translator from FOL to SQL was implemented.
The example Hvilke menn ligger i en kvinnes seng?
(Which men lie in a woman's bed?
) would be translated dryly into the SQL query: SELECT DISTINCT T3.name,Tl.sex,T2.reg_no,T3.sex, T4.reg_no,T4.bed_no,T5.hosp_no,T5.ward_no FROM PATIENT TI,OCCUPANCY T2,PATIENT T3, OCCUPANCY T4,WARD T5 WHERE (Tl.sex='f') AND (T2.reg_no=Tl.reg_no) AND (T3.sex='m') AND (T4.reg_no=T3.reg_no) AND (T4.bed_no=T2.bed_no) AND (T5.hosp_no=T4.hosp_no) AND (T5.ward_no=T4.ward_no) 2.3 T h e T h e U n d e r s t a n d i n g C o m p u t e r The HSQL was a valuable experience in the effort to make transportable natural language interfaces.
However, the underlying system CHAT-80 restricted the further development.
After the HSQL Project was finished, an internal reseach project TUC (the Understanding Computer) was initiated at NTNU to carry on the results from HSQL.
The project goals differed from those of HSQL in a number of ways, and would not be concerned with multimedia interfaces. On the other hand, portability and versatility were made central issues concerning the generality of the language and its applications.
The research goals could be summarised as to Give computers an operational understanding of natural language.
 Build intelligent systems with natural language capabilities.
 Study common sense reasoning in natural language.
A test criterion for the understanding capacity is that after a set of definitions in a Naturally Readable Logic, NRL, the system's answer to queries in NRL should conform to the answers of an idealised rational agent.
( A translation is beside the point o.f being a long query in Norwegian.) 2.2 HSQL H e l p S y s t e m for SQL A Nordic project HSQL (Help System for SQL) was accomplished in 1988-89 to make a joint Nordic effort interfaces to databases.
The HSQL project was led by the Swedish State Bureau (Statskontoret), with participants from Sweden, Denmark, Finland and Norway (Amble et al., 1990).
The aim of HSQL was to build a natural language interface to SQL databases for the Scandinavian languages Swedish, Danish and Norwegian.
These languages are very similar, and the Norwegian version of CHAT-80 was easily extended to the other Scandinavian languages.
Instead of Geography, a more typical application area was chosen to be a query system for hospital administration.
We decided to target an SQL database of a hospital administration which had been developed already.
The next step was then to change the domain of discourse from Geography to hospital administration, using the same knowledge representation techniques used in CHAT-80.
A semantic model of this domain was made, and then implemented in the CHAT-80 framework.
The modelling technique that proved adequate was to use an extended Entity Relationship (ER) model with a class (type) hierarchy, attributes belonging to each class, single inheritance of attributes and relationships.
Every man that lives loves Mary.
John is a man.
John lives.
Who loves Mary?
==> John 3 Anatomy of the bus route oracle The main components of the bus route information systems are:  A parser system, consisting of a dictionary, a lexical processor, a grammar and a parser.
 A knowledge base (KB), divided into a semantic KB and an application KB  A query processor, contalng a routing logic system, and a route data base.
The system is bilingual and contains a double set of dictionary, morphology and grammar.
Actually, it detects which language is most probable by counting the number of unknown words related to each language, and acts accordingly.
The grammars are surprisingly similar, but no effort is made to coalesce them.
The Norwegian grammar is slightly bigger than the English grammar, mostly because it is more elaborated but also because Norwegian allows a freer word order.
3.1 Features
of BusTUC For the Norwegian systems, the figures give an indication of the size of the domain: 420 nouns, 150 verbs, 165 adjectives, 60 prepositions, etc.
There are 1300 grammar rules ( 810 for English) although half of the rules are very low level.
The semantic net described below contains about 4000 entries.
A big name table of 3050 names in addition to the official station names, is required to capture the variety of naming.
A simple spell correction is a part of the system ( essentially 1 character errors).
The pragmatic reasoning is needed to translate the output from the parser to a route database query language . This is done by a production system called Pragma, which acts like an advanced rewriting system with 580 rules.
In addition, there is another rule base for actually generating the natural language answers (120 rules).
The system is mainly written in Prolog (Sicstus Prolog 3.7), with some Perl programs for the communication and CGI-scripts.
At the moment, there are about 35000 lines of programmed Prolog code (in addition to route tables which are also in Prolog).
Average response time is usually less than 2 seconds, but there are queries that demand up to 10 seconds.
The error rate for single, correct, complete and relevant questions is about 2 percent.
NRL is defined in a closed context.
Thus interfaces to other systems are in principle defined through simulating the environment as a dialogue partner.
TUC is a prototypical natural language processor for English written in Prolog.
It is designed to be a general purpose easily adaptable natural language processor.
It consists of a general grammar for a subset of English, a semantic knowledge base, and modules for interfaces to other interfaces like UNIX, SQL-databases and general textual information sources.
2.4 The
TABOR Project It so happened that a Universtity Project was starteded in 1996, called T A B O R ( " Speech based user interfaces and reasoning systems "), with the aim of building an automatic public transport route oracle, available over the public telephone.
At the onset of the project, the World Wide Web was fresh, and not as widespread as today, and the telephone was still regarded as the main source of information for the public.
Since then, the Internet became the dominant medium, and it is as likeley to find a computer with Internet connection, as to find a local busroute table.
( The consequtive wide spreading of cellular phones changed the picture in favour of the telephone, but that is another story).
It was decided that a text based information system should be built, regardless of the status of the speech rocgnition and speech synthesis effort, which proved to lag behind after a while.
The BusTUC system The resulting system BusTUC grew out as a natural application of TUC, and an English prototype could be built within a few months (Bratseth, 1997).
Since the summer 1996, the prototype was put onto the Internet, and been developed and tested more or less continually until today.
The most important extension was that the system was made bilingual (Norwegian and English) during the fall 1996.
In spring 1999, the BusTUC was finally adopted by the local bus company in Trondheim ( A/S Trondheim Trafikkselskap), which set up a server ( a 300 MHz PC with Linux).
Until today, over 150.000 questions have been answered, and BusTUC seems to stabilize and grow increasingly popular.
3.2 The
Parser S y s t e m The G r a m m a r S y s t e m The grammar is based on a simple grammar for statements, while questions and commands are derived by the use of movements.
The grammar 3 fiformalism which is called Consensical Grammar, (CONtext SENSitive CompositionAL Grammar) is an easy to use variant of Extraposition Grammar (Pereira and Warren, 1980), which is a generalisation of Definite Clause Grammars.
Compositional grammar means that the semantics of a a phrase is composed of the semantics of the subphrases; the basic constituents being a form of verb complements.
As for Extraposition grammars, a grammar is translated to Definite Clause Grammars, and executed as such.
A characteristic syntactic expression in Consensical G r a m m a r m a y define an incomplete construct in terms of a "difference " between complete constructs.
W h e n possible, the parser will use the subtracted part in stead of reading from the input, after a gap if necessary.
The effect is the same as for Exwhich is analysed as for which X is it true that the (X) person has a dog that barked? where the last line is analysed as a s t a t e m e n t . Movement is easily handled in Consensical Grammar without making special phrase rules for each kind of movement.
The following example shows how TUC manages a variety of analyses using movements: Max said Bill thought Joe believed Fido Barked.
Who said Bill thought Joe believed Fido barked?
Who did Max say thought Joe believed Fido barked? traposition grammars, but the this format is more intuitive.
Examples of grammar rules.
Who did Max say Bill thought believed Fido barked?
T h e parser The experiences with Consensical grammars are a bit mixed however.
The main problem is the parsing method itself, which is top down with backtracking.
Many principles that would prove elegant for small domains turned out to be too costly for larger domains, due to the wide variety of modes of expressions, incredible ambiguities and the sheer size of the covered language.
The disambiguation is a major problem for small grammars and large languages, and was solved by the following guidelines:  a semantic type checking was integrated into the parser, and would help to discard sematica/ly wrong parses from the start.
 a heuristics was followed that proved almost irreproachable: The longest possible phrase of a category that is semantically correct is in most cases the preferred interpretation.
 due to the perplexity of the language, some committed choices (cuts) had to be inserted into the grammar at strategic places.
As one could fear however, this implied that wrong choices being made at some point in the parsing could not be recovered by backtracking.
These problems also made it imperative to introduce a timeout on the parsing process of embarassing 10 seconds.
Although most sentences, would be parsed within a second, some legal sentences of moderate size actually need this time.
4 Example: Whose dog barked? is analysed as if the sentence had been Who has a dog t h a t barked? which is analysed as Which p e r s o n has a dog t h a t barked? fi3.3 The semantic knowledge base Adaptability means that the system does not need to be reprogrammed for each new application.
The design principle of TUC is that most of the changes are made in a tabular semantic knowledge base, while there is one general grammar and dictionary.
In general, the logic is generated automatically from the semantic knowledge base.
The nouns play a key role in the understanding part as they constitute the class or type hierarchy.
Nouns are defined in an a k i n d o f hierarchy.
The hierarchy is tree-structured with single inheritance.
The top level also constitute the top level ontology of TUC's world.
In fact, a type check of the compliances of verbs, nouns adjectives and prepositions is not only necessary for the semantic processing but is essential for the syntax analysis for the disambiguation as well.
In TUC, the legal combinations are carefully assembled in the semantic network, which then serves a dual purpose.
These semantic definitions are necessary to allow for instance the following sentences The dog saw a man with a telescope.
The man saw a dog with a telescope.
gives exactly the same code.
% Type of question % tuc is a program % A is a real bus % B isa saturday % Nidar is a place % D is an event Y.
C was known at D Y.
E is an event in C action(go,E), Y.
the action of E is Go actor(A,E), Y.
the actor of E is A srel(to,place,nidar,E),Y.
E is to nidar srel(on,time,B,E), y, E is on the saturday B to be treated differently because with telescope m a y modify the noun man but not the noun dog, while with telescope modifies the verb see, restricted to person.
The event parameter plays an important role in the semantics.
It is used for various purposes.
The most salient role is to identify a subset of time and space in which an action or event occured.
Both the actual time and space coordinates are connected to the actions through the event parameter.
Pragmatic reasoning The TQL is translated to a route database query language (BusLOG) which is actually a Prolog program.
This is done by a production system called Pragma, which acts like an advanced rewriting system with 580 rules.
In addition, there is another rule base for actually generating the natural language answers (120 rules).
4 Conclusions
The TUC approach has as its goal to automate the creation of new natural language interfaces for a well defined subset of the language and with a minimum of explicit programming.
The implemented system has proved its worth, and is interesting if for no other reason.
There is also an increasing interest from other bus companies and route information companies alike to get a similar system for their customers.
Further work remains to make the parser really efficient, and much work remains to make the language coverage complete within reasonable limits.
It is an open question whether the system of this kind will be a preferred way of offering information to the public.
If it is, it is a fair amount of work to make it a portable system that can be implemented elsewhere, also connecting various travelling agencies.
If not, it will remain a curiosity.
But anyway, a system like this will be a contribution to the development of intelligent systems.
3.4 The
Query Processor Event Calculus The semantics of the phrases are built up by a kind of verb complements, where the event play a central role.
The text is translated from Natural language into a form called TQL (Temporal Query Language/ TUC Query Language) which is a first order event calculus expression, a self contained expression containing the literal meaning of an utterance.
A formalism TQL that was defined, inspired by the Event Calculus by Kowalski and Sergot (Kowalski and Sergot, 1986).
The TQL expressions consist of predicates, functions, constants and variables.
The textual words of nouns and verbs are translated to generic predicates using the selected interpretation.
The following question Do you know whether the bus goes to Nidar on Saturday ? would give the TQL expression below.
BusTUC A natural language bus route oracle Tore A m b l e Dept.
of computer and information science University of Trondheim Norway, N-7491 amble@idi, nt nu.
no Abstract The paper describes a natural language based expert system route advisor for the public bus transport in Trondheim, Norway.
The system is available on the Internet,and has been intstalled at the bus company's web server since the beginning of 1999.
The system is bilingual, relying on an internal language independent logic representation.
In between the question and the answer is a process of lexical analysis, syntax analysis, semantic analysis, pragmatic reasoning and database query processing.
One could argue that the information content could be solved by an interrogation, whereby the customer is asked to produce 4 items: s t a t i o n of departure, station of arrival, earliest departure timeand/or latest arrival time.
It Introduction A natural language interface to a computer database provides users with the capability of obtaining information stored in the database by querying the system in a natural language (NL).
With a natural language as a means of communication with a computer system, the users can make a question or a statement in the way they normally think about the information being discussed, freeing them from having to know how the computer stores or processes the information.
The present implementation represents a a major effort in bringing natural language into practical use.
A system is developed that can answer queries about bus routes, stated as natural language texts, and made public through the Internet World Wide Web is a myth that natural language is a better way of communication because it is "natural language".
The challenge is to prove by demonstration that an NL system can be made that will be preferred to the interrogative mode.
To do that, the system has to be correct, user friendly and almost complete within the actual domain.
P r e v i o u s Efforts, C H A T 8 0, P R A T 8 9 and HSQL Trondheim is a small city with a university and 140000 inhabitants.
Its central bus systems has 42 bus lines, serving 590 stations, with 1900 departures per day (in average).
T h a t gives approximately 60000 scheduled bus station passings per day, which is somehow represented in the route data base.
The starting point is to automate the function of a route information agent.
The following example of a system response is using an actual request over telephone to the local route information company: Hi, I live in Nidarvoll and tonight i must reach a train to Oslo at 6 oclock.
The system, called BusTUC is built upon the classical system CHAT-80 (Warren and Pereira, 1982).
CHAT-80 was a state of the art natural language system that was impressive on its own merits, but also established Prolog as a viable and competitive language for Artificial Intelligence in general.
The system was a brilliant masterpiece of software, efficient and sophisticated.
The natural language system was connected to a small query system for international geography.
The following query could be analysed and answered in a split second: Which country bordering the Mediterranean borders a country that is bordered by a country whose population exceeds the population of India?
(The answer 'Turkey' has become incorrect as time has passed.
The irony is that Geography was chosen as a domain without time.) and a typical answer would follow quickly: Bus number 54 passes by Nidarvoll skole at 1710 and arrives at Trondheim Railway Station at 1725.
The abi!ity to answer ridiculously long queries is of course not the main goal.
The main lesson is that complex sentences are analysed with a proper understanding without sacrificing efficiency.
Any superfificial pattern matching technique would prove futile sooner or later.
Making a N o r w e g i a n CHAT-80, PRAT-89 At the University of Trondheim (NTNU), two students made a Norwegian version of CHAT-80,called PRAT-89 (Teigen and Vetland, 1988),(Teigen and Vetland, 1989).
(Also, a similar Swedish project SNACK-85 was reported).
The dictionary was changed from English to Norwegian together with new rules for morphological analysis.
The change of grammar from English to Norwegian proved to be amazingly easy.
It showed that the langauges were more similar than one would believe, given that the languages are incomprehensible to each other's communities.
After changing the dictionary and graramar, the following Norwegian query about the same domain could be answered correctly in a few seconds.
Hvilke afrikanske land som hat en befolkning stoerre enn 3 millioner og mindre enn 50 millioner og er nord for Botswana og oest for Libya hat en hovedstad som hat en befolkning stoerre enn 100 tusen.
Coupling the s y s t e m to an SQL database.
After the remodelling, the system could answer queries in "Scandinavian" to an internal hospital database as well as CHAT-80 could answer Geography questions.
HSQL produced a Prolog-like code FOL (First Order Logic) for execution.
A mapping from FOL to the data base Schema was defined, and a translator from FOL to SQL was implemented.
The example Hvilke menn ligger i en kvinnes seng?
(Which men lie in a woman's bed?
) would be translated dryly into the SQL query: SELECT DISTINCT T3.name,Tl.sex,T2.reg_no,T3.sex, T4.reg_no,T4.bed_no,T5.hosp_no,T5.ward_no FROM PATIENT TI,OCCUPANCY T2,PATIENT T3, OCCUPANCY T4,WARD T5 WHERE (Tl.sex='f') AND (T2.reg_no=Tl.reg_no) AND (T3.sex='m') AND (T4.reg_no=T3.reg_no) AND (T4.bed_no=T2.bed_no) AND (T5.hosp_no=T4.hosp_no) AND (T5.ward_no=T4.ward_no) 2.3 T h e T h e U n d e r s t a n d i n g C o m p u t e r The HSQL was a valuable experience in the effort to make transportable natural language interfaces.
However, the underlying system CHAT-80 restricted the further development.
After the HSQL Project was finished, an internal reseach project TUC (the Understanding Computer) was initiated at NTNU to carry on the results from HSQL.
The project goals differed from those of HSQL in a number of ways, and would not be concerned with multimedia interfaces. On the other hand, portability and versatility were made central issues concerning the generality of the language and its applications.
The research goals could be summarised as to Give computers an operational understanding of natural language.
 Build intelligent systems with natural language capabilities.
 Study common sense reasoning in natural language.
A test criterion for the understanding capacity is that after a set of definitions in a Naturally Readable Logic, NRL, the system's answer to queries in NRL should conform to the answers of an idealised rational agent.
( A translation is beside the point o.f being a long query in Norwegian.) 2.2 HSQL H e l p S y s t e m for SQL A Nordic project HSQL (Help System for SQL) was accomplished in 1988-89 to make a joint Nordic effort interfaces to databases.
The HSQL project was led by the Swedish State Bureau (Statskontoret), with participants from Sweden, Denmark, Finland and Norway (Amble et al., 1990).
The aim of HSQL was to build a natural language interface to SQL databases for the Scandinavian languages Swedish, Danish and Norwegian.
These languages are very similar, and the Norwegian version of CHAT-80 was easily extended to the other Scandinavian languages.
Instead of Geography, a more typical application area was chosen to be a query system for hospital administration.
We decided to target an SQL database of a hospital administration which had been developed already.
The next step was then to change the domain of discourse from Geography to hospital administration, using the same knowledge representation techniques used in CHAT-80.
A semantic model of this domain was made, and then implemented in the CHAT-80 framework.
The modelling technique that proved adequate was to use an extended Entity Relationship (ER) model with a class (type) hierarchy, attributes belonging to each class, single inheritance of attributes and relationships.
Every man that lives loves Mary.
John is a man.
John lives.
Who loves Mary?
==> John 3 Anatomy of the bus route oracle The main components of the bus route information systems are:  A parser system, consisting of a dictionary, a lexical processor, a grammar and a parser.
 A knowledge base (KB), divided into a semantic KB and an application KB  A query processor, contalng a routing logic system, and a route data base.
The system is bilingual and contains a double set of dictionary, morphology and grammar.
Actually, it detects which language is most probable by counting the number of unknown words related to each language, and acts accordingly.
The grammars are surprisingly similar, but no effort is made to coalesce them.
The Norwegian grammar is slightly bigger than the English grammar, mostly because it is more elaborated but also because Norwegian allows a freer word order.
3.1 Features
of BusTUC For the Norwegian systems, the figures give an indication of the size of the domain: 420 nouns, 150 verbs, 165 adjectives, 60 prepositions, etc.
There are 1300 grammar rules ( 810 for English) although half of the rules are very low level.
The semantic net described below contains about 4000 entries.
A big name table of 3050 names in addition to the official station names, is required to capture the variety of naming.
A simple spell correction is a part of the system ( essentially 1 character errors).
The pragmatic reasoning is needed to translate the output from the parser to a route database query language . This is done by a production system called Pragma, which acts like an advanced rewriting system with 580 rules.
In addition, there is another rule base for actually generating the natural language answers (120 rules).
The system is mainly written in Prolog (Sicstus Prolog 3.7), with some Perl programs for the communication and CGI-scripts.
At the moment, there are about 35000 lines of programmed Prolog code (in addition to route tables which are also in Prolog).
Average response time is usually less than 2 seconds, but there are queries that demand up to 10 seconds.
The error rate for single, correct, complete and relevant questions is about 2 percent.
NRL is defined in a closed context.
Thus interfaces to other systems are in principle defined through simulating the environment as a dialogue partner.
TUC is a prototypical natural language processor for English written in Prolog.
It is designed to be a general purpose easily adaptable natural language processor.
It consists of a general grammar for a subset of English, a semantic knowledge base, and modules for interfaces to other interfaces like UNIX, SQL-databases and general textual information sources.
2.4 The
TABOR Project It so happened that a Universtity Project was starteded in 1996, called T A B O R ( " Speech based user interfaces and reasoning systems "), with the aim of building an automatic public transport route oracle, available over the public telephone.
At the onset of the project, the World Wide Web was fresh, and not as widespread as today, and the telephone was still regarded as the main source of information for the public.
Since then, the Internet became the dominant medium, and it is as likeley to find a computer with Internet connection, as to find a local busroute table.
( The consequtive wide spreading of cellular phones changed the picture in favour of the telephone, but that is another story).
It was decided that a text based information system should be built, regardless of the status of the speech rocgnition and speech synthesis effort, which proved to lag behind after a while.
The BusTUC system The resulting system BusTUC grew out as a natural application of TUC, and an English prototype could be built within a few months (Bratseth, 1997).
Since the summer 1996, the prototype was put onto the Internet, and been developed and tested more or less continually until today.
The most important extension was that the system was made bilingual (Norwegian and English) during the fall 1996.
In spring 1999, the BusTUC was finally adopted by the local bus company in Trondheim ( A/S Trondheim Trafikkselskap), which set up a server ( a 300 MHz PC with Linux).
Until today, over 150.000 questions have been answered, and BusTUC seems to stabilize and grow increasingly popular.
3.2 The
Parser S y s t e m The G r a m m a r S y s t e m The grammar is based on a simple grammar for statements, while questions and commands are derived by the use of movements.
The grammar 3 fiformalism which is called Consensical Grammar, (CONtext SENSitive CompositionAL Grammar) is an easy to use variant of Extraposition Grammar (Pereira and Warren, 1980), which is a generalisation of Definite Clause Grammars.
Compositional grammar means that the semantics of a a phrase is composed of the semantics of the subphrases; the basic constituents being a form of verb complements.
As for Extraposition grammars, a grammar is translated to Definite Clause Grammars, and executed as such.
A characteristic syntactic expression in Consensical G r a m m a r m a y define an incomplete construct in terms of a "difference " between complete constructs.
W h e n possible, the parser will use the subtracted part in stead of reading from the input, after a gap if necessary.
The effect is the same as for Exwhich is analysed as for which X is it true that the (X) person has a dog that barked? where the last line is analysed as a s t a t e m e n t . Movement is easily handled in Consensical Grammar without making special phrase rules for each kind of movement.
The following example shows how TUC manages a variety of analyses using movements: Max said Bill thought Joe believed Fido Barked.
Who said Bill thought Joe believed Fido barked?
Who did Max say thought Joe believed Fido barked? traposition grammars, but the this format is more intuitive.
Examples of grammar rules.
Who did Max say Bill thought believed Fido barked?
T h e parser The experiences with Consensical grammars are a bit mixed however.
The main problem is the parsing method itself, which is top down with backtracking.
Many principles that would prove elegant for small domains turned out to be too costly for larger domains, due to the wide variety of modes of expressions, incredible ambiguities and the sheer size of the covered language.
The disambiguation is a major problem for small grammars and large languages, and was solved by the following guidelines:  a semantic type checking was integrated into the parser, and would help to discard sematica/ly wrong parses from the start.
 a heuristics was followed that proved almost irreproachable: The longest possible phrase of a category that is semantically correct is in most cases the preferred interpretation.
 due to the perplexity of the language, some committed choices (cuts) had to be inserted into the grammar at strategic places.
As one could fear however, this implied that wrong choices being made at some point in the parsing could not be recovered by backtracking.
These problems also made it imperative to introduce a timeout on the parsing process of embarassing 10 seconds.
Although most sentences, would be parsed within a second, some legal sentences of moderate size actually need this time.
4 Example: Whose dog barked? is analysed as if the sentence had been Who has a dog t h a t barked? which is analysed as Which p e r s o n has a dog t h a t barked? fi3.3 The semantic knowledge base Adaptability means that the system does not need to be reprogrammed for each new application.
The design principle of TUC is that most of the changes are made in a tabular semantic knowledge base, while there is one general grammar and dictionary.
In general, the logic is generated automatically from the semantic knowledge base.
The nouns play a key role in the understanding part as they constitute the class or type hierarchy.
Nouns are defined in an a k i n d o f hierarchy.
The hierarchy is tree-structured with single inheritance.
The top level also constitute the top level ontology of TUC's world.
In fact, a type check of the compliances of verbs, nouns adjectives and prepositions is not only necessary for the semantic processing but is essential for the syntax analysis for the disambiguation as well.
In TUC, the legal combinations are carefully assembled in the semantic network, which then serves a dual purpose.
These semantic definitions are necessary to allow for instance the following sentences The dog saw a man with a telescope.
The man saw a dog with a telescope.
gives exactly the same code.
% Type of question % tuc is a program % A is a real bus % B isa saturday % Nidar is a place % D is an event Y.
C was known at D Y.
E is an event in C action(go,E), Y.
the action of E is Go actor(A,E), Y.
the actor of E is A srel(to,place,nidar,E),Y.
E is to nidar srel(on,time,B,E), y, E is on the saturday B to be treated differently because with telescope m a y modify the noun man but not the noun dog, while with telescope modifies the verb see, restricted to person.
The event parameter plays an important role in the semantics.
It is used for various purposes.
The most salient role is to identify a subset of time and space in which an action or event occured.
Both the actual time and space coordinates are connected to the actions through the event parameter.
Pragmatic reasoning The TQL is translated to a route database query language (BusLOG) which is actually a Prolog program.
This is done by a production system called Pragma, which acts like an advanced rewriting system with 580 rules.
In addition, there is another rule base for actually generating the natural language answers (120 rules).
4 Conclusions
The TUC approach has as its goal to automate the creation of new natural language interfaces for a well defined subset of the language and with a minimum of explicit programming.
The implemented system has proved its worth, and is interesting if for no other reason.
There is also an increasing interest from other bus companies and route information companies alike to get a similar system for their customers.
Further work remains to make the parser really efficient, and much work remains to make the language coverage complete within reasonable limits.
It is an open question whether the system of this kind will be a preferred way of offering information to the public.
If it is, it is a fair amount of work to make it a portable system that can be implemented elsewhere, also connecting various travelling agencies.
If not, it will remain a curiosity.
But anyway, a system like this will be a contribution to the development of intelligent systems.
3.4 The
Query Processor Event Calculus The semantics of the phrases are built up by a kind of verb complements, where the event play a central role.
The text is translated from Natural language into a form called TQL (Temporal Query Language/ TUC Query Language) which is a first order event calculus expression, a self contained expression containing the literal meaning of an utterance.
A formalism TQL that was defined, inspired by the Event Calculus by Kowalski and Sergot (Kowalski and Sergot, 1986).
The TQL expressions consist of predicates, functions, constants and variables.
The textual words of nouns and verbs are translated to generic predicates using the selected interpretation.
The following question Do you know whether the bus goes to Nidar on Saturday ? would give the TQL expression below.
Machine Translation of Very Close Languages Jan HAJI(~ Computer Science Dept.
Johns Hopkins University 3400 N.
Charles St., Baltimore, MD 21218, USA hajic@cs.jhu.edu Jan HRIC KTI MFF UK Malostransk6 nfim.25 Praha 1, Czech Republic, 11800 hric@barbora.m ff.cuni.cz Vladislav KUBON OFAL MFF UK Malostransk6 mim.25 Praha 1, Czech Republic, 11800 vk@ufal.mff.cuni.cz Abstract Using examples of the transfer-based MT system between Czech and Russian RUSLAN and the word-for-word MT system with morphological disambiguation between Czech and Slovak (~ESILKO we argue that for really close languages it is possible to obtain better translation quality by means of simpler methods.
The problem of translation to a group of typologically similar languages using a pivot language is also discussed here.
demonstrate that this assumption holds only for really very closely related languages.
1. Czech-to-Russian MT system RUSLAN 1.1 History Introduction Although the field of machine translation has a very long history, the number of really successful systems is not very impressive.
Most of the funds invested into the development of various MT systems have been wasted and have not stimulated a development of techniques which would allow to translate at least technical texts from a certain limited domain.
There were, of course, exceptions, which demonstrated that under certain conditions it is possible to develop a system which will save money and efforts invested into human translation.
The main reason why the field of MT has not met the expectations of sci-fi literature, but also the expectations of scientific community, is the complexity of the task itself.
A successful automatic translation system requires an application of techniques from several areas of computational linguistics (morphology, syntax, semantics, discourse analysis etc).
as a necessary, but not a sufficient condition.
The general opinion is that it is easier to create an MT system for a pair of related languages.
In our contribution we would like to The first attempt to verify the hypothesis that related languages are easier to translate started in mid 80s at Charles University in Prague.
The project was called RUSLAN and aimed at the translation of documentation in the domain of operating systems for mainframe computers.
It was developed in cooperation with the Research Institute of Mathematical Machines in Prague.
At that time in former COMECON countries it was obligatory to translate any kind of documentation to such systems into Russian.
The work on the Czech-to-Russian MT system RUSLAN (cf.
Oliva (1989)) started in 1985.
It was terminated in 1990 (with COMECON gone) for the lack of funding.
System description The system was rule-based, implemented in Colmerauer's Q-systems.
It contained a fullfledged morphological and syntactic analysis of Czech, a transfer and a syntactic and morphological generation of Russian.
There was almost no transfer at the beginning of the project due to the assumption that both languages are similar to the extent that does not require any transfer phase at all.
This assumption turned to be wrong and several phenomena were covered by the transfer in the later stage of the project (for example the translation of the Czech verb "b~" [to be] into one of the three possible Russian equivalents: empty form, the form "byt6" in future fitense and the verb "javljat6sja"; or the translation of verbal negation).
At the time when the work was terminated in 1990, the system had a main translation dictionary of about 8000 words, accompanied by so called transducing dictionary covering another 2000 words.
The transducing dictionary was based on the original idea described in Kirschner (1987).
It aimed at the exploitation o f the fact that technical terms are based (in a majority o f European languages) on Greek or Latin stems, adopted according to the particular derivational rules o f the given languages.
This fact allows for the "translation" o f technical terms by means of a direct transcription of productive endings and a slight (regular) adjustment o f the spelling of the stem.
For example, the English words localization and discrimination can be transcribed into Czech as "lokalizace" and "diskriminace" with a productive ending -ation being transcribed to -ace.
It was generally assumed that for the pair Czech/Russian the transducing dictionary would be able to profit from a substantially greater number o f productive rules.
This hypothesis proved to be wrong, too (see B6mov~, Kubofi (1990)).
The set o f productive endings for both pairs (English/Czech, as developed for an earlier MT system from English to Czech, and Czech/Russian) was very similar.
The evaluation o f results o f RUSLAN showed that roughly 40% o f input sentences were translated correctly, about 40% with minor errors correctable by a human post-editor and about 20% of the input required substantial editing or re-translation.
There were two main factors that caused a deterioration of the translation.
The first factor was the incompleteness o f the main dictionary of the system.
Even though the system contained a set of so-called fail-soft rules, whose task was to handle such situations, an unknown word typically caused a failure o f the module o f syntactic analysis, because the dictionary entries contained besides the translation equivalents and morphological information very important syntactic information.
The second factor was the module of syntactic analysis o f Czech.
There were several reasons of parsing failures.
Apart from the common inability of most rule-based formal grammars to cover a particular natural language to the finest detail o f its syntax there were other problems.
One o f them was the existence of non-projective constructions, which are quite common in Czech even in relatively short sentences.
Even though they account only for 1.7/'o of syntactic dependencies, every third Czech sentence contains at least one, and in a news corpus, we discovered as much as 15 non-projective dependencies; see also Haji6 et al.(1998). An example o f a non-projective construction is "Soubor se nepodafilo otev~it".
[lit.: File Refl.
was_not._possible to_open.
It was not possible to open the file].
The formalism used for the implementation (Q-systems) was not meant to handle non-projective constructions.
Another source of trouble was the use o f so-called semantic features.
These features were based on lexical semantics o f individual words.
Their main task was to support a semantically plausible analysis and to block the implausible ones.
It turned out that the question o f implausible combinations o f semantic features is also more complex than it was supposed to be.
The practical outcome o f the use o f semantic features was a higher ratio of parsing failures semantic features often blocked a plausible analysis.
For example, human lexicographers assigned the verb 'to run' a semantic feature stating that only a noun with semantic features o f a human or other living being may be assigned the role o f subject of this verb.
The input text was however full o f sentences with 'programs' or 'systems' running etc.
It was o f course very easy to correct the semantic feature in the dictionary, but the problem was that there were far too many corrections required.
On the other hand, the fact that both languages allow a high degree o f word-order freedom accounted for a certain simplification o f the translation process.
The grammar relied on the fact that there are only minor word-order differences between Czech and Russian.
1.3 Lessons
learned from RUSLAN We have learned several lessons regarding the MT o f closely related languages:  The transfer-based approach provides a similar quality o f translation both for closely related and typologically different languages  Two main bottlenecks o f full-fledged transfer-based systems are: ficomplexity o f the syntactic dictionary relative unreliability o f the syntactic analysis of the source language Even a relatively simple component (transducing dictionary) was equally complex for English-to-Czech and Czech-to-Russian translation Limited text domains do not exist in real life, it is necessary to work with a high coverage dictionary at least for the source language.
2. Translation and localization 2.1 A pivot language Localization o f products and their documentation is a great problem for any company, which wants to strengthen its position on foreign language market, especially for companies producing various kinds o f software.
The amounts o f texts being localized are huge and the localization costs are huge as well.
It is quite clear that the localization from one source language to several target languages, which are typologically similar, but different from the source language, is a waste of money and effort.
It is o f course much easier to translate texts from Czech to Polish or from Russian to Bulgarian than from English or German to any o f these languages.
There are several reasons, why localization and translation is not being performed through some pivot language, representing a certain group o f closely related languages.
Apart from political reasons the translation through a pivot language has several drawbacks.
The most important one is the problem o f the loss o f translation quality.
Each translation may to a certain extent shift the meaning o f the translated text and thus each subsequent translation provides results more and more different from the original.
The second most important reason is the lack of translators from the pivot to the target language, while this is usually no problem for the translation from the source directly to the target language.
MAHT (Machine-aided human translation) systems.
We have chosen the TRADOS Translator's Workbench as a representative system o f a class o f these products, which can be characterized as an example-based translation tools.
IBM's Translation Manager and other products also belong to this class.
Such systems uses so-called translation memory, which contains pairs o f previously translated sentences from a source to a target language.
When a human translator starts translating a new sentence, the system tries to match the source with sentences already stored in the translation memory.
If it is successful, it suggests the translation and the human translator decides whether to use it, to modify it or to reject it.
The segmentation o f a translation memory is a key feature for our system.
The translation memory may be exported into a text file and thus allows easy manipulation with its content.
Let us suppose that we have at our disposal two translation memories one human made for the source/pivot language pair and the other created by an MT system for the pivot/target language pair.
The substitution o f segments o f a pivot language by the segments of a target language is then only a routine procedure.
The human translator translating from the source language to the target language then gets a translation memory for the required pair (source/target).
The system o f penalties applied in TRADOS Translator's Workbench (or a similar system) guarantees that if there is already a human-made translation present, then it gets higher priority than the translation obtained as a result o f the automatic MT.
This system solves both problems mentioned above the human translators from the pivot to the target language are not needed at all and the machinemade translation memory serves only as a resource supporting the direct human translation from the source to the target language.
3. M a c h i n e translation o f (very) closely related Slavic languages In the group o f Slavic languages, there are more closely related languages than Czech and Russian.
Apart from the pair o f Serbian and Croatian languages, which are almost identical and were Translation memory is the key The main goal of this paper is to suggest how to overcome these obstacles by means o f a combination of an MT system with commercial ficonsidered one language just a few years ago, the most closely related languages in this group are Czech and Slovak.
This fact has led us to an experiment with automatic translation between Czech and Slovak.
It was clear that application of a similar method to that one used in the system RUSLAN would lead to similar results.
Due to the closeness of both languages we have decided to apply a simpler method.
Our new system, (~ESILKO, aims at a maximal exploitation of the similarity of both languages.
The system uses the method of direct word-for-word translation, justified by the similarity of syntactic constructions of both languages.
Although the system is currently being tested on texts from the domain of documentation to corporate information systems, it is not limited to any specific domain.
Its primary task is, however, to provide support for translation and localization of various technical texts.
3.1 System
( ~ E S i L K O and its governing noun.
An alternative way to the solution of this problem was the application of a stochastically based morphological disambiguator (morphological tagger) for Czech whose success rate is close to 92/'0.
Our system therefore consists of the following modules: 1.
Import of the input from so-called 'empty' translation memory 2.
Morphological analysis of Czech 3.
Morphological disambiguation 4.
Domain-related bilingual glossaries (incl.
singleand multiword terminology) 5.
General bilingual dictionary 6.
Morphological synthesis of Slovak 7.
Export of the output to the original translation memory Letus now look in a more detail at the individual modules of the system: ad 1.
The input text is extracted out of a translation memory previously exported into an ASCII file.
The exported translation memory (of TRADOS) has a SGML-Iike notation with a relatively simple structure (cf.
the following example): Example 1.
A sample of the exported translation memory <RTF Preamble>...</RTF Preamble> <TrU> <CrD>23051999 <CrU>VK <Seg L=CS_01>Pomoci v~kazu ad-hoc m65ete rychle a jednoduge vytv~i~et regerge.
<Seg L=SK_01 >n/a </TrU> Our system uses only the segments marked by <Seg L=CS_01>, which contain one source language sentence each, and <Seg L=SK_01>, which is empty and which will later contain the same sentence translated into the target language The greatest problem of the word-for-word translation approach (for languages with very similar syntax and word order, but different morphological system) is the problem of morphological ambiguity of individual word forms.
The type of ambiguity is slightly different in languages with a rich inflection (majority of Slavic languages) and in languages which do not have such a wide variety of forms derived from a single lemma.
For example, in Czech there are only rare cases of part-of-speech ambiguities (st~t [to stay/the state], zena [woman/chasing] or tri [three/rub(imperative)]), much more frequent is the ambiguity of gender, number and case (for example, the form of the adjective jam[ [spring] is 27-times ambiguous).
The main problem is that even though several Slavic languages have the same property as Czech, the ambiguity is not preserved.
It is distributed in a different manner and the "form-for-form" translation is not applicable.
Without the analysis of at least nominal groups it is often very difficult to solve this problem, because for example the actual morphemic categories of adjectives are in Czech distinguishable only on the basis of gender, number and case agreement between an adjective by CESiLKO.
ad 2.
The morphological analysis of Czech is based on the morphological dictionary developed by Jan Haji6 and Hana Skoumalov~i in 1988-99 (for latest description, see Haji~ (1998)).
The dictionary contains over 700 000 dictionary entries and its typical coverage varies between fi99% (novels) to 95% (technical texts).
The morphological analysis uses the system of positional tags with 15 positions (each morphological.category, such as Part-of-speech, Number, Gender, Case, etc.
has a fixed, singlesymbol place in the tag).
Example 2 tags assigned to the word-form "pomoci" (help/by means of) pomoci: NFP2 ......
A ....
]NFS7 ......
A ....
I R--2 ........... where : N noun; R preposition F feminine gender S singular, P plural 7, 2 case (7 instrumental, 2 genitive) A affirmative (non negative) ad 3.
The module of morphological disambiguation is a key to the success o f the translation.
It gets an average number of 3.58 tags per token (word form in text) as an input.
The tagging system is purely statistical, and it uses a log-linear model of probability distribution see Haji~, Hladkfi (1998).
The learning is based on a manually tagged corpus of Czech texts (mostly from the general newspaper domain).
The system learns contextual rules (features) automatically and also automatically determines feature weights.
The average accuracy o f tagging is between 91 and 93% and remains the same even for technical texts (if we disregard the unknown names and foreign-language terms that are not ambiguous anyway).
The lemmatization immediately follows tagging; it chooses the first lemma with a possible tag corresponding to the tag selected.
Despite this simple lemmatization method, and also thanks to the fact that Czech words are rarely ambiguous in their Part-of-speech, it works with an accuracy exceeding 98%.
The multiple-word terms are sequences of lemmas (not word forms).
This structure has several advantages, among others it allows to minimize the size of the dictionary and also, due to the simplicity of the structure, it allows modifications of the glossaries by the linguistically naive user.
The necessary morphological information is introduced into the domain-related glossary in an off-line preprocessing stage, which does not require user intervention.
This makes a big difference when compared to the RUSLAN Czech-to-Russian MT system, when each multiword dictionary entry cost about 30 minutes of linguistic expert's time on average.
ad 5.
The main bilingual dictionary contains data necessary for the translation o f both lemmas and tags.
The translation of tags (from the Czech into the Slovak morphological system) is necessary, because due to the morphological differences both systems use close, but slightly different tagsets.
Currently the system handles the 1:1 translation of tags (and 2:2, 3:3, etc.).
Different ratio of translation is very rare between Czech and Siovak, but nevertheless an advanced system of dictionary items is under construction (for the translation 1:2, 2:1 etc.).
It is quite interesting that the lexically homonymous words often preserve their homonymy even after the translation, so no special treatment of homonyms is deemed necessary.
ad 6.
The morphological synthesis of Slovak is based on a monolingual dictionary of SIovak, developed by J.Hric (1991-99), covering more than ]00,000 dictionary entries.
The coverage of the dictionary is not as high as o f the Czech one, but it is still growing.
It aims at a similar coverage of Slovak as we enjoy for Czech.
ad 7.
The export o f the output of the system (~ESILKO into the translation memory (of TRADOS Translator's Workbench) amounts mainly to cleaning of all irrelevant SGML markers.
The whole resulting Slovak sentence is inserted into the appropriate location in the original translation memory file.
The following example also shows that the marker <CrU> contains an information that the target language sentence was created by an M T system.
ad 4.
The domain-related bilingual glossaries contain pairs of individual words and pairs of multiple-word terms.
The glossaries are organized into a hierarchy specified by the user; typically, the glossaries for the most specific domain are applied first.
There is one general matching rule for all levels of glossaries the longest match wins.
languages, namely for Czech-to-Polish translation.
Although these languages are not so similar as Czech and Slovak, we hope that an addition of a simple partial noun phrase parsing might provide results with the quality comparable to the fullfledged syntactic analysis based system RUSLAN (this is of course true also for the Czechoto-Slovak translation).
The first results of Czech-to Polish translation are quite encouraging in this respect, even though we could not perform as rigorous testing as we did for Slovak.
Acknowledgements 3.2 Evaluation of results The problem how to evaluate results of automatic translation is very difficult.
For the evaluation of our system we have exploited the close connection between our system and the TRADOS Translator's Workbench.
The method is simple the human translator receives the translation memory created by our system and translates the text using this memory.
The translator is free to make any changes to the text proposed by the translation memory.
The target text created by a human translator is then compared with the text created by the mechanical application of translation memory to the source text.
TRADOS then evaluates the percentage of matching in the same manner as it normally evaluates the percentage of matching of source text with sentences in translation memory.
Our system achieved about 90% match (as defined by the TRADOS match module) with the results of human translation, based on a relatively large (more than 10,000 words) test sample.
This project was supported by the grant GAt~R 405/96/K214 and partially by the grant GA(~R 201/99/0236 and project of the Ministry of Education No.
VS96151. The accuracy of the translation achieved by our system justifies the hypothesis that word-forword translation might be a solution for MT of really closely related languages.
The remaining problems to be solved are problems with the oneto many or many-to-many translation, where the lack of information in glossaries and dictionaries sometimes causes an unnecessary translation error.
A u t o m a t i c construction of parallel English-Chinese corpus for cross-language information retrieval Jiang Chen and Jian-Yun Nie D ~ p a r t e m e n t d ' I n f o r m a t i q u e et R e c h e r c h e O p ~ r a t i o n n e l l e Universit~ de M o n t r e a l C.P. 6128, succursale C E N T R E V I L L E M o n t r e a l (Quebec), C a n a d a H 3 C 3 J 7 {chen, nie} @iro.
umontreal, ca Abstract A major obstacle to the construction of a probabilistic translation model is the lack of large parallel corpora.
In this paper we first describe a parallel text mining system that finds parallel texts automatically on the Web.
The generated Chinese-English parallel corpus is used to train a probabilistic translation model which translates queries for Chinese-English cross-language information retrieval (CLIR).
We will discuss some problems in translation model training and show the preliminary C U R results.
1 Introduction
2 Parallel Text Mining Algorithm The PTMiner system is an intelligent Web agent that is designed to search for large amounts of parallel text on the Web.
The mining algorithm is largely language independent.
It can thus be adapted to other language pairs with only minor modifications.
Taking advantage of Web search engines as much as possible, PTMiner implements the following steps (illustrated in Fig.
1): 1 Search for candidate sites Using existing Web search engines, search for the candidate sites that may contain parallel pages; 2 File name fetching For each candidate site, fetch the URLs of Web pages that are indexed by the search engines; 3 Host crawling Starting from the URLs collected in the previous step, search through each candidate site separately for more URLs; 4 Pair scan From the obtained URLs of each site, scan for possible parallel pairs; 5 Download and verifying Download the parallel pages, determine file size, language, and character set of each page, and filter out non-parallel pairs.
2.1 Search
for candidate Sites We take advantage of the huge number of Web sites indexed by existing search engines in determining candidate sites.
This is done by submitting some particular requests to the search engines.
The requests are determined according to the following observations.
In the sites where parallel text exists, there are normally some pages in one language containing links to the parallel version in the other language.
These are usually indicated by those links' anchor texts 1.
For example, on some English page there may be a link to its Chinese version with the anchor text "Chinese Version" or "in Chinese".
1An a n c h o r t e x t is a piece of text on a W e b page which, w h e n clicked on, will take you to a n o t h e r linked page.
To be helpful, it u s u a l l y c o n t a i n s t h e key i n f o r m a t i o n about the linked page.
Parallel texts have been used in a number of studies in computational linguistics.
Brown et al.(1993) defined a series of probabilistic translation models for MT purposes.
While people may question the effectiveness of using these models for a full-blown MT system, the models are certainly valuable for developing translation assistance tools.
For example, we can use such a translation model to help complete target text being drafted by a human translator (Langlais et al., 2000).
Another utilization is in cross-language information retrieval (CLIR) where queries have to be translated from one language to another language in which the documents are written.
In CLIR, the quality requirement for translation is relatively low.
For example, the syntactic aspect is irrelevant.
Even if the translated word is not a true translation but is strongly related to the original query, it is still helpful.
Therefore, CLIR is a suitable application for such a translation model.
However, a major obstacle to this approach is the lack of parallel corpora for model training.
Only a few such corpora exist, including the Hansard English-French corpus and the HKUST EnglishChinese corpus (Wu, 1994).
In this paper, we will describe a method which automatically searches for parallel texts on the Web.
We will discuss the text mining algorithm we adopted, some issues in translation model training using the generated parallel corpus, and finally the translation model's performance in CLIR.
21 Figure 1: The workflow of the mining process.
T h e same phenomenon can be observed on Chinese pages.
Chances are t h a t a site with parallel texts will contain such links in some of its documents.
This fact is used as the criterion in searching for candidate sites.
Therefore, to determine possible sites for EnglishChinese parallel texts, we can request an English document containing the following anchor: Host Crawling anchor : "english version H ["in english",...].
Similar requests are sent for Chinese documents.
From the two sets of pages obtained by the above queries we extract two sets of Web sites.
T h e union of these two sets constitutes then the candidate sites.
T h a t is to say, a site is a candidate site when it is found to have either an English page linking to its Chinese version or a Chinese page linking to its English version.
A host crawler is slightly different from a Web crawler.
Web crawlers go through innumerable pages and hosts on the Web.
A host crawler is a Web crawler t h a t crawls through documents on a given host only.
A breadth-first crawling algorithm is applied in P T M i n e r as host crawler.
The principle is t h a t when a link to an unexplored document on the same site is found in a document, it is added to a list t h a t will be explored later.
In this way, most file names from the candidate sites are obtained.
Pair Scan File N a m e Fetching We now assume t h a t a pair of parallel texts exists on the same site.
To search for parallel pairs on a site, P T M i n e r first has to obtain all (or at least p a r t of) the H T M L file names on the site.
From these names pairs are scanned.
It is possible to use a Web crawler to explore the candidate sites completely.
However, we can take advantage of the search engines again to accelerate the process.
As the first step, we submit the following query to the search engines: to fetch the Web pages t h a t they indexed from this site.
If we only require a small a m o u n t of parallel texts, this result m a y be sufficient.
For our purpose, however, we need to explore the sites more thoroughly using a host crawler.
Therefore, we continue our search for files with a host crawler which uses the documents found by the search engines as the starting point.
After collecting file names for each candidate site, the next task is to determine the parallel pairs.
Again, we t r y to use some heuristic rules to guess which files m a y be parallel texts before downloading them.
The rules are based on external features of the documents.
By external feature, we mean those features which m a y be known without analyzing the contents of the file, such as its URL, size, and date.
This is in contrast with the internal features, such as language, character set, and H T M L structure, which cannot be known until we have downloaded the page and analyzed its contents.
The heuristic criterion comes from the following observation: We observe t h a t parallel text pairs usually have similar n a m e patterns.
The difference between the names of two parailel pages usually lies in a segment which indicates the language.
For example, "file-ch.html" (in Chinese) vs.
"file-en.html" (in English).
T h e difference m a y also appear in the path, such as ".../chinese/.../file.html" vs.
".../english/.../file.html'. T h e n a m e patterns described above are commonly used by webmasters to help organize their sites.
Hence, we can suppose t h a t a pair of pages with this kind of p a t t e r n are probably parallel texts.
First, we establish four lists for English prefixes, English suffixes, Chinese prefixes and Chinese suffixes.
For example: E n g l i s h P r e f i x = {e, en, e_, en_, e -, e n -, ...}.
For each file in one language, if a segment in its name corresponds to one of the language affixes, several new names are generated by changing the segment to the possible corresponding affixes of the other language.
If a generated name corresponds to an existing file, then the file is considered as a candidate parallel document of the original file.
Filtering Next, we further examine the contents of the paired files to determine if they are really parallel according to various external and internal features.
This may further improve the pairing precision.
The following methods have been implemented in our system.
usually have similar H T M L structures.
However, we also noticed that parallel texts may have quite different HTML structures.
One of the reasons is that the two files may be created using two HTML editors.
For example, one may be used for English and another for Chinese, depending on the language handling capability of the editors.
Therefore, caution is required when measuring structure difference numerically.
Parallel text alignment is still an experimental area.
Measuring the confidence values of an alignment is even more complicated.
For example, the alignment algorithm we used in the training of the statistical translation model produces acceptable alignment results but it does not provide a confidence value that we can "confidently" use as an evaluation criterion.
So, for the moment this criterion is not used in candidate pair evaluation.
Generated Corpus and Translation Model Training In this section, we describe the results of our parallel text mining and translation model training.
3 Text
Length Parallel files often have similar file lengths.
One simple way to filter out incorrect pairs is to compare the lengths of the two files.
The only problem is to set a reasonable threshold that will not discard too many good pairs, i.e. balance recall and precision.
The usual difference ratio depends on the language pairs we are dealing with.
For example, ChineseEnglish parallel texts usually have a larger difference ratio than English-French parallel texts.
The filtering threshold had to be determined empirically, from the actual observations.
For Chinese-English, a difference up to 50% is tolerated.
2.5.2 L
a n g u a g e a n d Character Set It is also obvious that the two files of a pair have to be in the two languages of interest.
By automatically identifying language and character set, we can filter out the pairs that do not satisfy this basic criterion.
Some Web pages explicitly indicate the language and the character set.
More often such information is omitted by authors.
We need some language identification tool for this task.
SILC is a language and encoding identification system developed by the RALI laboratory at the University of Montreal.
It employs a probabilistic model estimated on tri-grams.
Using these models, the system is able to determine the most probable language and encoding of a text (Isabelle et al., 1997).
2.5.3 H
T M L Structure and Alignment In the STRAND system (Resnik, 1998), the candidate pairs are evaluated by aligning them according to their H T M L structures and computing confidence values.
Pairs are assumed to be wrong if they have too many mismatching markups or low confidence values.
Comparing H T M L structures seems to be a sound way to evaluate candidate pairs since parallel pairs 23 The Corpus Using the above approach for Chinese-English, 185 candidate sites were searched from the domain hk.
We limited the mining domain to hk because Hong Kong is a bilingual English-Chinese city where high quality parallel Web sites exist.
Because of the small number of candidate sites, the host crawler was used to thoroughly explore each site.
The resulting corpus contains 14820 pairs of texts including 117.2Mb Chinese texts and 136.5Mb English texts.
The entire mining process lasted about a week.
Using length comparison and language identification, we refined the precision of the corpus to about 90%.
The precision is estimated by examining 367 randomly picked pairs.
Statistical Translation Model Many approaches in computational linguistics try to extract translation knowledge from previous translation examples.
Most work of this kind establishes probabilistic models from parallel corpora.
Based on one of the statistical models proposed by Brown et al.(1993), the basic principle of our translation model is the following: given a corpus of aligned sentences, if two words often co-occur in the source and target sentences, there is a good likelihood that they are translations of each other.
In the simplest case (model 1), the model learns the probability, p(tls), of having a word t in the translation of a sentence containing a word s.
For an input sentence, the model then calculates a sequence of words that are most probable to appear in its translation.
Using a similar statistical model, Wu (1995) extracted a largescale English-Chinese lexicon from the H K U S T corFigure 2: An alignment example using pure length-based method.
pus which is built manually.
In our case, the probabilistic translation model will be used for CLIR.
The requirement on our translation model may be less demanding: it is not absolutely necessary that a word t with high p(tls ) always be a true translation of s.
It is still useful if t is strongly related to s.
For example, although "railway" is not a true translation of "train" (in French), it is highly useful to include "railway" in the translation of a query on "train".
This is one of the reasons why we think a less controlled parallel corpus can be used to train a translation model for CLIR.
very noisy.
Aligning English-Chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.
A number of alignment techniques have been proposed, varying from statistical methods (Brown et al., 1991; Gale and Church, 1991) to lexical methods (Kay and RSscheisen, 1993; Chen, 1993).
The method we adopted is t h a t of Simard et al.(1992). Because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length-based methods.
Cognates are identical sequences of characters in corresponding words in two languages.
T h e y are commonly found in English and French.
In the case of English-Chinese alignment, where there are no cognates shared by the two languages, only the H T M L markup in both texts are taken as cognates.
Because the H T M L structures of parallel pages are normally similar, the markup was found to be helpful for alignment.
To illustrate how markup can help with the alignment, we align the same pair with both the pure length-based method of Gale & Church (Fig.
2), and the method of Simard et al.(Fig. 3).
First of all, we observe from the figures that the two texts are Parallel Text Alignment Before the mined documents can be aligned into parallel sentences, the raw texts have to undergo a series of some preprocessing, which, to some extent, is language dependent.
For example, the major operations on the Chinese-English corpus include encoding scheme transformation (for Chinese), sentence level segmentation, parallel text alignment, Chinese word segmentation (Nie et al., 1999) and English expression extraction.
The parallel Web pages we collected from various sites are not all of the same quality.
Some are highly parallel and easy to align while others can be Figure 3: An alignment example considering cognates.
divided into sentences.
The sentences are marked by <s i d = " x x x x " > and < / s > . Note that we determine sentences not only by periods, but also by means of H T M L markup.
We further notice that it is difficult to align sentences 0002.
The sentence in the Chinese page is much longer than its counterpart in the English page because some additional information (font) is added.
The length-based method thus tends to take sentence 0002, 0003, and 0004 in the English page as the translation of sentence 0002 in the Chinese page (Fig.
2), which is wrong.
This in turn provocated the three following incorrect alignments.
As we can see in Fig.
3, the cognate method did not make the same mistake because of the noise in sentence 0002.
Despite their large length difference, the two 0002 sentences are still aligned as a 1-1 pair, because the sentences in the following 4 alignments (0003 0003; 0004 0004, 0005; 0005 0006; 0006 0007) have rather similar H T M L markups and are taken by the program to be the most likely alignments.
Beside HTML markups, other criteria may also be incorporated.
For example, it would be helpful to consider strong correspondence between certain English and Chinese words, as in (Wu, 1994).
We hope to implement such correspondences in our future research.
3.4 Lexicon
Evaluation To evaluate the precision of the English-Chinese translation model trained on the Web corpus, we examined two sample lexicons of 200 words, one in each direction.
The 200 words for each lexicon were randomly selected from the training source.
We examined the most probable translation for each word.
The Chinese-English lexicon was found to have a precision of 77%.
The English-Chinese lexicon has a higher precision of 81.5%.
Part of the lexicons are shown in Fig.
4, where t / f indicates whether a translation is true or false.
These precisions seem to be reasonably high.
They are quite comparable to that obtained by Wu (1994) using a manual Chinese-English parallel corpus.
Effect of Stopwords We also found that stop-lists have significant effect on the translation model.
Stop-list is a set of the most frequent words that we remove from the train3.5 English word t/f access adaptation add adopt agent agree airline amendment, appliance apply attendance auditor -,average base_on t t t t t t t t t t t/f t t t t t t t t t t Translation office protection report prepare local follow standard adult inadequate part financial visit bill vehicle saving Figure 4: Part of the evaluation lexicons.
Figure 5: Effect of stop lists in C-E translation.
ing source.
Because these words exist in most alignments, the statistical model cannot derive correct translations for them.
More importantly, their existence greatly affects the accuracy of other translations.
They can be taken as translations for many words.
A priori, it would seem that both the English and Chinese stop-lists should be applied to eliminate the noise caused by them.
Interestingly, from our observation and analysis we concluded that for better precision, only the stop-list of the target language should be applied in the model training.
We first explain why the stop-list of the target language has to be applied.
On the left side of Fig.
5, if the Chinese word C exists in the same alignments with the English word E more than any other Chinese words, C will be the most probable translation for E.
Because of their frequent appearance, some Chinese stopwords may have more chances to be in the same alignments with E.
The probability of the translation E --+ C is then reduced (maybe even less than those of the incorrect ones).
This is the reason why many English words are translated to " ~ ' (of) by the translation model trained without using the Chinese stop-list.
We also found that it is not necessary to remove the stopwords of the source language.
In fact, as illustrated on the right side of Fig.
5, the existence of the English stopwords has two effects on the probability of the translation E -~ C: 1 They may often be found together with the Chinese word C.
Owing to the Expectation Maximization algorithm, the probability of E -~ C may therefore be reduced.
2 On
the other hand, there is a greater likelihood that English stopwords will be found together with the most frequent Chinese words.
Here, we use the term "Chinese frequent words" instead of "Chinese stopwords" because even if a stop-list is applied, there may still remain some common words that have the same effect as the stopwords.
The coexistence of English and Chinese frequent words reduces the probability that the Chinese frequent words are the translations of E, and thus raise the probability of E -+ C.
The second effect was found to be more significant than the first, since the model trained without the English stopwords has better precision than the model trained with the English stopwords.
For the correct translations given by both models, the model Mono-Lingual IR Translation Model Dictionary trained without considering the English stopwords gives higher probabilities.
4 English-Chinese CLIR Results Our final goal was to test the performance of the translation models trained on the Web parallel corpora in CLIR.
We conducted CLIR experiments using the Smart IR system.
4.1 Results
The English test corpus (for C-E CLIR) was the AP corpus used in TREC6 and TREC7.
The short English queries were translated manually into Chinese and then translated back to English by the translation model.
The Chinese test corpus was the one used in the TREC5 and TREC6 Chinese track.
It contains both Chinese queries and their English translations.
Our experiments on these two corpora produced the results shown in Tab.
1. The precision of monolingual IR is given as benchmark.
In both E-C and C-E CLIR, the translation model achieved around 40% of monolingual precision.
To compare with the dictionary-based approach, we employed a ChineseEnglish dictionary, CEDICT (Denisowski, 1999), and an English-Chinese online dictionary (Anonymous, 1999a) to translate queries.
For each word of the source query, all the possible translations given by the dictionary are included in the translated query.
The Chinese-English dictionary has about the same performace as the translation model, while the English-Chinese dictionary has lower precision than that of the translation model.
We also tried to combine the translations given by the translation model and the dictionary.
In both C-E and E-C CLIR, significant improvements were achieved (as shown in Tab.
1). The improvements show that the translations given by the translation model and the dictionary complement each other well for IR purposes.
The translation model may give either exact translations or incorrect but related words.
Even though these words are not correct in the sense of translation, they are very possibly related to the subject of the query and thus helpful for IR purposes.
The dictionary-based approach expands a query along another dimension.
It gives all the possible translations for each word including those that are missed by the translation model.
4.2 C
o m p a r i s o n W i t h M T S y s t e m s One advantage of a parallel text-based translation model is that it is easier to build than an MT system.
Now that we have examined the CLIR performance of the translation model, we will compare it with two existing MT systems.
Both systems were tested in E-C CLIR.
4.2.1 S
u n s h i n e W e b T r a n Server Using the Sunshine WebTran server (Anonymous, 1999b), an online Engiish-Chinese MT system, to translate the 54 English queries, we obtained an average precision of 0.2001, which is 50.3% of the mono-lingual precision.
The precision is higher than that obtained using the translation model (0.1804) or the dictionary (0.1427) alone, but lower than the precison obtained using them together (0.2232).
4.2.2 Transperfect
Kwok (1999) investigated the CLIR performance of an English-Chinese MT software called Transperfect, using the same TREC Chinese collection as we used in this study.
Using the MT software alone, Kwok achieved 56% of monolingual precision.
The precision is improved to 62% by refining the translation with a dictionary.
Kwok also adopted pretranslation query expansion, which further improved the precison to 70% of the monolingual results.
In our case, the best E-C CLIR precison using the translation model (and dictionary) is 56.1%.
It is lower than what Kwok achieved using Transperfect, however, the difference is not large.
4.3 F
u r t h e r P r o b l e m s The Chinese-English translation model has a fax lower CLIR performance than that of the EnglishFrench model established using the same method (Nie et al., 1999).
The principal reason for this is the fact that English and Chinese are much more different than English and French.
This problem surfaced in many phases of this work, from text alignment to query translation.
Below, we list some further factors affecting CLIR precision.
 The Web-collected corpus is noisy and it is difficult to align English-Chinese texts.
The alignment method we employed has performed more poorly than on English-French alignment.
This in turn leads to poorer performance of the translation model.
In general, we observe a higher fivariability in Chinese-English translations than in English-French translations.
 For E-C CLIR, although queries in both languages were provided, the English queries were not strictly translated from the original Chinese ones.
For example, A J g, ~ (human right situation) was translated into human right issue.
We cannot expect the translation model to translate issue back to ~ (situation).
 The training source and the CLIR collections were from different domains.
The Web corpus are retrieved from the parallel sites in Hong Kong while the Chinese collection is from People's Daily and Xinhua News Agency, which are published in mainland China.
As the result, some important terms such as ~ $ $ (mostfavored-nation) and --I!!
~ ~ (one-nation-twosystems) in the collection are not known by the model.
5 Summary
The goal of this work was to investigate the feasibility of using a statistical translation model trained on a Web-collected corpus to do English-Chinese CLIR.
In this paper, we have described the algorithm and implementation we used for parallel text mining, translation model training, and some results we obtained in CLIR experiments.
Although further work remains to be done, we can conclude that it is possible to automatically construct a Chinese-English parallel corpus from the Web.
The current system can be easily adapted to other language pairs.
Despite the noisy nature of the corpus and the great difference in the languages, the evaluation lexicons generated by the translation model produced acceptable precision.
While the current CLIR results are not as encouraging as those of English-French CLIR, they could be improved in various ways, such as improving the alignment method by adapting cognate definitions to HTML markup, incorporating a lexicon and/or removing some common function words in translated queries.
We hope to be able to demonstrate in the near future that a fine-tuned English-Chinese translation model can provide query translations for CLIR with the same quality produced by MT systems.
D i s t i l l i n g dialogues A m e t h o d using natural dialogue c o r p o r a for dialogue s y s t e m s d e v e l o p m e n t Arne Department JSnsson and Nils Dahlb~ick of Computer and Information Science LinkSping University S-581 83, L I N K O P I N G SWEDEN nilda@ida.liu.se, arnjo@ida.liu.se Abstract We report on a method for utilising corpora collected in natural settings.
It is based on distilling (re-writing) natural dialogues to elicit the type of dialogue that would occur if one the dialogue participants was a computer instead of a human.
The method is a complement to other means such as Wizard of Oz-studies and un-distilled natural dialogues.
We present the distilling method and guidelines for distillation.
We also illustrate how the method affects a corpus of dialogues and discuss the pros and cons of three approaches in different phases of dialogue systems development.
1 Introduction
on measures for inter-rater reliability (Carletta, 1996), on frameworks for evaluating spoken dialogue agents (Walker et al., 1998) and on the use of different corpora in the development of a particular system (The Carnegie-Mellon Communicator, Eskenazi et al.(1999)). The question we are addressing in this paper is how to collect and analyse relevant corpora.
We begin by describing what we consider to be the main advantages and disadvantages of the two currently used methods; studies of human dialogues and Wizard of Oz-dialogues, especially focusing on the ecological validity of the methods.
We then describe a m e t h o d called 'distilling dialogues', which can serve as a supplement to the other two.
It has been known for quite some time now, that the language used when interacting with a computer is different from the one used in dialogues between people, (c.f.
JSnsson and Dahlb~ick (1988)).
Given that we know that the language will be different, but not how it will be different, we need to base our development of natural language dialogue systems on a relevant set of dialogue corpora.
It is our belief that we need to clarify a number of different issues regarding the collection and use of corpora in the development of speech-only and multimodal dialogue systems.
Exchanging experiences and developing guidelines in this area are as important as, and in some sense a necessary pre-requisite to, the development of computational models of speech, language, and dialogue/discourse.
It is interesting to note the difference in the state of art in the field of natural language dialogue systems with that of corpus linguistics, where issues of the usefulness of different samples, the necessary sampling size, representativeness in corpus design and other have been discussed for quite some time (e.g.
(Garside et al., 1997; Atkins et al., 1992; Crowdy, 1993; Biber, 1993)).
Also the neighboring area of evaluation of NLP systems (for an overview, see Sparck Jones and Galliers (1996)) seems to have advanced further.
Some work have been done in the area of natural language dialogue systems, e.g. on the design of Wizard of Oz-studies (Dahlb~ck et al., 1998), 2 Natural and Wizard of Oz-Dialogues The advantage of using real dialogues between people is that they will illustrate which tasks and needs that people actually bring to a particular service provider.
Thus, on the level of the users' general goals, such dialogues have a high validity.
But there are two drawbacks here.
First; it is not self-evident that users will have the same task expectations from a computer system as they have with a person.
Second, the language used will differ from the language used when interacting with a computer.
These two disadvantages have been the major force behind the development of Wizard of Ozmethods.
The advantage here is that the setting will be human-computer interaction.
But there are important disadvantages, too.
First, on the practical side, the task of setting up a high quality simulation environment and training the operators ('wizards') to use this is a resource consuming task (Dahlb~ck et al., 1998).
Second, and probably even more important, is that we cannot then observe real users using a system for real life tasks, where they bring their own needs, motivations, resources, and constraints to bear.
To some extent this problem can be overcome using well-designed so called 'scenarios'.
As pointed out in Dahlb~ck (1991), on many levels of analysis the artificiality of the situation will not affifect the language used.
An example of this is the pattern of pronoun-antecedent relations.
But since the tasks given to the users are often pre-described by the researchers, this means t h a t this is not a good way of finding out which tasks the users actually want to perform.
Nor does it provide a clear enough picture on how the users will act to find something t h a t satisfies their requirements.
If e.g. the task is one of finding a charter holiday trip or buying a TVset within a specified set of constraints (economical and other), it is conceivable t h a t people will stay with the first item t h a t matches the specification, whereas in real life they would probably look for alternatives.
In our experience, this is primarily a concern if the focus is on the users' goals and plans, but is less a problem when the interest is on lowerlevel aspects, such as, syntax or patterns of pronounantecedent relationship (c.f.
Dahlb~ick (1991)).
To summarize; real life dialogues will provide a reasonably correct picture of the way users' approach their tasks, and what tasks they bring to the service provider, but the language used will not give a good approximation of what the system under construction will need to handle.
Wizard of Ozdialogues, on the other hand, will give a reasonable approximation of some aspects of the language used, but in an artificial context.
The usual approach has been to work in three steps.
First analyse real h u m a n dialogues, and based on these, in the second phase, design one or more Wizard of Oz-studies.
The final step is to fine-tune the system's performance on real users.
A good example of this method is presented in Eskenazi et al.(1999). But there are also possible problems with this approach (though we are not claiming that this was the case in their particular project).
Eskenazi et al.(1999) asked a h u m a n operator to act 'computerlike' in their Wizard of Oz-phase.
The advantage is of course that the h u m a n operator will be able to perform all the tasks t h a t is usually provided by this service.
The disadvantage is t h a t it puts a heavy burden on the h u m a n operator to act as a computer.
Since we know that lay-persons' ideas of what computers can and cannot do are in m a n y respects far removed from what is actually the case, we risk introducing some systematic distortion here.
And since it is difficult to perform consistently in similar situations, we also risk introducing non-systematic distortion here, even in those cases when the 'wizard' is an NLP-professional.
Our suggestion is therefore to supplement the above mentioned methods, and bridge the gap between them, by post-processing h u m a n dialogues to give them a computer-like quality.
The advantage, compared to having people do the simulation on the fly, is both that it can be done with more consistency, and also that it can be done by researchers t h a t actually know what h u m a n c o m p u t e r natural language dialogues can look like.
A possible disadvantage with using both Wizard of Oz-and real computer dialogues, is that users will quickly a d a p t to what the system can provide t h e m with, and will therefore not try to use it for tasks they know it cannot perform.
Consequently, we will not get a full picture of the different services they would like the system to provide.
A disadvantage with this method is, of course, t h a t post-processing takes some time compared to using the natural dialogues as they are.
There is also a concern on the ecological validity of the results, as discussed later.
Distilling dialogues Distilling dialogues, i.e. re-writing h u m a n interactions in order to have them reflect what a humancomputer interaction could look like involves a number of considerations.
The main issue is t h a t in corp o r a of natural dialogues one of the interlocutors is not a dialogue system.
The system's task is instead performed by a h u m a n and the problem is how to anticipate the behaviour of a system that does not exist based on the performance of an agent with different performance characteristics.
One important aspect is how to deal with h u m a n features that are not part of what the system is supposed to be a b l e to handle, for instance if the user talks about things outside of the domain, such as discussing an episode of a recent T V show.
It also involves issues on how to handle situations where one of the interlocuters discusses with someone else on a different topic, e.g. discussing the up-coming Friday party with a friend in the middle of an information providing dialogue with a customer.
It is i m p o r t a n t for the distilling process to have at least an outline of the dialogue system t h a t is under development: Will it for instance have the capacity to recognise users' goals, even if not explicitly stated?
Will it be able to reason about the discourse domain?
W h a t services will it provide, and what will be outside its capacity to handle?
In our case, we assume that the planned dialogue system has the ability to reason on various aspects of dialogue and properties of the application.
In our current work, and in the examples used for illustration in this paper, we assume a dialogue model that can handle any relevant dialogue phenomenon and also an interpreter and speech recogniser being able to understand any user input that is relevant to the task.
There is is also a powerful domain reasoning module allowing for more or less any knowledge reasoning on issues that can be accomplished within the domain (Flycht-Eriksson, 1999).
Our current system does, however, not have an explicit user task model, as opposed to a system task model (Dahlb~ick fiand JSnsson, 1999), which is included, and thus, we can not assume that the 'system' remembers utterances where the user explains its task.
Furthermore, as our aim is system development we will not consider interaction outside the systems capabilities as relevant to include in the distilled dialogues.
The context of our work is the development a multi-modal dialogue system.
However, in our current work with distilling dialogues, the abilities of a multi-modal system were not fully accounted for.
The reason for this is that the dialogues would be significantly affected, e.g. a telephone conversation where the user always likes to have the n e x t connection, please will result in a table if multi-modal output is possible and hence a fair amount of the dialogne is removed.
We have therefore in this paper analysed the corpus assuming a speech-only system, since this is closer to the original telephone conversations, and hence needs fewer assumptions on system performance when distilling the dialogues.
distilling. The system might in such cases provide less information.
The principle of providing all relevant information is based on the assumption that a computer system often has access to all relevant information when querying the background system and can also present it more conveniently, especially in a multimodal system (Ahrenberg et al., 1996).
A typical example is the dialogue fragment in figure 1.
In this fragment the system provides information on what train to take and how to change to a bus.
The result of distilling this fragment provides the revised fragment of figure 2.
As seen in the fragment of figure 2 we also remove a number of utterances typical for human interaction, as discussed below.
* S y s t e m utterances are m a d e m o r e computer-like and do n o t include irrelevant i n f o r m a t i o n. The Distillation guidelines Distilling dialogues requires guidelines for how to handle various types of utterances.
In this section we will present our guidelines for distilling a corpus of telephone conversations between a human information provider on local buses 1 to be used for developing a multimodal dialogue system (Qvarfordt and JSnsson, 1998; Flycht-Eriksson and JSnsson, 1998; Dahlb~ick et al., 1999; Qvarfordt, 1998).
Similar guidelines are used within another project on developing Swedish Dialogue Systems where the domain is travel bureau information.
We can distinguish three types of contributors: 'System' (i.e.
a future systems) utterances, User utterances, and other types, such as moves by other speakers, and noise.
latter is seen in $9 in the dialogue in figure 3 where the provided information is not relevant.
It could also be possible to remove $5 and respond with $7 at once.
This, however, depends on if the information grounded in $5-U6 is needed for the 'system' in order to know the arrival time or if that could be concluded from U4.
This in turn depends on the system's capabilities.
If we assume that the dialogue system has a model of user tasks, the information in $5-U6 could have been concluded from that.
We will, in this case, retain $5-U6 as we do not assume a user task model (Dahlb/ick and JSnsson, 1999) and in order to stay as close to the original dialogue as possible.
The next problem concerns the case when 'system' utterances are changed or removed.
 Dialogue contributions provided by s o m e t h i n g or s o m e o n e other than the u s e r or the ' s y s t e m ' are removed.
These are regarded as not being part Modifying system utterances The problem of modifying 'system' utterances can be divided into two parts: how to change and when to change.
They are in some respects intertwined, but as the how-part affects the when-part more we will take this as a starting point.
 The ' s y s t e m ' provides as m u c h relevant inform a t i o n as possible at once.
This depends on the capabilities of the systems output modalities.
If we have a screen or similar output device we present as much as possible which normally is all relevant information.
If we, on the other hand, only have spoken output the amount of information that the hearer can interpret in one utterance must be considered when 1The bus time table dialogues are collected at LinkSping University and are available (in Swedish) on http://www.ida.liu.se/~arnjo/kfb/dialoger.html of the interaction.
This means that if someone interrupts the current interaction, say that the telephone rings during a face-to-face interaction, the interrupting interaction is normally removed from the corpus.
Furthermore, 'system' interruptions are removed.
A human can very well interrupt another human interlocuter, but a computer system will not do that.
However, this guideline could lead to problems, for instance, when users follow up such interruptions.
If no information is provided or the interrupted sequence does not affect the dialogue, we have no problems removing the interruption.
The problem is what to do when information from the 'system' is used in the continuing dialogue.
For such cases we have no fixed strategy, yes I wonder if you have any m m buses or ().
like express buses leaving from LinkSping to Vadstena ().
on sunday ja ville undra om ni hade ndgra 5h bussar eUer ().
typ expressbussar sore dkte frdn LinkSping till Vadstena ().
pd sSnda $5: no the bus does not run on sundays nej bussen g~r inte pd sSndagar U6: how can you ().
can you take the train and then change some way ().
because ().
to MjSlby 'n' so hur kan man ().
kan man ta tdg d sen byta p~ ndtt sStt ().
fSr de ().
till mjSlby ~ sd $7: that you can do too yes de kan du gSra ocksd ja U8: how ().
do you have any such suggestions hut ().
har du n~ra n~gra s~na fSrslag $9: yes let's see (4s) a m o m e n t (15s) now let us see here ().
was it on the sunday you should travel ja ska se h~ir (4s) eft 5gonblick (15s) nu ska vise hSr ().
va de p~ sSndagen du skulle dka pd U10: yes right afternoon preferably ja just de eftermidda ggirna $11: afternoon preferable ().
you have train from LinkSping fourteen twenty nine eftermidda gSrna ().
du hat t~g frdn LinkSping fjorton d tjugonie U12: m m mm S13: and then you will change from MjSlby station six hundred sixty sd byter du frdn MjSlby station sexhundrasexti sexhundrasexti $15: fifteen and ten Figure 1: Dialogue fragment from a real interaction on bus time-table information U4: S5: U6: $7: I wonder if you have any buses or ().
like express buses going from LinkSping to Vadstena ().
on sunday no the bus does not run on sundays how can you ().
can you take the train and then change some way ().
because ().
to MjSlby and so you can take the train from LinkSping fourteen and twenty nine and then you will change at MjSlby station to bus six hundred sixty at fifteen and ten Figure 2: A distilled version of the dialogue in figure 1 the dialogue needs to be rearranged depending on how the information is to be used (c.f.
the discussion in the final section of this paper).
in figure 4).
A common case of this is when the ' s y s t e m ' is talking while looking for information, $5 in the dialogue fragment of figure 4 is an example of this.
Related to this is when the system provides its own comments.
If we can assume that it has such capabilities they are included, otherwise we remove them.
 'System' utterances which are no longer valid are removed.
Typical examples of this are the utterances $7, $9, $11 and $13 in the dialogue fragment of figure 1.
* Remove sequences of utterances where the 'system' behaves in a way a computer would not do.
For instance jokes, irony, humor, commenting on the other dialogue participant, or dropping the telephone (or whatever is going on in $7 The system does not repeat information that has already been provided unless explicitly asked to do so.
In human interaction it is not uncommon to repeat what has been uttered for purposes other than to provide grounding information or feedback.
This is for instance common during 'n' I must be at Resecentrum before fourteen and thirty five ().
'cause we will going to the interstate buses $5: U6: $7: aha ().
'n' then you must be there around twenty past two something then yes around t h a t ja ungefgir let's see here ( l l s ) two hundred and fourteen R y d end station leaves forty six ().
thirteen 'n' forty six then you will be down fourteen oh seven (.) jaha 'n' ().
the next one takes you there ().
fourteen thirty seven ().
but t h a t is too late Figure 3: Dialogue fragment from a real interaction on bus time-table information U2: $3: U4: $5: U6: $7: Well, hi ().
I a m going to Ugglegatan eighth ja hej ().
ja ska till Ugglegatan dtta Yes ja and ().
I wonder ().
it is somewhere in Tannefors och ().
jag undrar ().
det ligger ndnstans i Tannefors Yes ().
I will see here one one I will look exactly where it is one m o m e n t please ja ().
jag ska se hhr eft eft jag ska titta exakt vat det ligger eft 6gonblick barn Oh Yeah (operator disconnects) (25s) m m ().
okey (hs) what the hell (2s) (operator connects again) hello yes ((Telefonisten kopplar ur sig)) (25s) iihh ().
okey (hs) de va sore ]aan (2s) ((Telefonisten kopplar in sig igen)) halld ja ja hej It is bus two hundred ten which runs on old tannefors road t h a t you have to take and get off at the bus stop at t h a t bus stop named vetegatan Figure 4: Dialogue fragment from a natural bus timetable interaction search procedures as discussed above.
want to develop systems where the user needs to restrict his/her behaviour to the capabilities of the dialogue system.
However, there are certain changes m a d e to user utterances, in most cases as a consequence of changes of system utterances.
 The system does not ask for information it has already achieved.
For instance asking again if it is on Sunday as in $9 in figure 1.
This is not uncommon in h u m a n interaction and such utterances from the user are not removed.
However, we can assume t h a t the dialogue system does not forget what has been talked about before.
4.2 M
o d i f y i n g u s e r u t t e r a n c e s The general rule is to change user utterances as little as possible.
The reason for this is that we do not Utterances that are no longer valid are removed.
The most common cases are utterances whose request has already been answered, as seen in the distilled dialogue in figure 2 of the dialogue in figure 1.
sixteen fifty five sexton ]emti/em U12: sixteen fifty five ().
aha sexton f e m t i / e m ().
jaha S13: bus line four hundred thirty five linje ]yrahundra tretti/em Figure 5: Dialogue fragment from a natural bus timetable interaction  Utterances are removed where the user discusses things that are in the environment.
For instance commenting the 'systems' clothes or hair.
This also includes other types of communicative signals such as laughter based on things outside the interaction, for instance, in the environment of the interlocuters.
 User utterances can also be added in order to make the dialogue continue.
In the dialogue in figure 5 there is nothing in the dialogue explaining why the system utters S13.
In such cases we need to add a user utterance, e.g.
Which bus is that?.
However, it might turn out that there are cues, such as intonation, found when listening to the tapes.
If such detailed analyses are carried out, we will, of course, not need to add utterances.
Furthermore, it is sometimes the case t h a t the telephone operator deliberately splits the information into chunks t h a t can be comprehended by the user, which then must be considered in the distillation.
5 Applying
the method To illustrate the m e t h o d we will in this section t r y to characterise the results from our distillations.
The illustration is based on 39 distilled dialogues from the previously mentioned corpus collected with a telephone operator having information on local bus time-tables and persons calling the information service.
The distillation took a b o u t three hours for all 39 dialogues, i.e. it is reasonably fast.
The distilled dialogues are on the average 27% shorter.
However, this varies between the dialogues, at most 73% was removed but there were also seven dialogues t h a t were not changed at all.
At the most 34 utterances where removed from one single dialogue and that was from a dialogue with discussions on where to find a parking lot, i.e. discussions outside the capabilities of the application.
There was one more dialogue where more t h a n 30 utterances were removed and that dialogue is a typical example of dialogues where distillation actually is very useful and also indicates what is normally removed from the dialogues.
This particular dialogue begins with the user asking for the telephone number to 'the Lost property office' for a specific bus operator.
However, the operator starts a discussion on what bus the traveller traveled on before providing the requested telephone number.
The reason for this discussion is probably t h a t the operator knows that different bus companies are utilised and would like to make sure that the user really understands his/her request.
The interaction t h a t follows can, thus, in t h a t respect be relevant, but for our purpose of developing systems based on an overall goal of providing information, not to understand human interaction, our dialogue system will not able to handle such phenomenon (JSnsson, 1996).
The dialogues can roughly be divided into five different categories based on the users task.
The discussion in twenty five dialogues were on bus times between various places, often one departure and one arrival but five dialogues involved more places.
In five dialogues the discussion was one price and various types of discounts.
Five users wanted to know the telephone number to 'the Lost property office', two discussed only bus stops and two discussed how they could utilise their season ticket to travel outside the trafficking area of the bus company.
It is interesting to note that there is no correspondence between the task being performed during the interaction and the amount of changes made to the d i a logue.
Thus, if we can assume that the amount of distillation indicates something about a user's interaction style, other factors t h a n the task are important when characterising user behaviour.
Looking at what is altered we find t h a t the most i m p o r t a n t distilling principle is that the 'system' provides all relevant information at once, c.f. figures 1 and 2.
This in turn removes utterances provided by both 'system' and user.
Most added utterances, both from the user and the 'system', provide explicit requests for information that is later provided in the dialogue, e.g. utterance $3 in figure 6.
We have added ten utterances in all 39 dialogues, five 'system' utterances and five user utterances.
Note, however, that we utilised the transcribed dialogues, without information on intonation.
We would probably not have needed to add this m a n y utterances if we had utilised the tapes.
Our reason for not using information on intonation is that we do not assume t h a t our system's speech recogniser can recognise intonation.
Finally, as discussed above, we did not utilise the full potential of multi-modality when distilling the dialogues.
For instance, some dialogues could be further distilled if we had assumed t h a t the system had presented a time-table.
One reason for this is t h a t we wanted to capture as m a n y interesting aspects intact as possible.
The advantage is, thus, that we have a better corpus for understanding humanYees hi Anna Nilsson is my name and I would like to take the bus from Ryd center to Resecentrum in LinkSping jaa hej Anna Nilsson heter jag och jag rill ~ka buss ~r~n Ryds centrum till resecentrum i LinkSping.
$3: U4: mm When do you want to leave? mm N~ir r i l l d u  k a ? 'n' I must be at Resecentrum before fourteen and thirty five ().
'cause we will going to the interstate buses ja ska va p~ rececentrum innan fjorton d trettifem ().
f5 vi ska till l~ngfiirdsbussarna Figure 6: Distilled dialogue fragment with added utterance computer interaction and can from t h a t corpus do a second distillation where we focus more on multimodal interaction.
One example of this is whether the system is meant to acquire information on the user's underlying motivations or goals or not.
In the examples presented, we have not assumed such capabilities, but this assumption is not an absolute necessity.
We believe, however, that the distilling process should be based on one such model, not the least to ensure a consistent t r e a t m e n t of similar recurring phenomena at different places in the corpora.
The validity of the results based on analysing distilled dialogues depends p a r t l y on how the distillation has been carried out.
Even when using natural dialogues we can have situations where the interaction is somewhat mysterious, for instance, if some of the dialogue participants behaves irrational such as not providing feedback or being too elliptical.
However, if careful considerations have been made to stay as close to the original dialogues as possible, we believe that distilled dialogues will reflect what a hum a n would consider to be a natural interaction.
Acknowledgments This work results from a n u m b e r of projects on development of natural language interfaces supported by The Swedish Transport & Communications Research Board (KFB) and the joint Research P r o g r a m for Language Technology ( H S F R / N U T E K ) . We are indebted to the participants of the Swedish Dialogue Systems project, especially to Staffan Larsson, Lena S a n t a m a r t a, and Annika Flycht-Eriksson for interesting discussions on this topic.
Discussion We have been presenting a method for distilling hum a n dialogues to make t h e m resemble h u m a n computer interaction, in order to utilise such dialogues as a knowledge source when developing dialogue systems.
Our own main purpose has been to use t h e m for developing multimodal systems, however, as discussed above, we have in this p a p e r rather assumed a speech-only system.
But we believe that the basic approach can be used also for multi-modal systems and other kinds of natural language dialogue systems.
It is i m p o r t a n t to be aware of the limitations of the method, and how 'realistic' the produced result will be, compared to a dialogue with the final system.
Since we are changing the dialogue moves, by for instance providing all required information in one move, or never asking to be reminded of what the user has previously requested, it is obvious t h a t what follows after the changed sequence would probably be affected one way or another.
A consequence of this is that the resulting dialogue is less accurate as a model of the entire dialogue.
It is therefore not an ideal candidate for trying out the systems over-all performance during system development.
But for the smaller sub-segments or sub-dialogues, we believe that it creates a good approximation of what will take place once the system is up and running.
Furthermore, we believe distilled dialogues in some respects to be more realistic than Wizard of Ozdialogues collected with a wizard acting as a computer.
Another issue, t h a t has been discussed previously in the description of the method, is t h a t the distilling is made based on a particular view of what a dialogue with a computer will look like.
While not necessarily being a detailed and specific model, it is at least an instance of a class of computer dialogue models.
Plan-Based Dialogue Management in a Physics Tutor Reva Freedman Learning Research and Development Center University of Pittsburgh Pittsburgh, PA 15260 freedrk+@pitt, edu http://www.pitt, edu/~freedrk Abstract This paper describes an application of APE (the Atlas Planning Engine), an integrated planning and execution system at the heart of the Atlas dialogue management system.
APE controls a mixedinitiative dialogue between a human user and a host system, where turns in the 'conversation' may include graphical actions and/or written text.
APE has full unification and can handle arbitrarily nested discourse constructs, making it more powerful than dialogue managers based on finitestate machines.
We illustrate this work by describing Atlas-Andes, an intelligent tutoring system built using APE with the Andes physics tutor as the host.
1 Introduction
The purpose of the Atlas project is to enlarge the scope of student interaction in an intelligent tutoring system (ITS) to include coherent conversational sequences, including both written text and GUI actions.
A key component o f Atlas is APE, the Atlas Planning Engine, a "just-intime" planner specialized for easy construction and quick generation of hierarchically organized dialogues.
APE is a domainand task-independent system.
Although to date we have used APE as a dialogue manager for intelligent tutoring systems, APE could also be used to manage other types of human-computer conversation, such as an advicegiving system or an interactive help system.
Planning is an essential component of a dialogue-based ITS.
Although there are many reasons for using natural language in an ITS, as soon as the student gives an unexpected response to a tutor question, the tutor needs to be able to plan in order to achieve its goals as well as respond appropriately to the student's statement.
Yet classical planning is inappropriate for dialogue generation precisely because it assumes an unchanging world.
A more appropriate approach is the "practical reason" approach pioneered by Bratman (1987, 1990).
According to Bratman, human beings maintain plans and prefer to follow them, but they are also capable of changing the plans on the fly when needed.
Bratman's approach has been introduced into computer science under the name of reactive planning (Georgeff and Ingrand 1989, Wilkins et al.1995). In this paper we discuss the rationale for the use of reactive planning as well as the use of the hierarchical task network (HTN) style o f plan operators.
Then we describe APE (the Atlas Planning Engine), a dialogue planner we have implemented to embody the above concepts.
We demonstrate the use of APE by showing how we have used it to add a dialogue capability to an existing ITS, the Andes physics tutor.
By showing dialogues that Atlas-Andes can generate, we demonstrate the advantages of this architecture over the finite-state machine approach to dialogue management.
Integrated planning and execution for dialogue generation 2.1 'Practical reason' and the BDI model For an ITS, planning is required in order to ensure a coherent conversation as well as to accomplish tutorial goals.
But it is impossible to plan a whole conversation in advance when the student can respond freely at every turn, just as human beings cannot plan their daily lives in advance because of possible changes in conditions.
Classical planning algorithms are inappropriate because the tutor must be able to change plans based on the This research was supported by NSF grant number 9720359 to CIRCLE, the Center for Interdisciplinary Research on Constructive Learning Environments at the University of Pittsburgh and Carnegie-Mellon University.
fistudent's responses.
For this reason we have adopted the ideas of the philosopher Michael Bratman (1987, 1990).
Bratman uses the term "practical reason" to describe his analysis since he is concerned with how to reason about practical matters.
For human beings, planning is required in order to accomplish one's goals.
Bratman's key insight is that human beings tend to follow a plan once they have one, although they are capable of dropping an intention or changing a partial plan when necessary.
In other words, human beings do not decide what to do from scratch at each turn.
Bratman and others who have adopted his approach use a tripartite mental model that includes beliefs, desires and intentions (Bratman, Israel and Pollack 1988, Pollack 1992, Georgeff et al.1998), hence the name "BDI model".
Beliefs, which are uninstantiated plans in the speaker's head, are reified by the plan library.
Desires are expressed as the agent's goals.
Intentions, or plan steps that the agent has committed to but not yet acted on, are stored in an agenda.
Thus the agent's partial plan for achieving a goal is a network of intentions.
A plan can be left in a partially expanded state until it is necessary to refine it further.
2.2 Implementation
via reactive planning can be achieved via a series of subgoals instead of relying on means-end reasoning.
Hierarchical decomposition is more appropriate to dialogue generation for a number of reasons.
First, decomposition is better suited to the type of largescale dialogue planning required in a real-world tutoring system, as it is easier to establish what a human speaker will say in a given situation than to be able to understand why in sufficient detail and generality to do means-end planning.
Second, Hierarchical decomposition minimizes search time.
Third, our dialogues are task-oriented and have a hierarchical structure (Grosz and Sidner 1986).
In such a case, matching the structure of the domain simplifies operator development because they can often be derived from transcripts of human tutoring sessions.
The hierarchy information is also useful in determining appropriate referring expressions.
Fourth, interleaved planning and execution is important for dialogue generation because we cannot predict the human user's future utterances.
In an HTN-based system, it is straightforward to implement interleaved planning and execution because one only needs to expand the portion of the plan that is about to be executed.
Finally, the conversation is in a certain sense the trace of the plan.
In other words, we care much more about the actions generated by the planner than the states involved, whether implicitly or explicitly specified.
Hierarchical decomposition provides this trace naturally.
3 Background: the Andes physics tutor Andes (Gertner, Conati and VanLehn 1998) is an intelligent tutoring system in the domain of firstyear college physics.
Andes teaches via coached problem solving (VanLehn 1996).
In coached problem solving, the tutoring system tracks the student as the latter attempts to solve a problem.
If the student gets stuck or deviates too far from a correct solution path, the tutoring system provides hints and other assistance.
A sample Andes problem is shown in midsolution in Figure 1.
A physics problem is given in the upper-left corner with a picture below it.
Next to the picture the student has begun to sketch the vectors involved using the GUI buttons along the left-hand edge of the screen.
As the fistudent draws vectors, Andes and the student cooperatively fill in the variable definitions in the upper-right corner.
Later the student will use the space below to write equations connecting the variables.
In this example, the elevator is decelerating, so the acceleration vector should face the opposite direction from the velocity vector.
(If the acceleration vector went the same direction as the velocity vector, the speed of the elevator would increase and it would crash into the ground).
This is an important issue in beginning physics; it occurs in five Andes problems.
When such errors occur, Andes turns the incorrect item red and provides hints to students in the lower-left corner of the screen.
A sample of these hints, shown in the order a student would encounter them, is shown in Fig.
2. But hints are an output-only form of natural language; the student can't take the initiative or ask a question.
In addition, there is no way for the system to ask the student a question or lead the student through a multi-step directed line of reasoning.
Thus there is no way to use some of the effective rhetorical methods used by skilled human tutors, such as analogy and reductio ad absurdum.
Current psychological research suggests that active methods, where students have to answer questions, will improve the performance of tutoring systems.
Structure of the Atlas Planning Engine Figure3 shows a sample plan operator.
For legibility, the key elements have been rendered in English instead of in Lisp.
The hiercx slot provides a way for the planner to be aware of the context in which a decomposition is proposed.
Items in the hiercx slot are instantiated and added to the transient database only so long as the operator which spawned them is in the agenda.
To initiate a planning session, the user invokes the planner with an initial goal.
The system searches the operator library to find all operators whose goal field matches the next goal on the agenda and whose filter conditions and preconAn elevator slows to a stop from an initial downward velocity of 10.0 m]s in 2.00 seconds.
A passenger in the elevator is holding a 3.00 kilogram package by a vertical string.
What is the tension in the string during the process? e',ev~o, at 10 m/s elev~or at a stop mass of p~:w'.,I,,~ magnitude of the inst~~taneous Velocity of pack,age ~ {rkneTO magnitude of the avelage Acceleratiorl of package,dudngTO... pkg Figure I: Screen shot of the Andes physics tutor fiS: T: S: T: S: T: S: T: S: (draws acceleration vector in same direction as velocity) Wrong.
What's wrongwith that?
Think about the direction of the acceleration vector.
Please explain further.
Remember that the direction of acceleration is the direction of the change in velocity.
Please explain further.
The'direction o f the acceleration vector is straight up.
(draws acceleration vector correctly) Figure 2: Andes hint sequence formatted as dialogue ditions are satisfied.
Goals are represented in first-order logic without quantifiers and matched via unification.
Since APE is intended especially for generation of hierarchically organized taskoriented discourse, each operator has a multi-step recipe in the style of Wilkins (1988).
When a match is found, the matching goal is removed from the agenda and is replaced by the steps in the recipe.
APE has two kinds of primitive actions; one ends a turn and the other doesn't.
From the point of view of discourse generation, the most important APE recipe items are those allowing the planner to change the agenda when necessary.
These three types of recipe items make APE more powerful than a classical planner.
 Fact: Evaluate a condition.
If false, skip the rest of the recipe.
Fact is used to allow run-time decision making by bypassing the rest o f an operator when circumstances change during its execution.
Fact can be used with retry-at to implement a loop just as in Prolog.
 Retry-at.
The purpose of retry-at is to allow the planner to back up to a choice point and make a new decision.
It removes goals sequentially from the top of the agenda, a full operator at a time, until the supplied argument is false.
Then it restores the parent goal of the last operator removed, so that further planning can choose a new way to achieve it.
Retry-at implements a Prolog-like choice of alternatives, but it differs from backtracking in that the new operator is chosen based on conditions that apply when the retry operation is executed, rather than on a list of possible operators formed when the original operator was chosen.
For retry-at to be useful, the author must provide multiple operators for the same goal.
Each operator must have a set of preconditions enabling it to be chosen at the appropriate time.
 Prune-replace: The intent of prune-replace is (def-operator handle-same-direction :goal ()... :filter () :precond ()...
We h a v e a s k e d a q u e s t i o n a b o u t a c c e l e r a t i o n ;...
a n d t h e s t u d e n t h a s g i v e n an a n s w e r ; ...
f r o m w h i c h we c a n d e d u c e t h a t s / h e t h i n k s a c c e l, a n d v e l o c i t y go in ; the same direction ; a n d we h a v e n o t g i v e n t h e e x p l a n a t i o n below yet : r e c i p e ()...
Tell the student: "But if the a c c e l e r a t i o n went the same direction as t h e v e l o c i t y, t h e n t h e e l e v a t o r w o u l d be s p e e d i n g u p . " ; M a r k t h a t we a r e g i v i n g t h i s e x p l a n a t i o n ; T e l l t h e s t u d e n t t h a t t u t o r is r e q u e s t i n g another answer ("Try again").
Edit the agenda ( u s i n g prune-replace) so t h a t r e s p o n d i n g to a n o t h e r a n s w e r is at t h e t o p of t h e a g e n d a :hiercx ()) Figure 3: Sample plan operator 55 fito allow the planner to remove goals from the agenda based on a change in circumstances.
It removes goals sequentially from the top of the agenda, one at a time, until the supplied argument becomes false.
Then it replaces the removed goals with an optional list of new goals.
Prune-replace allows a type of decision-making frequently used in dialogue generation.
When a conversation partner does not give the expected response, one would often like to remove the next goal from the agenda and replace it with one or more replacement goals.
Prune-replace implements a generalized version of this concept.
APE is domain-independent and communicates with a host system via an API.
As a partner in a dialogue, it needs to obtain information from the world as well as produce output turns.
Preconditions on plan operators can be used to access information from external knowledge sources.
APE contains a recipe item type that can be used to execute an external program such as a call to a GUI interface.
APE also has recipe items allowing the user to assert and retract facts in a knowledge base.
Further details about the APE planner can be found in (Freedman, 2000).
I m p l e m e n t a t i o n of Atlas-Andes 5.1 Architecture of Atlas-Andes The first system we have implemented with APE is a prototype Atlas-Andes system that replaces the hints usually given for an incorrect acceleration vector by a choice of generated subdialogues.
Figure 4 shows the architecture of Atlas-Andes; any other system built with APE would look similar.
Robust natural language understanding in Atlas-Andes is provided by Ros6's CARMEL system (Ros6 2000); it uses the spelling correction algorithm devised by Elmi and Evens (1998).
5.2 Structure
of human tutorial dialogues In an earlier analysis (Kim, Freedman and Evens 1998) we showed that a significant portion of human-human tutorial dialogues can be modeled with the hierarchical structure o f task-oriented dialogues (Grosz and Sidner 1986).
Furthermore, a main building block o f the discourse hierarchy, corresponding to the transaction level in Conversation Analysis (Sinclair and Coulthard 1975), matches the tutoring episode defined by VanLehn et al.(1998). A tutoring episode consists of the turns necessary to help the student make one correct entry on the interface.
NLU (CARMEL) Plan Library User Interface Host (Andes) GUI Interpreter (Andes) Transient Knowledge Base Figure 4: Interface between Atlas and host system fiTo obtain empirical data for the Atlas-Andes plan operators, we analyzed portions of a corpus of human tutors helping students solve similar physics problems.
Two experienced tutors were used.
Tutor A was a graduate student in computer science who had majored in physics; tutor B was a professional physics tutor.
The complete corpus contained solutions to five physics problems by 41 students each.
We analyzed every tutoring episode dealing with the acceleration vector during deceleration, totaling 29 examples divided among 20 students and both tutors.
The tutors had very different styles.
Tutor A tended to provide encouragement rather than content, making those transcripts less useful for deriving an information-based approach.
Tutor B used an information-based approach, but after one wrong answer tended to complete the solution as a monologue.
Largely following tutor B's approach to sequence and content, we isolated six ways of teaching the student about direction of acceleration.
Tutoring schemata Switching between schemata API and GUI handling Answer handling Domain-dep.
lex. insertion Domain-indep.
lex. insertion TOTAL 5.3 Sample output and evaluation Figure 5 shows an example of text that can be generated by the Atlas-Andes system, showing an analogy-based approach to teaching this content.
The operator library used to generate this text could generate a combinatorially large number of versions of this dialogue as well as selected examples of other ways o f teaching about direction of acceleration.
This operator library used to generate this text contained 1 l 1 plan operators, divided as follows: We are currently working on components that will allow us to increase the number of physics concepts covered without a corresponding increase in the number of operators.
The schema switching operators prevent the tutor from repeating itself during a physics problem.
They could be reduced or eliminated by a general discourse history component that tutoring schema operators could refer to.
Domain-dependent lexical insertion refers to the choice of lexical items such as car and east in the sample dialogue, while domain-independent iexical insertion refers to items such as O K and exactly.
Both categories could be eliminated, or at least severely reduced, through the use of a text realization package.
Together that would provide a one-third reduction in the number o f operators needed.
As the set of API and GUI handling operators is fixed, that would reduce by half the number of application operators needed.
The largest remaining category of operators is the answer handlers.
These operators handle a variety of answers for each o f the five questions that the system can ask.
The answers we recognize include categories such as "don't know" as well as specific answers (e.g.
a direction perpendicular to the correct answer) which we recognize because the tutor has specific replies for them.
In order to reduce the number o f S: T: S: T: S: T: S: (draws acceleration vector in same direction as velocity) What is the definition of acceleration?
Don't know.
OK, let's try this.
If a car was driving along east, which way would you have to push on it to make it stop?
West. Exactly.
The opposite direction.
So the net force goes the opposite direction, and so does the acceleration.
Try to draw the acceleration vector again now.
(draws acceleration vector correctly) Figure 5: Example of generated dialogue 57 fioperators further, we must investigate more general methods of handling student errors.
In particular, we plan to investigate error-classifying predicates that apply to more than one question as well as the use of intention-based predicates.
Since the system only covers one rule of physics, albeit in a variety of ways, we plan to make some of these efficiency improvements before adding new rules o f physics and testing it with users.
Preconditions for the operators in the plan library utilize discourse or interaction history, the current goal hierarchy, recent information such as the tutor's current goal and the student's latest response, shared information such as a model o f objects on the screen, and domain knowledge.
As an example of the latter, if the student draws an acceleration vector which is incorrect but not opposite to the velocity vector, a different response will be generated.
Related work Wenger (1987), still the chief textbook on ITSs, states that using a global planner to control an ITS is too inefficient to try.
This is no longer true, if indeed it ever was.
Vassileva (1995) proposes a system based on AND-OR graphs with a separate set of rules for reacting to unexpected events.
Lehuen, Nicolle and Luzzati (1996) present a method of dialogue analysis that produces schemata very similar to ours.
Earlier dialoguebased ITSs that use augmented finite-state machines or equivalent include CIRCSIM-Tutor (Woo et al.1991, Z h o u e t al.
1999) and the system described by Woolf (1984).
Cook (1998) uses levels of finite-state machines.
None of these systems provides for predicates with variables or unification.
Conclusions 5.4 Discussion Many previous dialogue-based ITSs have been implemented with finite-state machines, either simple or augmented.
In the most common finite state mode[, each time the human user issues an utterance, the processor reduces it to one of a small number of categories.
These categories represent the possible transitions between states.
Thus history can be stored, and context considered, only by expanding the number o f states.
This approach puts an arbitrary restriction on the amount of context or depth of conversational nesting that can be considered.
More importantly, it misses the significant generalization that these types of dialogues are hierarchical: larger units contain repeated instances of the same smaller units in different sequences and instantiated with different values.
Furthermore, the finite-state machine approach does not allow the author to drop one line of attack and replace it by another without hardcoding every possible transition.
It is also clear that the dialogue-based approach has many benefits over the hint-sequence approach.
In addition to providing a multi-step teaching methods with new content, it can respond flexibly to a variety of student answers at each step and take context into account when generating a reply.
In this paper we described APE, an integrated planner and execution system that we have implemented as part o f the Atlas dialogue manager.
APE uses HTN-style operators and is based on reactive planning concepts.
Although APE is intended largely for use in domains with hierarchical, multi-turn plans, it can be used to implement any conversation-based system, where turns in the 'conversation' may include graphical actions and/or text.
We illustrated the use of APE with an example from the Atlas-Andes physics tutor.
We showed that previous models based on finite-state machines are insufficient to handle the nested subdialogues and abandoned partial subdialogues that occur in practical applications.
We showed how APE generated a sample dialogue that earlier systems could not handle.
Acknowledgments We thank Abigail Gertner for her generous assistance with the Andes system, and Michael Ringenberg for indispensible programming support.
Carolyn Ros6 built the CARMEL natural language understanding component.
Mohammed EImi and Michael Glass of Illinois Institute o f Technology provided the spelling correction code.
We thank Pamela Jordan and the referees for their comments.
Bratman's approach has been elaborated in a computer science context by subsequent researchers (Bratman, Israel and Pollack 1988, Pollack 1992, Georgeff et al.1998). Reactive planning (Georgeff and Ingrand 1989, Wilkins et al.1995), originally known as "integrated planning and execution," is one way of implementing Bratman's model.
Originally developed for real-time control of the space shuttle, reactive planning has since been used in a variety of other domains.
For the Atlas project we have developed a reactive planner called APE (Atlas Planning Engine) which uses these ideas to conduct a conversation.
After each student response, the planner can choose to continue with its previous intention or change something in the plan to respond better to the student's utterance.
Like most reactive planners, APE is a hierarchical task network (HTN) style planner (Yang 1990, Erol, Hendler and Nau 1994).
A Framework for MT and Multilingual NLG Systems Based on Uniform Lexico-Structural Processing Benoit Lavoie CoGenTex, Inc.
840 Hanshaw Road Ithaca, NY USA, 14850 benoit@cogentex.com Richard Kittredge CoGenTex, Inc.
840 Hanshaw Road Ithaca, NY USA, 14850 richard @cogentex.com Tanya Korelsky CoGenTex, Inc.
840 Hanshaw Road Ithaca, NY USA, 14850 tanya @cogentex.com Owen Rambow * ATT Labs-Research, B233 180 Park Ave, PO Box 971 Florham Park, NJ USA, 07932 rambow @research.att.com Abstract In this paper we describe an implemented framework for developing monolingual or multilingual natural language generation (NLG) applications and machine translation (MT) applications.
The framework demonstrates a uniform approach to generation and transfer based on declarative lexico-structural transformations of dependency structures of syntactic or conceptual levels ("uniform lexico-structural processing").
We describe how this framework has been used in practical NLG and MT applications, and report the lessons learned.
1 Introduction
The framework consists of a portable Java environment for building NLG or MT applications by defining modules using a core tree transduction engine and single declarative ASCII specification language for conceptual or syntactic dependency tree structures 1 and their transformations.
Developers can define new modules, add or remove modules, or modify their connections.
Because the processing of the transformation engine is restricted to transduction of trees, it is computationally efficient.
Having declarative rules facilitates their reuse when migrating from one programming environment to another; if the rules are based on functions specific to a programming language, the implementation of these functions might no longer be available in a different environment.
In addition, having all lexical information and all rules represented declaratively makes it relatively easy to integrate into the framework techniques for generating some of the rules automatically, for example using corpus-based methods.
The declarative form of transformations makes it easier to process them, compare them, and cluster them to achieve proper classification and ordering.
In this paper we present a linguistically motivated framework for uniform lexicostructural processing.
It has been used for transformations of conceptual and syntactic structures during generation in monolingual and multilingual natural language generation (NLG) and for transfer in machine translation (MT).
Our work extends directions taken in systems such as Ariane (Vauquois and Boitet, 1985), FoG (Kittredge and Polgu6re, 1991), JOYCE (Rainbow and Korelsky, 1992), and LFS (Iordanskaja et al., 1992).
Although it adopts the general principles found in the abovementioned systems, the approach presented in this paper is more practical, and we believe, would eventually integrate better with emerging statistics-based approaches to MT.
* The work performed on the framework by this coauthor was done while at CoGenTex, Inc.
1 In
this paper, we use the term syntactic dependency (tree) structure as defined in the Meaning-Text Theory (MTT; Mel'cuk, 1988).
However, we extrapolate from this theory when we use the term conceptual dependency (tree) structure, which has no equivalent in MTT (and is unrelated to Shank's CD structures proposed in the 1970s).
Thus, the framework represents a generalized processing environment that can be reused in different types of natural language processing (NLP) applications.
So far the framework has been used successfully to build a wide variety of NLG and MT applications in several limited domains (meteorology, battlefield messages, object modeling) and for different languages (English, French, Arabic, and Korean).
In the next sections, we present the design of the core tree transduction module (Section 2), describe the representations that it uses (Section 3) and the linguistic resources (Section 4).
We then discuss the processing performed by the tree transduction module (Section 5) and its instantiation for different applications (Section 6).
Finally, we discuss lessons learned from developing and using the framework (Section 7) and describe the history of the framework comparing it to other systems (Section 8).
2 The
Framework's Tree Transduction Module Input Lexico-Structm'al Processing Dependency SUucturc Lexico-Structural Postprocessing Figure 1: Design of the Tree Transduction Module 3 The Framework's Representations The core processing engine of the framework is a generic tree transduction module for lexicostructural processing, shown in Figure 1.
The module has dependency stuctures as input and output, expressed in the same tree formalism, although not necessarily at the same level (see Section 3).
This design facilitates the pipelining of modules for stratificational transformation.
In fact, in an application, there are usually several instantiations of this module.
The transduction module consists of three processing steps: lexico-structural preprocessing, main lexico-structural processing, and lexico-structural post-processing.
Each of these steps is driven by a separate grammar, and all three steps draw on a common feature data base and lexicon.
The grammars, the lexicon and the feature data base are referred to as the linguistic resources (even if they sometimes apply to a conceptual representation).
All linguistic resources are represented in a declarative manner.
An instantiation of the tree transduction module consists of a specification of the linguistic resources.
The representations used by all instantiations of the tree transduction module in the framework are dependency tree structures.
The main characteristics of all the dependency tree structures are:  A dependency tree is unordered (in contrast with phrase structure trees, there is no ordering between the branches of the tree).
 All the nodes in the tree correspond to lexemes (i.e., lexical heads) or concepts depending on the level of representation.
In contrast with a phrase structure representation, there are no phrase-structure nodes labeled with nonterminal symbols.
Labelled arcs indicate the dependency relationships between the lexemes.
The first of these characteristics makes a dependency tree structure a very useful representation for MT and multilingual NLG, since it gives linguists a representation that allows them to abstract over numerous crosslinguistic divergences due to language specific ordering (Polgu~re, 1991).
We have implemented 4 different types of dependency tree structures that can be used for NLG, MT or both:  Deep-syntactic structures (DSyntSs);  Surface syntactic structures (SSyntSs); Conceptual structures (ConcSs); Parsed syntactic structures (PSyntSs).
The DSyntSs and SSyntSs correspond closely to the equivalent structures of the Meaning-Text Theory (MTT; Mel'cuk, 1988): both structures are unordered syntactic representations, but a DSyntS only includes full meaning-bearing lexemes while a SSyntS also contains function words such as determiners, auxiliaries, and strongly governed prepositions.
In the implemented applications, the DSyntSs are the pivotal representations involved in most transformations, as this is also often the case in practice in linguistic-based MT (Hutchins and Somers, 1997).
Figure 2 illustrates a DSyntS from a meteorological application, MeteoCogent (Kittredge and Lavoie, 1998), represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework (Lavoie and Rambow, 1997).
As Figure 2 illustrates, there is a straightforward mapping between the graphical notation and the ASCII notation supported in the framework.
This also applies for all the transformation rules in the framework which illustrates the declarative nature of our approach, Figure 3 illustrates the mapping between an interlingua defined as a ConcS and a corresponding English DSyntS.
This example, also taken from MeteoCogent, illustrates that the conceptual interlingua in NLG can be closer to a database representation of domain data than to its linguistic representations.
As mentioned in (Polgu~re, 1991), the high level of abstraction of the ConcSs makes them a suitable interlingua for multilingual NLG since they bridge the semantic discrepancies between languages, and they can be produced easily from the domain data.
However, most off-the-shelf parsers available for MT produce only syntactic structures, thus the DSyntS level is often more suitable for transfer.
Cones TO ItlGH LOW Low 5 to Mlgh 20 Figure 3: ConcS Interlingua and English DSyntS Finally, the PSyntSs correspond to the parser outputs represented using RealPro's dependency structure formalism.
The PSyntSs may not be valid directly for realization or transfer since they may contain unsupported features or dependency relations.
However, the PSyntSs are represented in a way to allow the framework to convert them into valid DSyntS via lexicostructural processing.
This conversion is done via conversion grammars customized for each parser.
There is a practical need to convert one syntactic formalism to another and so far we have implemented converters for three off-theshelf parsers (Palmer et al., 1998).
4 The
Framework's Linguistic Resources 't Low S to high 20 Figure 2: DSyntS(Graphicaland ASCIINotation) The ConcSs correspond to the standard framelike structures used in knowledge representation, with labeled arcs corresponding to slots.
We have used them only for a very limited meteorological domain (in MeteoCogent), and we imagine that they will typically be defined in a domain-specific manner.
As mentioned previously, the framework is composed of instantiations of the tree fitransduction module shown in Figure 1.
Each module has the following resources:  Feature Data-Base: This consists of the feature system defining available features and their possible values in the module.
 Lexicon: This consists of the available lexemes or concepts, depending on whether the module works at syntactic or conceptual level.
Each lexeme and concept is defined with its features, and may contain specific lexico-structural rules: transfer rules for MT, mapping rules to the next level of representation for surface realization of DSyntS or lexicalization of ConcS.
 Main Grammar: This consists of the lexicostructural mapping rules that apply at this level and which are not lexemeor conceptspecific (e.g.
DSynt-rules for the DSyntmodule, Transfer-rules for the Transfer module, etc).
 Preprocessing grammar: This consists of the lexico-structural mapping rules for transforming the input structures in order to make them compliant with the main grammar, if this is necessary.
Such rules are used to integrate new modules together when discrepancies in the formalism need to be fixed.
This grammar can also be used for adding default features (e.g.
setting the default number of nouns to singular) or for applying default transformations (e.g.
replacing non meaning-bearing lexemes with features).
Postprocessing grammar: This consists of lexico-structural mapping rules for transforming the output structures before they can be processed by the next module.
As for the preprocessing rules, these rules can be used to fix some discrepancies between modules.
Our representation of the lexicon at the lexical level (as opposed to conceptual) is similar to the one found in RealPro.
Figure 4 shows a specification for the lexeme SELL.
This lexeme is defined as a verb of regular morphology with two lexical-structural mappings, the first one introducing the preposition TO for its 3r actant, and the preposition FOR for its 4 th actant: (a seller) X1 sells (merchandise) X2 to (a buyer) X3 f o r (a price) X4.
What is important is that 63 each mapping specifies a transformation between structures at different levels of representation but that are represented in one and the same representation formalism (DSyntS and SSyntS in this case).
As we will see below, grammar rules are also expressed in a similar way.
( c o m p l e t i v e 3 FOR ( prepositional Figure 4: Specification of Lexeme SELL At the conceptual level, the conceptual lexicon associates lexical-structural mapping with concepts in a similar way.
Figure 5 illustrates the mapping at the deep-syntactic level associated with the concept #TEMPERATURE.
Except for the slight differences in the labelling, this type of specification is similar to the one used on the lexical level.
The first mapping rule corresponds to one of the lexico-structural transformations used to convert the interlingual ConcS of Figure 3 to the corresponding DSyntS.
SY SX Note that since each lexicon entry can have more than one lexical-structural mapping rule, the list of these rules represents a small grammar specific to this lexeme or concept.
Realization grammar rules of the main grammar include generic mapping rules (which are not lexeme-specific) such as the DSyntS-rule illustrated in Figure 6, for inserting a determiner.
DSYNT-RULE: More general lexico-structural rules for transfer can also be implemented using our grammar rule formalism.
Figure 8 gives an English-French transfer rule applied to a weather domain for the transfer of a verb modified by the adverb ALMOST: It almost rained.
--o II a failli pleuvoir.
Figure 6: Deep-Syntactic Rule for Determiner Insertion The lexicon formalism has also been extended to implement lexeme-specific lexico-structural transfer rules.
Figure 7 shows the lexicostructural transfer of the English verb lexeme MOVE to French implemented for a military and weather domain (Nasr et al., 1998): Cloud will move into the western regions.
Des nuages envahiront les rdgions ouest.
They moved the assets forward.
-.9 lls ont amen~ les ressources vers l 'avant.
The 79 dcg moves forward.
---~La 79 dcg a v a n c e vers l'avant.
A disturbance will move north of Lake Superior.
--~ Une perturbation se diplacera au nord du lac supdrieur.
Figure 8: English to French Lexico-Structural Transfer Rule with Verb Modifier ALMOST More details on how the structural divergences described in (Dorr, 1994) can be accounted for using our formalism can be found in (Nasr et al., 1998).
5 The
Rule Processing Before being processed, the rules are first compiled and indexed for optimisation.
Each module applies the following processing.
The rules are assumed to be ordered from most specific to least specific.
The application of the rules to the structures is top-down in a recursive way from the f'n-st rule to the last.
For the main grammar, before applying a grammar rule to a given node, dictionary lookup is carried out in order to first apply the lexemeor conceptspecific rules associated with this node.
These are also assumed to be ordered from the most specific to the least specific.
If a lexico-structural transformation involves switching a governor node with one of its dependents in the tree, the process is reapplied with the new node governor.
When no more rules can be applied, the same process is applied to each dependent of the current governor.
When all nodes have been processed, the processing is completed, 6 Using the Framework to build Applications Figure 7: Lexico-Structural Transfer of English Lexerne MOVE to French Figure 9 shows how different instantiations of the tree transduction module can be combined to fibuild NLP applications.
The diagram does not represent a particular system, but rather shows the kind of transformations that have been implemented using the framework, and how they interact.
Each arrow represents one type of processing implemented by an instantiation of the tree transduction module.
Each triangle represents a different level of representation.
Sentence interlingua can also support the generation of French but this functionality has not yet been implemented).
MT:  Transfer on the DSyntS level and realization via SSyntS level for English--French, English--Arabic, English---Korean and Korean--English.
Translation in the meteorology and battlefield domains (Nasr et al., 1998).
 Conversion of the output structures from off-the-shelf English, French and Korean parsers to DSyntS level before their processing by the other components in the framework (Palmer et al., 1998).
7 Lessons
Learned Using the Framework PI "ng Scopeof the Framework SSyntSLI Parsing Input Sentence LI yntS ealization Generated Sentence 1.2 Generated Sentence LI Sentence SSyntS Figure 9: Scope of the Framework's Transformations For example, in Figure 9, starting with the "Input Sentence LI" and passing through Parsing, Conversion, Transfer, DSyntS Realization and SSyntS Realization to "Generated Sentence L2" we obtain an Ll-to-L2 MT system.
Starting with "Sentence Planning" and passing through DSyntS Realization, and SSyntS Realization (including linearization and inflection) to "Generated Sentence LI", we obtain a monolingual NLG system for L1.
So far the framework has been used successfully for building a wide variety of applications in different domains and for different languages: NLG:  Realization of English DSyntSs via SSyntS level for the domains of meteorology (MeteoCogent; Kittredge and Lavoie, 1998) and object modeling (ModelExplainer; Lavoie et al., 1997).
 Generation of English text from conceptual interlingua for the meteorology domain (MeteoCogent).
(The design of the Empirical results obtained from the applications listed in Section 6 have shown that the approach used in the framework is flexible enough and easily portable to new domains, new languages, and new applications.
Moreover, the time spent for development was relatively short compared to that formerly required in developing similar types of applications.
Finally, as intended, the limited computational power of the transduction module, as well as careful implementation, including the compilation of declarative linguistic knowledge to Java, have ensured efficient run-time behavior.
For example, in the MT domain we did not originally plan for a separate conversion step from the parser output to DSyntS.
However, it quickly became apparent that there was a considerable gap between the output of the parsers we were using and the DSyntS representation that was required, and furthermore, that we could use the tree transduction module to quickly bridge this gap.
Nevertheless, our tree transduction-based approach has some important limitations.
In particular, the framework requires the developer of the transformation rules to maintain them and specify the order in which the rules must be applied.
For a small or a stable grammar, this does not pose a problem.
However, for large or rapidly changing grammar (such as a transfer grammar in MT that may need to be adjusted when switching from one parser to another), the fiburden of the developer's task may be quite heavy.
In practice, a considerable amount of time can be spent in testing a grammar after its revision.
Another major problem is related to the maintenance of both the grammar and the lexicon.
On several occasions during the development of these resources, the developer in charge of adding lexical and grammatical data must make some decisions that are domain specific.
For example, in MT, writing transfer rules for terms that can have several meanings or uses, they may simplify the problem by choosing a solution based on the context found in the current corpus, which is a perfectly natural strategy.
However, later, when porting the transfer resources to other domains, the chosen strategy may need to be revised because the context has changed, and other meanings or uses are found in the new corpora.
Because the current approach is based on handcrafted rules, maintenance problems of this sort cannot be avoided when porting the resources to new domains.
An approach such as the one described in (Nasr et al., 1998; and Palmer and al., 1998) seems to be solving a part of the problem when it uses corpus analysis techniques for automatically creating a first draft of the lexical transfer dictionary using statistical methods.
However, the remaining work is still based on handcrafting because the developer must refine the rules manually.
The current framework offers no support for merging handcrafted rules with new lexical rules obtained statistically while preserving the valid handcrafted changes and deleting the invalid ones.
In general, a better integration of linguistically based and statistical methods during all the development phases is greatly needed.
8 History
of the Framework and Comparison with Other Systems realization of deep-syntactic structures in NLG (Lavoie and Rambow, 1997).
It was later extended for generation of deep-syntactic structures from conceptual interlingua (Kittredge and Lavoie, 1998).
Finally, it was applied to MT for transfer between deep-syntactic structures of different languages (Palmer et al., 1998).
The current framework encompasses the full spectrum of such transformations, i.e. from the processing of conceptual structures to the processing of deep-syntactic structures, either for NLG or MT.
Compared to its predecessors (Fog, LFS, JOYCE), our approach has obvious advantages in uniformity, declarativity and portability.
The framework has been used in a wider variety of domains, for more languages, and for more applications (NLG as well as MT).
The framework uses the same engine for all the transformations at all levels because all the syntactic and conceptual structures are represented as dependency tree structures.
In contrast, the predecessor systems were not designed to be rapidly portable.
These systems used programming languages or scripts for the implementation of the transformation rules, and used different types of processing at different levels of representation.
For instance, in LFS conceptual structures were represented as graphs, whereas syntactic structures were represented as trees which required different types of processing at these two levels.
Our approach also has some disadvantages compared with the systems mentioned above.
Our lexico-structural transformations are far less powerful than those expressible using an arbitrary programming language.
In practice, the formalism that we are using for expressing the transformations is inadequate for long-range phenomena (inter-sentential or intra-sentential), including syntactic phenomena such as longdistance wh-movement and discourse phenomena such as anaphora and ellipsis.
The formalism could be extended to handle intrasentential syntactic effects, but inter-sentential discourse phenomena probably require procedural rules in order to access lexemes in The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory: FoG (Kittredge and Polgu~re, 1991), LFS (Iordanskaja et al., 1992), and JOYCE (Rambow and Korelsky, 1992).
The framework was originally developed for the fiother sentences.
In fact, LFS and JOYCE include a specific module for elliptical structure processing.
Similarly, the limited power of the tree transformation rule formalism distinguishes the framework from other NLP frameworks based on more general processing paradigms such as unification of FUF/SURGE in the generation domain (Elhadad and Robin, 1992).
9 Status
The framework is currently being improved in order to use XML-based specifications for representing the dependency structures and the transformation rules in order to offer a more standard development environment and to facilitate the framework extension and maintenance.
Acknowledgements A first implementation of the framework (C++ processor and ASCII formalism for expressing the lexico-structural transformation rules) applied to NLG was developed under SBIR F30602-92-C-0015 awarded by USAF Rome Laboratory.
The extensions to MT were developed under SBIR DAAL01-97-C-0016 awarded by the Army Research Laboratory.
The Java implementation and general improvements of the framework were developed under SBIR DAAD17-99-C-0008 awarded by the Army Research Laboratory.
We are thankful to Ted Caldwell, Daryl McCullough, Alexis Nasr and Mike White for their comments and criticism on the work reported in this paper.
REES: A Large-Scale Relation and Event Extraction System Abstract This paper reports on a large-scale, end-toend relation and event extraction system.
At present, the system extracts a total of 100 types of relations and events, which represents a much wider coverage than is typical of extraction systems.
The system consists of three specialized pattem-based tagging modules, a high-precision coreference resolution module, and a configurable template generation module.
We report quantitative evaluation results, analyze the results in detail, and discuss future directions.
Introduction One major goal of information extraction (IE) technology is to help users quickly identify a variety of relations and events and their key players in a large volume of documents.
In contrast with this goal, state-of-the-art information extraction systems, as shown in the various Message Understanding Conferences (MUCs), extract a small number of relations and events.
For instance, the most recent MUC, MUC-7, called for the extraction of 3 relations (person-employer, maker-product, and organization-location) and 1 event (spacecraft launches).
Our goal is to develop an IE system which scales up to extract as many types of relations and events as possible with a minimum amount of porting effort combined with high accuracy.
Currently, REES handles 100 types of relations and events, and it does so in a modular, configurable, and scalable manner.
Below, Section 1 presents the ontologies of relations and events that we have developed.
Section 2 describes REES' system architecture.
Section 3 evaluates the system's performance, and offers a qualitative analysis of system errors.
Section 4 discusses future directions.
1 Relation
and Event Ontologies As the first step in building a large-scale relation and event extraction system, we developed ontologies of the relations and events to be extracted.
These ontologies represent a wide variety of domains: political, financial, business, military, and life-related events and relations.
"Relations" covers what in MUC-7 are called Template Elements (TEs) and Template Relations (TRs).
There are 39 types of relations.
While MUC TE's only dealt with singular entities, REES extracts both singular and plural entities (e.g., "five executives").
The TR relations are shown in italic in the table below.
Relations 'Artifact Relations Artifact-Name&Aliases Artifact-Type Artifact-Subtype Artifact-Descriptor Place Relations Place-Name&Aliases Place-Type Place-Subtype Place-Descriptor Place-Country Artifact-Maker Artifact-Owner Person Relations Person-Name&Aliases Person-Type Person-Subtype Person-Descriptor Person-Honorific Person-Age Person-PhoneNumber Person-Nationality Organization Relations Org-Name&Aliases Org-Descriptor Org-FoundationDate Org-Nationality Org-TickerSymbol Org-Location Org-ParentOrg Org-Owner Org-Founder Org-StockMarket Person-Affiliation Person-Sibling Person-Spouse Person-Parent Person-Grandparent Person-OtherRelative Person-BirthPlace Person-BirthDate Table 1: Relation Ontology "Events" are extracted along with their event participants, e.g., "who did what to whom when and where"?
For example, for a BUYING event, REES extracts the buyer, the artifact, the seller, and the time and location of the BUYING event.
REES currently covers 61 types of events, as shown below.
Figures 1 and 2 show sample relation and event templates.
Figure 1 shows a Person-Affiliation relation template for "Frank Ashley, a spokesman for Occidental Petroleum Corp'".
<PERSON TYPE: PERSON: ORG: AFFILIATION-AP8802230207-54> := PERSON AFFILIATION [TE for"Frank Ashley"] [TE for "Occidental Petroleum"] Figure 1: Example of Relation Template Figure 2 shows an Attack Target event template for the sentence "an Iraqi warplane attacked the frigate Stark with missiles May 17, 1987.
" <ATTACK TARGET-AP8804160078-12>: = TYPE: CONFLICT SUBTYPE: ATTACK TARGET ATTACKER: [TE for "an Iraqi warplane"] TARGET: [TE for "the frigate Stark"] WEAPON: [TE for "missiles"] TIME: "May 17, 1987" PLACE: [TE for "the gulf'] COMMENT: "attacked" Events Vehicle Vehicle departs Vehicle arrives Spacecraft launch Vehicle crash Personnel Change Hire Terminate contract Promote Succeed Start office Transaction Buy artifact Sell artifact Import artifact Export artifact Give money Business Start business Close business Make artifact Acquire company Sell company Sue organization Merge company Financial Currency moves up Currency moves down Stock moves up Stock moves down Stock market moves up Stock market moves down Stock index moves up Stock index moves down Conflict Kill Injure Hijack vehicle Hold hostages Attack target Fire weapon Weapon hit Invade land Move forces Retreat Surrender Evacuate Figure 2: Example of Event Template Crime Sexual assault Steal money Seize drug Indict Arrest Try Convict Sentence Jail Political Nominate Appoint Elect Expel person Reach agreement Hold meeting Impose embargo Topple Family Die Marry System Architecture and Components Figure 3 illustrates the REES system architecture.
REES consists of three main components: a tagging component (cf.
Section 2.1), a co-reference resolution module (cf.
Section 2.2), and a template generation module (cf.
Section 2.3).
Figure 3 also illustrates that the user may run REES from a Graphical User Interface (GUI) called TemplateTool (cf.
Section 2.4).
Tagging Modules The tagging component consists of three modules as shown in Figure 3: NameTagger, NPTagger and EventTagger.
Each module relies on the same pattern-based extraction engine, but uses different sets o f patterns.
The NameTagger recognizes names o f people, organizations, places, and artifacts (currently only vehicles).
Table 2: Event Ontology GUI interaction Figure 3: The REES System Architecture syntactically-based generic patterns.
These The NPTagger then takes the XML-tagged output of the NameTagger through two phases.
First, it recognizes non-recursive Base Noun Phrase (BNP) (our specifications for BNP resemble those in Ramshaw and Marcus 1995).
Second, it recognizes complex NPs for only the four main semantic types of NPs, i.e., Person, Organization, Location, and Artifact (vehicle, drug and weapon).
It makes postmodifier attachment decisions only for those NPs that are crucial to the extraction at hand.
During this second phase, relations which can be recognized locally (e.g., Age, Affiliation, Maker) are also recognized and stored using the XML attributes for the NPs.
For instance, the XML tag for "President of XYZ Corp".
below holds an AFFILIATION attribute with the ID for "XYZ Corp".
<PNP ID="03" AFFILIATION="O4">Presidentof <ENTITY ID="04">XYZ Corp.</ENTITY> </PNP> patterns tag events in the presence of at least one of the arguments specified in the lexical entry for a predicate.
Subsequent pattems try to find additional arguments as well as place and time adjunct information for the tagged event.
As an example of the EventTagger's generic patterns, consider the simplified pattern below.
This pattem matches on an event-denoting verb that requires a direct object of type weapon (e.g., "fire a gun") (& {AND $VP {ARG2_SYN=DO} {ARG2_SEM=WEAPON}} {AND $ARTIFACT {SUBTYPE=WEAPON}})1 The important aspect of REES is its declarative, lexicon-driven approach.
This approach requires a lexicon entry for each event-denoting word, which is generally a I &=concatenation, AND=Boolean operator, $VP and SARTIFACT are macro references for complex phrases.
71:1 Building upon the XML output of the NPTagger, the EventTagger recognizes events applying its lexicon-driven, fiverb.
The lexicon entry specifies the syntactic and semantic restrictions on the verb's arguments.
For instance, the following lexicon entry is for the verb "attack".
It indicates that the verb "attack" belongs to the CONFLICT ontology and to the ATTACK_TARGET type.
The first argument for the verb "attack" is semantically an organization, location, person, or artifact (ARGI_SEM), and syntactically a subject (ARGI_SYN).
The second argument is semantically an organization, location, person or artifact, and syntactically a direct object.
The third argument is semantically a weapon and syntactically a prepositional phrase introduced by the preposition "with".
ATTACK {{{CATEGORY VERB} {ONTOLOGY CONFLICT} {TYPE ATTACK_TARGET} {ARGI_SEM {ORGANIZATION LOCATION PERSON ARTIFACT} } {ARGI_SYN {SUBJECT}} {ARG2_SEM {ORGANIZATION LOCATION PERSON ARTIFACT} } {ARG2_SYN {DO}} {ARG3_SEMWEAPON} } {ARG3_SYN {WITH}}}} About 50 generic event extraction patterns, supported by lexical information as shown above, allow extraction of events and their arguments in cases like: An lraqi warplane attacked the frigate Stark with missiles May 17, 1987.
This generic, lexicon-driven event extraction approach makes REES easily portable because new types of events can be extracted by just adding new verb entries to the lexicon.
No new patterns are required.
Moreover, this approach allows for easy customization capability: a person with no knowledge of the pattern language would be able to configure the system to extract new events.
While the tagging component is similar to other pattern-based IE systems (e.g., Appelt et al.1995; Aone et al.1998, Yangarber and Grishman 1998), our EventTagger is more portable through a lexicon-driven approach.
Co-reference Resolution After the tagging phase, REES sends the XML output through a rule-based co-reference resolution module that resolves:   definite noun phrases of Organization, Person, and Location types, and singular person pronouns: he and she.
Only "high-precision" rules are currently applied to selected types of anaphora.
That is, we resolve only those cases of anaphora whose antecedents the module can identify with high confidence.
For example, the pronoun rules look for the antecedents only within 3 sentences, and the definite NP rules rely heavily on the head noun matches.
Our highprecision approach results from our observation that unless the module is very accurate (above 80% precision), the coreference module can hurt the overall extraction results by over-merging templates.
Template Generation Module A typical template generation module is a hard-coded post-processing module which has to be written for each type of template.
By contrast, our Template Generation module is unique as it uses declarative rules to generate and merge templates automatically so as to achieve portability.
Declarative Template Generation REES outputs the extracted information in the form of either MUC-style templates, as illustrated in Figure 1 and 2, or XML.
A crucial part of a portable, scalable system is to be able to output different types of relations and events without changing the template generation code.
REES maps XML-tagged output of the co-reference module to templates using declarative template definitions, which specifies the template label (e.g., ATTACK_TARGET), XML attribute names (e.g., ARGUMENT l), corresponding template slot names (e.g., ATTACKER), and the type restrictions on slot values (e.g., string).
Event Merging One of the challenges of event extraction is to be able to recognize and merge those event descriptions which refer to the same event.
The Template Generation module uses a set of declarative, customizable rules to merge coreferring events into a single event.
Often, the rules reflect pragmatic knowledge of the world.
For example, consider the rule below for the DYING event type.
This rule establishes that if two die events have the same subject, then they refer to the same event (i.e., a person cannot die more than once).
{merge {EVENT 1 {AND {SUBTYPE DIE} {PERSON training set (200 texts) and the blind set (208 texts) from about a dozen news sources.
Each set contains at least 3 examples of each type of relations and events.
As we mentioned earlier, "relations" includes MUC-style TEs and TRs.
Text Set Task Train Blind Rel.
Events Rel.
& Events Rel.
Events Rel.
& Events Templates in keys 9955 2525 10707 F-M {EVENT 2 {AND {SUBTYPE DIE} {PERSON Table 3: Evaluation Results 2.4 Graphical User Interface (GUI) For some applications such as database population, the user may want to validate the system output.
REES is provided with a Javabased Graphical User Interface that allows the user to run REES and display, delete, or modify the system output.
As illustrated in Figure 4, the tool displays the templates on the bottom half of the screen, and the user can choose which template to display.
The top half of the screen displays the input document with extracted phrases in different colors.
The user can select any slot value, and the tool will highlight the portion of the input text responsible for the slot value.
This feature is very useful in efficiently verifying system output.
Once the system's output has been verified, the resulting templates can be saved and used to populate a database.
3 System
Evaluation The blind set F-Measure for 31 types of relations (73.95%) exceeded our initial goal of 70%.
While the blind set F-Measure for 61 types o f events was 53.75%, it is significant to note that 26 types of events achieved an FMeasure over 70%, and 37 types over 60% (cf.
Table 4).
For reference, though not exactly comparable, the best-performing MUC-7 system achieved 87% in TE, 76% in TR, and 51% in event extraction.
F-M in blind set 90-100 80-89 Event types 2 : Buy artifact.
Marry 9 : Succeed, Merge company, Kill, Surrender, Arrest, Convict, Sentence, Nominate, Expel.
15 : Die, Sell artif~/ct,Export Artifact, Hire, Start office, Make artifact, Acquire company, Sue organization, Stock Index moves down, Steal money, Indict, Jail, Vehicle crash, Elect, Hold meeting.
Table 4: Top-performing Event Types The table below shows the system's recall, precision, and F-Measure scores for the Regarding relation extraction, the difference in the score between the training and blind sets was very small.
In fact, the total F-Measure on the blind set is less than 2 points lower than that of the training set.
It is also interesting to note that for 8 of the 12 relation types where the F-Measure dropped more than 10 points, the training set includes less than 20 instances.
In other words, there seems to be a natural correlation between low number of instances in the training set and low performance in the blind set.
There was a significant drop between the training and blind sets in event extraction: 11 points.
We believe that the main reason is that the total number of events in the training set is fairly low: 801 instances of 61 types of events (an average of 13/event), where 35 o f the event types had fewer than 10 instances.
In fact, 9 out of the 14 event types which scored lower than 40% F-Measure had fewer than I0 examples.
In comparison, there were 34,000 instances of 39 types of relations in the training set.
The contribution o f the co-reference module is illustrated in the table below.
Co-reference resolution consistently improves F-Measures both in training and blind sets.
Its impact is larger in relation than event extraction.
Text set Task Coreference rules No coreference rules Training Blind Relations Events Relations & Events Relations Events Relations & Events Table 5: Comparative results with and without co-reference rules In the next two sections, we analyze both false positives and false negatives.
False Positives (or Precision Errors) REES produced precision errors in the following cases:  Most of the errors were due to overgeneration of templates.
These are mostly cases of co-referring noun phrases that the system failed to resolve.
For example: "Panama...
the nation ...
this country.., his country" Rules for the co-reference module are still under development, and at present REES handles only limited types of plural noun phrase anaphora.
Spurious events resulted from verbs in conditional constructions (e.g., "if ...
then")... or from ambiguous predicates.
For instance, "appoint" as a POLITICAL event vs.
a PERSONNEL CHANGE event.
The subject of a verb was misidentified.
This is particularly frequent in reduced relative clauses.
Kabul radio said the latest deaths brought to 38 the number o f people killed in the three car bomb explosions, (Wrong subject: "the number of people" as the KILLER instead of the victim) False Negatives (or Recall Errors) Below, we list the most frequent recall errors in the training set.
 Some event arguments are mentioned with event nouns instead of event verbs.
The current system does not handle noun-based event extraction.
India's acquisition last month of the nuclear submarine from the Soviet Union...
(SELLER="Soviet Union" and TIME="last month'" come with the nounbased event "acquisition").
 Pronouns "it" and "they," which carry little semantic information, are currently not resolved by the co-reference module.
It also has bought three late-1970s vintage ICilo class Soviet submarines and two West German HDW 209 subs (Missed BUYER=India because of unresolved it).
Verb arguments are a conjunction of noun phrases.
The current system does not handle coordination of verb arguments.
Hezbollah killed 21 lsraelis and 43 o f Lahad's soldiers (The system gets only the first object: 21 Israelis.
) Ellipsis cases.
The current system does not handle ellipsis.
The two were sentenced to five-year prison terms with hard labor by the state security court...
(Missed PERSON_SENTENCED fill because of unresolved the two).
The subject of the event is relatively far from the event-denoting verb: Vladislav Listyev, 38, who brought television interview shows in the style of Phil Donahue or Larry King to Russian viewers and pioneered hard-hitting television journalism in the 1980s, was shot in the heart by unknown assailants and died immediately...
(The system missed subject Vladislav Listyev for attack event shot) Missed ORG LOCATION relations for locations that are part o f the organization's name.
Larnaca General Hospital (Missed ORG_LOCATION TR for this and Larnaca.
) We asked a person who is not involved in the development of REES to review the event extraction output for the blind set.
This person reported that:  In 35% of the cases where the REES system completely missed an event, it was because the lexicon was missing the predicate.
REES's event predicate lexicon is rather small at present (a total of 140 verbs for 61 event types) and is mostly based on the examples found in the training set,  In 30% of the cases, the subject or object was elliptical.
The system does not currently handle ellipsis.
In 25% of the cases, syntactic/semantic argument structures were missing from existing lexical entries.
It is quite encouraging that simply adding additional predicates and predicate argument structures to the lexicon could significantly increase the blind set performance.
Desmond Tutu and Albertina Sisulu are important...
We plan to develop a generic set of patterns for noun-based event extraction to complement the set of generic verb-based extraction patterns.
5 4 Future Directions We believe that improving co-reference resolution and adding noun-based event extraction capability are critical to achieving our ultimate goal of at least 80% F-Measure for relations and 70% for events.
4.1 Co-reference Resolution Conclusions As discussed in Section 3.1 and 3.2, accurate co-reference resolution is crucial to improving the accuracy of extraction, both in terms of recall and precision.
In particular, we identified two types of high-payoff coreference resolution:  definite noun phrase resolution, especially plural noun phrases  3 rd person neutral pronouns "it" and "they".
4.2 Noun-based Event Extraction In this paper, we reported on a fast, portable, large-scale event and relation extraction system REES.
To the best of our knowledge, this is the first attempt to develop an IE system which can extract such a wide range of relations and events with high accuracy.
It performs particularly well on relation extraction, and it achieves 70% or higher F-Measure for 26 types of events already.
In addition, the design of REES is highly portable for future addition of new relations and events.
Acknowledgements This project would have not been possible without the contributions of Arcel Castillo, Lauren Halverson, and Sandy Shinn.
Our thanks also to Brandon Kennedy, who prepared the hand-tagged data.
REES currently handles only verb-based events.
Noun-based event extraction adds more complexity because: Nouns are often used in a generic, nonreferential manner (e.g., "We see a m e r g e r as being in the consumer's interest"), and When referential, nouns often refer to verb-based events, thus requiring nounverb co-reference resolution ("An F-14 crashed shortly after takeoff...
The crash").
PROBLEMS IN NATURAL-LANGUAGE INTERFACE WITH EXAMPLES FROM EUFID Marjorie T e m p l e t o n John Burger S y s t e m Development Corporation Santa Mortice, California TO DSMS ABSTRACT For five years t h e End-User Friendly Interface to Data management (EUFID) project team at System Development Corporation worked on the design and implementation of a Natural-Language Interface (NLI) system that was to be independent of both the application and the database management system.
In this paper we describe application, n a t u r a l -l a n g u a g e and d a t a b a s e management problems involved in NLI development, with specific reference to the EUFID system as an example.
I INTRODUCTION users.
Tools that could assist in automating this process are badly needed.
The second set of issues involves language processing techniques: how to assign constituent structure and interpretation to queries using robust and general methods that allow extension to additional lexical items, sentence types and semantic relationships.
Some NLI systems d i s t i n g u i s h the assignment of syntactic structure, o r parsing, from the interpretation.
Other systems, including EUFID, combine information about constituent and semantic structure into an integrated semantic grammar.
The third class involves database issues: how to actually perform the intent of the natural-language question by formulating the correct structured query and e f f i c i e n t l y n a v i g a t i n g through the database to retrieve the right answer.
This involves a thorough understanding of the DBMS structure underlying the a p p l i c a t i o n, the operations and functions the query language supports, and the nature and volatility of the database.
Obviously issues in these three areas are related, and the knowledge needed to deal with them may be distributed throughout a natural-language interface system.
The purpose of this paper is to show how such issues might be addressed in NLI development, with illustrations from EUFID.
The next section includes a brief review of related work, and an o v e r v i e w of the EUFID system.
The third section describes the goals that EUFID achieved, and section four discusses in detail ~ome of the major application, language, and database problems that arose.
Section five suggests guidelines for determining whether an application is an appropriate target for a n a t u r a l l a n g u a g e interface.
From 1976 t o 1981 SDC was involved in the development of the End-User Friendly Interface to Data management (EUFID) system, a n a t u r a l l a n g u a g e interface (NLI) that is designed to be independent of both the application and the underlying d a t a b a s e management system (DBMS).
[TEMP79, TEMP80, BURG80, BURG82].
The EUFID system permits users to communicate with database management systems in natural English rather than formal query languages.
It is assumed that the application domain is well defined and bounded, that users share a common language to address the application, and that users may have little experience with computers or DBMSs but are competent in the application area.
At least three broad categories of issues had to be addressed during EUFID development, and it is apparent that they are common to any general naturallanguage interface to database management systems.
The first category involves the application: how to c h a r a c t e r i z e the requirements of the human-machine dialogue and interaction, capture that information efficiently, formalize the information and incorporate that knowledge into a framework that can be used by the system.
The major problems in this area are knowledge acquisition and representation.
For many NLI systems, bringing up a new application requires extensive effort by system designers with cooperation from a representative set of endfiII BACKGROUND Over the past two decades a considerable amount of work has gone into the d e v e l o p m e n t of natural-language systems.
Early developments were in the areas of text processing, syntactic parsing techniques, machine translation, and early attempts at English-language question answering systems.
Several early question-answering experiments are reviewed by R.
F. Simmons in [SIMM65].
Waltz has edited a collection of short papers on topics related to naturallanguage and artificial intelligence in a survey of NLI research [WALT77].
A survey of NLIs and evaluation of several systems with respect t o their applicability to command and control environments can be found in [OS179].
A. RELATED WORK involved with problems of semantics and has three separate layers of semantic u n d e r s t a n d i n g. The layers are called "English Formal Language", "World Model Language", and "Data Base Language" and appear to c o r r e s p o n d roughly to the "external", "conceptual", and "internal" views of data as d e s c r i b e d by C.
J. Date [DATE77].
PHLIQAI can interface to a v a r i e t y of d a t a b as e structures and DBMSs.
5. The Programmed LANguage-based Enquiry System (PLANES) [WALT78] uses an ATN based parser and a semantic case frame analysis to understand questions.
Case frames are used to handle pronominal and elliptical reference and to g e n e r a t e responses to clarify partially interpreted questions.
REL [THOM69], initially written entirely in assembler code for an IBM36@, has been in continuous development since 1967.
REL allows a user to make interactive extensions to the g r a m m a r and semantics of the system.
It uses a formal grammar expressed as a set of general re-write rules with semantic transformations attached to each rule.
Answers are obtained from a b u i l t i n database.
RENDEZVOUS [CODD74] addresses the problem of c e r t a i n t y regarding the machine's understanding of the user's question.
It engages the user in d i a l o g u e to specify and disambiguate the question and will not route the formal query to the relational DBMS until the user is satisfied with the machine's interpretation.
ROBOT [HARR78] is one of the few NLI systems currently a v a i l a b l e on the commercial market.
It is the basis for Cullinane's OnLine English [CULL80] and Artificial Intelligence C o r p o r a t i o n ' s Intellect [EDP82].
It uses an extracted version of the database for lexical data to assist the ATN parser.
TORUS [MYLO76], like RENDEZVOUS, engages the user in a d i a l o g u e to specify and d i s a m b i g u a t e the user's question.
It is a research o r i e n t e d system looking at the problems of knowledge representation, and some effort has been spent on the understanding of text as well as questions.
While few NLIs have reached the commercial marketplace, many systems have c o n t r i b u t e d to advancing the state of the art.
Several representative systems and the problems they addressed are described in this section.
i. CONVERSE [KELLT1] used formal syntactic analysis to g e n e r a t e surfaceand d e e p s t r u c t u r e parsings together with formal semantic t r a n s f o r m a t i o n rules to produce queries for a built-in relational DBMS.
It was written in SDC LISP and ran on IBM 37@ computers.
Started in 1968, it was one of the first naturallanguage processors to be built for the purpose of querying a separate data m a n a g e m e n t system.
LADDER [HEND77] was designed to access large d i s t r i b u t e d databases.
it is implemented in INTERLISP, runs on a PDP-I@, and can interface to different DBMSs with proper configuration.
It uses a semantic g r a m mar and, like EUFID and most NLIs, a different grammar must be defined for each application.
The Lunar Rocks system LSNLIS [WOOD72] was the first to use the Augmented Transition Network (ATN) grammar.
Wrl~ten in LISP, it transformed formally parsed questions into representations of the first-order predicate calculus for deductive processing against a built-in DBMS.
PHLIQAI [SCHA77] uses a syntactic parser which runs as a separate pass from the semantic understanding passes.
This system is mainly fiB.
OVERVIEW OF EUFID EUFID is a general purpose naturallanguage front-end for database management.
The original design goals for EUFID were: to b e application independent.
This means that the program must be table driven.
The tables contain the dictionary and semantic information and are loaded with a p p l i c a t i o n s p e c i f l c data.
It was desired that the tables could be constructed by someone other than the EUFID staff, so t h a t users could build new applications on their o w n . to be database independent.
This means that the organization of the data in the database must be representable in tables that drive the query generator.
~ A database reorganization that does not change the semantics of the application should be transparen~ to the user.
written in a high level language; initially a customer required code to be written in FORTRAN, later we were able to use the "C" programming language.
to support different views data for security purposes.
of the The design which met these requirements is a modular system which uses an Intermediate Language (IL) as the output of the natural-language analysis system [BURG82].
This language represents, in many ways, the union of the c a p a b i l i t i e s of many "target" DBMS q u e r y languages.
The EUFID system consists of three major modules, not counting the DBM3 (see Figure I).
The analyzer (parser) module is table driven.
It is n e c e s s a r y only to properly build and load the tables to interface EUFID to a new application.
Mapping a question from its d i c t i o n a r y (user) representation to DBMS representation is handled by mapping functions contained in a table and applied by a separate module, t h e "mapper".
Each c o n tent (application dependent) word in the d i c t i o n a r y has one or more mapping functions defined for it.
A final stage of the mapper is a q u e r y l a n g u a g e generator containing the syntax of IL.
This stage writes a query in IL using the group/field names found by the mapper t o represent the user's concepts and the structural relationships between them.
This design satisfies t h e requirement of application independence.
ENGLISH QUESTION to be DBMS independent.
This means that it must be able to generate requests to different DBMSs in the DBMS's query language and that the interface of EUF~D to a different DBMS should not require changes to t h e NLI modules.
Transferring the same database with the same semantic content to another DBMS should be transparent to the natural-language users.
to run on a mini-computer that might possibly be different from the computer with the DBMS.
to have a fast response time, even when the question cannot be interpreted.
This means it must be able quickly to recognize unanalyzable constructs.
Figure i: EUFID Block Diagram to handle nonstandard or poorlyformed (but, nevertheless, meaningful) questions.
to be portable to various machines.
This means that the system had to be * We make a technical distinction between the words "question" and "query".
A question is any string entered by the user to the EUFID analyzer, regardless of the terminating punctuation.
This is consistent with the design since EUFID treats all input as a request for information.
A query is a formal representation of a question in either the EUFID intermediate language IL, or in the formal query language of a DBMS.
For each different DBMS used by a EUFID application, a "translator" module needs to be written to convert a query in IL to the equivalent in the DBMS query language.
This design satisfies the requirement of DBMS independence.
Other modules are the system controller, a "help" module, and a " s y n o n y m editor".
An "Application Definition Module" is used off-line to assist in the creation of the run-time application description tables.
The following subsections descrloe each of the modules of the EUFID system, and give our m o t i v a t i o n for design.
i. A~plication Definitions Bringing up a new a p p l i c a t i o n is a long and complex process.
The d a t a b a s e d e f i n i t i o n must be transmitted to EUFID.
A large corpus of "typical" user questions must be collected from a representative set of users and from these the dictionary and mapping tables are designed.
A "semantic graph" is defined for the application.
This graph is implicitly realized in t h e dictionary where the nodes of the graph are the definitions of English content words and the c o n n e c t i v i t y of the graph is implied by the case-structure relationships defined for the nodes.
All d i c t i o n a r y and mapping-function are then entered into computer files which are processed by the Application Definition Module (ADM) to produce t h e run-time tables.
These final tables are complex structures of pointers, character strings, and index tables, designed to decrease access time to the information required by the analyzer and mapper modules.
data considered.
Frequently, desig~ :o,~s i d e r a t i o n s in the m a p p i n g f u n c t i o n list necessitate going back and m o d i f y i n g the content of the d i c t i o n a r y . This is an example of the o v e r l a p of the l i n g u i s t i c and database issues in assigning an interpretation to a question.
c. Database Representation The ADM, typically, needs to be run several times to "debug" the tables.
EUFID interfaces to three applications currently exist, and building tables for each new a p p l i c a t i o n took less time than the previous one, b u t it still requires several staff-months to bring up a new application.
a. User-View Representation The structure o f the data in the user's database is represented in two tables, called the CAN (for canonical) and REL (for relationships) tables.
Taking advantage of the fact that any database can be represented in relational form, EUFID lists each d a t a b a s e g r o u p as if it were a relation.
Group-to-group linkage (represented in the REL table) is d e a l t with as if a join* were necessary to implement the link.
For h i e r a r c h i c a l and network DBMSs the join will not be needed: the link is "wired in" to the d a t a b a s e structure.
EUFID nevertheless assumes a join m a i n l y in order to facilitate the writing of g r o u p t o g r o u p links in IL, which is a relational language.
The CAN table includes database-specific information for each field (attribute) of each group (relation), such as field name, containing group, name of d o m a i n from which attributed gets its values, and a pointer to a set of c o n v e r s i o n functions for numeric v a l u e s which can be be used to convert from one unit of m e a s ure to another (e.g., feet to meters).
These data are used by the run-time modules which map and translate the t r e e s t r u c t u r e d output of the analyzer to IL on the actual g r o u p / f i e l d names of the database, and then co the language of the DBMS.
These modules are d i s c u s s e d in the next sections.
2. The EUFID Analyzer All information on the user's view of the database is kept in the d i c t i o n ary.
The dictionary consists of two kinds of words and definitions.
Function words, such as p r e p o s i t i o n s and Conjunctions, are pre-stored in each a p p l i c a t i o n ' s d i c t i o n a r y and are used by the analyzer for direction on how to connect the semantic-graph nodes during analysis.
Content words are application dependent.
The d c r O o n s of content words are semantic-graph nodes.
The connectivity o the graph is indicated by semantic case slots and pointers contained in the nodes.
A form of semantic-case is used to indicate the attributes of an entity (e.g., adjectives, prepositional phrases, and other modifiers of a noun).
b. Mapping Functions The current version of the EUFID analyzer employs a variant of the CockeK a s a m i Y o u n g e r algorithm for parsing its input.
This classical nonpredictive b o t t o m u p algorithm has been used in a family of "chart parsers" developed by Kay, Earley, and others [AHO72].
The main features of these parsers are: (i) They use a r b i t r a r y c o n t e x t f r e e grammars.
There are no r e s t r i c t i o n s on rules which have l e f t r e c u r s i o n or other c h a r a c t e r i s tics which sometimes cause difficulty.
(2) They produce all possible parses of a given input string.
The g r a m m a r s they use may be ambiguous at either the nonterminalor t e r m i n a l s y m b o l levels.
In natural-language processing, this allows for a precise r e p r e s e n t a t i o n of * The t e r m "join" refers to a composite o p e r a t i o n between two relations in a relational DBMS.
The list of mapping functions is derived from the dictionary.
Every possible connection of every node has to be fiboth the syntactic and lexical ambiguities which may be present in an input sentence.
(3) They provide partial parses of the input.
Each non-terminal symbol derives some input substring.
Even if no such substring spans the entire sentence, i.e., no complete parse is achieved, analyses of various regions o f t h e s e n t e n c e a r e available.
(4) They are conceptually straightforward and easy t o implement.
The speed and storage considerations which have kept such parsers from being widely used in compilers are less relevant in the analysis o f short strings such as queries to a DBMS.
The grammar used b y the EUFID parser is essentially semantic.
The symbols of the grammar r e p r e s e n t t h e concepts underlying lexical items, and the rules specify the ways in which these concepts can be combined.
More s p e c i f i c a l l y, the concepts are o r g a n i z e d into a case system.
Each rule states that a given pair of constituents can be linked if the conc e p t u a l head of o n e constituent fills a case on the conceptual head of t h e other.
A degree of context sensitivity is achieved b y attaching predicates to the rules.
These predicates b l o c k application of t h e rules unless certain (usually syntactic) conditions hold true.
The parser uses syntactic information only "on demand", that is, only when such information is necessary to resolve semantic ambiguities.
This a d d s to its coverage and robustness, and makes it relatively insensitive to the phrasing variations which must be explicitly accounted for in many other systems.
3. Mapping to-field and g r o u p t o g r o u p tions of t h e database.
connecThe mapper makes use of a table of mapping functions.
The table contains at least one mapping function for every content word in the dictionary.
The analyzer's tree is traversed bottom up, applying mapping functions to each node on t h e way.
Mapping f u n c t i o n s are context sensitive with respect to those nodes below it in the tree: nodes that have already been mapped.
A new tree is g r a d u a l l y formed and connected this way.
Mapping functions may indicate that the map of a semantic-graph node is a database node (that is, a group or field name), o r a pre-connected sub-tree of database nodes.
The mapping function may also indicate removal of a database node or m o d i f i c a t i o n to the existing structure of the tree being constructed.
The new t r e e i s c r e a t e d in terms of the database groups and f i e l d s and i t s structure reflects the connectivity of the database.
A final stage of the mapper traverses this new tree and generates the EL statement of the query using a table of the syntax and keywords of EL and the database names from the tree.
The mapper module converts the output of the analyzer to input for the translator module.
Analyzer output is a tree structure where the nodes are semantic-graph nodes corresponding to the content words in the user's question and obtained from the dictionary.
An alternative method of mapping that is now being investigated involves breaking the process into two basic parts.
The first step would be to map the tree o u t p u t o f the analyzer t o an IL query on what C.
J. Date calls the "conceptual schema" of the database [DATE77].
A second step would take this IL input and re-arrange the schema connectivity (and names of groups and fields) from that of the conceptual schema to that of the actual target database, generating another IL query as input to the current translators.
Input to the translator module is a string in the syntax of IL which contains the names of actual groups and fields in the database.
The mapping algorithm, thus, has to make several levels of conversion simultaneously: it must convert a into a linear string it must convert into database names, and tree structure of tokens, semantic-graph nodes groupand fieldit must convert the connectivity of the tree (representing concept-toconcept linkage in English) into the (frequently very different) groupThe final run-time module in EUFID is a syntax translator that converts IL to the actual DBMS query language.
If necessary, the translator can also add access-path information related t o database search.
Currently, two translators have been written.
One converts IL to QUEL, a relatively simple conversion into the language of the relational database management system INGRES [STONY6].
The other translator converts IL into the query language of the World-Wide Data Management System (WWDMS) [HONE76] used by the Department of Defense, and also handles additional access path information.
This translator was quite difficult to design and build because of the highly procedural nature of the WWDMS query system.
The output of a translator is sent to the appropriate DBMS.
In the EUFID system running at SDC, a QUEL query is submitted directly to INGRES running on the same PDP-II/70 as EUFID.
For testing purposes, queries generated by the WWDMS translator were transmitted from a PDP11/70 to a Honeywell H6000 with a WWDMS database.
5. Application Description some coming from open-ended domains.
A I R E P has a network database structure and contains the same data s t r u c t u r e in four d i f f e r e n t files.
III LEVEL OF SUCCESS EUFID runs o n three d i f f e r e n t application databases.
The METRO a p p l i c a t i o n involves monitoring of shipping transactions between companies in a city called "Metropolis".
There are ten companies located in any one of three n e i g h b o r hoods.
Each company rents warehouse space for shipping/recelving transactions, and has local offices which receive goods.
The data is organized telationally using the INGRES database m a n a g e m e n t system.
That means that there are no n a v i g a t i o n a l links stored in the records (called "relations") and there is no predefined "root" to the database structure.
Access may be made from any relation to any other relation as long as there is a field in each of the two relations which has the same "domain" (set of values).
AIREP (ADP Incident REPorting) is a network database, implemented in WWDMS.
It c o n t a i n s reports about hardware and software failures and resolution of the problems in a large computer system.
Active problems are maintained in an active file and old, solved problems are moved to an historical file.
If a problem [s reported more than once, an abbreviated record is made for the additional report, called the "duplicate incident" record.
This means that there are four basic type of report: active incidents, duplicate incidents, historical incidents, and historical duplicate incidents.
In addition, there are records about sites, problems, and solutions.
The A P P L I C A N T database is a relational database implemented in INGRES that contains information about job applicants and their backgrounds.
The central entity is the "applicant", while other relations describe the a p p l i c a n t ' s specialties, education, previous employment, computer experience, and interviews.
Each database has d i f f e r e n t features chat may present problems for a naturallanguage interface but which are typical of 'real-world' applications.
METRO has relatively few entities but has complex relationships among them.
APPLICANT has many updates and many different values, Most of the EUFID d e s i g n g o a l s were actually met.
EUFID runs on a minicomputer, a DEC PDP 11/70.
It is application, database, and DBMS independent.
A typical q u e s t i o n is analyzed, mapped and translated in five to fifteen seconds even with g r a m m a t i c a l l y incorrect input.
The analyzer c o n t a i n s a good spelling corrector and a good morphology a l g o r i t h m that strips inflectional endings so that all inflected forms of words need not be stored explicitly.
A "synonym editor" permits the user to replace any word or string of words in the dicionary with another word or string, to accommodate personal jargon and expressability.
A "Concept Graph Editor s allows a database administrator to m o d i f y tables and define user profiles so that d i f f e r e n t users may have limited views of the data for s e c u r i t y purposes.
The analysis strategy, based on a semantic grammar, permits easy and natural paraphrase recognition, although there are linguistic c o n s t r u c t s it cannot handle.
These are d i s c u s s e d below.
An English word may have more than one definition without c o m p l i c a t i n g the analysis strategy.
For example, "ship" as a vessel and as a verb meaning "to send" can be defined in the same d i c t i o n ary.
Words used as database values, such as names, may also have m u l t i p l e definitions, e.g., "New York" used as the name of both a city and a state.
The mapper, despite its many limitations, can c o r r e c t l y map almost all trees output by the analyzer.
It is able to handle English c o n j u n c t i o n s, mapping them a p p r o p r i a t e l y to logical ANDs or ORs, and understanding that some "ands" may need to be interpreted as OR and vice-versa under certain c i r c u m s t a n c e s . It is able to g e n e r a t e calls on DBMS calculations (e.g., average) and user-defined functions (e.g., marine great-circle distance) if the user-function exists and is supported by the DBMS.
Questions involving time are interpreted in a reasonable way.
Functions are defined for "between" and "during" in the METRO application.
The AIREP application allows time comparisons such as "What system was running when incident J123 occurred" which require a test to see if a point in time is within an interval.
The mapper can translate "user values" (e.g., "Russian") to database values (e.g., "USSR"), and convert one unit of measure (e.g., feet) to another (e.g., meters).
EUFID c a n i n t e r f a c e to very complex relational and CODASYL-type databases having difficult n a v i g a t i o n and parallel structures.
In t h e AIREP application a consistent WWDMS navigational m e t h o d o l o g y is used to access non-key records.
The system c a n also map to t h e parallel, but not identical, structures for duplicate and historical incidents.
I n the INGRES applications, EUFID is able to use and correctly map to = r e l a tionship relations" which relate two or more other relations.
For example, the METRO relation =cw" contains a company name, a warehouse name, a n d a date.
This represents the initial business contact.
A user might ask, =When d i d C o l o n i a l start t o do b u s i n e s s w i t h Superior?
= or  When d i d b u s i n e s s b e g i n b e t w e e n C o l o n i a l and S u p e r i o r ? =, e i t h e r of which must ~oin both t h e c o m p a n y ( " c =) a n d t h e w a r e h o u s e ('w') relations t o the =cw" relation.
The system c o n t r o l module keeps a journal of all user-system interaction together with internal module-to-module data such as the IL for the user's question and the generated DBMS query.
The system also employs a very effective HELP module which, under certain circumstances, is context sensitive t o the problem affecting the user.
IV PROBLEMS APPLICANT database may wish to fill a specific Job opening while others may collect statistics on types of appli~ cants.
The language used for these two functions can be quite different, and it is n e c e s s a r y to have extensive interaction with cooperative users in order to characterize the kinds of dialogues they will have with the system.
Not only must representative language protocols be collected, but desired responses must be understood.
For example, to answer a question such as =What is t h e status of our forces in Europe =, the system must know whether 'our' refers to U.S. or NATO or some other unit.
The importance of this interaction between potential users and system developers should n o t b e underestimated, as it is the basis for defining much of the knowledge base needed by the system, and may also be t h e basis for eventual user acceptance o r rejection of the NLI system.
2. Value R e c o g n i t i o n This section describes problems associated with EUFID development that appear to be common to natural-language interfaces to database management systems.
They are loosely classified into the major areas Of application, language and database management issues, although there may be overlap.
Criteria for evaluating whether an application is appropriate for a natural-language front-end are also described.
A. APPLICATION DEFINITION PROBLEMS A "value = is a specific datum stored in the database, and is the smallest piece of data obtainable as the result o f a database query.
For example, in response to the question "What companies in North Hills shipped light freight to Superior?
= the METRO DBMS returns two values: "Colonial" and "Supreme'.
Values can also be used in a query to qualify or select certain records for output, e.g., in t h e above question "North Hills" and "Superior" are values that must be represented in the query to the DBMS.
As long as the alphanumeric values used in a particular database field are the same as words in t h e English questions, there are no difficult problems involved in recognizing values as selectors in a query.
There are three basic ways to recognize these value words in a question.
They can be explicitly listed in the dictionary, recognized by a pattern or context, or found in the database itself.
If the value words are stored in the dictionary, they can be subject to spelling correction because the spelling corrector uses the dictionary to locate words which are a close match to unrecognized words in a question.
This means, though, that all possible values and variant legitimate spellings of values for a concept must be put either into the dictionary or into the synonym list.
This is reasonable for concepts which have a small and controlled set of _values* such as the names of the * A set of v a l u e s is called a "domain,r.
The primary issue in this area is concerned with problems of defining, creating, and bringing up the necessary data for a new application.
The discussion points out the difficulties associated with systematic knowledge acquisition.
I. User Model A single database may be used by different groups of users for different purposes.
For example, some users of the ficompanies in METRO, but may u n w i e l d y for large sets of values.
become If a value can be recognized by a pattern, it is not n e c e s s a r y to itemize all instances in the dictionary.
For example, a date may be entered as "yy/mm/dd" so that any input matching the pattern "nn/nn/nn" is recognized as a date.
This is the approach used for dates and for names of applicants in the A P P L I C A N T database, where names of people match the pattern "I.I.Lastname".
In another approach, OnLine English [CULL80] and Intellect [HARR78, EDP82] (two v a r i a t i o n s of ROBOT) used the database to recognize values.
This is a s a t i s f a c t o r y solution if the database is small or if the small number of d i f f e r e n t values is stored in an index accessible to the NLI, and if the values in the database are suitable for use in English questions.
Each of these solutions has disadvantages.
If values are stored in the d i c t i o n a r y there may be many different ways to spell each particular value.
For example, the company name for "System Development Corporation" may also be given as "S.D.C.", "S D C", or "System Development Cotp".
While each d i f f e r e n t spelling could be entered as a synonym for the "correct" spelling in the database, this would result in an enormous proliferation of the d i c t i o n a r y entries and problems with concurrency control between the updates directed to the data m a n a g e m e n t system and the updates to the dictionary.
A creative solution might he to define rules for synonym generation and apply them to database updates.
A somewhat different example is from the A P P L I C A N T application which has many open ended domains, such as names of applicants and previous employers.
In this case, the application designer may have to treat certain fields as "retrieve-only", meaning that the data can be asked ~or but not used as a selection criterion.
A database with a large number of retrieve-only fields may be a poor candidate for an NLI.
Patterns can be used only if they can be enforced, and probably few values really fit the patterns nicely.
Proper names ate a poor choice for patterns because of variations such as middle initial or title such as "Dr".
or "Jr.".
Also, spelling correction cannot be performed unless the value is stored in the dictionary.
Finally, the solution of using the database itself to recognize v a l u e s is u n s a t i s f a c t o r y to a general NLI for anything other than trivial databases, unless an inverted index of values is easily accessible.
There are the problems of spelling c o r r e c t i o n and synonyms for database values, the inefficiency involved in accessing the DBMS for every unrecognized word, and the d i f f i culty of knowing which fields in the d a t a b a s e to search.
3. Semantic Variation By Value Databases are generally designed with a m i n i m u m number of d i f f e r e n t record types.
When there are entities which are similar, but p o s s i b l y have a small number of a t t r i b u t e s which are not shared, the entities will be stored in the same record type with null values for the attributes that do not apply.
The user, in his questions, may view these similar entities as very d i f f e r e n t e nt i t i e s and talk about them d i f f e r e n t l y . We did not encounter the problem with METRO or AIREP.
For example, in METRO, the user asks the same type of questions about the c o m p a n y named "Colonial" as about the company named "Supreme".
In APPLICANT, however, each a p p l i c a n t has a set of "specialties" such as "computer programmer", "a c c o u n t i n g clerk", or "gardener".
These are all stored as values of the s p e c i a l t y field in the database.
Unfortunately, in this case different specialties evoke completely d i f f e r e n t concepts to the end user.
The user may ask q u e s t i o n s such as, "What p r o g r a m m e r s know COBOL?", "Who can program in COBOL?", and "How m a n y a p p l i c a n t s with a s p e c i a l t y in computer programming applied in 1982?".
Notice the new nouns and verbs that are introduced by this s p e c i a l t y name.
A value domain such as specialties should be handled with an ISA hierarchy.
Each d i f f e r e n t type of s p e c i a l t y such as gardener or programmer could have a different concept that is a subset of the concept "specialty".
Some questions could be asked about all s p e c i al t i e s and others could be directed only to certain subconcepts.
However, there is no [SA hierarchy in EUFID, and it would have been inefficient to treat each specialty and subspecialty as a separate concept since there are 30 specialties and 196 subspecialties.
Therefore, we required the users to know the exact values, to know which values are for s p e c i a l t i e s and which are for subspecialties, and to ask q u e s t i o n s using the values only as nouns.
This is not "user friendly".
Even if it were possible to build a different concept for each different skill, there is an update problem.
When a new value is a d d e d to a v a l u e domain where there ace uniform semantics (as in adding a new company name in METRO), the new value is simply attached to the existing concept, when the new value has different semantics, t h e newly associated concepts, nouns, and verbs cannot be added automatically.
If t h e NLI supports an ISA hierarchy, someone w i l l need to categorize t h e new value and add a new node to the hierarchy or specify a position in the hierarchy.
4. Automation of D e f i n i t i o n subset who l i v e in Nevada.
One s o l u t i o n is to provide commands that allow u s e r s to d e f i n e s u b s e t s of the database to which to address questions.
This removes the ambiguity and speeds up retrieval time on a large d a t a b a s e . However, it moves the NLI interaction toward that of a structured query language, and forces the user to be a w a r e of the level of subset b e i n g accessed.
It is also difficult to implement because a subset may involve projections and joins to build a new relation containing the subset.
The NLI must be able dynamically and temporarily to change the mapping tables t o map t o this new relation.
2. Intelll~ent Interaction A natural-language interface system will not be practical u n t i l a new a p p l i c a t i o n can b e installed easily.
"Easily" means that the end-user organization must be able to create and modify the driving tables for the application relatively quickly without the help of the NLI developer, and must b e able to use the NLI without restructuring the d a t a b a s e . Each EUFID application required "handcrafted" tables that were built by the development staff.
Each new application was done in less time than the previous one, but still required several staff-months to bring up.
Clearly, the goal of facilitating the building of the tables by end users was not met.
Computer-assisted tools for defining new applications are a prerequisite for practical NLIs.
B. LANGUAGE PROBLEMS One of the EUFID design goals was to r e s p o n d promptly either with an answer or with a message that the question could not be interpreted.
The system handles spelling or typographical errors by interacting with the user t o select the correct word.
However, when all of the words are recognized but do n o t connect semantically, It is difficult to identify a single point in analysis which caused the failure.
It is i n this a r e a that the absence of a syntactic mechanism for determining well-formedness was most noticeable.
There are times when a question has a proper syntactic structure, but co n t a i n s semantic relationships u n r e c o g n i z a b l e to the application as in "What is the locatlon of North Hills?".
A response of "Location is not defined f o r North Hills in this appllcacion" should be derivable from the recognizable semantic failure.
Similarly, it would be useful to have a framework for interpreting partial trees, as in the question "What companies does Mohawk ship to"? where Mohawk is not a recognized word within the application.
An appropriate response might be "Companies ship to receiving offices and companies; Mohawk is neither a receiving office nor a company.
The names of offices and companies are ...".
Interpretation of partial a n a l y s e s is not possible within the EUFID system; it either succeeds or fails completely.
3. Yes/No Questions The basic approach to language analysis in EUFID involves a bottom up parser using a semantic grammar.
The symbols of the grammar are concepts underlying lexical items, and the rules of the grammar ace based o n a case framework.
Essentially syntactic information is used only when needed to resolve ambiguity.
The language features that this technique has t o handle are common to any NLI, and some of the problem areas are described in the following sections.
I. Anaphora and Ellipsis To support natural interaction it is desirable to allow the use of anaphoric reference and elliptical constructions across sentence sequences, such as "What applicants know Fortran and C?", "Which of them live in California?", "In Nevada?", "How many know Pascal?'.
One of the biggest problems is to define the scope of the reference in such cases.
In the example, it is not clear whether the user wishes to retrieve the set of all applicants who know Pascal or only the II In normal NLI interaction users may wish to ask "yes/no" questions, yet no DBMS has the ability to answer "yes" or "no" explicitly.
The EUFID mapper maps a yes/no question into a query which will retrieve some data, such as an " o u t p u t identifier" or default name for a concept, if the answer is "yes" and no data if the answer if "no".
However, the answer may be "no" for several reasons.
For example, a "no" response to the question "Has John Smith been interviewed"? may mean that the database has knowledge about John Smith and about interviews and Smith is not listed as having had an interview*, or the database knows about John Smith and no data about interviews is available.
A third p o s s i b i l i t y could be that the database has information about John Smith and his employment situation (already hired), and the response might include that information, as in "No, but he has already been hired'.
4. Conjunctions uncertain whether they should be returned in the answer.
It is also d i f f i c u l t to take a c o m p l e m e n t of a set of data using the m a n y data m a n a g e m e n t systems that do not support set o p e r a t o r s between relations.
Questions which require a "yes" or "no" response are difficult to answer because often the "no" is due to a p r e s u p p o s i t i o n which is invalid.
This is e s p e c i a l l y true with negation.
For example, if the user asks, "Does e v e r y company in North Hills except Supreme use NH2?", the answer may be "no" because Supreme is not in North Hills.
The current i m p l e m e n t a t i o n of EUFID does not allow explicit negation, a l t h o u g h some n e g a t i v e concepts are handled such as "What c o m p a n i e s ship to companies other than Colonial?".
"Other than" is interpreted as the "!-" o p e r a t o r in e x a c t l y the same way that "greater than" is interpreted as ">".
C. INTERPRETATION AND DATABASE ISSUES T h e s c o p e of c o n j u n c t i o n s is a difficult problem for any parsing or analyzing algorithm.
The n a t u r a l l a n g u a g e use of "and" and "or" does not n e c e s s a r i l y correspond to the logical meaning, as in the question "List the applicants who live in C a l i f o r n i a a n d Arizona.".
Multiple c o n j u n c t i o n s in a single q u e s t i o n can be ambiguous as in "which minority and female applicants know Fortran and Cobol?'.
This could be interpreted with logical "and" or with logical "or" as in "Which a p p l i c a n t s who are minority or female know either Fortran or Cobol?".
The EUFID mapper will change English "and" to logical "or" when the two phrases within the scope of the conjunction are values for the same field.
In the example above, an applicant has only one state of residence.
Many q u e s t i o n s make perfect sense semantically but are difficult to map into DBMS q u e r i e s because of the d a t a b a s e structure.
The problems become worse when access is through an NLI because of increased e x p e c t a t i o n s on the part of the user and because it may be d i f f i c u l t for a help system a d e q u a t e l y to d e s c r i b e the problem to the user who is unaware of the database structure.
I. IL Limitations Nepption Negative requests may contain explicit negative words such as "not" and "never" or may contain implicit negatives such as "only", "except" and "other than" [OLNE78].
The interpretation of negatives can be very difficult.
For example, "Which c o m p a n i e s did not ship any perishable freight in 1976" could mean either "Which (of all the companies) shipped no perishable freight in 1976"? or "Which (of the companies that ship perishable freight) shipped none in 1976?'.
Moreover, if some companies were only receivers and never shippers it is "-"~e is the important d i s t i n c t i o n between a "closed world" database in which the assumption is that the database covers the whole world (of the application) and an "open world" database in which it is understood that the database does not represent all there is to the real world of the application.
In the open world database, which we encounter most of the time, a response of "not that this database knows of" might be more appropriate.
~Z The design of the IL is critical.
It must be rich enough to support retrieval from all the underlying DBMSs.
However, if it c o n t a i n s c a p a b i l i t i e s that do not exist in a specific DBMS, it is difficult to d e s c r i b e this d e f i c i e n c y to the user.
In APPLICANT, the user cannot get both the major and minor fields of study by asking "List applicants and field of study", because a limitation in the EUFID IL prevents making two joins between education and subject records.
This problem was corrected in a subsequent version of IL with the addition of a "range" statement similar to that used by QUEL [STON76].
The current IL does not contain an "EXISTS" or "FAILS" operator which can test for the existence of a record.
Such an operator is frequently used to test an interrecord link in a network or hierarchical DBMS.
It is needed to express "What problems are unsolved"? to the AIREP application, which requires a test for a database link between a set and a solution Mixed Case Values set.
generate the IL q u e r y EUFID allows a value in the database to be upper or lower case and will c o n v e r t a value in the question either to all upper or all lower case in the IL, or leave it as input b y the user.
If the d a t a b a s e values are mixed case, it is not possible to convert the user's input to a single case.
If the user does not enter each letter in t h e p r o p e r c a s e, t h e v a l u e will n o t match.
3. Granularit~ Differences retrieve [cct.scname] where (cct.date  198~) and (cct.lf >{retrieve [avg (cct.lf)] where (cct.date 1980)}) Here, =cct" i s t h e name o f the companyto-company transaction relation.
" S c n a m e " is the name of a shipping company in this relation.
Note again that the qualification on " 1 9 8 ~ " n e e d s to be done both inside and o u t s i d e the nested p a r t o f t h e query.
In the query language for INGRES such a request is expressed in a manner very similtar t o t h e IL e x p r e s s i o n s . For WWDMS a very complex procedure is generated.
In all cases, t h e DBMS n e e d s to answer the inner request and s a v e t h e result for usa in qualifying the outer request.
There are many database management systems that cannot handle such questions and t h e s e I L s t a t e m e n t s cannot be translated into the system's query language.
5. Inconsistency In Retrieval The NLI user is n o t expected to understand exactly how d a t a is stored, and yet must understand something about the g r a n u l a r i t y of the data.
Time fields often cause problems because time m a y be given by year or by fractions of a second.
U s e r s may make t i m e comparisons that require more granularity than is stored in t h e database.
For example, t h e user can ask "What incidents were reported at SAC while system release 3.4 was installed?".
If incidents were reported by day but system release dates were given by month, the system would return i n c i d e n t s which occurred in the days of the month before the system release was i n s t a l l e d . 4.
Nested Queries A very simple question in English can turn into a very complicated request in t h e query language if it involves retrieval of data which must b e used f o r qualification in another part of the same query.
In IL these are called "nested queries".
Most o f t e n some qualification needs to be done b o t h "inside" and "outside" t h e clause of the query that does the internal retrieve.
For example, t h e question "What i n c i d e n t at SAC had the longest d o w n t i m e ? " f r o m o u r AIREP a p p l i cation i s e x p r e s s e d i n I L as retrieve [INCA.
ID] where (INCA.SITENAME = "SAC") and (INCA.DNTM [retrieve [ max (INCA.DNTM)] where (INCA.SITENAME = "SAC")}) The nested part of t h e query is enclosed in braces.
"INCA" is the database name of the active incident records.
Notice that removing the "INCA.SITENAME = 'SAC'" clause from either the inner or outer query would result in an incorrect formulation of the question.
A similar example from the METRO application is the question, "What company shipped more than the average amount of light freight in 198~"? which will 13 The NLI presents a uniform view of all d a t a b a s e s a n d DBMSs, but it is difficult to truly mask all differences in the behavior o f t h e DBMSS b e c a u s e t h e y d o n o t all process the equivalent query in the same way.
For example, when data are retrieved from two relations in a relational database, the two relations must be J o i n e d on a common attribute.
The answer forms a new relation which may be displayed to t h e user o r stored.
Since the join clause acts as qualification, a record (tuple) in either relation which has no corresponding t u p l e in t h e other relation does not participate in the result.
This is a different concept from the hierarchical and network models where the system retrieves all records from a master record and then retrieves corresponding records from a subfile.
This difference can cause anomalies with retrieval.
For example, in a pure relational system "List applicants and thei~ interviews" would be treated as "List applicants who have had interviews together with their interview information".
A h i e r a r c h i c a l or network DBMS would treat it as "List all applicants (whether or n o t they have been interviewed) plus any interview information that exists".
This second interpretation is more likely to be the correct one.
fiD. OVERALL NLI DESIGN There are several problems that affect the selection of a p p l i c a t i o n s for the NLI.
Some d a t a b a s e s and data m a n a g e ment systems may not be a p p r o p r i a t e targets for natural-language interfaces.
Some DBMS functions may be d i f f i c u l t to support.
It is important to have a clear understanding of these problems so that the NLI can mediate between the user view, as represented by the naturallanguage questions, and the underlying d a t a b a s e structure.
i. ~ Design C o n s i d e r a t i o n map q u e r i e s and t o explain problems to t h e u s e r when t h e m a p p i n g c a n n o t b e m a d e . However, there can be "reasonable" queries that cannot be answered d i r e c t l y because of the database structure.
Hierarchical DBMSs present the most problems with n a v i g a t i o n because access must start from the root.
For example, if the APPLICANT database were under an hierarchical DBMS, the q u e s t i o n "List t h e s p e c i a l t i e s for each applicant" could be answered directly but not "What are the specialties"? as there would be no way to get to the s p e c i a l t y records except via particular applicant records.
An array allows more than one instance of a field or set of fields in a single record.
There may be arrays of values or even arrays of sets of values in nonrelatlonal databases.
When the user retrieves a field that is an array the DBMS requires a subscript into the array.
Either the user must s p e c i f l y this s u b s c r i p t or the NLI must map to all members of the array with a test for missing data.
3. Class of DBMS to Supp%rt For any d a t a b a s e there are naturallanguage q u e s t i o n s that cannot be interpreted because the concepts involved lle outside the world of the database.
Questions can also involve structural complexity that is n o t r e p r e s e n t a b l e in the DBMS q u e r y language.
A p a r t i c u l a r l y difficult d e c i s i o n in the overall design of an NLI is the issue of where in the chain of events of processing a user's question into a DBMS q u e r y to trap these q u e s t i o n s and stop processing.
One approach is to decide that if a question is not meaningful to the world of the d a t a b a s e it should not be m e a n i n g ful to the NLI and, therefore, not analyzable on semantic grounds.
Another assumes that if the NLI can analyze a question that cannot be asked of the database, it has a much better chance of d e s c r i b i n g to the user what is wrong with the question and how it might be rephrased to get the desired information.
Codd made good use of the dialogue procedures of the RENDEZVOUS [CODD74] system to avoid questions that the DBMS could not handle, as well as avoiding g e n e r a t i o n of DBMS queries that did not represent the user's intent.
Such a system, however, requires a very large semantic base (much larger than that of the database) in order to make meaningful communication with the user during the dialogue.
2. Class of Database to Support For systems such as EUFID, the database must be organized within a data m a n a g d m e n t system so that the data is structured and individual fields are named.
If the data is just text, the EUFID approach cannot be used.
Current NLI systems are de s i g n e d to be used interactively by a user, which means that the DBMS should also have an interactive query language.
However, noc all data m a n a g e m e n t systems are interactive.
WWDMS [HONE76] has a user query language, b u t queries are entered into a batch job queue and answers may not return for many minutes.
If an Nil front end is to be added to such a DBMS, i~ must have the capability to generate query programs without any access to the database for parsing or for processing the returned answer.
The query language should support operations equivalent to the relational o p e r a t i o n s of select, project, and join.
Also, the query language should support some arithmetic capability.
Most have aggregate functions such as SUM and COUNT.
WWDMS does not have an easy-touse average operation, but it does have a procedural language with arithmetic operators so that EUFID can produce a "query" that p r o c e d u r a l l y calculates an average.
Basic c a l c u l a t i o n s should be supported such as " a g e = t o d a y b i r t h d a t e " . It is also d e s i r a b l e to be able to call special functions to do complex c a l c u l a t i o n s Some databases are simply not good candidates for an NLI because of characteristics mentioned in previous sections such as many retrieve-only fields, or domains that have a high update rate but cannot be recognized by a pattern.
There are also some structural problems chat must be recognized.
If the database contains "flat" files about one basic entity, it is reasonably easy to fisuch as required in navigational calculations a naval database.
the input standardize must be values, controlled to Support for Metadata Metadata is data about the data in the database.
It would be able to tell the user of the METRO application, for example, the kind of information the database has for warehouses and other entities in the application.
Such metadata might be extensions of active integrated data dictionaries now available i n some DBMSs.
I n an a p p l i c a t i o n l e v e l system the user should be able to query the metadata to learn about the structure of the database.
A different mode, such as the menus used by the EUFID help system, could be used to access metadata, or English language questions to both meta information and the database could be supported.
there should be few fields than have values that change rapidly, cannot be recognized by a pattern, and that must be used in qualification, the users of the NLI should have a common use for the data and a common vlew of the data, and there must be some user who understands the questions that will be asked and is available to work with the d e v e l o p e r s of the NLI.
Updates Some potential users would like a n a t u r a l l a n g u a g e interface to include the capability to update the database.
Currently, updating through any high level view of the database should be avoided, especially when the view contains joins or derlve4 data, because of the risk of inadvertently entering incorrectly-interpreted data.
SUMMARY AND CONCLUSIONS We believe that current system development is limited by the need for good semantic modelling techniques and the length of time needed to build the knowledge base required to interface with a new application.
When the knowledge base for the NLI is developed, the database as well as sample input must be considered in the design.
Parsing of questions to a database cannot be divorced from the database contents since semantic interpretation can only be determined in the context of that database.
On the other hand, a robust system cannot be developed by considering only database structure and content, because the range of the questions allowed would not accurately reflect the user view of the application and also would not account for all the information that is inferred at some level.
For many years, researchers have been attempting to build robust systems for natural-language access to databases.
It is not clear that such a system exists for general use [0SI79].
There are problems that need to be solved on both the front end, the parsing of the English question, and the back end, the translation of the question into a data management system query.
It is important to understand the types of requests, types of functions, and types of databases that can be supported by a specific NLI.
Some general guidelines that can be applied to the selection of applications for current NLI front ends are suggested below: lo the underlying DBMS should interactive query language, have an the DMS view should be relational or at least support multiple access paths, the database arrays either tures, should not contain of values or of strucACKNOWLEDGEMENTS We would like to acknowledge the many people who have contributed to EUFID development: David Brill, Marilyn Crilley, Dolores Dawson, LeRoy Gates, Iris Kameny, Philip Klahr, Antonio Leal, Charlotte Linde, Eric Lund, Fillp Machi, Kenneth Miller, Eileen Lepoff, Beatrice Oshika, Roberta Peeler, Douglas Pintar, Arie Shoshani, Martin Vago, and Jim Weiner.
REFERENCES [AHO72] Aho, A.
V. and J.
D. Ullman.
"The Theory of Parsing, Translation, and Compiling", Vol.
I: Parsing, Prentice-Hall, 1972, pp.
314-23Z. [BURGBff] Burger, J.
F . "Semantic Database Mapping in EUFID", Proceedings of the 198Z ACM/SIGMOD Conference, ~3"n~-a'-o~caY-'-Ca~., May 14-16, 198ff.
[BURG82] Burger, J.
F. and Marjorie Templeton.
"Recommendations for an Internal Input Language Eor the Knowledge-Based System', System Development Corporation internal paper N-(L)-24890/021/00, January 5, 1982.
[CODD74] Codd, E.
F., "Seven Steps to Rendezvous with the Casual User', Proc.
IFIP TC-2 Working Conference on Data-'5"~e'-~a~a~emen~ ~ystems, Car ~ gese, Corsica, April 1-5, 1974, in J.
W. Kimbie and K.
I. Koffeman (Eds.), "Data Base Management" North-Holland, 1974.
[CULL80] Cullinane Corporation, "IQS Summary Description", May 1980.
[DATE77] Date, C.
J., "An Introduction to Database Systems', second edition, Addison-Wesley Publishing, Menlo Park, CA, 1977.
[EDP82] "Query Systems for End Users", EDP Analyzer, Vol.
20, No.
9, September, 1982.
[HARR78] Harris, L.
R., "The ROBOT System: Natural Language Processing Applied to Data Base Query', Proceedings ACM 78 Annual Conference, 1978.
[HEND77] Hendrix, G.
G., E.
D. Sacerdoti, D.
Sagalowicz, and J.
Slocum, "Developing a Natural Language Interface to Complex Data" SRI Report 78-305, August 1977.
[HONE76] Honeywell, WWMCCS: World Wide Data Management System User's Guide, Honeywell DE97 Ray.3, April 1976.
[KELL71] Kellogg, C.
H., J.
F. Burger, T.
billet, and K.
Fogt, "The CONVERSE Natural Language Data management System: Current Status and Plans", Proceedings of the ACM SZmposium on :ntormation ~ ~ a n d ~etrleval-~, University o Maryland, College Park, MD, 1971, pp.
33-46. [MYLO76] Mylopoulos, J., A.
8 o r g i d a, P.
Cohen, N.
Roussopoulos, J.
Tsotsos, and H.
Wong, "TORUS: A Step Towards Bridging the Gap between Data Bases and the Casual User", in Information Volume 2 1976, Pergamon Press, pp 49-64.
[OLNE78] Olney, John, "Enabling EUFID to Handle Negative Expressions", SDC SP-3996, August 1978.
[OS179] Operating Systems, Inc., "An Assessment of Natural Language Interfaces for Command and Control Database Query", Logicon/OSI Division report for WWMCCS System 16 [SCHA77] Scha, R.
J. H., "Phillips Question-Answering System PHLIQAI", in SIGART Newsletter Number 61, February 1977, Association for Computing machinery, New York.
[SIMM65] Simmons, R.
F., "Answering English Questions by Computer -a Survey', Comm.
ACM 8,1, January 1965, 53-70.
[STON76] Stonebraker, M., et.
al., "The Design and Implementation of INGRES', Electronics Research Laboratory, College of Engineering, University of California at Berkeley, Memorandum No.
ERL-M577, 27 January 1976.
[TEMP79] Templeton, M.
P., "EUFID: A Friendly and Flexible Frontend for Data Management Systems", Proceedings of the 1979 National Conference Association of Computational Linguistics, August, 1979.
[TEMP80] Templeton, M.
P., "A Natural Language User Interface", Proceedings of "Pathwazs ~o System rn~ri%7", washington DYC.
C a h ~ o ACM, 1980.
[THOM69], Thompson, F.
B., P.
C. Lockemann, B.
H. Dostert, and R.
Deverill, "REL: A Rapidly Extensible Language System", in Proceedings of the 24th ACM National Conference, s~ociation--"~or Computing machinery, New York, 1969, pp 399-417.
[WALT77] Waltz, D.
L., " N a t u r a l Language Interfaces", in SIGART Newsletter Number 61, F e b r u a r y ' ~ 7, Association for Computing machinery, New York.
[WALT78] Waltz, D.
L., "An English language Question Answering System for a Large Relational Database", Communications of the ACM 21, 7(July 1978), pp 526-539.
[WOOD72] Woods, W.
A., R.
M. Kaplan, B.
Nash-Webber, The Lunar Sciences N a t u r a l L a n @ u a ~ e " r n f o r m a t i o n ' System ~'~Report, Report number~, Bolt, Beranek, and Newman, Inc., Cambridge, MA, 15 June 1972 .
INTRODUCING ASK, A SIMPLE KNOWLEDGEABLE SYSTEM Bozenn H.
Thompson F r e d e r i c k B.
Thompson California Inatitnce of Technology Pasadena, California 91125 ABSTRACT ASK, ~ ~ i m p l e K n o w l e d g e a b l e S y s t e m, i s a t o t a l system for the structuring, manipulation and communication of information.
It is a simple system in t h e sense thaC its development concentrated on c l e a n e n g i n e e r i n g solutions to w h a t c o u l d be d o n e now w i t h g o o d r e s p o n s e t i m e s. The user interface is a limited dialect of English.
In contrast to expert systems, in which experts build the knowledge base and users make u s e o f t h i s e x p e r t k n o w l e d g e, ASK i s a i m e d a t t h e u s e r who w i s h e s t o c r e a t e, test, modify, extend a n d m a k e u s e o f h i s own k n o w l e d g e b a s e . It is a s y s t e m for a research team, a m a n a g e m e n t or military staff, or a business office.
some h a v e t h e f o l l o w i n g n u m b e r a t t r i b u t e s : speed length beam >List the destinations a n d home p o r t o f each ship, ship destination home p o r t Ubu New York Naples Tokyo --Morn 0slo Tokyo Kittyhawk Naples Boston Boston -London This paper is designed to give you a feel for the general performance of t h e ASK S y s t e m a n d overview of its operational capabilities.
To Chin end, the movie you see will continue throughout the talk.
Indeed, the talk itself is a commentary on t h i s b a c k g r o u n d m o v i e . The m o v i e i s bona f i d e and in real time, i t i s o f t h e ASK S y s t e m i n action.
(Many o f t h e i l l u s t r a t i o n s from the movie are reproduced in the written paper.) I.
ASK AS A DATABASE SYSTEM A.
Examples o f ASK English To i n t r o d u c e a few examples you to ASK, we w i l l s t a r t o u t w i t h of queries of a simple data base The uninitiated user may wish London London N e w York --North Scar London New York gimitz London Norfolk Saratoga unknown Norfolk >What c i t i e s a r e t h e home p o r t s o f s h i p s whose d e s t i n a t i o n i s London?
Boston London New York Norfolk >Are t h e r e s h i p s t h a t do n o t h a v e a c a r g o ? yes >What i s t h e number o f New York s h i p s ? There are 2 answers: ( 1 ) New York ( d e s t i n a t i o n ) ships 2 ( 2 ) New York (home p o r t ) s h i p s 1 >How many s h i p s a r e t h e r e w i t h l n e g t h g r e a t e r t h a n 600 f e e t ? Spelling correction: " l n e g t h " to " l e n g t h " 4 >What ships t h a t carry wheat go to London or Alamo Oslo? ships that carry wheat London Maru Oslo Alamo >Does the Maru carry wheat and go co London? yes concerning ships.
simply to ask: >How many ships are there? 7 >What is known about ships? some are in the following classes: Navy freighter old S.
The ASK Data Structures A l t h o u g h in the t e r m i n o l o g y of data base theory, ASK can be considered as an "entityrelation" system, ASK retains its information in records w h i c h are interlinked in a s e m a n t i c net.
One reason we refer to ALE as simple is because ic uses only a few kinds of nodes in its s e m a n t i c tanker a l l have t h e f o l l o w i n g a t t r i b u t e s : destination home p o r t some have t h e f o l l o w i n g a t t r i b u t e s : cargo a l l have t h e f o l l o w i n g number a t t r i b u t e s : age 17 net, namely: fio Attributes Relations and the ctbvious c o r r e s p o n d i n g arcs.
We speak of this as the COAR structure.
A~tributes are single valued, e.g., "father", "home port", " t i t l e " ; relations may be m u l t i p l e valued, e.g., "child"~ "cargo", "author".
The d i f f e r e n c e between attributes and relations can be seen in the following p r o t o c o l . >What is the cargo and home port of the Maru? cargo home port wheat London >The home port of Maru is Boston.
London has been replaced by Boston as the home port of Maru.
>The cargo of Maru is coal.
coal has been added as the cargo of Maru.
>What i s the cargo and home port of the Maru? cargo home port wheat BosCon coal -->definition:long:paper whose number of pages e x c e e d s 49 Defined.
>definition:long:book whose number o f p a g e s e x c e e d s 800 Defined.
>What AI bibliography i t e m s a r e long?
There are 2 answers: (1) long:paper whose number of pages exceeds 49 Physical Symbol Systems A General Syntactic P r o c e s s o r (2) long:book whose number of pages exceeds 800 Human Problem Solving >What long books were written in 19727 long:book whose number of pages exceeds 800 Human Problem Solving Family relationships make for a g o o d illustration of definitions; we switch to a small family relationship context.
>What are attributes? individual/individual attributes : spouse >What are relations? individua I/individual relations : parent >What are classes? individual classes : male female >What are definitions? definition:mother :female parent definition: father :male parent definition:child:converse of parent definition:sibling:child of parent bur not oneself definition'cousin:child of sibling of parent >List the father and mother of each of Billy Smith's cousins.
Billy Smith's cousins father mother Baby Boyd R o b e r t Boyd J i l l Boy C.
Extendin K and Hodifyin~ I.
Definitions t h e Dat~ To make such a system more knowledgeable, one needs to be able co add d e f i n i t i o n s that e m b o d y interrelationships a m o n g the basic classes, objects, a t t r i b u t e s and relations of the data.
The simplest form of definition is synonym: >definition:tub:old Defined.
ship Although this form of definition allows one to introduce abbreviations and many forms of jargon, more extensive forms of definition are desirable.
Here are three illustrations using the same "ship" file as above.
In the third definition, note the use of quotes to create local '~ariables".
>definition:area:length * beam Defined >List the length, beam and area of each tub.
tub length beam area foot foot foot**2 Ubu 231.667 48 11120.016 Alamo 564.5 84 47418.
>definition:meter:39.37 * (foot / 12) Defined.
>beam of the Alamo squared in square meters? 655.526472343 square meters >definition:longest "ship":"ship" whose length is the maximum length of "ship"s Defined.
>What is the length in meters of the longest ship whose home port is Naples? 121.920243840 meters T h e n o t i o n of w h a t is l o n g m a y be q u i t e different in another context, say in the context of b i b l i o g r a p h y of a r t i f i c i a l intelligence literature.
18 2.
Verbs Most verbs e m b o d y k n o w l e d g e specific to the application in which they are used, the exceptions being the copula verbs.
Therefore the only verbs initially known to the ASK System are "to be" and "to have".
The user c a n add n e w v e r b s by paraphrase.
>verb:ships "go" to New York:destination of ships is New York Defined.
>verb:ships "carry" coal from London to Boston:ships have coal as cargo, have L o n d o n as home port and go to Boston Defined.
>Each old ship carries what cargo to each port? old ship port cargo Ubu New York oil Tokyo oil Alamo London wheat coal fi>What i s c a r r i e d by t h e Alamo? wheat coal >Wheat i s c a r r i e d to London from what p o r t s ? New York >What c i t i e s does t h e Alamo c a r r y wheat to?
London Pronouns and Ellinses >Create t h e a t t r i b u t e : r a t i n g The a t t r i b u t e r a t i n g h a s been a d d e d . >Create i n d i v i d u a l s : s e m i n a l, e x c e l l e n t, f a i r and i m p o s s i b l e The f o l l o w i n g i n d i v i d u a l s have been added: seminal excellent fair impossible >The r a t i n g o f W i n o g r a d ' s 1980 p a p e r i n Cognitive Science is excellent.
e x c e l l e n t h a s been added a s t h e r a t i n g o f W i n o g r a d ' 8 1980 p a p e r in C o g n i t i v e S c i e n c e >Rating o f A Framework f o r R e p r e s e n t i n g In p r a c t i c a l s y s t e m s f o r e x p e r t s, a b b r e v i a t e d f o r m s of a d d r e s s i n g t h e c o m p u t e r a r e common.
Thus the ability to h a n d l e p r o n o m i n a l and e l l i p t i c a l constructions are of considerable importance.
A l t h o u g h t h e r e has been p r o g r e s s i n t h e l a s t few years in the linguistic understanding of these c o n s t r u c t i o n s, many d i f f i c u l t i e s r e m a i n . However, b u i l d i n g on t h e work t h a t ham been a c c o m p l i s h e d, many o f t h e s e c o n s t r u c t i o n s can be h a n d l e d by t h e ASK System.
In o r d e r to a v o i d m i s l e a d i n g t h e u s e r when t h e c o m p u t a t i o n a l a l g o r i t h m does not make t h e c o r r e c t i n t e r p r e t a t i o n, echo i s u s e d t o i n f o r m t h e u s e r of t h e i n t e r p r e t a t i o n t h a t h a s been t a k e n . >Is t h e r e a s h i p whose d e s t i n a t i o n i s unknown? yes >What is it?
What is it [ship whose destination is ..knov.] ? Saratoga >Is its cargo wheat or c o a l ? I s i t s [ s h i p whose d e s t i n a t i o n i s unknown] cargo wheat or c o a l ? wheat no coal yes >Does t h e A l a m o ' s l e n g t h exceed t h a t of t h e Ubu? yes >What about t h e N o r t h e r n S t a r ? [the Northern Star?] does t h e A l a m o ' s l e n g t h exceed t h a t of t h e Northern Star? no >Is i t s cargo wheat?
Is its [Northern Star] cargo wheat? no >Is it that of the Ubu?
Is i t [Norther, Star cargo] that [cargo] of the Obu? yes >Wheat is carried by how many ships? 2 >Coal and oil? c o a l and o i l i s c a r r i e d by how many ship*? coal I oil 2 >by the Ubu? c o a l and o i l i s c a r r i e d by t h e Ubu? coal no oil yes Knowledge i s i m p o s s i b l e . i m p o s s i b l e h a s been added as t h e r a t i n g o f A remework f o r R e p r e s e n t i n g Knowledge.
>Scripts, P l a n s, Goals and U n d e r s t a n d i n g ' s rating is se~.nal.
s e m i n a l h a s been added a s t h e r a t i n g o f S c r i p t s, P l a n s, Goals and U n d e r s t a n d i n g . >The r a t i n g o f M i n s k y ' s paper i n The P s y c h o l o g y of Computer V i s i o n is seminal.
i m p o s s i b l e has been r e p l a c e d by s e m i n a l as t h e r a t i n g o f M i n s k y ' s p a p e r i n The P s y c h o l o g y o f Computer V i s i o n . > L i s t t h e a u t h o r of e a c h paper t h a t h a s a r a t i n g o f e i t h e r s e m i n a l or e x c e l l e n t . paper S c r i p t s, Plans, Goals and Understanding R.
C. Schauk A Framework f o r R e p r e s e n t i n g Knowledge M.
Minsky What Does I t Mean to U n d e r s t a n d N a t u r a l Language To Winograd > r e l a t i o n : keyword The r e l a t i o n keyword h a s been added.
>individuals:context, natural language, knowledge r e p r e s e n t a t i o n The f o l l o w i n g i n d i v i d u a l s have been added: c o n t e x t n a t u r a l l a n g u a g e knowledge representation >Context and knowledge r e p r e s e n t a t i o n a r e k e ~ o r d a of S c r i p t s, P l a n s, Goals and U n d e r s t a n d i n g and A Framework f o r R e p r e s e n t i n g Knowledge.
c o n t e x t knowledge r e p r e s e n t a t i o n have been added as keywords o f S c r i p t s, P l a n s, Goals and U n d e r s t a n d i n g . c o n t e x t knowledge r e p r e s e n t a t i o n have been added as keywords o f A Framework f o r R e p r e s e n t i n g Knowledge.
> N a t u r a l l a n g u a g e i s a keyword of S c r i p t s, P l a n s, Goals and U n d e r s t a n d i n g . n a t u r a l l a n g u a g e h a s been added a s keyword of Scripts, Plans, Goals and Understanding.
>definition:about "booka'*:converse of keyword of "books" Defined.
>What AI Sibliosraphy items are about knowledge r e p r e s e n t a t i o n ? S c r i p t s, P l a n s, Goals and U n d e r s t a n d i n g A Framework f o r R e p r e s e n t i n g Knowledge D.
Inheritanc~ 4.
E x t e n d i n 2 th_...eeVocabular 7 an...~dDa~a Bas~ New o b j e c t s, c l a s s e s, a t t r i b u t e s and r e l a t i o n s may be a d d e d a t w i l l . T h i s w i l l be i l l u s t r a t e d using the AI Bibliography.
19 Classes, as well as objects, may be t h e a r g u m e n t s or v a l u e s o f a t t r i b u t e s and r e l a t i o n s . T h i s r a i s e s t h e q u e s t i o n of i n h e r i t a n c e of properties and attributes.
The a n s w e r is to be fifound in the following protocol, using attribute of color; inheritance for relations h a n d l e d i n s i m i l a r ways.
>class: r a c i n g sloop, sports b o a t, Norwegian The following classes have been added: racing sloop sports boat Norwegian >attribute: color The a t t r i b u t e c o l o r h a s been a d d e d . >individuals: white, blue, b l a c k, g r a y The following i n d i v i d u a l s have b e e n added: white blue black gray >Saratoga is a racing sloop.
S a r a t o g a has b e e n added to r a c i n g s l o o p . >Racing s l o o p s a r e s p o r t s b o a t s . r a c i n g s l o o p s have b e e n added to s p o r t s boats.
>Alamo is industrial and Norwegian.
Alamo has b e e n added to i n d u s t r i a l Norwegian.
>The c o l o r o f s p o r t s b o a t s i s w h i t e . w h i t e h a s b e e n added a s c o l o r o f s p o r t s boats  >Maru's color is gray.
g r a y has b e e n added a s c o l o r o f Maru.
>Black i s t h e c o l o r of i n d u s t r i a l . b l a c k has b e e n added as t h e c o l o r o f industrial.
>Blue is the color of Norwegian.
b l u e h a s b e e n added a s t h e color of Norwegian.
>What i s the color of each s h i p ? There are 3 answers: (t) ship color Maru gray Saratoga white (2) Alamo as industrial ship color Alamo black (3) Alamo as Norwegian ship color Alamo blue the is There are 2 answers: ( i ) T h e r e a r e no p o r t s ( 2 ) T h e r e a r e no U.S. d e s t i n a t i o n s o f Maru.
> I s some E u r o p e a n p o r t a p o r t o f Maru?
The f o l l o w i n g word i s n o t i n t h e v o c a b u l a r y : portof Correction: Is some European port a port of Maru?
There is no port.
>London is Alamo's port.
London h a s b e e n added a s t h e p o r t o f Alamo.
> I s som E u r o p e a n p o r t a p r o t o f Maru?
Spelling corrections: "son" to "some" "prot" tO " p o r t " T h e r e i s no p o r t o f Maru.
>New York i s M a r u ' s p o r t . New York h a s b e e n added a s p o r t o f Maru.
> I s some E u r o p e a n p o r t a p o r t o f Maru.
I s some E u r o p e a n p o r t a p o r t o f Maru? no II.
INTEGRATION OF MULTIPLE OBJECT TYPES A.
Extension of COAR ~o Multiple Object Types So far we have i l l u s t r a t e d ASK using only two types of objects: capabilities individuals, e.g., "John Jones", "Maru" numbers, e.g., "34.6 feet", "length of Maru", "number of ships".
ASK has been designed, however, to facilitate many kinds of objects.
This is a capability orthogonal to the simple COAR s t r u c t u r e in that for any types of objects there m a y be c o r r e s p o n d i n g classes, attributes and relations.
B. An Examvle: E.
DiaKnostics Our objectives in the design respond to the experienced user: o o of ASK are to with the responses s/he desires in at l e a s t 90% o f t h e c a s e s ; w i t h a r e s p o n s e that includes what s/he desires, but perhaps additional information also, in 90% of t h e remaining cases; with a response, although oot the desired one, but one which clearly indicates the system's interpretation of the request in the remaining cases.
We will illustrate this m u l t i p l e object type capability with the additional object type: text.
Once this new object type was added (together with p r o c e d u r e s t o m a n i p u l a t e texts, i.e., a "word processor") then text classes, individual/text and text/individual a t t r i b u t e s and r e l a t i o n s w e r e immediately available.
It was a small task to add an e l e c t r o n i c mail s y s t e m to ASK; all that w a s required was an a d d i t i o n to the a u t h o r i z a t i o n procedure that assigned to each newly authorized person a new text class as his/her mail box.
>Create the t e x t / t e x t relation:comment The t e x t / t e x t r e l a t i o n co-~.ent has been added.
>Create a file: budget correspondence The text class budget correspondence has been added.
>File the Budget Proposal in budget correspondence.
Budget Proposal has been added to the text class budget correspondence.
>Who have I received mail from?
J o h n Smith Peter Jackson 20 This third objective calls for good diagnostics.
The diagnostic capabilities of ASK are illustrated in the following protocol.
>attribute:port The a t t r i b u t e p o r t has been added.
>What a r e t h e p o r t s and U.S. d e s t i n a t i o n s t h e Maru? of fi> D i s p l a y my m a i l f r o m J o h n S m i t h . F r e d, Your B u d g e t P r o p o s a l l o o k s good t o me.
S u g g e s t you s u b m i t i t a s i s . John > F i l e i t i n my b u d g e t c o r r e s p o n d e n c e f i l e . F i l e i t [my m a i l f r o m J o h n S m i t h ] i n my budget correspondence file.
my m a i l f r o m J o h n S m i t h h a s b e e n a d d e d co the text class budget correspondence.
>amm-va i t f r o m my m a i l b o x . Bemove i c [my m a i l f r o m J o h n S m i t h ] f r o m my m a i l box.
my m a i l f r o m J o h n S m i t h h a s b e e n r e m o v e d from your mail.
> C r e a t e a b u d g e t c o r r e s p o n d e n c e named Budget Plans Please e n t e r t e x t : S t a f f l e v e l b u d g e t m e e t i n g on Wed.
a t 3 i n Tom's o f f i c e . P l e a s e s e n d me y o u r c o m m e n t s b e f o r e t h e m e e t i n g ; f i l e t h e m a s "commenCe on B u d g e t P l a n e " . \ Budget Plane class budget >Mail Budget Budget plane manager.
h a s b e e n a d d e d Co t h e t e x t correspondence.
Plans to each section manager.
h a s b e e n s e n t to e a c h s e c t i o n Ill.
MORE GENERAL ASPECTS OF THE ASK SYSTEM A.
R e s v o n s e Times The movie, which accompanied the oral presentation of this paper, demonstrated that the response rime, i.e., the time between completion of t h e t y p i n g of t h e i n p u t by t h e u s e r Co t h e appearance of t h e r e s p o n s e on t h e t e r m i n a l, is very good.
But the data bases used in the illustrations have been small, Coy d a t a b a s e s . The f o l l o w i n g t a b l e g i v e s a v e r a g e r e s p o n s e t i m e s for a few cases using larger data bases.
The query used for this illustration is: >What arm t h e d e s t i n a t i o n s of tankers?
The r e s p o u s e t i m e i s r a t h e r i n s e n s t i t i v e to the Coral number of individuals, classes, attributes and relations in the data base, depending primarily on the size of the relation (destination) and i t s a r g u m e n t ( C a n k e r s ) . Suppose t h a t t h e r e a r e m t a n k e r s i n t h e d a t a b a s e and t h a t n individuals have destinations, i.e., the size of the destination relation i s n.
T h e t a b l e g i v e s time in seconds.
no. of tankers > D i s p l a y t h e commence on B u d g e t P l a n s by e a c h section manager.
D i s p l a y i n g commence on B u d g e t P l a n s by e a c h section manager: J o h n Dobbs: D ( i s p l a y ), S(kip), o r Q(uit): 2 2 a 9 dastinscions I I I I C.
A d d i n z New O b i e c t T y ~ e s A l t h o u g h t h e ASK S y s t e m h a s b e e n d e s i g n e d t o allow the addition o f new o b j e c t t y p e s, t h i s c a n be d o n e o n l y by a n a p p l i c a t i o n programmer.
The major obstacle is the necessity to provide a procedure to initialize instances of the new object type and procedures that carry out their intrinsic manipulation.
However, we expect the addition of new object types to be a c o m m o n occurrence in the applications of the ASK System.
In any potential applicaion areas, using groups have accumulations of data already structured in specific ways and families of procedures that they have developed to manipulate these structures.
In ASK, they can identify these data structures as a new object type, design simple syucax for them to invoke their procedures, and thus embed their familar objects and manipulations within the ASK English dialect and within the same context as other associated aspects of their tasks.
The class, attributed and relation constructions become immediately available.
B e s p o n e e Time i n S e c o n d s f o r : >What a r e t h e d e s t i n a t i o n s of tankers?
B. The C q n c e v t o f A Use ~ C o n t e x t an_...dd the Basing Ooeration In the t e r m i n o l o g y of ASK, a user "Context" is a knowledge base together with the vocabulary and definitions that S o with it.
A given user will usually have several Contexts for v a r i o u s purposes, some of which may be the small "Ships" Context, a (truncated) bibliography of Artificial Intelligence literature and an a d m i n i s t r a t i v e Context concerning budget matters.
When one initiates a session with the ASK System, one is initially in the Command Context: >Welcome to ASK Please identify yourself.
>Fred >Pass word: You have mail.
Fred is in COMMAND, proceed.
At this point, you can list the Directory of Contexts available to you, create or delete Contexts, authorize others to use Contexts which you have created, and enter any of the Contexts in 21 fi>Directory context BASE Ships AI Bibliography Family Management Matters creator MASTER Fred Fred Fred Fred enter no yes yes yes yes b~s~ yes yes yes yes yes >enter Management Matters You are in Management Matters, proceed.
>Who have I received mail from?
Peter Jackson John Dobbs A new C o n t e x t is c r e a t e d by basing it on an already existing one.
Consider a u s e r who h a s b e e n a u t h o r i z e d f o r b a s i n g on t h e AI B i b l i o g r a p h y Context illustrated a b o v e and who w a n t s t o b u i l d a w i d e r b i b l i o g r a p h y C o n t e x t ( a d d i n g new i n f o r m a t i o n --vocabulary, data and definitions), however, without disturbing the old one.
To do s o, a l l s / h e n e e d s t o do i s s e l e c t a new n a m e, s a y CS B i b l i o g r a p h y, and t y p e : >exit You a r e >Base CS The new created >individual: E x p e r i e n c e w i t h ROBOT, L.
H a r r i s The f o l l o w i n g i n d i v i d u a l s have been added: E x p e r i e n c e w i t h ROBOT L.
H a r r i s >The a u t h o r o f E x p e r i e n c e w i t h ROBOT i s L.
Harris. L.
H a r r i s h a s b e e n a d d e d a s a u t h o r o f E x p e r i e n c e w i t h ROBOT.
>Keyword o f E x p e r i e n c e w i t h ROBOT i s d a t a b a s e . database has been added as keyword of E x p e r i e n c e with ROBOT.
>Who wrote what about databases? author D.
L. W a l t z N a t u r a l L a n g u a g e A c c e s s t o a Large Data Base L.
H a r r i s E x p e r i e n c e w i t h ROBOT > e x i t t o CB B i b l i o g r a p h y, You a r e i n CS B i b l i o g r a p h y, proceed.
>Who w r o t e w h a t a b o u t d a t a b a s e s ? author D.
L. W a l t z N a t u r a l L a n g u a g e A c c e s s t o a Large Data Base C.
J. D a t e An I n t r o d u c t i o n to Database Systems L.
H a r r i s E x p e r i e n c e w i t h ROBOT Several C o n t e x t s can be based on a g i v e n one, and one C o n t e x t can be b a s e d on several, thus a hierarchical structure of Contexts can be realized.
All Contexts are directly or indirectly based upon the BASE Context, w h i c h c o n t a i n s the f u n c t i o n words and g r a m m a r of the ASK d i a l e c t of English, the mathematical and statistical capabilities, and the word processor.
i n COMMAND, p r o c e e d . Bibliography on AI Bibliography context CS Bibliography has been b a s e d on AI B i b l i o g r a p h y basing action is t h i s new C o n t e x t : a ne w The r e s u l t of this Context.
Upon e n t e r i n g > E n t e r CS B i b l i o g r a p h y You a r e in CS Bibliography, one c a n make a d d i t i o n s : proceed.
C. T~anspo~tabilitv It is easy and fast to apply ASK to a n e w domain, given that a data base for this new domain is a v a i l a b l e in m a c h i n e r e a d a b l e form.
The vehicle is t h e ASK dialogue-driven Bulk Data Input capability, w h i c h can be called upon to build an existing database into one's Context.
The result not only i n t e g r a t e s this n e w data w i t h that already in the C o n t e x t and under the ASK d i a l e c t of English, but in m a n y c i r c u m s t a n c e s w i l l make the use of this data m o r e r e s p o n s i v e to users" > i n d i v i d u a l s :An I n t r o d u c t i o n to Database S y s t e m s, C.
J . D a t e The f o l l o w i n g i n d i v i d u a l s h a v e b e e n a d d e d : An I n t r o d u c t i o n to D a t a b a s e S y s t e m s C.
J . D a t e >The a u t h o r o f An I n t r o d u c t i o n to Database S y s t e m s i s C.
J . D a t e . C.
J . D a t e h a s been added a s a u t h o r of An Introduction to Database Systems.
>Keyword o f An I n t r o d u c t i o n t o D a t a b a s e Systems is database.
d a t a b a s e h a s been a d d e d a s k e y w o r d o f An Introduction to Database Systems.
>Who w r o t e w h a t a b o u t d a t a b a s e s ? author D.
L. W a l t z N a t u r a l L a n g u a g e A c c e s s t o a L a r g e D a t a Base C.
J. D a t e An I n t r o d u c t i o n to D a t a b a s e Systems These additions to the CS B i b l i o g r a p h y would not, of c o u r s e, e f f e c t She AI B i b l i o g r a p h y Context.
However, a d d i t i o n s and m o d i f i c a t i o n s that are subsequently made in the AI Bibliography Context would automatically be reflected in the CS Bibliography.
>exit You are in COMMAND, proceed.
>Enter AI Bibliography You are in AI Bibliography, proceed.
22 needs.
The Bulk Data Input D i a l o g u e p r o m p t s the user for n e c e s s a r y i n f o r m a t i o n to (i) e s t a b l i s h t h e physical structure of the d a t a b a s e to be included, (2) add necessary classes and attributes as needed for the new data entries.
The user also indicates, using E n g l i s h c o n s t r u c t i o n s, the i n f o r m a t i o n a l r e l a t i o n s h i p s a m o n g the fields in the p h y s i c a l records of the d a t a b a s e file that s/he wishes carried over to the ASK Context.
IV. DIALOGUES IN ASK Some have raised the question whether natural language is always the most desirable medium for a user to c o m m u n i c a t e w i t h the computer.
Expert systems, for example, have tended to use computer guided dialogues.
One simple form such a dialogue fimight take is illustrated by t h e f o l l o w i n s in w h i c h a new e n t r y i s a d d e d t o t h e AI B i b l i o g r a p h y : >New b i b l i o g r a p h y i t e m >Add to what b i b l i o g r a p h y ? AI B i b l i o g r a p h y >Title: Natural Language Processing >Author: Harry Tennant >Keyword: n a t u r a l l a n g u a g e >Keyword: s y n t a x p r o c e s s i n g >Keyword: s p e e c h a c t s >Keyword: Natural Language Processing has been added t o AI B i b l i o g r a p h y . >Title: The "new b i b l i o g r a p h y item" dialogue i8 completed.
>What A I B i b l i o g r a p h y items were written by Harry Tennant?
E x p e r i e n c e with the Evaluation of Natural Language Question Answerers Natural Language Processing necessary, respond with a diagnostic, (2) f i l l in other fields with data developed from the knowledge base, (3) extend the knowledge base, adding to the vocabulary and adding or changing the data itself, (4) file the completed form in p r e s c r i b e d f i l e s o r i n t h o s e i n d i c a t e d by t h e u s e r and a l s o m a i l it t o a s p e c i f i e d d i s t r i b u t i o n list through the electronic mail subsystem.
Since the Form p r o c e s s i n g c a n c h e c k c o n s i s t e n c y and m o d i f y the knowledge base, Forms can be used to facilitate data input.
S i n c e Form p r o c e s s i n g c a n fill f i e l d s in t h e Form, the forms c a p a b i l i t y includes the functions of a report generator.
L e t t e r s and memos c a n be written a s s p e c i a l c a s e s of Form filling, automatically adding dates, addresses, etc.
and filing and dispatching the result.
It must be easy and natural to add new Forms, if they are to be a convenient tool.
That is the function of the Forms Designing Dialogue.
Much like the Bulk Data Input Dialogue, the Forms Designing Dialogue holds a dialogue with the the user through which s/he can specify the fields of the Form itself and the processing of the above k i n d s t o be a u t o m a t i c a l l y accomplished at the time the Form is filled in.
Here is a simple example of a from that was designed using the Forms Designing Dialogue.
>What i s t h e bona p o r t and c o ~ a n d e r old ship?
There are 2 answers: (i) The~e is no c o ~ . n d e r . of each Other alternative media for user/system communication are menu boards, selection arrays and q u e r y by e x a m p l e . Many o t h e r c r y p t i c w a y s to communicate user needs to a knowledgeable system c a n be t h o u g h t o f ; o f t e n t h e m o s t u s e f u l m e a n s will be highly specific to the particular application.
For e x a m p l e, i n p o s i t i o n i n g c a r g o i n t h e h o l d o f a s h i p, o n e w o u l d l i k e t o be a b l e t o display the particular cargo space, showing its current cargo, and call for and move into place o t h e r i t e m s t h a t a r e to be i n c l u d e d . In the past, enabling the system to respond more intelligently to the user's needs required the provision of elaborate programs since the u s e r ' s t a s k s m a y be q u i t e i n v o l v e d, w i t h c o m p l e x decision structures.
The introduction of terse, effective communication has incurred lout delays and thus the changing needs of a user had little c h a n c e o f b e i n g m e t . I n t h e ASK S y s t e m, t h e u s e r s themselves can provide this knowledge.
They c a n i n s t r u c t t h e system on how to e l i c i t the necessary i n f o r m a t i o n and how to c o m p l e t e t h e r e q u i r e d t a s k . This ASK capability is quite facile, opening the way f o r i t s u b i q u i t o u s use in extending the knowledgeable responsiveness of the computer to user's immediate needs.
ASK i n c l u d e s two s y s t e m guided dialogues, similar to the Bulk Data Input d i a l o g u e by w h i c h u s e r s c a n i n s t r u c t t h e S y s t e m on how to be more r e s p o n s i v e t o t h e i r n e e d s . A.
Forms Desi~nin2 Dialogue The Form is an efficient means of communication with which we are all familiar.
A number of computer systems include a Forms package.
For most of these, however, filling in a Form results only in a document; the Form does not constitute a medium for interacting with the knowledge base or controllin K the actions of the system.
The ASK Forms capability enlarges the roles and ways in which Forms can be used as a m e d i u m for user interaction.
As the user fills in the fields of a Form, the System can make use of the information being supplied to (1) check its consistency with the data already in the k n o w l e d g e base and, if old ship hone port Ubu Naples Alamo London >Who i s J o h n S m i t h ? The f o l l o w i n s w o r d s a r e n o t i n t h e v o c a b u l a r y : John Smith > I n v e n t o r y o f wheat and c o r n o i l ? w h e a t and c o r n o i l i n v e n t o r y wheat 86.7 corn oil 123~00.
Note that the home port of the Alamo is London and that it does not have a commander, further that John Smith is not known to the System.
>Fill s h i p p i n g (For the purposes of the published paper, in contrast to the film shown at the presentation of the paper, only the initial and final copies of the form are given, under~ines indicate fields filled in by the "user", the o t h e r f i e l d s automatically being filled by the System).
(before) Shipping Form ship: port: quantity item $ price $ total commander: 23 fi(after) S h i p p i n g Form ship: port: A;amQ London item whvac corn oi~ J@hn SmiC~ price $ 35.75 $ 2.50 total $ 107.25 $1250.00 quantity ! 500 colmander: natural language programming capabiltty.
We hasten to add that it is not a general purpose program environment.
It is for "ultra-high" level programming, gaining its programming efficiency t h r o u g h t h e a s s u m p t i o n o f an e x t e n s i v e v o c a b u l a r y and knowledge base on which it can draw.
The illustrative d i a l o g u e a b o v e, w h i c h a d d s ' a new i t e m to a bibliography, is an example of a simple d i a l o g u e d e s i g n e d u s i n g DDD.
V. ACKNOWLEDGEMENTS AND CURRENT STATUS Shipping List for Alamo has been filed in Shipping Invoice File.
S h i p p i n g L i s t for Alamo h a s b e e n m a i l e d to J o n e s . mail t o : Fill shipping has been completed.
> L i s t t h e home p o r t and co-w,a n d e r o f e a c h old ship.
old ship home p o r t commander Ubu Naples -Alamo London John Smith >Inventory of wheat and corn oil? w h e a t and c o r n o i l i n v e n t o r y wheat 83.7 c o r n oil 122900.
>What is in the Shipping Invoice File?
Shipping List for Alamo The three System guided dialogues, Bulk Data Input, Dialogue Designing Dialogue and Forms D e s i g n i n g D i a l o g u e, are f r o m the d o c t o r a l dissertation of Tai-Ping Ho.
The aspects of ASK c o n c e r n i n g b a s i n g o n e C o n t e x t on a n o t h e r a r e f r o m the doctoral dissertation o f K w a n 8 I Yu.
The methods for handling anaphora, fragments and correction of inputs are from the doctoral dissertation of David Trawick.
ASK is implemented on the Hewlett Packard HP9836 desktop computer.
To handle Contexts of r e a s o n a b l e s i z e, one n e e d s a b a r d d i s k . An HP9836 with an HP9725 disk was used in the illustrations in this paper.
Our work is supported by the Hewlett Packard Corporation, Desktop Computer Division.
B. DialoKue Desi~nin~ Dialogue In the day-by-day use of an interactive system, users are very often involved in repetitive tasks.
They c o u l d be r e l i e v e d o f much o f t h e d r u d g e r y o f such tasks if the system were more knowledgeable.
Such a knowledgeable system, as it goes about a t a s k f o r t h e u s e r, may need a d d i t i o n a l information from the user.
What information it needs aCa particular point may depend on earlier user inputs and the current state of the database.
The user must provide the system with knowledge of a particular cask; more precisely s/he must program this knowledge into t h e system.
The result of this programming will be a system guided dialogue which the user can subsequently initiate and which will then elicit the necessary inputs.
Using these inputs in c o n j u n c t i o n w i t h the knowledge already available, particularly the data base, the system completes the task.
It is this system-guided dialogue chat the user needs to be able to d e s i g n . In the ASK System, there is a special dialogue w h i c h can be used co d e s i g n s y s t e m g u i d e d dialogues Co accomplish particular casks.
We call this the Dialogue Designing Dialogue (DDD).
Using DDD, the user becomes a computer-aided designer.
Since DDD, in conducting its dialogue with the user, only requires simple responses or responses phrased in ASK English, the user need have little programming skill or experience.
Using DDD, the user alone can replace a tedious, repetitive cask with an efficient system guided dialogue, all in a natural language environment.
The ASK Dialogue Designing Dialogue constitutes a high level, 24
A Robust P o r t a b l e N a t u r a l L a n g u a g e D a t a B a s e I n t e r f a c e Jerrold M.
Ginsparg Bell Laboratories Murray Hill, New Jersey 07974 A BSTRA C T This paper describes a NL data base interface which consists oF two parts: a Natural Language Processor (NLP) and a data base application program (DBAP).
The NLP is a general pur!~se language processor which builds a formal representation of the meaning of the English utterances it is given.
The DBAP is an algorithm with builds a query in a augmented relational algebra from the output of the NLP.
This approach yields an interface which is both extremely robust and portable.
where "colored", "'color" and "'house" are system primitives called concepts.
Each concept is an extended case frame, [Fillmore 2].
The meaning of each concept to the system is implicit in its relation to the other system concepts and the way the system manipulates it.
Each concept has case preferences associated with =IS cases.
For example, the case preference of color is "color and the case preference of coloredis "physical-object.
The case preferences induce a network among the concepts.
For example, "color is connected to "physical-object via the path: ['physical-object colored'colored color "color].
In addition.
"color is connected to "writing,implement, a refinement ot" "physicalobject, by a path whose meaning is that the writing implement writes with that color.
This network is used by the NLP to determine the meaning of many modifications, For example, "red pencil" is either a pencil which is red or a pencil that writes red, depending on which path is chosen.
In the absence of contextual information, the NLP chooses the shortest path.
In normal usage, case preferences are often broken.
The meaning of the broken preference involves coercing the offending concept to another one via a path in the network.
Examples are: "Turn on the soup".
"Turn on the burner that has soup on it".
"My car will drink beer".
"The passengers in my car will drink beer" This paper describes an extremely robust and portable NL data base interface which consists of two parts: a Natural Language Processor (NLP) and a data base application program (DBAP).
The NLP is a general purpose language processor which builds a formal representation of the meaning of the English utterances it is given.
The DBAP is an algorithm with builds a query in an augmented relational algebra from the output of the NLP.
The system is portable, or data base independent, because all that is needed to set up a new data base interface are definitions for concepts the NLP doesn't have, plus what I will call the +data base connection", i.e,, the connection between the relations in the data base and the NLP's concepts.
Demonstrating the portability and the robustness gained from using a general purpose NLP are the main subjects of this paper Discussion of the NLP will be limited to its interaction with the DBAP and the data base connection, which by design, is minimal.
[Ginsparg 5] contains a description of the NLP parsing algorithm.
3. The Data Base Connection 2.
NLP overview The formal language the NLP uses to represent meaning is a variant of semantic nets [Quillian 8].
For example, the utterances "The color of the house is green," "The house's color is green".
"Green,~ the color that the house is".
would all be ~ransformed to: Consider the data base given by the following scheme: Parts(pno,pname,color.cosl,weight) Spj( sno,pno,jno.quantity,m, y ) Suppliers and proiects have a number, )~ame and c~tV Parts ha'.,: a number, name, color, cost and weight Supplier wl(~,,unphe,, a quanntYof parts pno to prolect /no in month,nor year The data base connection has four parts: gl Isa: "colored Tense: present Colored: g2 Color: g3 lsa: "house Definite: the Isa: "color Value: green I.
Connecting each relation to the appropriate concept: Suppliers > "supplier fi2.
Connecting each attribute to the appropriate concept: leg., Spj(sno,pno,jno,cost,quantity)) depended on the supplier in which the cost ol a part 4.
C r e a t i n g pseudo relations Pseudo Cities jcity,scity This creates a pseudo relation, Cities(cname), so that the query building algorithm can treat all attributes as if they belong to a relation.
The query produced by the system will refer to the Cities relation.
A postprocessor is used to remove references to pseudo relations from the final query.
Pseudo relations are important because they ensure uniform handling of attributes.
With the pseudo Cities relation, questions like "Who supplies every city?
= and "List the cities".
can be treated identically to "Who supplies every project'"? and "List the suppliers".
The remainder of the data base connection is a set of switches which provide information on how to print out the relations.
whether all proper nouns have been defined or are to be inferred.
whether relations are multivalued, etc.
The switch settings and the four components above constitute the entire data base connection, Nothing else iS needed.
The network of concepts in the N L P should only be augmented for a particular data base; never changed.
Yet different data base schemes will require different representations for the same word.
For example, depending on the data base scheme, it could be correct to represent "box" as either, gl g2 g3 [sa: "part Conditions: "named(gl,box) Isa: "container Conditions: "named(g2.box) [sa: "box 3.
Capturing the information implicit in each relation: Parts(pno,pname,color,cost,weight ) "indexnumberp i n d e x n u m b e r > pno n u m b e r e d > Parts "named n a m e > pname n a m e d > Parts "colored color > color c o l o r e d > Parts costobj > Parts "weighs w e i g h t > weight w e i g h t o b j > Parts Projects(jno.jnamedcity) "indexnumberp indexnumber > jno n u m b e r e d > Projects "named name > jname n a m e d > Projects "located location > jetty located > Prolects Suppliers(sno,sname,scity) "indexnumberp indexnumber > sno n u m b e r e d > Suppliers "named name > sname n a m e d > Suppliers "located location > sctty located > Suppliers %pl O~no.pno.lno.quant Hv.m.y ) "supply supplier > '.no supplied > pno suppliee > mo (cardinality-of pno) > quantity u m e > m.y "spend spender > 1no s p e n d f o r -> pno amount (" cost quantity) The a m o u m case of "spend maps to a c o m p u t a t i o n rather than a,~mgle a t t r i b u t e It' all the attributes in the c o m p u t a h o n are not present,n the relation being defined, the query building program ioms,n the necessary extra relations.
So the definition of "spend ~mrks equally well irl tile example scheme as well as in a scheme 26 The solution is to define each word to map to the lowest possible concept.
W h e n a concept is e n c o u n t e r e d that has a data base relation associated with )t.
there is no problem.
If there )s no relauon associated with a concept, the N L p searchs For a concept that d o e s correspond to a relation and is also a generalization ot" the concept in question.
I f one is found, it is used with an appropriate condilion, usually "tilled or "named.
So "box" has a definition which m a p s to "box.
In the data base c o n n e c t i o n given above.
"box" would be instantiated as a "=part" since " ' b o x " is a r e f i n e m e n t of "'part" and no relation maps to "box," Using the Connection The information in the data base connection ts primarily used m building the query (section.~).
But It IS ~llso used Io augment the knowledge base of Ihe N L P The data base connection is used to overwrite the NLP's ca~e preferences.
Since I o c a w d > S u p p h e r s ()r Projects.
the preference ot" localed ts spec)fied to "suppliers or "protects.
This enables the NLP to interpret the first noun group )n "Do,m', suppliers that supply widgets located nl london also supply,~cre',vs )" as "'suppliers in London that supply widgets" rather than "supphers that,;upph London wldgets" This )s in contrast to [Gawron 31 which u'..es,i separate "disambiguator" phase to ehmlnale parses that do 11()i make sense =n the conceptual scheme of the dala base.
Tile additional preference informamm supplied bv the data base connection is used to induce coercions (section 2).
thai would rlot be made in the absence of the connection (~r under,mother data fibase scheme.
"Who supplies London" does not break any real world preferences, but does break one of the preferences induced by this data base scheme, namely that Suppliee is a "project.
London. a "city, is coerced to "project via the path [*project located *located /ocanon cityl and the question is understood to mean "Who supplies projects which are in London".
As mentioned in Section 2., the NLP determines the meanin~ of many modifications by searching for connections in a semantic net.
The data base connection is used to augment and highlight the existing network of the NLP.
I f the user says, "What colors do parts come in?', the NLP can infer that the meaning of "come-in" intended by the user is "colored since the only path through the net between "color and "part derived from the case preferences induced by the data base connection is ['part colored "colored color "color] Similarly, when given the noun group "London suppliers" the meaning is determined by tracing the shortest path through the highlighted net, ['supplier located'located Iocanon "city] The longer path connecting "supplier and "city, ['supplier supplier "supply suppliee *project located "location location *city] which means "the suppliers that supply to london projects" is found when the NLP rejects the first meaning because of context, If the user says "What are the locations of the London suppliers" the system assumes the second meaning since the first (in the domain of this data base scheme) leads to a tautological reading.
The NLP is able to infer that "The locations of the suppliers located in London" is tautological while "The locations of the suppliers located in England" is not, because the data base connection has specified "located to be a single valued concept with its Iocarton case typed to "city.
I f the system were asked for the locations of suppliers in England, and it knew England was a country, the question would be interpreted as "the cities of the suppliers that are located in cities located in England." The NLP treats most true-l'aise questions with indefinites as requests for the data which would make the statement true.
The question's meaning is "to show the subset of london proiects that are supplied by Blake".
The query building algorithm builds up the query recursively Given an instantiated concept with cases, =t expands the contents of each case and links the results together with the relation corresponding to the concept.
Given an instantiated concept with conditions, it expands each condition.
For the example, we have.
Expand gl Expand g2, the Element of gl Expand gg, the Condition of g2.
Expand g3, the Supplier case of gg.
Expand g9, the Condition of g3.
From the data base connection, a "named whose named case is a *supplier is realized by the Suppliers relation using the sname attribute So we have, g9 select tram Suppliers where sname -blake From the data base connection, a "supply is realized by the Spj relation.
This results in, gga -project]no/i'om.joinSpj to g9 g8 -joingga toProjects g8 is the projects supplied by Blake.
Expand 84, the set gl is a subset of, by expanding its element.
g6 Expand glO.
the Condition of gb Expand g7, the location case of glO yielding g l l -select #am Cities wherecname london A "located with a "project in the Iocotedcase ~s realized by the Projects relation using the ]city attribute.
So we have.
glOa -join Projects Io gl I where]city = cname glOb proiect ]no /'romglOa glO ]oinglOb toProjects g[0 is the projects in London.
Intersect the expansions of g2 and g4 and project the prolect names.
gl3 = pro/eel]name lrom imersectton g 8 glO 5.
A trtee of the query building algorithm.
The query budding algorithm is illustrated by tracmg its operation on the question, "Does blake supply any prolects in london'?" The entire query is, The NLP's meaning representation I'or this question ts shown below.
gO Isa: "show g5 Isa: "name Value: blake g9 Isa: "named Tense: present Named: g3 Name: g5 Isa: "tocated Tense: present Located: gb Location: g7 Isa: "named Tense: present Named: g7 Name: g12 Isa: *name Value: london Tense: present Toshow: g l gO [sa: "set Element: 2 Subset-of': g4 Isa: "protect Isa: "project Element-of: g4 Conditions: glO glO lsa: city Conditions: gll Isa: "supply Tense: Present Suppler: g3 Suppliee: g2 gl [ g9 = select/romSuppliers where s n a m e = blake g8a -/~'oiecrjno #om ioin Spj to g9 g8 = loin gga to Projects gl0 = select #am Projects where icily = london g 13 -prelect iname lrom mter'~e('tlo~t g8 g I0 where the: extra loin resulting f'rom the pseudo (:h=e~ relation ha', been rernoved by the post processor (section 3 ) Entirely as a side effe,'t of the way the query rs generated, the -,,,,,tern can easily correct any l'alse assumptions made by the u~,,2r [Kaplan 71.
For example, if there were no projects in London.
gill would be empty and system would respond, generating Irom the instantiated concept glO li.e., the names used in query correspond to the names used in the knowledge representatmnL "There arc no suppliers located in London".
No additional "'.=oiated presupposition" mechanism is requ+red.
The remainder of this section discusses several aspects o the query building process that the trace does not show.
Negations are handled by introducing a set difference when necessary If the example query were "Does Blake supply any projects that aren't in London?", the expansion of g7 would have been.
I f we ask.
"Who frequents a bar that serves a beer John likes?".
we get the following query.
81 82 g3 84 85 =" select from Likes where drinker john project beer l'rom g 1 .join Serves to 82 =" project bar I~om g3 "" join Frequents to 84 Expand g7.
the location case of glO yielding g i l a select [romCities wherecname -london gl 1 difference o f Cities a n d g l la Conjunctions are handled by introducing an an intersection or union.
I f the example query were "Does Blake supply any projects in London or Paris'?', the /ocanon case of g10 would have.
been the conjunction 813.
I f we ask "Who frequents a bar that serves a beer that he likes"? the correct query, is.
isa "conjunction Type: or Conjoins: g7 g14 [sa: "city Conditions: g l 5 lsa: "named Named: g15 Name: g l 6 Isa: "name Value: paris glb In the first query "beer" was the only attribute projected from g l [n the second, the system projected both "beer" and "drinker", because in expanding "a beer he likes" it needed to expand an instantiated concept (the one representing "who") that was already being expanded.
All of these cases interact gracefully with one another.
For example.
there is no problem in handling "Who supplies every project that is not supplied by blake and bowles".
The result of expanding gl3 would be, [n general, "or" becomes a union and "and" becomes an intersection.
However, if an "and" conjunction is in a single valued case (information obtained from the data base connection), a union is used instead.
Thus "Who supplies london and paris"? is interpreted as "Who supplies both London and Paris'"? and "Who is in London and Pans"? is interpreted as "Who is in London and who ~s m Paris"?
)n the example data base scheme.
6. Advantages of this approach The system can understand anything it has a concept about.
regardless o f whether the concept is attached to a relation in the data base scheme.
In the Suppliers data base from Secuon 4., parts had costs and weights associated with them, but not sizes.
I f a user asks "How big are each of the parts"? and the interface has a "size primitive (which it does), the query building process wdl attempt to find the relation which "size maps to and on fading wdl report back to the user.
"There is no information in the data base about the size of the parts".
This gives the user some informatmn about the what the data base contains, An answer like "1 don't know what "big" means".
would leave the user wondering whether size information was in the data base and obtainable if only the "right" word was used.
The system can interpret user statements that are not queries.
If the user says "A big supplier is a supplier that supplies more than 3 projects" the NLP can use, the definition qn answering later queries.
The definition is not made on a "string" basis e.g., substttuting the words of one side of the definition for the other Instead.
whenever the query building algorithm encounters an mstantiated concept that is a supplier wnh the condition "size~x.
big) it builds a query substnuting the condiuon from the definition that it can expand as a data base query Thus the .~vstern can handle "big london suppliers" and answer "Which sunpliers are big" which it couldn't if ~t were doing strlct string substitution.
This Facility can be used to bootstrap common definitions In,~ commercial flights application, with data base scheme, Flights(fl#,carrier,from.to,departure,arrival.stops.cost ) the word "nonstop" is defined to the system in English as, "A nonstop flight is a night that does not make any stops " and then saved along wuh the rest of the system's defimt~ons.
28 Quantifiers are handled by a post processing phase.
"Does blake supply every project in London"? is handled identically to "Does Blake supply a prolect in London'"? except that the expansion of "projects m London" is marked so that the post processor will be called.
The post processor adds on a set of commands which check that the set difference of London projects and London prolects that Blake supplies is empty.
The rasulhn 8 query is.
gl = =_2 g3 = g4 = =_5 = gO = g7 = g8 = ~e/ect lrom Suppliers w/weresname = blake ~elect l m m Projects where jcity london /otnSpl togl tomg3 to g2 protect jno from g2 protect ino /tom g4 {hl]~'rem'e org5 andgO empn, g7 ] h e first tour commands are the query for "Does Blake supply a llrolect m London'?".
The last tour check that no project in London is not supplied by Blake.
-\ minor modification is needed to cover cases in which the query building algorithm is expanding an instantiated concept that refercnces an instuntiated concept that is being expanded in a higher recursmve call The following examples illustrate this.
Consider the data base scheme below, taken from [Ullman ql.
Coercions (section 2).
can be used solve problems that may require inferences in other systems.
[Grosz 6] discusses the query "Is there a doctor within 200 miles of Philadelphia" in the context of a scheme in which doctors are on ships and ships have distances from cities, and asserts that a system which handles this query must be able to inter that if a doctor is on a ship, and the ship is with 200 miles of Philadelphia, then the doctor is within 200 miles of Philadelphia.
Using coercions, the query would be understood as "is there a ship with a doctor on it that is within 200 miles of Philadelphia?', which solves the problem immediately.
Since the preference information is only used to choose among competing interpretations, broken preferences can still be understood and responded to.
The preference for the supplier case is specified to supplier but if the user says "How many parts does the sorter project supply"? the NLP will find the only interpretation and respond "projects do not supply parts, suppliers do".
Ambiguities inherent in attribute values are handled using the same methods which handles words with multiple definitions.
For example, 1980 may be an organization number, a telephone extension, a number, or a year.
The NLP has a rudimentary (so far) expert system inference mechanism which can easily be used by the DBAP.
One of the rules it uses is "If x is a precondition of y and z knows y is true then z knows x was and may still be true" One of the ['acts in the NLP knowledge base is that being married is a precondition of being divorced or widowed.
I f a user asks "Did Fred Smith used to be married"? in a data base with the relation Employees(name, marital-status) the system can answer correctly by using its inference mechanism.
The exact method is as follows.
The data base application receives the true-false question: "Fred Smith was married and Fred Smith is no longer married" The system handles all the examples in this paper as well as a wide range of others (Appendix A.).
Several different data bases schemes have been connected to the system for demonstrations, including one "real data base" abstracted from the on-line listing of the Bell Laboratories Company Directory.
9. Since the data base includes only current marital status information.
the only way to answer the first part of the question is to inl'cr it from some other information in the data base.
The data base application sends the query to the NLP inference mechanism which would ordinarily attempt to answer it by matching it against its knowledge base or by finding a theorem which would gives it something else to match ['or When called by the data base application, the inference mechanism simply uses its rules base to decide what it should match ['or, and then returns to the data base program.
In this, example, the inference mechanism receives "Fred Smith was married" and using the precondition rule mentioned above, returns to the data base program, "Is Fred Smith divorced" or "is Fred Smith widowed", which can be answered by the data base.
The DBAP can call the inference mechanism recursively if necessary.
Appendix A: Sample Questions and Answers  does every department head in 1135 have an office in holmde[?
No. all heads Ihat work For center number 11.]5 are not located in an office in ho  who isn't alles,h g is the head not located in an ofi~ce in ho that works for center number [ 135  who has been with us for longer than fred smith? 7.
Implementation Status and Details The DBAP ~s fully implemented and debugged.
The NLP is ~mplemented and sail growing.
Both are implemented in Franz Lisp.
a dialect of LISP Language processing and query generai~on are performed m virtually real time (average 1-3 cpu seconds) on a Vax I 1-780 The system ~s intended to be used with u Data Base Management system.
The interface between the DBAP and the I-)BMS is a,,tralghtforward translator from relational algebra to the query language of Ihe DflMS I have written a Ir;.mslator I'or Polaris [Gielan 41.
There are no employees that have worked ('or you ['or more than the length or" the time fred smith has worked for ~.'~'~u  list the employees who used to be married and are not married now, paul rochester, verlon biggs and willie davis are the empin~ee,~ that were married and that are not married now.
TEAM: A TRANSPORTABLE NATURAL-LANGUAGE INTERFACE SYSTEM B a r b a r a J.
G r o s z Artificial Intelligence Center SRI I n t e r n a t i o n a l Menlo P a r k, CA 94025 A.
Overview A major benefit of u s i n g n a t u r a l language to the i n f o r m a t i o n in a database is that it shifts o n t o t h e system t h e b u r d e n of m e d i a t i n g b e t w e e n two v i e w s o f t h e d a t a : t h e way i n which t h e d a t a i s s t o r e d ( t h e " d a t a b a s e v i e w " ), and t h e way i n which an e n d u s e r thinks about it (the "user*s view").
Database information is recorded in terms of files, r e c o r d s, and fields, while natural-language expressions refer t o the same information i n terms of entities and relationships in the world.
A major problem in constructing a natural-language interface is determining how to encode and use the information needed to bridge these two views.
Current natural-language interface systems require extensive efforts by specialists in natural-language processing to p r o v i d e them w i t h t h e i n f o r m a t i o n t h e y need t o do the bridging.
The systems are, in effect, handtallored to provide access to particular databases.
access how Co o b t a i n t h e information requested.
Moving s u c h s y s t e m s to a new d a t a b a s e r e q u i r e s c a r e f u l handcrafting that involves d e t a i l e d knowledge o f such things ae p a r s i n g p r o c e d u r e s, t h e p a r t i c u l a r way i n which domain i n f o r m a t i o n i s stored, and data-access procedures.
To provide for transportability, TEAM s e p a r a t e s i n f o r m a t i o n a b o u t language, about the domain, and a b o u t the database.
The d e c i s i o n t o p r o v i d e t r a n s p o r t a b i l i t y to existing conventional databases (which d i s t i n g u i s h e s TEAM from CHAT [ W a r r e n, 1981]) means that t h e d a t a b a s e c a n n o t be r e s t r u c t u r e d t o make t h e way i n w h i c h i t s t o r e s d a t a more c o m p a t i b l e w i t h t h e way i n which a u s e r may a s k a b o u t t h e data.
A l t h o u g h many p r o b l e m s can be a v o i d e d i f one i s a l l o w e d t o d e s i g n t h e d a t a b a s e a s w e l l a s the natural-language system, given the prevalence of existing conventional databases, approaches w h i c h make t h i s assumption are likely t o have limited applicability in the near-term.
This paper f o c u s e s on the p r o b l e m of constructing transportable natural-language interfaces, i. e ., s y s t e m s t h a t can be a d a p t e d t o p r o v i d e a c c e s s t o d a t a b a s e s f o r which t h e y were not specifically handtailored.
It describes an initial version of a transportable system, called TEAM (for ~ransportable E_ngllsh A_ccess Data manager).
The hypothesis underlying the research described in this paper is that the i n f o r m a t i o n required for the adaptation can be obtained through an Lnteractlve dialogue with database management personnel who are not familiar with natural-language processing techniques.
The TEAM s y s t e m h a s three major components: ( 1 ) an a c q u Z s t t i o n component, ( 2 ) t h e DIALOGIC language system [Grosz, et al., 1982], and (3) a data-access ccaponent.
Section C descrlbes how the language and data-access components were designed to accommodate the needs of transportability.
S e c t i o o D d e s c r i b e s the d e s i g n of the acquisition component to allow flexible interaction ~rlth a database expert and discusses acquisition problems caused by the differences between the database view and user view.
Section E shows how end-user queries are interpreted after an acquisition has been completed.
Section F describes the current state of development of TEAM and lists several problems currently under investigation.
B. I s s u e s of T r a n s p o r t a b i l i t y C.
System Design The insistence on transportability distinguishes TEAM from previous systems such as LADDER [Hendrlx ec al., [978] LUNAR [Woods, Kaplan, and Webber, 1972], PLANES [Waltz, 1975], REL [Thompson, [975], and has affected ~he design of the natural-language processln~ system in several ways.
Most previously built naturallanguage interface systems have used techniques that make them inherently difficult to transfer to new domains and databases.
The internal representations [n these systems typically intermix (in their data structures and procedures) information about language with information about the domain and the database.
In addition, in Interpretln~ a query, the systems conflate what a user is requesting (what hls query "means") with 39 I n TEAM, t h e t r a n s l a t i o n o f an E n g l i s h q u e r y into a database query takes place in two s t e p s . First, the DIALOGIC system constructs a representation of the literal meaning or "logical form" of the query [Moore, 1981].
Second, the data-access component translates the logical form into a formal database query.
Each of these steps requires a combination of some information that is dependent on the domain or the database wlth some information that is not.
To provide for transportability, the TEAM system carefully separates these two kinds of information.
fiI. Domainand Database-Dependent Information To adapt TEAM to a new database three kinds of information must be acquired: information about words, about concepts, and about the structure of the database.
The data structures that encode this information--and the language processing and data-access procedures that use them--are designed to allow for acquiring new information automatically.
Information about words, lexlcal information, includes the syntactic properties of the words that will be used in querying the database and semantic information about the kind of concept t o which a particular word refers.
TEAM records the lexlcal information specific to a given domain in a lexicon.
Conceptual information includes information about taxonomic relationships, about the kinds of objects that can serve as arguments to a predicate, and a b o u t t h e k i n d s o f p r o p e r t i e s an object can have.
I n TEAM, t h e internal representation of information about the entities in the domain of discourse and the relationships that can hold among them is provided by a conceptual schema.
This schema includes a sort hierarchy encoding the taxonomic relationships among objects in the domain, information about constraints on arguments to predicates, and information about relationships among certain types of predicates.
database schema encodes information about how concepts in the conceptual schena map onto the structures of a particular database.
In particular, it links conceptual-schema representations of entities and relationships in the domain to their realization in a particular database.
TEAM currently assumes a relational database with a number of f i l e s . (No languageprocesslng-related problems are entailed in moving TEAM to other database models).
Each file is about some kind of object (e.g., employees, students, ships, processor chips); the fields of the file record properties of the object (e.g., department, age, length).
A To provide access to the informa=,on in a particular database, each of the components of DIALOG~C must access domain-speciflc information about the words and concepts relevant to that database.
The information required by the syntactic rules is found in the lexicon.
Information required by the semantic and pragmatic rules is found in the lexicon or the conceptual schema.
The rules themselves however do not include such domain-dependent information and therefore do not need to be changed for different databases.
In a similar manner, the data-access component separates general rules for translating logical forms into database queries from information about a particular database.
The rules access information i n the conceptual and database schemata to interpret queries for a particular database.
D. Acquisition TEAM i s d e s i g n e d t o i n t e r a c t w i t h two k i n d s o f u s e r s : a d a t a b a s e e x p e r t (DBE) and an e n d u s e r . The DBE provides information about the files and fields in the database through a system-dlrected acquisition dialogue.
As a result of this dlaloEue, the language-processlng and data-access components are extended so that the end-user may query the new database in natural-language.
i. Acquisition Questions Because the DBE is assumed to be familiar with database structures, but not with language-processlng techniques, the acquisition dialogue is oriented around database structures.
That is, the questions are about the kinds of things in the files and fields of the database, rather than about lexlcal entries, sort hierarchies, and predicates.
The disparity between the database view of the data and the end-user's view make the acquisition process nontrlvlal.
For instance, consider a database of information about students in a university.
From the perspective of an enduser "sophomore" refers to a subset of all of the students, those who are in their second year at the university.
The fact that a particular student is a sophomore might be recorded in the database in a number of ways, including: (l) in a separate file containing information about the sophomore students; (2) by a special value in a symbolic field (e.g., a CLASS field [n which the value SOPH indicates "sophomore"); (3) by a "true" value in a Boolean field (e.g., a * in an [S-$O?H field).
For natural-language querying to be useful, the end-user must be protected from having to know which type of representation was chosen.
The questions posed to the DBE for each kind of database construct must be sufficient to allow DIALOGIC to handle approximately the same range of Domain-lndependent Information The language executive [Grosz, e t a l ., 1982; Walker, 1978|, DIALOGIC, coordinates syntactic, semantic, and basic pragmatic rules in translating an English query into logical form.
DIALOGIC's syntactic rules provide a general grammar of English [Robinson, 1982].
A semantic "translation" rule associated with each syntactic phrase rule specifies how the constituents of the phrase are to be interpreted.
Basic pragmatic functions take local context into account in providing the interpretation of such things as noun-noun combinations.
DIALOGIC also includes a quantlfler-scoping algorithm.
filinguistic expressions (e.g., for referring to "students i n t h e sophomore c l a s s ' ) r e g a r d l e s s o f the particular database implementation chosen.
In all c a s e s, TEAM w i l l c r e a t e a l e x i c a l e n t r y f o r " s o p h o m o r e " and an e n t r y i n t h e c o n c e p t u a l schema to represent the concept of sophomores.
The database attachment for thls concept will depend on t h e p a r t i c u l a r d a t a b a s e s t r u c t u r e, as w i l l the kinds of predicates f o r which i t can be an argument.
Example of Acquisition Queeclons I n d e s i g n i n g TEAM we f o u n d i t i m p o r t a n t to distinguish three differanc kinds of fields N arlthmeCic, feature (Boolean), and s y m b o l l c o n the b a s i s of t h e r a n g e of l i n g u i s t i c expressions to which each gives r i s e . AriChmetic fields contain numeric values on which comparisons and computations llke averaging are likely to be done.
(Fields containing dates a r e n o t y e t h a n d l e d by TEAM).
Feature fields contain true/false values w h i c h r e c o r d w h e t h e r o r n o t some a t t r i b u t e i s a property of the object d e s c r i b e d by t h e file.
Symbolic f i e l d s typically contain values that c o r r e s p o n d to n o u n s o r a d j e c t i v e s t h a t d e n o t e t h e s u b t y p e s o f t h e domain d e n o t e d by t h e f i e l d . D i f f e r e n t a c q u i s i t i o n q u e s t i o n s a r e asked f o r each type of field.
These are illustrated in the example i n S e c t i o n D.3.
To illustrate the acquisition of information, consider a database, called CHIP, containing information about processor chips.
In particular, the fields in this database contain the following information: the identification number o f a c h i p ( I D ), its m a n u f a c t u r e r (MAKER) its width i n b i t s (WIDTH), ice speed in m e g a h e r t z (SPEED), its cost i n d o l l a r s (PRICE), the kind of technology (FAMILY), and a flag indicating wheCher o r noc t h e r e is an e x p o r t l i c e n s e f o r t h e c h i p (EXP).
In the figures discussed below, the DBE's r e s p o n s e is indicated in uppercase.
For many quesClone the DBE is presented wlch a llst of options from which ha can choose.
For these questions, the complete llst is shown and the answer indicated in boldface.
F i g u r e i shows t h e short-form of the questions asked about the file itself.
In r e s p o n s e to q u e s t i o n ( 1 ), t h e DBE t e l l s TEAM w h a t fields are in the file.
Responses to the r e m a i n i n g quesCloms allow TEAM t o identify t h e kind of object the file contains information about (2), types of linguistic expressions used to refer to It [ (6) and (7)], how to identify individual objects in the database (4), and how to s p e c i f y i n d i v i d u a l o b j e c t s to the u s e r ( 5 ) . These responses result in the words "chip" and " p r o c e s s o r " b e i n g added t o t h e l e x i c o n, a new s o r t added to the taxonomy (providing the interpretation f o r t h e s e w o r d s ), and a l i n k made i n t h e d a t a b a s e schema b e t w e e n t h i s sort and records i n the file CHIP.
Figure 2 gives the short-form of the most central questions asked about symbolic fields, using the field MAKER (chip manufacturers) as exemplar.
These questions are used to determine the kinds of properties represented, how t h e s e r e l a t e t o p r o p e r t i e s i n o t h e r f i e l d s, and the k i n d s of linguistic expressions the field values can give rise to.
Question (4) allows TEAM to determine that individual field values refer to manufacturers rather than chips.
The long-form of Q u e s t i o n (7) i s : Will you want to ask, for example, "How many MOTOROLA processors are there"? to get a count of the number of PROCESSORS with CHIP-MAKER-MOTOROLA?
Question (8) expands to: Will you want to ask, for example, "How many HOTOROLAS are there"? to get a count of the number of PROCESSORS with CHIP-MAKER-MOTOROLA?
Acquisition Strategy The ~ a J o r features of the s tra te gy developed for acquiring information about a database from a DBE include: (1) providiu E multiple levels of detail for each question posed to the DBE; (2) allowing a DBE to review previous answers and change them; and (3) checking for legal answers.
At present, TEAM initially presents the DBE wlth the short-form of a quesclou.
A more detailed version ("long-form') of the question, including examples illustratlng different kinds of responses, can be requested by the DBE.
An obvious excenslon to this strategy would be to present different Inltial levels t o different users ( d e p e n d i n g, f o r e x a m p l e, on t h e i r p r e v i o u s experience wlth the system).
A c q u i s i t i o n I s e a s i e r i f e a c h new p i e c e of information is immediately i n t e g r a t e d into the u n d e r l y i n g knowledge s t r u c t u r e s o f t h e p r o g r a m . 8 o w e v e r, we a l s o wanted Co a l l o w t h e DSE t o change a n s w e r s to p r e v i o u s q u e s t i o n s ( t h i s has t u r n e d o u t to be an essential feature of TEAM).
Some questions (e.g., those about irregular plural forms and synonyms) affect only a single part of TEAM (the lexicon).
Other questions (e.g., those about feature fields) affect all components of the system.
Because of the complex interaction between acquisition questions and components of the system to be updated, immediate integration of new information is not possible.
As a result, updating of the lexicon, conceptual schema, and database schema Is not done until an acqulsition dialogue is completed.
In t h i s ease, t h e a n s w e r to q u e s t i o n ( 7 ) I s " y e s " and to q u e s t i o n ( 8 ) " n o " ; the field has v a l u e s that can be used as explicit, but not implicit, classifiers.
Contrast this wlth a symbolic field in a file about students that contains the class of a student; in this case the answer to both fiauesclons would be affirmative because, for example, the phrases "sophomore woman" and "sophomores" can be used to refer to refer to STUDENTS with CLASS=SOPHOMORE.
In other cases, the values may serve neither as explicit nor as implicit classifiers.
For example, one cannot say *"the shoe employees" or *"the shoes" to mean "employees in the SHOE department".
For both questions (7) and (8) a positive answer i s the default.
It i s i m p o r t a n t to allow the user to override thls default, because TEAM must be able to avoid spurious ambiguities (e.g., where two fields have identical field values, but where the values can be classifiers for only one field.).
Following acquisition of this field, lexical entries are made for "maker" and any synonyms supplied by the user.
Again a new s o n is created.
It i s marked a s h a v i n g v a l u e s t h a t can be explicit, b u t not implicit, classifiers.
Later, when the actual connection to the database is made, individual field values (e.g., "Motorola") will be made individual instances of this new sort.
Figure (3) presents the questions asked about arithmetic fields, using the PRICE field as exemplar.
Because dates, measures, and count quantities are all handled differently, TEAM must first determine which kind of arithmetic object is in the field (2).
In this case we have a unit of "worth" (6) measured in "dollars" (4).
Questions (8) and (9) supply information needed for interpreting expressions Involvlng comparatives (e.g., "What chips are more expensive than the Z8080")? and superlatives (e--~7, "What is the cheapest chip?").
Figure 4 gives the expanded version of these questions.
As a result of thls acquisition, a new subsort of the (measure) sort WORTH i s added to the taxonomy for PRICE, and is noted as measured in dollars.
In addition, lexlcal entries are created for adjectives indicating positive ("expensive") and negative ("cheap") degrees of price and are linked to a binary predicate that relates a chip to its price.
Feature fields are the most difficult fields to handle.
They represent a single (arbitrary) property of an entity, with values that indicate whether or not the entity has the property, and they give rise to a wide range of linguistic expresslons--adJectlvals, nouns, phrases.
The short-form of the questions asked about feature fields are given in Figure 5, using the field EXP; the value YES indicates there is an export license for a given processor, and NO indicates there is not.
Figures 6, 7, and 8 give the expanded form of questions (4), (6), and (B) respectively.
The expanded form illustrates the kinds of end-user queries that TEAM can handle after the DBE has answered these questions (see also Figure 9).
Providing thls kind of illustration has turned out to be essential for getting these questions answered correctly.
Each of these types of expression leads to new lexlcal, conceptual schema, and database schema entries.
I n general in the conceptual schema, feature field adJectlvals and abstract nouns result in the creation of new predicates (see Section E for an example); count nouns result in the creation of new subsorts of the file subject sort.
The database schema contains informatlon about which field to access and what field value is required.
TEAM also includes a limlted capability for acqulrln8 verbs.
At present, only transitive verbs can be acquired.
One of the arguments to the predicate cozTespondlng to a verb must be of the same sort as the file subject.
The other argument must correspond to the sort of one of the fields.
For the CHIP database, the DBE could specify that the verb "make" (and/or "manufacture") takes a CHIP as one argument and a MAKER as the second argument.
E. Sample Q u e r i e s and T h e i r [nterpretatlons After the DBE has completed an acquisition session for a file, TEAM can interpret and respond Co end-user queries.
Figure 9 lists some sample end-user queries for the file illustrated in the previous section.
The role of the different kinds of informatlon acquired above can be seen by considering the logical forms produced for several queries and the database attachments for the sorts and predicates that appear in them.
The following examples illustrate the information acquired for the three different fields described in the preceding section.
Given the query, What are the Motorola chips?
DIALOGIC produces the following logical form: (Query (WHAT tl (THING tl) (THE p2 (AND (PROCESSOR p2) (MAKER-OF p2 MOTOROLA)) (EQ p2 tl)))) where WHAT and THE are quantifiers; 1 tl and p2 are variables; AND and EQ have their usual interpretation.
The predicates PROCESSOR and MAKER-OF and the constant MOTOROLA were created as a result of acquisition.
The schema: PROCESSOR: MAKER-OF: following information in the database 1 Because the current version of DIALOGIC takes no account of the slngular/plural distinction, the uniqueness presupposition normally associated with "the" is not enforced.
42 fii s u s e d, a l o n g with s o r ~ h i e r a r c h y i n f o r m a t i o n i n the conceptual schema, t o g e n e r a t e the actual database query.
Similarly, t h e e n d u s e r query chips? new acqulslClon component allows t h e user more flexibility i n answering questions and provides a wider range of default answers.
TEAM c u r r e n t l y h a n d l e s m u l t i p l e files and provides transportability to a l i m i t e d r a n g e o f databases.
As menCloned previously, a relational database model is assumed.
Currently, TEAM also assumes all files are In third normal form.
The acquisition of verbs is limited Co allowing t h e DBE Co s p e c i f y t r a n s I C l v e v e r b s, as described in S e c t i o n D.3.
We a r e c u r r e n t l y excending TEAM t o What a r e t h e e x p o r t a b l e would l e a d to t h e l o g i c a l form: ( Q u e r y (WHAT t l (THING cl) (THE p2 (AND (PROCESSOR p2) where EXP-POS is a predlcace created by acquisIClon; it is true if its argumanC is exportable.
In thls case the relevant database scheme information I s : PROCESSOR: EXP-POS: file-CHIP keyfleld-[D file-CHIP fleld-EXP fieldvalue-T (I) Provide for interpretation of expressions involving such things as mass terms, aggregates, quantified c o a m a n d s, and commands t h a c r e q u i r e t h e s y s t e m Co p e r f o r m f u n c t i o n s o t h e r t h a n q u e r y i n g che d a t a b a s e . Provide for efficient p r o c e s s i n g of the m o s t common f o r m s o f c o n j u n c t i o n . Generalize the verb acquisition p r o c e d u r e s and e x t e n d TEAM t o h a n d l e more complex verbs, including such Chings as verbs wlth mulClple delineations, verbs chat require special prepositions, and verbs that allow senCenclel complements.
Handle d a t a b a s e s encoding time-related information and e x t e n d DIALOGIC to handle expressions involving clme and tense.
Finally, co illustrate how TEAM h a n d l e s arithmetic f i e l d s, and I n p a r t i c u l a r the use of comparatives, consider the query: What c h i p i s c h e a p e r chart 5 d o l l a r s ? The l o g i c a l form f o r Chin q u e r y I s ( Q u e r y (WHAT pl (PROCESSOR pl) ((MORE C ~ A P ) pl (DOLLAH 5)))) The conceptual schema encodes the relationship between the predicates CHEAP and PRICE-OF (again, both concepts created as a result of acquisition), wlCh t h e following information CHEAP: measure-predlcate-PRICE-OF scale-negative G.
Acknowledgments The d e v e l o p m e n t of TEAM has involved the efforts of many people.
Doug Appelc, Armar Archbold, Bob Moore, Jerry Hobbs, Paul Marcln, Pernando Pereira, Jane Robinson, Daniel Sagalowicz, and David Warren have made ~ a J o r contributions.
This research was supported by the Defense Advanced Research Projects Agency with the Naval Electronic Systems Command under Contract N0003980-'<:-0645.
The views and conclusions contained in Chin document are chose of the author and should not be interpreted as representative of the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the United States Government.
And the relevant database schema Informaclon is: PROCESSOR: PRICE-OF: file-CHIP keyfield-[D flit-CHIP field(argl)=[D fleld(arg2)-PRICE F.
Status and Future Research An initial version of TEAM was implemented in a combination of Incerlisp (acquisition and DIALOGIC components) and Prolog (data access component) on the DEC2060, but address space llmicatlons made continued development difficult.
Current research on TEAM is being done on the Symbolics LISP machine.
The acquisition component has been redesigned co cake advantage of capabilities provided by che blcmap display.
The 43 File nameC H ~ (1} Fields (ID MAKER WIDTH SPEED PRICE FAMILY EXP) (2) Subject P R O C E S S O R (31 Synonyms for P R O C E S S O R C H I P (4} Primazy key ID {5} IdentifyingfieldsM A K E R ID (8) Can one say W h o are the P R O C E S S O R S ? Y E S N O (7) Pronouns for filesubject H E S H E IT T H E Y (8) Field containing the name of each file subject ID Figure 1: Questions About File Field P R I C E ( 1) Type of field SYMBOLIC A R I T l t M E T I C FEATURE (2) Value t y p e . DATES M E A S U R E S COUNTS [3) Are the units implicit?
Y E S N O (4) Enter implicit unit DOLLAR (5) Abbreviation for this unit.~ (6) Measure type of this trait TIME WEIGHT SPEED VOLUME LINEAR AREA W O R T H OTHER {7) Minimum and maximum numeric valucs(1,100) (8} Positive adjectives (EXPENSIVE COSTLY) (9) Negative adjective (CHEAP) Figure 3: Questions for Arithmetic Field P R I C E Please specify any adjectives that can be used in their comparative or superlative form to indicate how much each P R O C E S S O R is in a positivedirectionon the scale measured by the values of CHIP-PRICE.
In a file about machine-tools with a numeric field called PRICE, one could ask: How E X P E N S I V E is each tool? to mean What is the price of each tool.~ EXPENSIVE, COSTLY, AND (HIGH PRICED) ~re positive adjectives designating the upper range of the PRICE scale.
C H E A P and (LOW PRICED), which designate the lower range of the PRICE scale, are negative adjectives.
Field M A K E R ( I ) Type of field S Y M B O L I C ARITHMETIC FEATURE (2) .Axe field values units of measure?
YES N O (3} Noun subvategory P R O P E R COUNT MASS (4} Domain of field value's reference SUBJECT F I E L D (5) Can you say W h o is the C H I P M A K E R t Y E S N O (6) Typical value M O R T O R O L A (7) Will values of this field be used as cia~sifers.~ E S N O Y {8) Will the values in this field be used alone as implicit classifiers?
YES N O Figure 2: Questions for Symbolic Field M A K E R Please enter any such adjectives you will want to ~ querying the database.
Figure 4: Expanded Version of Adjective Questions (Arithmetic Field} in Field E X P (I) Type of field SYMBOLIC ARITHMETIC F E A T U R E (2) Positive value YES (3) Negative value NO (4) Positive adjectives EXPORTABLE (5) Negative adjectives UNEXPORTABLE (6) Positive abstraA't nouns EXPORT AUTHORIZATION (7) Negative abstract no~.1 (8) Pmitive common nouns (9) Negative common nouns Figure 5: Questions for Feature Field ]gXP List any count nous~ ammciated with positive field value YES.
In general, this is any word wwww such that you might want to u k : What PROCESSORS are wwww-s! to mean What PROCESSORS have a CHIP-EXP of YES?
For example, in a file about EMPLOYEEs with  feature field CITIZEN having a positive field value Y and n e ~ t i v e field value N, you might want to aek: Which employees are citizens? instead of Which employees have a CITIZEN of Y?
Figure 8: Feature Field Count Nouns What adjectivab are aasoeiated with the field values YES in this field?
In general these are word.5 wwww such that you might want to Mk: Which PROCESSORS are www~' Which PROCESSORS have  CHIP-EXP of YES!
For example, in s medical file about PATIENTs with a feature field IMM having a positive field value Y and a negative filed value N, you might want to ask: Which patients are IMMUNE (or RESISTANT, PROTECTED)!
Figure 6: Feature Field Adjectivals ~,Vhat 8 bit chips are cheaper than the fastest exportable chip made by Zilogt Who makes the fastest exportable N M O $ chip costing less than 10 dollars!
By whom is the most expensive chip reader Who b the cheapest exportable chip made by!
Who is the most expensive chip made?
What is the fastest exportable chip that Motorola makes?
What 16 bit chips does Zilog make?
Who makes the fastest exportable N M O S chip?
Who makes the faatest exportable chip.~ Does Zilog make a chip that is faster than every chip that Intel makes?
Are there any 8 bit Ziiog chipe? is some exportable chip faster than 12 mhz?
Is every Ziiog chip that is f ~ t e r than 5 mhz exportable?
How faat is the faate~t exportable chip?
How expensive is the f~stest ~'~MOS chipt Figure 9: Sample questions for CHIP databaae List any abstrart nouns ~k~tociated with the positive feature value YES.
In general this is any word wwww such that you might want to ask a question of the form: Which PROCESSORS hove wwww? tO m e a n Which PROCESSORS have CHIP-EXP of YES!
For example, in a medical databaae about PATIENTs with a feature field IMM having a positive field value Y and a negative field value N, you might want to a~k: ~,Vhich patients have IMMUNITY? instead of Which patients have aa IMM of Y?
Figure 7: Feature Field Abstract Nouns REFERENCES Grosz, B.
et al . [1982] "DIALOGIC: A Core Natural Language Processing System," Proceedings of the Ninth International Conference on Computational Linguistics, Prague, Czechoslovakia (July 1982).
Moore, R.
C. [1981] "Problems in Logical Form," in Proceedings of the 19th Annual Meeting of the Association for Computaional Linguistics, pp.
117-L24. The Association for Computaional Linguistics, SRI International, Menlo Park, Californla (June 1981)..
Waltz, D.
[1975] "Natural Language Access to a Large Data Base: An Engineering Approach," Proc.
4th International Joint Conference on Artificial Intelligence, Tbillsl, USSR, pp.
868-872 (September 1975).
Warren, D . R . [1981] "Efficient Processing of Interactive Relational Database Queries Expressed in Logic," Proc.
Seventh International Conference on Very Large DataBases, Cannes, France, pp.
2"'2~-2--~', Robinson, J.
[1982] "DIAGRAM: A Grammar for Dialogues," Communications of the ACM, Vol.
25, No.
1, pp.
27-47 (January 1982).
Thompson, g . B . and Thompson, B . H . [1975] "Practical Natural Language Processing: The REL System as Prototype," H.
Rubinoff and M.
C. Yovlts, eds., pp.
109-168, Advances in Computers 13, Academic Press, New York, (New York 1975).
Woods, W.
A., R.
M. Kaplan, and B.
N-Nebber [I972] "The Lunar Sciences Natural Language Information System," BBN Report 2378, Bolt Beranek and Newman, Cambridge, Massachusetts (1972).
Walker, D.
E. (ed).
[1978] Understanding Spoken Language, Elsevier North-Hollam~, New York, New York, (1978) .
Customizable Descriptions of Object-Oriented Models Benoit Lavoie CoGenTex, Inc.
840 Hanshaw Road Ithaca, NY 14850, USA benoitOcogentex, com Owen Rambow CoGenTex, Inc.
840 Hanshaw Road Ithaca, NY 14850, USA owen~cogentex, com Ehud Reiter Department of Computer Science University of Aberdeen Aberdeen AB9 2UE, Scotland ereiter~csd, abdn.
ac. uk 1 Introduction: Object Models With the emergence of object-oriented technology and user-centered software engineering paradigms, the requirements analysis phase has changed in two important ways: it has become an iterative activity, and it has become more closely linked to the design phase of software engineering (Davis, 1993).
A requirements analyst builds a formal object-oriented (OO) domain model.
A user (domain expert) validates the domain model.
The domain model undergoes subsequent evolution (modification or adjustment) by a (perhaps different) analyst.
Finally, the domain model is passed to the designer (system analyst), who refines the model into a OO design model used as the basis for implementation.
Thus, we can see that the OO models form the basis of many important flows of information in OO software engineering methodologies.
How can this information best be communicated?
It is widely believed that graphical representations are easy to learn and use, both for modeling and for communication among the engineers and domain experts who tqgether develop the OO domain model.
This belief is reflected by the large number of graphical OO modeling tools currently in research labs and on the market.
However, this belief is not accurate, as some recent empirical studies show.
For example, Kim (1990) simulated a modeling task with experienced analysts and a validation task with sophisticated users not familiar with the particular graphical language.
Both user groups showed semantic error rates between 25% and 70% for the separately scored areas of entities, attributes, and relations.
Relations were particularly troublesome to both analysts and users.
Petre (1995) compares diagrams with textual representations of nested conditional structures (which can be compared to OO modeling in the complexity of the "paths" through the system).
She finds that "the intrinsic difficulty of the graphics mode was the strongest effect observed" (p.35).
We therefore conclude that graphics, in order to assure maximum communicative efficiency, needs to be complemented by an alternate view of the data.
We claim that the alternate view should be provided by an explanation tool that represents the data in the form of a fluent English text.
This paper presents such a tool, the MODELEXPLAINER, or MODEx for short, and focuses on the customizability of the system.1 Automatically generating natural-language descriptions of software models and specifications is not a new idea.
The first such system was Swartout's GIST Paraphraser (Swartout, 1982).
More recent projects include the paraphraser in ARIES (Johnson et al., 1992); the GEMA data-flow diagram describer (Scott and de Souza, 1989); and Gulla's paraphraser for the PPP system (Gulla, 1993).
MoDEx certainly belongs in the tradition of these specification paraphrasers, but the combination of features that we will describe in the next section (and in particular the customizability) is, to our knowledge, unique.
2 Features
of MoDEx MODEx was developed in conjunction with Andersen Consulting, a large systems consulting company, and the Software Engineering Laboratory at the Electronic Systems Division of Raytheon, a large Government contractor.
Our design is based on initial interviews with software engineers working on a project at Raytheon, and was modified in response to feedback during iterative prototyping when these software engineers were using our system.
 MoDEx output integrates tables, text generated automatically, and text entered freely by the user.
Automatically generated text includes paragraphs describing the relations between classes, and paral(Lavoie et al., 1996) focuses on an earlier version of MoDEx which did not yet include customization.
253 graphs describing examples.
The human-anthored text can capture information not deducible from the model (such as high-level descriptions of purpose associated with the classes).
 MoDEx lets the user customize the text plans at run-time, so that the text can reflect individual user or organizational preferences regarding the content and/or layout of the output.
 MoDEx uses an interactive hypertext interface (based on standard HTML-based WWW technology) to allow users to browse through the model.
 Input to MoDEx is based on the ODL standard developed by the Object Database Management Group (Cattell, 1994).
This allows for integration with most existing commercial off the shelf OO modeling tools.
Some previous systems have paraphrased complex modeling languages that are not widely used outside the research community (GIST, PPP).
 MODEX does not have access to knowledge about the domain of the OO model (beyond the OO model itself) and is therefore portable to new domains.
3 A
MoDEx Scenario Suppose that a university has hired a consulting company to build an information system for its administration.
Figure 1 shows a sample object model for the university domain (adapted from (Cattell, 1994, p.56), using the notation for cardinality of Martin and Odell (1992)) that could be designed by a requirements analyst.
Figure 1: The University OoO Diagram Once the object model is specified, the analyst must validate her model with a university administrator (and maybe other university personnel, such as dataentry clerks); as domain expert, the university administrator may find semantic errors undetected by the analyst.
However, he is unfamiliar with the "crow's foot" notation used in Figure 1.
Instead, he uses MoDEx to generate fluent English descriptions of the model, which uses the domain terms from the model.
Figure 2 shows an example of a description generated by MoDEx for the university model.
Suppose that in browsing through the model 254 using the hypertext interface, the university administrator notices that the model allows a section to belong to zero courses, which is in fact not the case at his university.
He points out the error to the analyst, who can change the model.
Suppose now that the administrator finds the texts useful but insufficient.
To change the content of the output texts, he can go to the Text Plan Configuration window for the text he has been looking at, shown in Figure 3.
He can add to the text plan specification one or more constituents (paragraphs) from the list of pre-built constituents (shown in the lower right corner of Figure 3).
After saving his modifications, he can return to browsing the model and obtain texts with his new specifications.
File Edit View Go Bookmarks Options Directory ~indow Help [List of Classes] [List of Models] [Reload Models] [Configuration] ~ [About ModeIF.xolame,] Description of the Class" Section' General Observations: A Section must be taught by exactly one F$ofesso, and may ~clong to zezo oz more Cqu~e s.
It must be tako by one ca more Students and may have at most one TA.
Examples: For example, Sectl is a Section and is taught by the professor Jolm Brown.
It belongs to two Courses, Math165 and Math201, and is take~ by two Students.
Frank Belfo~d and Sue Jones.
It has the TA Sally Blake.
Figure 2: Description Used for Validation............,-.,m~ .................
= ....................
I;|? i[Jlc Edll ~ew Go Bookmarks ~Jans Dlrc~r/ ~qndow Help Text Plsm Conflgm'aflon Tat Plmv V -'~'4'~"-(2~  ..,:L ~..cr=,.on o= = .=.,c~s ] 0 z~.~ ~.: ~===~==) i --=~'~ ~omponent I . -[ ~ose ~butes 3peretions :telafions-Teble :~elQ~ons-Te)d -:xemples-Long :xemples-Shod ~~ ~ ~'~ " Rle-Reference Figure 3: Text Plan Configuration Interface Once the model has been validated by the univerFile Edit View Go Bookmarks Options Directory Window _Help [List of Classes] [List of Model.~] [Reload Models] [Co:ffi~otation] [H~ [About ModelEx~01amer] [Q3_~] ~==:==~=~=~=~==::~==~==~:~:~====~===~::::::::::::::::::::::::::::~=~====~ Business Class: "Section' Purpose/Role: Course unit a student can take.
Ed11. Pu~o.~e Attributes: ii Am~u~ JiDeser~ t~n . iiTY~e .................. i i i ...................................... n~ber iSecUo n T'--"T""'7""" identifier ~#1NTF~3 ~ .................
]~'~ :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: Edit Attdbutee Relationships: A Section must be taught by exactly one Ptofee~ot and may belong to zero or more Cotuses.
It must b e taken by one or more Stud~nt.~ and may have at most one TAD server which receives requests via a standard Web CGI interface and returns HTML-formatted documents which can be displayed by any standard Web browser.
The documents generated by MoDEx are always generated dynamically in response to a request, and are composed of human-authored text, generated text and/or generated tables.
The main requests are the following: ModEx m Request i Figure 4: Description Used for Documentation sity administrator, the analyst needs to document it, including annotations about the purpose and rationale of classes and attributes.
To document it, she configures an output text type whose content and structure is compatible with her company's standard for OO documentation.
An example of a description obtained in modifying the text plan of Figure 3 is shown in Figure 4.
(This description follows a format close to Andersen Consulting's standard for documentation).
This description is composed of different types of information: text generated automatically (section Relationships), text entered manually by the analyst because the information required is not retrievable from the CASE tool object model (section Purpose), and tables composed both of information generated automatically and information entered manually (section Attributes).
The analyst then saves the text plan under a new name to use it subsequently for documentation purposes.
Note that while the generated documentation is in hypertext format and can be browsed interactively (as in the I-DOC system of Johnson and Erdem (1995)), it can of course also be printed for traditional paperbased documentation and/or exported to desktop publishing environments.
4 How
MODEX Works As mentioned above, MODEx has been developed as a WWW application; this gives the system a platform-independent hypertext interface.
Figure 5 shows the MoDEx architecture.
MoDEx runs as a Figure 5: MODEx Server Architecture  Text Plan Editing.
This generates an HTML document such as that shown in Figure 3 which allows a user to load/edit/save a text plan macro-structure specification.
A representation corresponding to the text plan of Figure 3 is shown in Figure 6.
Once edited, this representation can be stored permanently in the library of text plans and can be used to generate descriptions.
In this representation, User Text indicates free text entered for a title, while RelationsText and Examples-Short are schema names referring to two of the eight predefined text functions found in a C++ class library supplied with MoDEx.
alidation-Class) Ti~e, User Text Ti~e Schema ~itle Schema i I I I User Text Relations-Text User Text Examples-Short Figure 6: Macro-Stucture for Text Plan of Figure 3  Object Model Loading.
This loads an object model specification and generates a document displaying the list of classes found in the model.
 Description Generation.
This returns a description such as that shown in Figures 2 or 4.
To generate a description, the text planner creates a text structure corresponding to the text plan configuration selected by the user.
This text structure is a constituency tree where the internal nodes define the text organization, while the bottom nodes define its content.
The text content can be specified as syntactic repre255 sentations, as table specification and/or as humanauthored text for the titles and the object model annotations.
The text structure is transformed by the sentence planner which can aggregate the syntactic representations (cf.
conjunctions and in description on Figure 2) or introduce cue words between constituents (cf.
expression For example on Figure 2).
The resulting text structure is then passed to the text realizer which uses REALPRO (Lavoie and Rambow, 1997), a sentence realizer, to realize each individual syntactic representation in the text structure.
Finally, a formatter takes the final text structure to produce an HTML document.
 Object Model Annotation Editing.
This allows the user to edit human-authored annotations of the object model.
This editing can be done via links labelled Edit ...
which appear in Figure 4.
These human-authored texts are used by some of the predefined text functions to generate the descriptions.
5 Outlook
MoDEx is implemented in C++ on both UNIX and PC platforms.
It has been integrated with two object-oriented modeling environments, the ADM (Advanced Development Model) of the KBSA (Knowledge-Based Software Assistant) (Benner, 1996), and with Ptech, a commercial off-the-shelf object modeling tool.
MoDEx has been fielded at a software engineering lab at Raytheon, Inc.
The evaluation of MoDEx is based on anecdotal user feedback obtained during iterative prototyping.
This feedback showed us that the preferences regarding the content of a description can vary depending on the organization (or type of user).
The control that MoDEx gives over the text macro-structure is one step toward satisfying different types of text requirements.
We are currently extending MoDEx in order to give the user a better control over the text micro-structure, by replacing the set of predefined C++ text functions with customizable ASCII specifications.
This feature should make MODEx more easely portable among different types of users.
In addition, we intend to port MODEX to at least two new OO modeling environments in the near future.
Acknowledgments The first version of MoDEx for ADM was supported by USAF Rome Laboratory under contract F30602-92-C0015.
General enhancements to the linguistic machinery were supported by SBIR F30602-92-C-0124, awarded by USAF Rome Laboratory.
Current work on MODEx is supported by the TRP-ROAD cooperative agreement F30602-95-2-0005 with the sponsorship of DARPA and Rome Laboratory.
We are thankful to K.
Benner, M.
DeBellis, J.
Silver and S.
Sparks of Andersen Consulting, and to F.
Ahmed and B.
Bussiere of Raytheon Inc., for their comments and suggestions made during the development of MoDEx.
We also thank T.
Caldwell, R.
Kittredge, T.
Korelsky, D.
McCullough, A.
Nasr and M.
White for their comments and criticism of MoDEx.
Identifying Terms by their Family and Friends Diana Maynard Sophia Ananiadou Dept.
of Computer Science University of Sheffield Regent Court, 211 Portobello St Sheffield, $1 4DP, UK d.
maynard0dcs, shef.
ac. uk Computer Science, School of Sciences University of Saltbrd, Newton Building Saltbrd, M5 4WT, U.K. s.
ananiadou@salf ord.
ac. uk Abstract Multi-word terms are traditionally identified using statistical techniques or, more recently, using hybrid techniques combining statistics with shallow linguistic information.
Al)proaches to word sense disambiguation and machine translation have taken advantage of contextual information in a more meaningflfl way, but terminology has rarely followed suit.
We present an approach to t e r m recognition which identifies salient parts of the context and measures their strength of association to relevant candidate terms.
The resulting list of ranked terms is shown to improve on that produced by traditional methods, in terms of precision and distribution, while the information acquired in the process can also be used for a variety of other applications, such as disambiguation, lexical tuning and term clustering.
Introduction Although contextual information has been previously used, e.g. in general language (Grefenstette, 1994) mid in the NC-Value method for term recognition (Frantzi, 1998; Frantzi and Ananiadou, 1999), only shallow syntactic information is used in these cases.
The T R U C K S approach identifies different; elements of the context which are combined to form the Information Weight, a measure of how strongly related the context is to a candidate term.
The hffbrmation Weight is then combined with the statistical information about a candidate t e r m and its context, acquired using the NC-Value method, to form the SNC-Value.
Section 2 describes the NCValue method.
Section 3 discusses the importance of contextual information and explains how this is acquired.
Sections 4 and 5 describe the hffbrmation Weight and the SNC-VMue respectively.
We finish with an evaluation of the method and draw some conclusions about the work and its fllture.
Although statistical approaches to automatic term recognition, e.g.
(Bourigault, 1992; Daille et al., 1994; Enguehard and Pantera, 1994; 3usteson and Katz, 1995; Lauriston, 1996), have achieved relative success over the years, the addition of suitable linguistic information has the potential to enhance results still further, particularly in the case of small corpora or very specialised domains, where statistical information may not be so accurate.
One of the main reasons for the current lack of diversity in approaches to term recognition lies in the difficulty of extracting suitable semantic information from speeialised corpora, particularly in view of the lack of appropriate linguistic resources.
The increasing development of electronic lexieal resources, coupled with new methods for automatically creating and fine-tuning them from corpora, has begun to pave the way for a more dominant appearance of natural language processing techniques in the field of terminology.
The T R U C K S approach to t e r m recognition (Term Recognition Using Combined Knowledge Sources) focuses on identifying relevant contextual information from a variety of sources, in order to enhance traditional statistical techniques of t e r m recognition.
The NC-Value m e t h o d The NC-Value method uses a combination of linguistic and statistical information.
Terms are first extracted from a corpus using the C-Value method (Frantzi and Ananiadou, 1999), a measure based on frequency of occurrence and term length.
This is defined formally as: is not nested l~('n,) ~b~T~f(b)) a is nested where a is the candidate string, f(a) is its frequency in the corpus, eT, is the set of candidate terms that contain a, P(Ta) is the number of these candidate terms.
Two different cases apply: one for terms t h a t are found as nested, and one for terms that are not.
If a candidate string is not found as nested, its termhood is calculated from its total frequency and length.
If it is found as nested, termhood is calculated from its total frequency, length, frequency as a nested string, fiand the tmmber of longer candidate terms it; ai)l)ears in.
The NC-Value metho(1 builds oil this by incorl)orating contextual information in the form of a context factor for each candidate term.
A context word can be any noun, adjective or verb apI)earing within a fixed-size window of tim candidate term.
Each context word is assigned a weight, based on how frequently it appears with a ca lldidate term.
Ttmse weights m'e titan SUllslned for all colltext words relative to a candidate term.
The Context l"actor is combined with the C-Value to form tlm NC-Value: Category Verb Prep Noun Adj Weight 1.2 1.1 0.9 0.7 Table 1: We.ights for categories of boundary words where a is tile candidate term, Cvahte(a) is the Cvalue fin' tlm candidate term, CF(a) is the context factor tbr the candidate term.
Terminological knowledge Ternfinological knowledge concerns the terminological sta.tus of context words.
A context word whicll is also a term (whicll we call a context term) is likely to 1)e a better indicator than one wlfich is not.
The terminological status is determined by applying the NC-Value at)proach to the corlms, and considering tile top third of the list; of ranked results as valid terms.
A context term (CT) weight is then produced fin" each candidate term, based on its total frequency of occurrence with all relewmt context terms.
The CT weight is formally described as follows: Contextual Information: a Term's where a is the candidate term, 7', is the set: of context terms of a, d is a word from Ta, fa(d) is the frequency of d as a context term of a.
Semantic knowledge Semantic knowledge is obtained about context terms using the UMLS Metathesaurus and Semantic Network (NLM, 1997).
The former provides a semantic tag for each term, such as Acquired Abnormality.
The latte, r provides a hierarchy of semantic types, from wlfich we compute the similarity between a candidate term and the context I;erms it occurs with.
An example of part of tim network is shown in Figure Social Life Just as a person's social life can provide valuable clues al)out their i)ersonality, so we can gather much information about the nature of a term by investigating the coral)any it keeps.
We acquire this knowledge by cxtra:ting three different types of contextual information: 1.
syntactic; 2.
terminologic~fl; Syntactic knowledge Syntactic knowledge is based on words in the context which occur immediately t)efore or afl;er a candidatc term, wtfich we call boundary words.
Following "barrier word" al)proaches to term recoglfition (Bourigault, 1992; Nelson et al., 1995), where partitular syntactic categories are used to delimit era> didate terms, we develop this idea fllrther by weighting boundary words according to tlmir category.
The weight for each category, shown in Table 1, is all)cate(1 according to its relative likelihood of occurring with a term as opposed to a non-term.
A verb, therefore, occurring immediately before or after a candidate, term, is statistically a better indicator of a term than an adjective is.
By "a better indicator", we mean that a candidate term occurring with it is more likely to be valid.
Each candidate term is assigned a syntactic weight, calculated by summing the category weights tbr the context bomsdary words occurring with it.
Similarity is measured because we believe that a context term which is semantically similar to a candidate term is more likely to be significant than one wlfieh is less similar.
We use tim method for semantic distance described in (M~\ynard and Ananiadou, 1999a), wtfich is based on calculating the vertical position and horizontal distance between nodes in a hierarchy.
Two weights are cMculated:  positionah measured by the combined distance from root to each node measured by the number of shared common ancestors multiplied by the munber of words (usuMly two).
Similarity between the nodes is calculated by dividing tim commomflity weight by the 1)ositional weight to t)roduce a figure between 0 and 1, I being the ease The Information Weight The three individual weights described above are calculated for all relevant context words or context terms.
The total weights for the context are then combined according to the following equation: beC.
[TAIII OIIGANISM ITAIIlll ALGA Figure 1: Fragment of the Semantic Network where tile two nodes are identical, and 0 being the case where there is no common ancestor.
This is formally defined as follows: where a is the candidate term, Cais the set of context words of a, b is a word from C,, f,(b) is tlm frequency of b as a context word of a, syn~(b) is the syntactic weight of b as a context word of a, T.
is the set of context terms of a, d is a word fl'om T., fi,(d) is the frequency of d as a context term of a, sims(d) is the similarity weight of d as a context term of a.
This basically means t h a t the Infornlation Weight is composed of the total terminological weight, 511151tiplied by tile total semantic weight, and then added to the total syntactic weight of all the context words or context terms related to the candidate term.
where corn(w1...w,~) is the commonality weight of words The SNC-Value pos('wl...w,~) is the positional weight of words Let us take an example from the UMLS.
The similarity between a term t)elonging to the semantic category Plant and one belonging to the category Fungus would be calculated as follows:Tile Information Weight gives a score for each candidate term based on the ilnt)ortance of the contextual intbrmation surrounding it.
To obtain the final SNCValue ranking, the Information Weight is combined with the statistical information obtained using the NC-Vahm nmthod, as expressed formally below: where  Plant has the semantic code T A l l l and Fungus has the semantic code T A l l 2 .  The commonality weight is the number of nodes in common, multiplied by the number of terms we are considering.
T A l l l and T A l l 2 have 4 nodes in common (T, TA, TA1 and T A l l ) . So the weight will be 4 * 2 = 8.
 The positional weight is the total height of each of the terms (where tile root node has a height of 1).
T A l l l has a height of 5 (T, TA, TA1, T A l l and T A l l 1 ), and TAl12 also has a height of 5 (T, TA, TA1, T A l l and T A l l 2 ) . The weight will therefore be 5 + 5 = 10.
 The similarity weight is tile comlnonality weight divided by the positional weight, i.e. a is the candidate t e r m NCValue(a) is the NC-Value of a I W is the Inqmrtance Weight of a For details of the NC-Value, see (l:5'antzi and Ananiadou, 1999).
An example of the final result is shown in Table 2.
This corot)ares tile top 20 results from the SNCValue list with the top 20 from the NC-Value list.
The terms in italics are those which were considered as not valid.
We shall discuss the results in more detail in the next section, but we can note here three points.
Firstly, the weights for the SNC-Value are substantially greater than those for the NC-Vahm.
This, in itself, is not important, since it, is the position in the list, i.e. the relative weight, rather t h a n the absolute weight, which is important.
Secondly, we can see that there are more valid terms in the SNC-Value results than in the NC-Value results.
It Table 2: Top 20 results for the SNC-VaIue and NC-Value in hard to make flu:ther judgements based on this list alone, 1)ecause we cmmot s~3; wlmther on(; ter]u is 1)etter than another, if tiE(; two terms are both valid.
Thirdly, we can nee that more of the top 20 terms are valid tin' tim SNC-Vahm than for the NCValue: 17 (851X,) as ot)t)osed to 10 (50%).
discrei)an(:y 1)etween this lint and the lint validated by the manual experts (only 20% of the terms they judged valid were fOtlEl(1 ill the UMLS).
There are also further limitations to the UMLS, such as the fact that it is only nl)e(:ific to medicine in general, 1)ut not to eye t)athology, and the fact that it; is organised ill nllch a way that only the preferred terms, and not lexical variants, m'e actively and (:onnistently 1)r(~sent.
We first evaluate the similarity weight individually, since this is the main 1)rinciple on which the SNC-\Sflue method relies.
We then ewduate the SNC-VaIue as a whole t)y comparing it with the NCValue, so I;hat we can ewfluate the impact of tile addition of the deel)er forms of linguistic information incorl)orated in {:he hnI)ortance Weight.
Evaluation The SNC-Value method wan initially t(;sted on a eorl)US of 800,000 eye t)athoh)gy reI)ortn, which had 1)een tagged with the Brill t)art-of-nl)eeeh tagger (Brill, 1992).
The ca.ndidate terms we,'e first extracted using the NC-Value method (lhantzi, 1998), and the SNC-Value was then (:alculated.
To exvduate the results, we examined the p(.'rformanee of the similarity weight alone, and the overall 1)erformance of the system.
Similarity Weight Evaluation m e t h o d s The main evaluation i)rocedure was carried out with resl)ect to a manual assessment of tim list of terms l)y 2 domain exI)erts.
There are, however, 1)roblems associated with such an evaluation.
Firstly, there ix no gold standm:d of evaluation, and secondly, manual evaluation is both fallil)le and sul)jective.
To avoid this 1)rol)lem, we measure the 1)erformance of the system ill relative termn rather than in absolute terms, by measuring the improveln(mt over the results of tile NC-Value as eomt)ared with mmmal evahlation.
Although we could have used the list of terms 1)rovided in the UMLS, instead of a manu~ ally evahlated list, we found that there was a huge One of the 1)roblems with our method of calculating similarity is that it relies on a 1)re-existing lexi(:al resource, which Eneans it is 1)rone to errors and omissions.
Bearing in mind its innate inadequacies, we can nevertheless evaluate the expected theoretical performance of tilt measure by concerning ourselves only with what is covered by the thesaurus.
This means that we assume COml)leteness (although we know that this in not the case) and evahtate it accordingly, ignoring anything which may be inissing.
The semantic weight ix based on the premise that tile more similar a context term is to the candidate term it occurs with, the better an indicator that context term is.
So the higher the total semantic weight Section top set middle set b o t t o m set Table 3: Semantic weights of terms and non-terms for the candidate term, the higher the ranking of the term and the better the chance that the candidate term is a valid one.
To test the performmme of the semantic weight, we sorted the terms in descending order of their semantic weights and divided the list into 3, such that the top third contained the terms with the highest semantic weights, and the b o t t o m third contained those with the lowest.
We then compared how m a n y valid and non-valid terms (according to the manual evaluation) were contained in each section of the list,.
Tile results, depicted in Table 3, can be interpreted as follows.
In the top third of the list;, 76% were terms and 24% were non-terms, whilst in the middle third, 56% were terms and 44% were non-terms, and so on.
This means that most of the valid terms are contained in the top third of tile list mid the fewest valid terms are contained in the bottom third of the list.
Also, the proportion of terms to non-terms in tile top of tile list is such that there are more terms than non-terms, whereas in the b o t t o m of the list; there are more non-terms than ternis.
This therefore demonstrates two things:  more of' the terms with the highest semantic weights are valid, and fewer of those with the lowest semmitic weights are valid;  more valid terms have high semantic weights than non-terms, mid more non-terms have lower semantic weights than valid terms.
We also tested the similarity measure to see whether adding sosne statistical information would improve its results, and regulate any discrepancies in tile uniformity of the hierarchy.
The methods which intuitively seem most plausible are based on information content, e.g.(Resnik, 1995; Smeaton and Quigley, 1996).
The informatiosl content of a node is related to its probability of occurrence in the corpus.
Tile snore fi'equently it appears, the snore likely it is to be important in terms of conveying information, and therefore the higher weighting it should receive.
We performed experiments to cosnpare two such methods with our similarity measure.
The first considers the probability of the MSCA of the two terms (the lowest node which is an ancestor of both), whilst the second considers the probability of the nodes of the terms being colnpared.
However, the tindings showed a negligible difference between the three methods, so we conchlde that there is no Table 4: Precision of SNC-Vahle and NC-Value advantage to be gained by adding statistical int'ormation, fbr this particular corpus.
It; is possible that with a larger corlms or different hierarchy, this might slot be the case.
Overall E v a l u a t i o n of t h e S N C V a l u e We first; compare the precision rates for the SNCValue and the NC-Value (Table 4), by dividing tile ranked lists into 10 equal sections.
Each section contains 250 terms, marked as valid or invalid by the manual experts.
In the top section, the precision is higher for the SNC-Value, and in the b o t t o m section, it is lower.
This indicates that the precision span is greater fl~r the SNC-Value, and therefore that the ranking is improved.
The distribution of valid terms is also better for the SNC-Value, since of the valid terms, more appear at the top of the list than at the bottom.
Looking at Figure 2, we can see that the SNCValue graph is smoother than that of the NC-Vahle.
We can compare the graphs niore accurately using a method we call comparative upward trend.
Becruise there is no one ideal graph, we instead measure how much each graph deviates from a monotonic line downwards.
This is calculated by dividing the total rise in precision percentage by the length of the graph.
A graph with a lower upward trend will therefore be better than a graph with a higher upward trend.
If we compare the upward trends of the two graphs, we find that the trend for the SNCValue is 0.9, whereas the trend for the NC-Value is 2.7.
This again shows that the SNC-Value rmiking is better thmi the NC-Value ranking, since it is more consistent.
Table 5 shows a more precise investigation of the top portion of the list, (where it is to be expected that ternis are most likely to be wflid, and which is therefore the inost imi)ortant part of the list) We see that the precision is most iml)roved here, both in terms of accuracy and in terms of distribution of weights.
At the I)ottom of the top section, the PlccJshm T~ T T I Scctionollist tics for creating such a thesaurus automatically, or entrancing an existing one, using the contextual information we acquire (Ushioda, 1996; MaynaM and Anmfiadou, 1999b).
There is much scope tbr filrther extensions of this research.
Firstly, it; could be extended to other (lomains and larger corpora, in order to see the true benefit of such a.n apl)roach.
Secondly, the thesaurus could be tailored to the corpus, as we have mentioncd.
An incremental approach might be possible, whereby the similarity measure is combined with statistical intbrmation to tune an existing ontology.
Also, the UMLS is not designed as a linguistic resource, but as an information resource.
Some kind of integration of the two types of resource would be usefifl so that, for example, lexical variation could be more easily handled.
Table 5: Precision of SNC-\Sdue and NC-Vahm for top 250 terms precision is much higher for the SNC-Value.
This is important because ideally, all the terms in this part of the list should be valid, 7 Conclusions In this paper, we have described a method for multiword term extraction which improves on traditional statistical at)proaches by incorporating more specific contextual information.
It focuses particularly on measuring the strength of association (in semantic terms) l)etween a candidate term and its context.
Evahlation shows imi)rovement over the NC-Vahm approach, although the percentages are small.
This is largely l)ecmlse we have used a very small corpus for testing.
The contextuM information acquired can also be used for a mmlber of other related tasks, such as disambiguation and clustering.
C1D2D8CTD6D0CTCPDACTCS D7CTD1CPD2D8CXCR CXD2D8CTD6D4D6CTD8CPD8CXD3D2 CXD2 CTD2DACXD6D3D2D1CTD2D8B9CQCPD7CTCS D4CPD6D7CXD2CV A3 CFCXD0D0CXCPD1 CBCRCWD9D0CTD6 BVD3D1D4D9D8CTD6 CPD2CS C1D2CUD3D6D1CPD8CXD3D2 CBCRCXCTD2CRCT BWCTD4D8BA CDD2CXDACTD6D7CXD8DD D3CU C8CTD2D2D7DDD0DACPD2CXCP C8CWCXD0CPCSCTD0D4CWCXCPB8 C8BT BDBLBDBCBF D7CRCWD9D0CTD6BSD0CXD2CRBACRCXD7BAD9D4CTD2D2BACTCSD9 BTCQD7D8D6CPCRD8 CCCWCXD7 D4CPD4CTD6 CTDCD8CTD2CSD7 CP D4D3D0DDD2D3D1CXCPD0B9D8CXD1CT D4CPD6D7CXD2CV CPD0B9 CVD3D6CXD8CWD1 D8CWCPD8 D6CTD7D3D0DACTD7 D7D8D6D9CRD8D9D6CPD0 CPD1CQCXCVD9CXD8DD CXD2 CXD2D4D9D8 D7CTD2D8CTD2CRCTD7 CQDD CRCPD0CRD9D0CPD8CXD2CV CPD2CS CRD3D1D4CPD6CXD2CV D8CWCT CSCTD2D3B9 D8CPD8CXD3D2D7 D3CU D6CXDACPD0 CRD3D2D7D8CXD8D9CTD2D8D7B8 CVCXDACTD2 D7D3D1CT D1D3CSCTD0 D3CU D8CWCT CPD4D4D0CXCRCPD8CXD3D2 CTD2DACXD6D3D2D1CTD2D8 B4CBCRCWD9D0CTD6B8 BEBCBCBDB5BA CCCWCT CPD0CVD3D6CXD8CWD1 CXD7 CTDCD8CTD2CSCTCS D8D3 CXD2CRD3D6D4D3D6CPD8CT CP CUD9D0D0 D7CTD8 D3CU D0D3CVCXCRCPD0 D3D4CTD6CPD8D3D6D7B8 CXD2CRD0D9CSCXD2CV D5D9CPD2D8CXACCTD6D7 CPD2CS CRD3D2CYD9D2CRB9 D8CXD3D2D7B8 CXD2D8D3 D8CWCXD7 CRCPD0CRD9D0CPD8CXD3D2 DBCXD8CWD3D9D8 CXD2CRD6CTCPD7CXD2CV D8CWCT CRD3D1D4D0CTDCCXD8DD D3CU D8CWCT D3DACTD6CPD0D0 CPD0CVD3D6CXD8CWD1 CQCTDDD3D2CS D4D3D0DDD2D3B9 D1CXCPD0 D8CXD1CTB8 CQD3D8CW CXD2 D8CTD6D1D7 D3CU D8CWCT D0CTD2CVD8CW D3CU D8CWCT CXD2B9 D4D9D8 CPD2CS D8CWCT D2D9D1CQCTD6 D3CU CTD2D8CXD8CXCTD7 CXD2 D8CWCT CTD2DACXD6D3D2D1CTD2D8 D1D3CSCTD0BA BD C1D2D8D6D3CSD9CRD8CXD3D2 CCCWCT CSCTDACTD0D3D4D1CTD2D8 D3CU D7D4CTCPCZCTD6B9CXD2CSCTD4CTD2CSCTD2D8 D1CXDCCTCSB9 CXD2CXD8CXCPD8CXDACT D7D4CTCTCRCWCXD2D8CTD6CUCPCRCTD7B8 CXD2 DBCWCXCRCW D9D7CTD6D7 D2D3D8 D3D2D0DD CPD2D7DBCTD6 D5D9CTD7D8CXD3D2D7 CQD9D8 CPD0D7D3 CPD7CZ D5D9CTD7D8CXD3D2D7 CPD2CS CVCXDACT CXD2B9 D7D8D6D9CRD8CXD3D2D7B8 CXD7 CRD9D6D6CTD2D8D0DD D0CXD1CXD8CTCS CQDD D8CWCT CXD2CPCSCTD5D9CPCRDD D3CU CTDCCXD7D8CXD2CV CRD3D6D4D9D7B9CQCPD7CTCS CSCXD7CPD1CQCXCVD9CPD8CXD3D2 D8CTCRCWD2CXD5D9CTD7BA CCCWCXD7 D4CPD4CTD6 CTDCD4D0D3D6CTD7 D8CWCT D9D7CT D3CU D7CTD1CPD2D8CXCR CPD2CS D4D6CPCVB9 D1CPD8CXCR CXD2CUD3D6D1CPD8CXD3D2B8 CXD2 D8CWCT CUD3D6D1 D3CU D8CWCT CTD2D8CXD8CXCTD7 CPD2CS D6CTD0CPD8CXD3D2D7 CXD2 D8CWCT CXD2D8CTD6CUCPCRCTCS CPD4D4D0CXCRCPD8CXD3D2B3D7 D6D9D2B9D8CXD1CT CTD2B9 DACXD6D3D2D1CTD2D8B8 CPD7 CPD2 CPCSCSCXD8CXD3D2CPD0 D7D3D9D6CRCT D3CU CXD2CUD3D6D1CPD8CXD3D2 D8D3 CVD9CXCSCT CSCXD7CPD1CQCXCVD9CPD8CXD3D2BA C1D2 D4CPD6D8CXCRD9D0CPD6B8 D8CWCXD7 D4CPD4CTD6 CTDCD8CTD2CSD7 CPD2 CTDCCXD7D8CXD2CV D4CPD6D7B9 CXD2CV CPD0CVD3D6CXD8CWD1 D8CWCPD8 CRCPD0CRD9D0CPD8CTD7 CPD2CS CRD3D1D4CPD6CTD7 D8CWCT CSCTB9 D2D3D8CPD8CXD3D2D7 D3CU D6CXDACPD0 D4CPD6D7CT D8D6CTCT CRD3D2D7D8CXD8D9CTD2D8D7 CXD2 D3D6CSCTD6 D8D3 D6CTD7D3D0DACT D7D8D6D9CRD8D9D6CPD0 CPD1CQCXCVD9CXD8DD CXD2 CXD2D4D9D8 D7CTD2D8CTD2CRCTD7 B4CBCRCWD9D0CTD6B8 BEBCBCBDB5BA CCCWCT CPD0CVD3D6CXD8CWD1 CXD7 CTDCD8CTD2CSCTCS D8D3 CXD2CRD3D6B9 D4D3D6CPD8CT CP CUD9D0D0 D7CTD8 D3CU D0D3CVCXCRCPD0 D3D4CTD6CPD8D3D6D7 CXD2D8D3 D8CWCXD7 CRCPD0CRD9B9 D0CPD8CXD3D2 D7D3 CPD7 D8D3 CXD1D4D6D3DACT D8CWCT CPCRCRD9D6CPCRDD D3CU D8CWCT D6CTD7D9D0D8CXD2CV CSCTD2D3D8CPD8CXD3D2D7 DF CPD2CS D8CWCTD6CTCQDD CXD1D4D6D3DACT D8CWCT CPCRCRD9D6CPCRDD D3CU D4CPD6D7CXD2CV DF DBCXD8CWD3D9D8 CXD2CRD6CTCPD7CXD2CV D8CWCT CRD3D1D4D0CTDCCXD8DD D3CU D8CWCT D3DACTD6CPD0D0 CPD0CVD3D6CXD8CWD1 CQCTDDD3D2CS D4D3D0DDD2D3D1CXCPD0 D8CXD1CT B4CQD3D8CW CXD2 D8CTD6D1D7 D3CU D8CWCT D0CTD2CVD8CW D3CU D8CWCT CXD2D4D9D8 CPD2CS D8CWCT D2D9D1CQCTD6 D3CU CTD2D8CXD8CXCTD7 CXD2 D8CWCT CTD2DACXD6D3D2D1CTD2D8 D1D3CSCTD0B5BA CCCWCXD7 D4CPD6D7CXD1D3D2DD CXD7 CPCRCWCXCTDACTCS CQDD D0D3CRCPD0CXDECXD2CV CRCTD6D8CPCXD2 CZCXD2CSD7 D3CU D7CTD1CPD2D8CXCR D6CTD0CPD8CXD3D2D7 CSD9D6CXD2CV D4CPD6D7CXD2CVB8 D4CPD6D8CXCRD9D0CPD6D0DD D8CWD3D7CT CQCTD8DBCTCTD2 D5D9CPD2D8CXACCTD6D7 CPD2CS D8CWCTCXD6 D6CTD7D8D6CXCRD8D3D6 CPD2CS CQD3CSDD CPD6CVD9D1CTD2D8D7 A3 CCCWCT CPD9D8CWD3D6 DBD3D9D0CS D0CXCZCT D8D3 D8CWCPD2CZ BWCPDACXCS BVCWCXCPD2CVB8 C3CPD6CXD2 C3CXD4B9 D4CTD6B8 CPD2CS BTD0CTDCCPD2CSCTD6 C3D3D0D0CTD6B8 CPD7 DBCTD0D0 CPD7 D8CWCT CPD2D3D2DDD1D3D9D7 D6CTDACXCTDBCTD6D7 CUD3D6 CRD3D1D1CTD2D8D7 D3D2 D8CWCXD7 D1CPD8CTD6CXCPD0BA CCCWCXD7 DBD3D6CZ DBCPD7 D4CPD6D8CXCPD0D0DD D7D9D4B9 D4D3D6D8CTCS CQDD C6CBBY C1C1CBB9BLBLBCBCBEBLBJ CPD2CS BWBTCAC8BT C6BIBIBCBCBDB9BCBCB9BDB9BKBLBDBHBA B4D7CXD1CXD0CPD6 D8D3 D8CWCT DBCPDD CSCTD4CTD2CSCTD2CRCXCTD7 CQCTD8DBCTCTD2 D4D6CTCSCXCRCPD8CT CPD2CS CPD6CVD9D1CTD2D8 CWCTCPCS DBD3D6CSD7 CPD6CT D0D3CRCPD0CXDECTCS CXD2 D0CTDCCXCRCPD0CXDECTCS CUD3D6D1CPD0CXD7D1D7 D7D9CRCW CPD7 D8D6CTCT CPCSCYD3CXD2CXD2CV CVD6CPD1D1CPD6D7B5B8 CXD2 D3D6B9 CSCTD6 D8D3 CPDAD3CXCS CRCPD0CRD9D0CPD8CXD2CV CTDCD4D3D2CTD2D8CXCPD0 CWCXCVCWCTD6B9D3D6CSCTD6 CSCTB9 D2D3D8CPD8CXD3D2D7 CUD3D6 CTDCD4D6CTD7D7CXD3D2D7 D0CXCZCT CVCTD2CTD6CPD0CXDECTCS D5D9CPD2D8CXACCTD6D7BA BE BUCPD7CXCR CPD0CVD3D6CXD8CWD1 CCCWCXD7 D7CTCRD8CXD3D2 CSCTD7CRD6CXCQCTD7 D8CWCT CQCPD7CXCR CTD2DACXD6D3D2D1CTD2D8B9CQCPD7CTCS D4CPD6D7CTD6 B4CBCRCWD9D0CTD6B8 BEBCBCBDB5 DBCWCXCRCW DBCXD0D0 CQCT CTDCD8CTD2CSCTCS CXD2 CBCTCRB9 D8CXD3D2 BFBA BUCTCRCPD9D7CT CXD8 DBCXD0D0 CRD6D9CRCXCPD0D0DD D6CTD0DD D3D2 D8CWCT CSCTD2D3D8CPB9 D8CXD3D2D7 B4D3D6 CXD2D8CTD6D4D6CTD8CPD8CXD3D2D7B5 D3CU D4D6D3D4D3D7CTCS CRD3D2D7D8CXD8D9CTD2D8D7 CXD2 D3D6CSCTD6 D8D3 CVD9CXCSCT CSCXD7CPD1CQCXCVD9CPD8CXD3D2B8 D8CWCT D4CPD6D7CTD6 DBCXD0D0 CQCT CSCTACD2CTCS D3D2 CRCPD8CTCVD3D6CXCPD0 CVD6CPD1D1CPD6D7 B4BTCYCSD9CZCXCTDBCXCRDEB8 BDBLBFBHBN BUCPD6B9C0CXD0D0CTD0B8 BDBLBHBFB5B8 DBCWD3D7CT CRCPD8CTCVD3D6CXCTD7 CPD0D0 CWCPDACTDBCTD0D0 CSCTB9 ACD2CTCS D8DDD4CTD7 CPD2CS DBD3D6D7D8B9CRCPD7CT CSCTD2D3D8CPD8CXD3D2D7BA CCCWCTD7CT CRCPD8B9 CTCVD3D6CXCTD7 CPD6CT CSD6CPDBD2 CUD6D3D1 CP D1CXD2CXD1CPD0 D7CTD8 D3CU D7DDD1CQD3D0D7 BV D7D9CRCW D8CWCPD8BM C6C8 BEBVCPD2CS CB BEBVBN CXCU ADBN BEBVD8CWCTD2 ADBP BEBVCPD2CS ADD2 BEBVBM C1D2D8D9CXD8CXDACTD0DDB8 D8CWCT CRCPD8CTCVD3D6DD C6C8 CSCTD7CRD6CXCQCTD7 CP D2D3D9D2 D4CWD6CPD7CT CPD2CS D8CWCT CRCPD8CTCVD3D6DD CB CSCTD7CRD6CXCQCTD7 CP D7CTD2D8CTD2CRCTB8 CPD2CS D8CWCT CRD3D1D4D0CTDC CRCPD8CTCVD3D6CXCTD7 ADBPCPD2CS ADD2 CSCTD7CRD6CXCQCT COCP AD D0CPCRCZCXD2CV CP  D8D3 D8CWCT D6CXCVCWD8B3 CPD2CS COCP AD D0CPCRCZCXD2CV CP  D8D3 D8CWCT D0CTCUD8B3 D6CTD7D4CTCRD8CXDACTD0DDBN D7D3 CUD3D6 CTDCCPD1D4D0CT CBD2C6C8 DBD3D9D0CS CSCTD7CRD6CXCQCT CP CSCTCRD0CPD6CPD8CXDACTDACTD6CQ D4CWD6CPD7CT D0CPCRCZCXD2CV CPD2 C6C8 D7D9CQCYCTCRD8 D8D3 CXD8D7 D0CTCUD8 CXD2 D8CWCT CXD2D4D9D8BA CCCWCT D8DDD4CT CC CPD2CS DBD3D6D7D8B9CRCPD7CT B4D1D3D7D8 CVCTD2CTD6CPD0B5 CSCTD2D3D8CPB9 D8CXD3D2 CF D3CU CTCPCRCW D4D3D7D7CXCQD0CT CRCPD8CTCVD3D6DD CPD6CT CSCTACD2CTCS CQCTD0D3DBB8 CVCXDACTD2 CP D7CTD8 D3CU CTD2D8CXD8CXCTD7 BX CPD7 CPD2 CTD2DACXD6D3D2D1CTD2D8BM CCB4CBB5 BP D8 BM D8D6D9D8CW DACPD0D9CT CFB4CBB5 BP CUCCCACDBXBNBYBTC4CBBXCV CCB4C6C8B5BPCT BMCTD2D8CXD8DD CFB4C6C8B5 BP BX CCB4ADBPB5BPCWCCB4B5BNCCB4ADB5CX CFB4ADBPB5BPCFB4B5 A2CFB4ADB5 CCB4ADD2B5BPCWCCB4B5BNCCB4ADB5CX CFB4ADD2B5BPCFB4B5 A2CFB4ADB5 CCCWCT CSCTD2D3D8CPD8CXD3D2 BW D3CU CPD2DD D4D6D3D4D3D7CTCS CRD3D2D7D8CXD8D9CTD2D8 CXD7 CRD3D2D7D8D6CPCXD2CTCS D8D3 CQCT CP D7D9CQD7CTD8 D3CU D8CWCT DBD3D6D7D8B9CRCPD7CT CSCTD2D3B9 D8CPD8CXD3D2 CF D3CU D8CWCT CRD3D2D7D8CXD8D9CTD2D8B3D7 CRCPD8CTCVD3D6DDBN D7D3 CP CRD3D2B9 D7D8CXD8D9CTD2D8 D3CU CRCPD8CTCVD3D6DD C6C8 DBD3D9D0CS CSCTD2D3D8CT CP D7CTD8 D3CU CTD2B9 D8CXD8CXCTD7B8 CUCT BD BNCT BE BNBMBMBMCVB8 CPD2CS CP CRD3D2D7D8CXD8D9CTD2D8 D3CU CRCPD8CTCVD3D6DD CBD2C6C8 DBD3D9D0CS CSCTD2D3D8CT CP D7CTD8 D3CU CTD2D8CXD8DD A2 D8D6D9D8CW DACPD0D9CT D4CPCXD6D7B8 CUCWCT BD BNCCCACDBXCXBNCWCT BE BNBYBTC4CBBXCXBNBMBMBMCVBA C6D3D8CT D8CWCPD8 D2D3 CSCTD2D3D8CPD8CXD3D2 D3CU CP CRD3D2D7D8CXD8D9CTD2D8 CRCPD2 CRD3D2D8CPCXD2 D1D3D6CT D8CWCPD2 C7B4CYBXCY DA B5 CSCXABCTD6CTD2D8 CTD0CTD1CTD2D8D7B8 DBCWCTD6CT DA CXD7 CP DACPD0CTD2CRDD D1CTCPB9 D7D9D6CT D3CU D8CWCT D2D9D1CQCTD6D3CUC6C8D7DDD1CQD3D0D7 D3CRCRD9D6D6CXD2CV DBCXD8CWCXD2 D8CWCT CRD3D2D7D8CXD8D9CTD2D8B3D7 CRCPD8CTCVD3D6DDBA CCCWCXD7 D4CPD4CTD6 DBCXD0D0 D9D7CT D8CWCT CUD3D0D0D3DBCXD2CV CSCTACD2CXD8CXD3D2 D3CU CP CRCPD8CTCVD3D6CXCPD0 CVD6CPD1D1CPD6 B4BVBZB5BM BWCTACD2CXD8CXD3D2 BT CRCPD8CTCVD3D6CXCPD0 CVD6CPD1D1CPD6 BZ CXD7 CP CUD3D6D1CPD0 CVD6CPD1D1CPD6 B4C6BNA6BNC8B5 D7D9CRCW D8CWCPD8BM AF A6 CXD7 CP ACD2CXD8CT D7CTD8 D3CU DBD3D6CSD7 DBBN AF C8 CXD7 CP ACD2CXD8CT D7CTD8 D3CU D4D6D3CSD9CRD8CXD3D2D7 CRD3D2D8CPCXD2CXD2CVBM AD AX DB CUD3D6 CPD0D0 DBBEA6B8 DBCXD8CW AD BEBVB8 AD AX ADBP  CUD3D6 CTDACTD6DD D6D9D0CT ADBP AX BMBMBM CXD2 C8B8 AD AX  ADD2 CUD3D6 CTDACTD6DD D6D9D0CT ADD2 AX BMBMBM CXD2 C8B8 CPD2CS D2D3D8CWCXD2CV CTD0D7CTBN AF C6 CXD7 D8CWCT D2D3D2D8CTD6D1CXD2CPD0 D7CTD8 CUAD CY AD AX BMBMBM BE C8CVBA CPD2CS D8CWCT CUD3D0D0D3DBCXD2CV CSCTCSD9CRD8CXDACT D4CPD6D7CTD6B8 BD DBCWCXCRCW DBCXD0D0 CQCT CTDCD8CTD2CSCTCS D0CPD8CTD6 D8D3 CWCPD2CSD0CT CP D6CXCRCWCTD6 D7CTD8 D3CU D7CTD1CPD2D8CXCR D3D4B9 CTD6CPD8CXD3D2D7BA CCCWCT D4CPD6D7CTD6 CXD7 CSCTACD2CTCS DBCXD8CWBM AF CRD3D2D7D8CXD8D9CTD2D8CRCWCPD6D8 CXD8CTD1D7 CJCXBNCYBNADCL CSD6CPDBD2 CUD6D3D1 C1 D2 BC A2 C1 D2 BC A2C6B8 CXD2CSCXCRCPD8CXD2CV D8CWCPD8 D4D3D7CXD8CXD3D2D7 CX D8CWD6D3D9CVCW CY CXD2 D8CWCT CXD2D4D9D8 CRCPD2 CQCT CRCWCPD6CPCRD8CTD6CXDECTCS CQDD CRCPD8CTCVD3D6DD ADBN AF CP D0CTDCCXCRCPD0 CXD8CTD1 CJCXBNCYBNADCL CUD3D6 CTDACTD6DD D6D9D0CT AD AX DB BE C8 CXCU DB D3CRCRD9D6D7 CQCTD8DBCTCTD2 D4D3D7CXD8CXD3D2D7 CX CPD2CS CY CXD2 D8CWCT CXD2D4D9D8BN AF CP D7CTD8 D3CU D6D9D0CTD7 D3CU D8CWCT CUD3D6D1BM CJCXBNCZBNADBPCLCJCZBNCYBNCL CJCXBNCYBNADCL CUD3D6 CPD0D0 AD AX ADBP  BE C8BN CXBNCYBNCZ BE C1 D2 BC B8 CJCZBNCYBNADD2CLCJCXBNCZBNCL CJCXBNCYBNADCL CUD3D6 CPD0D0 AD AX  ADD2 BE C8BN CXBNCYBNCZ BE C1 D2 BC BA CPD2CS CRCPD2 D6CTCRD3CVD2CXDECT CPD2 D2B9D0CTD2CVD8CW CXD2D4D9D8 CPD7 CP CRD3D2D7D8CXD8D9CTD2D8 D3CU CRCPD8CTCVD3D6DD AD B4CUD3D6 CTDCCPD1D4D0CTB8 CPD7 CPD2 CBB5 CXCU CXD8 CRCPD2 CSCTCSD9CRCT D8CWCT CRCWCPD6D8 CXD8CTD1 CJBCBND2BNADCLBA CCCWCXD7 D4CPD6D7CTD6 CRCPD2 CQCT CXD1D4D0CTD1CTD2D8CTCS CXD2 CP CSDDD2CPD1CXCR D4D6D3B9 CVD6CPD1D1CXD2CV CPD0CVD3D6CXD8CWD1B8 D9D7CXD2CV D8CWCT D6CTCRD9D6D7CXDACT CUD9D2CRD8CXD3D2BM BYB4DCB5BP CN CP BD BMBMBMCP CZ D7BMD8BM CP BD BMBMBMCP CZ DC CZ CM CXBPBD BYB4CP CX B5 B4DBCWCTD6CT DCBNCP BD BMBMBMCP CZ CPD6CT D4D6D3D4D3D7CTCS CRD3D2D7D8CXD8D9CTD2D8D7 CSD6CPDBD2 CUD6D3D1 C1 D2 BC A2C1 D2 BC A2C6B8 CF BN BPBYBTC4CBBXB8 CPD2CS CE BN BPCCCACDBXB5B8 CQDD D6CTCRD3D6CSCXD2CV D8CWCT D6CTD7D9D0D8 D3CU CTDACTD6DD D6CTCRD9D6D7CXDACT D7D9CQB9CRCPD0D0 D8D3 BYB4DCB5CXD2CPCRCWCPD6D8B8 D8CWCTD2 CRD3D2D7D9D0D8CXD2CV D8CWCXD7 CRCWCPD6D8 D3D2 D7D9CQB9 D7CTD5D9CTD2D8 CRCPD0D0D7 D8D3 BYB4DCB5 CUD3D6 D8CWCT D7CPD1CT DC CRD3D2D7D8CXD8D9CTD2D8BA BE CBCXD2CRCT D8CWCT CXD2CSCXCRCTD7 CXD2 CTDACTD6DD D6D9D0CTB3D7 CPD2D8CTCRCTCSCTD2D8 CRD3D2B9 D7D8CXD8D9CTD2D8D7 CP BD BMBMBMCP CZ CTCPCRCW CRD3DACTD6 D7D1CPD0D0CTD6 D7D4CPD2D7 D8CWCPD2 D8CWD3D7CT CXD2 D8CWCT CRD3D2D7CTD5D9CTD2D8 DCB8 D8CWCT CPD0CVD3D6CXD8CWD1 DBCXD0D0 D2D3D8 CTD2D8CTD6 CXD2D8D3 CPD2 CXD2ACD2CXD8CT D6CTCRD9D6D7CXD3D2BN CPD2CS D7CXD2CRCT D8CWCTD6CT CPD6CT D3D2D0DD D2 BE CYC6CY CSCXABCTD6CTD2D8DACPD0D9CTD7 D3CU DCB8 CPD2CS D3D2D0DD BED2 CSCXABCTD6B9 CTD2D8 D6D9D0CTD7 D8CWCPD8 CRD3D9D0CS D4D6D3DACTCPD2DDCRD3D2D7CTD5D9CTD2D8 DC B4D8DBD3D6D9D0CT CUD3D6D1D7 CUD3D6 BP CPD2CS D2B8 CTCPCRCW DBCXD8CW D2 CSCXABCTD6CTD2D8DACPD0D9CTD7 D3CU CZB5B8 D8CWCT CPD0CVD3D6CXD8CWD1 D6D9D2D7 CXD2 D4D3D0DDD2D3D1CXCPD0 D8CXD1CTBM C7B4D2 BF CYC6CYB5BA CCCWCT D6CTD7D9D0D8CXD2CV CRCWCPD6D8 CRCPD2 D8CWCTD2 CQCT CPD2D2D3D8CPD8CTCS DBCXD8CW CQCPCRCZ D4D3CXD2D8CTD6D7 D8D3 D4D6D3CSD9CRCT CP D4D3D0DDD2D3D1CXCPD0B9D7CXDECTCS D7CWCPD6CTCS CUD3D6CTD7D8 BD BYD3D0D0D3DBCXD2CV CBCWCXCTCQCTD6 CTD8 CPD0BA B4BDBLBLBHB5BA BE BYD3D0D0D3DBCXD2CV BZD3D3CSD1CPD2 B4BDBLBLBLB5BA D6CTD4D6CTD7CTD2D8CPD8CXD3D2 D3CU CPD0D0 D4D3D7D7CXCQD0CT CVD6CPD1D1CPD8CXCRCPD0 D8D6CTCTD7 B4BUCXD0B9 D0D3D8 CPD2CS C4CPD2CVB8 BDBLBKBLB5BA CCD6CPCSCXD8CXD3D2CPD0 CRD3D6D4D9D7B9CQCPD7CTCS D4CPD6D7CTD6D7 D7CTD0CTCRD8 D4D6CTCUCTD6D6CTCS D8D6CTCTD7 CUD6D3D1 D7D9CRCW CUD3D6CTD7D8D7 CQDD CRCPD0CRD9D0CPD8CXD2CV CECXD8CTD6CQCX D7CRD3D6CTD7 CUD3D6 CTCPCRCW D4D6D3D4D3D7CTCS CRD3D2D7D8CXD8D9CTD2D8B8 CPCRCRD3D6CSCXD2CV D8D3 D8CWCT D6CTB9 CRD9D6D7CXDACT CUD9D2CRD8CXD3D2BM CB CE B4DCB5BP D1CPDC CP BD BMBMBMCP CZ D7BMD8BM CP BD BMBMBMCP CZ DC AW CZ CH CXBPBD CB CE B4CP CX B5 AX A1C8B4CP BD BMBMBMCP CZ CY DCB5 CCCWCTD7CT D7CRD3D6CTD7 CRCPD2 CQCT CRCPD0CRD9D0CPD8CTCS CXD2 D4D3D0DDD2D3D1CXCPD0 D8CXD1CTB8 D9D7CXD2CV D8CWCT D7CPD1CT CSDDD2CPD1CXCR D4D6D3CVD6CPD1D1CXD2CV CPD0CVD3D6CXD8CWD1 CPD7 D8CWCPD8 CSCTD7CRD6CXCQCTCS CUD3D6 D4CPD6D7CXD2CVBA BT D8D6CTCT CRCPD2 D8CWCTD2 CQCT D7CTB9 D0CTCRD8CTCSB8 CUD6D3D1 D8CWCT D8D3D4 CSD3DBD2B8 CQDD CTDCD4CPD2CSCXD2CV D8CWCT CWCXCVCWCTD7D8B9 D7CRD3D6CXD2CV D6D9D0CT CPD4D4D0CXCRCPD8CXD3D2 CUD3D6 CTCPCRCW CRD3D2D7D8CXD8D9CTD2D8BA CCCWCT CTD2DACXD6D3D2D1CTD2D8B9CQCPD7CTCS D4CPD6D7CTD6 CSCTD7CRD6CXCQCTCS CWCTD6CT D9D7CTD7 CP D7CXD1CXD0CPD6D1CTCRCWCPD2CXD7D1 D8D3 D7CTD0CTCRD8 D4D6CTCUCTD6D6CTCSD8D6CTCTD7B8 CQD9D8 D8CWCT D7CRD3D6CTD7 CPD6CT CQCPD7CTCS D3D2 D8CWCT D4D6CTD7CTD2CRCT D3D6 CPCQD7CTD2CRCT D3CU CTD2D8CXB9 D8CXCTD7 CXD2 D8CWCT CSCTD2D3D8CPD8CXD3D2 B4CXD2D8CTD6D4D6CTD8CPD8CXD3D2B5 D3CU CTCPCRCW D4D6D3B9 D4D3D7CTCS CRD3D2D7D8CXD8D9CTD2D8BM BF CB BW B4DCB5BP D1CPDC CP BD BMBMBMCP CZ D7BMD8BM CP BD BMBMBMCP CZ DC AW CZ CG CXBPBD CB BW B4CP CX B5 AX B7 B4 BD CXCU BWB4DCB5BIBPBN BC D3D8CWCTD6DBCXD7CT DBCWCTD6CT D8CWCT CSCTD2D3D8CPD8CXD3D2 BWB4DCB5 D3CU CP D4D6D3D4D3D7CTCS CRD3D2D7D8CXD8D9CTD2D8 DC CXD7 CRCPD0CRD9D0CPD8CTCS D9D7CXD2CV CPD2D3D8CWCTD6 D6CTCRD9D6D7CXDACT CUD9D2CRD8CXD3D2BM BWB4DCB5BP CJ CP BD BMBMBMCP CZ D7BMD8BM CP BD BMBMBMCP CZ DC AW AP CZ D3D2 CXBPBD BWB4CP CX B5 AX D3D2 B4 CAB4DCB5 CXCU CZ BPBC CUCWCXCV D3D8CWCTD6DBCXD7CT CXD2 DBCWCXCRCW CAB4DCB5 CXD7 CP D0CTDCCXCRCPD0 D6CTD0CPD8CXD3D2 CSCTACD2CTCS CUD3D6 CTCPCRCW CPDCCXD3D1 DC D3CU CRCPD8CTCVD3D6DD AD CTD5D9CPD0 D8D3 D7D3D1CT D7D9CQD7CTD8 D3CU ADB3D7 DBD3D6D7D8B9CRCPD7CT CSCTD2D3D8CPD8CXD3D2 CFB4ADB5B8 CPD7 CSCTACD2CTCS CPCQD3DACTBA BG CCCWCT D3D4CTD6CPD8D3D6 D3D2 CXD7 D2CPD8D9D6CPD0 B4D6CTD0CPD8CXD3D2CPD0B5 CYD3CXD2 D3D2 D8CWCT ACCTD0CSD7 D3CU CXD8D7 D3D4CTD6CPD2CSD7BM BTD3D2BU BP CUCWCT BD BMBMBMCT D1CPDCB4CPBNCQB5 CXCYCWCT BD BMBMBMCT CP CXBEBTBNCWCT BD BMBMBMCT CQ CXBEBUCV DBCWCTD6CT CPBNCQ AL BCBN CPD2CS AP CXD7 CP D4D6D3CYCTCRD8CXD3D2 D8CWCPD8 D6CTD1D3DACTD7 D8CWCT ACD6D7D8 CTD0CTD1CTD2D8 D3CU D8CWCT D6CTD7D9D0D8 B4CRD3D6D6CTD7D4D3D2CSCXD2CV D8CWCT D1D3D7D8 D6CTCRCTD2D8D0DD CSCXD7CRCWCPD6CVCTCS CPD6CVD9D1CTD2D8 D3CU D8CWCT CWCTCPCS D3D6 CUD9D2CRD8D3D6 CRCPD8CTCVD3D6DDB5BM APBT BP CUCWCT BE BMBMBMCT CP CXCYCWCT BD BMBMBMCT CP CXBEBTCV CCCWCXD7 CXD2D8CTD6D0CTCPDACXD2CV D3CU D7CTD1CPD2D8CXCR CTDACPD0D9CPD8CXD3D2 CPD2CS D4CPD6D7B9 CXD2CV CUD3D6 D8CWCT D4D9D6D4D3D7CT D3CU CSCXD7CPD1CQCXCVD9CPD8CXD3D2 CWCPD7 D1D9CRCW CXD2 CRD3D1D1D3D2 DBCXD8CW D8CWCPD8 D3CU BWD3DBCSCXD2CV CTD8 CPD0BA B4BDBLBLBGB5B8 CTDCCRCTD4D8 BF C0CTD6CTB8 D8CWCT D7CRD3D6CT CXD7 D7CXD1D4D0DD CTD5D9CPD0 D8D3 D8CWCT D2D9D1CQCTD6 D3CU D2D3D2B9 CTD1D4D8DD CRD3D2D7D8CXD8D9CTD2D8D7 CXD2 CPD2 CPD2CPD0DDD7CXD7B8 CQD9D8 D3D8CWCTD6 D1CTD8D6CXCRD7 CPD6CT D4D3D7B9 D7CXCQD0CTBA BG CBD3 CP D0CTDCCXCRCPD0 D6CTD0CPD8CXD3D2 CUD3D6 D8CWCT CRD3D2D7D8CXD8D9CTD2D8 COD0CTD1D3D2B3 D3CU CRCPD8CTCVD3D6DD C6C8 DBD3D9D0CS CRD3D2D8CPCXD2 CPD0D0 CPD2CS D3D2D0DD D8CWCT D0CTD1D3D2D7 CXD2 D8CWCT CTD2DACXD6D3D2D1CTD2D8B8 CPD2CS CP D0CTDCCXCRCPD0 D6CTD0CPD8CXD3D2 CUD3D6 D8CWCT CRD3D2B9 D7D8CXD8D9CTD2D8 COCUCPD0D0CXD2CVB3 D3CU CRCPD8CTCVD3D6DD CBD2C6C8 DBD3D9D0CS CRD3D2D8CPCXD2 CP D1CPD4B9 D4CXD2CV CUD6D3D1 CTDACTD6DD CTD2D8CXD8DD CXD2 D8CWCT CTD2DACXD6D3D2D1CTD2D8 D8D3 D7D3D1CT D8D6D9D8CW DACPD0D9CT B4CCCACDBX CXCU D8CWCPD8 CTD2D8CXD8DD CXD7 CUCPD0D0CXD2CVB8 BYBTC4CBBX D3D8CWCTD6DBCXD7CTB5BM CTBACVBA CUCWD0CTD1D3D2 BD BNCCCACDBXCXBNCWD0CTD1D3D2 BE BNBYBTC4CBBXCXBNBMBMBMCVBA C6C8CJD0CTD1D3D2CL CUD0 BD BND0 BE BND0 BF BND0 BG CV C8BMC6C8D2C6C8BBC6C8CJCXD2CL CUCWCQ BD BNCWD0 BD BND0 BD CXCXBNCWD1 BD BNCWD0 BE BND0 BE CXCXCV C6C8CJCQCXD2CL CUCQ BD BNCQ BE CV C8BMC6C8D2C6C8BBC6C8CJCQDDCL CUCWD1 BD BNCWCQ BD BNCQ BD CXCXBNCWD1 BE BNCWCQ BE BNCQ BE CXCXCV C6C8CJD1CPCRCWCXD2CTCL CUD1 BD BND1 BE BND1 BF CV C8C8BMC6C8D2C6C8CJCXD2CL CUCWD0 BD BND0 BD CXCV C8C8BMC6C8D2C6C8CJCQDDCL CUCWCQ BD BNCQ BD CXBNCWCQ BE BNCQ BE CXCV C6C8CJD0CTD1D3D2CL CUD0 BD CV C6C8CJCQCXD2CL CUCQ BD BNCQ BE CV C8C8BMC6C8D2C6C8CJCXD2CL CUCWD0 BD BND0 BD CXCV C6C8CJD0CTD1D3D2CL CUD0 BD CVCJBN BYCXCVD9D6CT BDBM BWCTD2D3D8CPD8CXD3D2B9CPD2D2D3D8CPD8CTCS CUD3D6CTD7D8 CUD3D6 COD0CTD1D3D2 CXD2 CQCXD2 CQDD D1CPCRCWCXD2CTBAB3 D8CWCPD8 CXD2 D8CWCXD7 CRCPD7CTB8 CRD3D2D7D8CXD8D9CTD2D8D7 CPD6CT D2D3D8 D3D2D0DD D7CTD1CPD2B9 D8CXCRCPD0D0DD D8DDD4CTB9CRCWCTCRCZCTCSB8 CQD9D8 CPD6CT CPD0D7D3 CUD9D0D0DD CXD2D8CTD6D4D6CTD8CTCS CTCPCRCW D8CXD1CT D8CWCTDD CPD6CT D4D6D3D4D3D7CTCSBA BYCXCVD9D6CT BD D7CWD3DBD7 CP D7CPD1D4D0CT CSCTD2D3D8CPD8CXD3D2B9CPD2D2D3D8CPD8CTCS CUD3D6CTD7D8 CUD3D6 D8CWCT D4CWD6CPD7CT COD8CWCT D0CTD1D3D2 CXD2 D8CWCT CQCXD2 CQDD D8CWCT D1CPCRCWCXD2CTB3B8 D9D7CXD2CV D8CWCT D0CTDCCXCRCPD0CXDECTCS CVD6CPD1D1CPD6BM D0CTD1D3D2B8 CQCXD2B8 D1CPCRCWCXD2CT BM C6C8 D8CWCTBMC6C8BPC6C8 CXD2B8 CQDDBMC6C8D2C6C8BPC6C8 CXD2 DBCWCXCRCW D8CWCT CSCTD2D3D8CPD8CXD3D2 D3CU CTCPCRCW CRD3D2D7D8CXD8D9CTD2D8 B4D8CWCT D7CTD8 CXD2 CTCPCRCW D6CTCRD8CPD2CVD0CTB5 CXD7 CRCPD0CRD9D0CPD8CTCS D9D7CXD2CV CP CYD3CXD2 D3D2 D8CWCT CSCTD2D3D8CPD8CXD3D2D7D3CU CTCPCRCWD4CPCXD6D3CU CRD3D2D7D8CXD8D9CTD2D8D7D8CWCPD8 CRD3D1CQCXD2CT D8D3 D4D6D3CSD9CRCT CXD8BA C1D2 D8CWCXD7 CTDCCPD1D4D0CTB8 D8CWCT D6CXCVCWD8B9CQD6CPD2CRCWCXD2CV D8D6CTCT DBD3D9D0CS CQCT D4D6CTCUCTD6D6CTCS CQCTCRCPD9D7CT D8CWCT CSCTD2D3D8CPD8CXD3D2 D6CTB9 D7D9D0D8CXD2CV CUD6D3D1 D8CWCT CRD3D1D4D3D7CXD8CXD3D2 CPD8 D8CWCT D6D3D3D8 D3CU D8CWCT D3D8CWCTD6 D8D6CTCT DBD3D9D0CS CQCT CTD1D4D8DDBA CBCXD2CRCT D8CWCXD7 D9D7CT D3CU D8CWCT CYD3CXD2 D3D4CTD6CPD8CXD3D2 CXD7 D0CXD2CTCPD6 D3D2 D8CWCT D7D9D1 D3CU D8CWCT CRCPD6CSCXD2CPD0CXD8CXCTD7 D3CU CXD8D7 D3D4CTD6CPD2CSD7B8 CPD2CS D7CXD2CRCT D8CWCT CSCTD2D3D8CPD8CXD3D2D7 D3CU D8CWCT CRCPD8CTCVD3D6CXCTD7 CXD2 CP CVD6CPD1D1CPD6 BZ CPD6CT CQD3D9D2CSCTCS CXD2 CRCPD6CSCXD2CPD0CXD8DDCQDD C7B4CYBXCY DA B5 DBCWCTD6CT DA CXD7 D8CWCT D1CPDCCXD1D9D1 DACPD0CTD2CRDD D3CU D8CWCT CRCPD8CTCVD3D6CXCTD7 CXD2 BZB8 D8CWCT D8D3D8CPD0 CRD3D1D4D0CTDCCXD8DD D3CU D8CWCT CPCQD3DACT CPD0CVD3D6CXD8CWD1 CRCPD2 CQCT D7CWD3DBD2 D8D3 CQCT C7B4D2 BF CYBXCY DA B5BM D4D3D0DDD2D3D1CXCPD0 D2D3D8 D3D2D0DD D3D2 D8CWCT D0CTD2CVD8CW D3CU D8CWCT CXD2D4D9D8 D2B8 CQD9D8 CPD0D7D3D3D2 D8CWCT D7CXDECT D3CU D8CWCT CTD2DACXD6D3D2D1CTD2D8BX B4CBCRCWD9D0CTD6B8 BEBCBCBDB5BA BF BXDCD8CTD2CSCTCS CPD0CVD3D6CXD8CWD1 CCCWCT CPCQD3DACT CPD0CVD3D6CXD8CWD1 DBD3D6CZD7 DBCTD0D0 CUD3D6 CPD8D8CPCRCWCXD2CV D3D6CSCXB9 D2CPD6DD CRD3D1D4D0CTD1CTD2D8D7 CPD2CS D1D3CSCXACCTD6D7B8 CQD9D8 CPD7 CP D7CTD1CPD2D8CXCR D8CWCTD3D6DD CXD8 CXD7 D2D3D8 D7D9CRCXCTD2D8D0DD CTDCD4D6CTD7D7CXDACTD8D3 D4D6D3CSD9CRCT CRD3D6B9 D6CTCRD8 CSCTD2D3D8CPD8CXD3D2D7CXD2 CPD0D0CRCPD7CTD7BA BYD3D6 CTDCCPD1D4D0CTB8 D8CWCT D0CTDCCXCRCPD0 D6CTD0CPD8CXD3D2D7 CSCTACD2CTCS CPCQD3DACT CPD6CT CXD2D7D9CRCXCTD2D8 D8D3 D6CTD4D6CTD7CTD2D8 D5D9CPD2D8CXACCTD6D7 D0CXCZCT COD2D3B3 B4D9D7CXD2CV CRCPD8CTCVD3D6DD C6C8BPC6C8B5CXD2D8CWCT D4CWD6CPD7CT COD8CWCT CQD3DD DBCXD8CW D2D3 CQCPCRCZD4CPCRCZBAB3 BH BT D7CXD1CXD0CPD6 D4D6D3CQB9 D0CTD1 D3CRCRD9D6D7 DBCXD8CW CRD3D2CYD9D2CRD8CXD3D2D7BN CUD3D6 CTDCCPD1D4D0CTB8 D8CWCT DBD3D6CS COCPD2CSB3 B4D9D7CXD2CV CRCPD8CTCVD3D6DDC6C8D2C6C8BPC6C8B5 CXD2 D8CWCT D4CWD6CPD7CT COD8CWCT CRCWCXD0CS DBCTCPD6CXD2CV CVD0CPD7D7CTD7 CPD2CS CQD0D9CT D4CPD2D8D7B3B8 CPD0D7D3 CRCPD2D2D3D8 CQCT D4D6D3D4CTD6D0DD D6CTD4D6CTD7CTD2D8CTCS CPD7 CP D0CTDCCXCRCPD0 D6CTD0CPD8CXD3D2BA BI CCCWCXD7 D6CPCXD7CTD7 D8CWCT D5D9CTD7D8CXD3D2BM CWD3DB D1D9CRCW CTDCD4D6CTD7D7CXDACXD8DD CRCPD2 CQCT CPD0D0D3DBCTCS CXD2 CP D7CWCPD6CTCS D7CTD1CPD2D8CXCR CXD2D8CTD6D4D6CTD8CPD8CXD3D2 DBCXD8CWD3D9D8 CTDCCRCTCTCSCXD2CV D8CWCT D8D6CPCRD8CPCQD0CT D4CPD6D7CXD2CV CRD3D1D4D0CTDCCXD8DD D2CTCRCTD7B9 D7CPD6DD CUD3D6 D4D6CPCRD8CXCRCPD0 CTD2DACXD6D3D2D1CTD2D8B9CQCPD7CTCS D4CPD6D7CXD2CVBR C1D2 D8D6CPCSCXD8CXD3D2CPD0 CRCPD8CTCVD3D6CXCPD0 D7CTD1CPD2D8CXCRD7 B4C5D3D2D8CPCVD9CTB8 BDBLBJBFBN BUCPD6DBCXD7CT CPD2CS BVD3D3D4CTD6B8 BDBLBKBDBN C3CTCTD2CPD2 CPD2CS CBD8CPDACXB8 BDBLBKBIB5 D5D9CPD2D8CXACCTD6D7 CPD2CS D2D3D9D2 D4CWD6CPD7CT CRD3D2CYD9D2CRD8CXD3D2D7 CSCTB9 D2D3D8CT CWCXCVCWCTD6B9D3D6CSCTD6 D6CTD0CPD8CXD3D2D7BM D8CWCPD8 CXD7B8 D6CTD0CPD8CXD3D2D7 CQCTB9 D8DBCTCTD2 DBCWD3D0CT D7CTD8D7 D3CU CTD2D8CXD8CXCTD7 CXD2D7D8CTCPCS D3CU CYD9D7D8 CQCTB9 D8DBCTCTD2 CXD2CSCXDACXCSD9CPD0D7BA CDD2CSCTD6 D8CWCXD7 CXD2D8CTD6D4D6CTD8CPD8CXD3D2B8 CP D5D9CPD2D8CXACCTD6 D0CXCZCT COD2D3B3 DBD3D9D0CS CSCTD2D3D8CT CP D7CTD8 D3CU D4CPCXD6D7 CUCWBT BD BNBU BD CXBNCWBT BE BNBU BE CXBNBMBMBMCV DBCWCTD6CT CTCPCRCW BT CX CPD2CS BU CX CPD6CT CSCXD7CYD3CXD2D8 D7D9CQD7CTD8D7 D3CU BXB8 CRD3D6D6CTD7D4D3D2CSCXD2CV D8D3 CPD2 CPCRCRCTD4D8B9 CPCQD0CT D4CPCXD6 D3CU D6CTD7D8D6CXCRD8D3D6 CPD2CS CQD3CSDD D7CTD8D7 D7CPD8CXD7CUDDCXD2CV D8CWCT D5D9CPD2D8CXACCTD6 COD2D3B3BA CDD2CUD3D6D8D9D2CPD8CTD0DDB8 D7CXD2CRCT D8CWCT CRCPD6CSCXD2CPD0CXD8CXCTD7 D3CU D8CWCTD7CT CWCXCVCWCTD6B9D3D6CSCTD6 CSCTD2D3D8CPD8CXD3D2D7 CRCPD2 CQCT CTDCD4D3D2CTD2D8CXCPD0 D3D2 D8CWCT D7CXDECT D3CU D8CWCT CTD2DACXD6D3D2D1CTD2D8 BX B4D8CWCTD6CT CPD6CT BE CYBXCY D4D3D7B9 D7CXCQD0CT D7D9CQD7CTD8D7 D3CU BX CPD2CS BE BECYBXCY D4D3D7D7CXCQD0CT CRD3D1CQCXD2CPD8CXD3D2D7 D3CU D8DBD3 D7D9CRCW D7D9CQD7CTD8D7B5B8 D7D9CRCW CPD2 CPD4D4D6D3CPCRCWDBD3D9D0CS CSCTD7D8D6D3DD D8CWCT D4D3D0DDD2D3D1CXCPD0 CRD3D1D4D0CTDCCXD8DD D3CU D8CWCT CTD2DACXD6D3D2D1CTD2D8B9CQCPD7CTCS D4CPD6D7CXD2CV CPD0CVD3D6CXD8CWD1BA BH BTD7D7CXCVD2CXD2CV D8CWCT CXCSCTD2D8CXD8DD D6CTD0CPD8CXD3D2 CUCWCT BD BNCT BD CXBNCWCT BE BNCT BE CXBNBMBMBMCV D8D3 D8CWCT D5D9CPD2D8CXACCTD6 DBD3D9D0CS CXD2CRD3D6D6CTCRD8D0DD DDCXCTD0CS D8CWCT D7CTD8 D3CU CQD3DDD7 DBCXD8CW CP CQCPCRCZD4CPCRCZ CPD7 CP CSCTD2D3D8CPD8CXD3D2 CUD3D6 D8CWCT CUD9D0D0 D2D3D9D2 D4CWD6CPD7CTBN CPD2CS CPD7D7CXCVD2B9 CXD2CV D8CWCT CRD3D2DACTD6D7CT D6CTD0CPD8CXD3D2 B4CUD6D3D1 CTCPCRCWCTD2D8CXD8DD CXD2 D8CWCT CTD2DACXD6D3D2D1CTD2D8 D8D3 CTDACTD6DD D3D8CWCTD6 CTD2D8CXD8DD CUCWCT BD BNCT BE CXBNCWCT BD BNCT BF CXBNBMBMBMCVB5DBD3D9D0CS CXD2CRD3D6D6CTCRD8D0DD DDCXCTD0CS D8CWCT D7CTD8 D3CU CQD3DDD7 DBCXD8CW CPD2DDD8CWCXD2CV D8CWCPD8 CXD7 D2D3D8 CP CQCPCRCZD4CPCRCZBA BI CCCWCT CXCSCTD2D8CXD8DD D6CTD0CPD8CXD3D2 CUCWCT BD BNCT BD BNCT BD CXBNCWCT BE BNCT BE BNCT BE CXBNBMBMBMCVB8 DBCWCXCRCW DDCXCTD0CSD7 CP CRD3D6D6CTCRD8 CXD2D8CTD6D4D6CTD8CPD8CXD3D2 CXD2 DACTD6CQ D4CWD6CPD7CT CRD3D2CYD9D2CRD8CXD3D2B8 DBD3D9D0CS DDCXCTD0CS CPD2 CXD2CRD3D6D6CTCRD8 CSCTD2D3D8CPD8CXD3D2 CUD3D6 D8CWCT D2D3D9D2 D4CWD6CPD7CT COCVD0CPD7D7CTD7 CPD2CS CQD0D9CT D4CPD2D8D7B8B3 CRD3D2D8CPCXD2CXD2CV D3D2D0DD CTD2D8CXD8CXCTD7 DBCWCXCRCW CPD6CT CPD8 D3D2CRCT CQD3D8CW CVD0CPD7D7CTD7 CPD2CS D4CPD2D8D7BA C0D3DBCTDACTD6B8 CXCU D8CWCT D2D9D1CQCTD6 D3CU D4D3D7D7CXCQD0CT CWCXCVCWCTD6B9D3D6CSCTD6 CUD9D2CRD8CXD3D2D7 CXD7 D6CTD7D8D6CXCRD8CTCS D8D3 CP ACD2CXD8CT D7CTD8 B4D7CPDDB8 D8D3 D7D3D1CT D7D9CQD7CTD8 D3CU DBD3D6CSD7 CXD2 CP D0CTDCCXCRD3D2B5B8 CXD8 CQCTCRD3D1CTD7 D8D6CPCRD8CPCQD0CT D8D3 D7D8D3D6CT D8CWCTD1 CQDD D2CPD1CT D6CPD8CWCTD6 D8CWCPD2 CQDD CSCTD2D3D8CPD8CXD3D2 B4CXBACTBA CPD7 D7CTD8D7B5BA CBD9CRCW CUD9D2CRD8CXD3D2 CRCPD2 D8CWCTD2 CSCXD7CRCWCPD6CVCT CPD0D0 D8CWCTCXD6 ACD6D7D8B9D3D6CSCTD6 CPD6CVD9D1CTD2D8D7 CXD2 CP D7CXD2CVD0CT CSCTD6CXDACPD8CXD3D2CPD0 D7D8CTD4 D8D3 D4D6D3CSD9CRCT CP ACD6D7D8B9D3D6CSCTD6 D6CTD7D9D0D8B8 CXD2 D3D6CSCTD6 D8D3 CPDAD3CXCS CVCTD2CTD6CPD8CXD2CV D3D6 CTDACPD0D9CPD8CXD2CV CPD2DD CWCXCVCWCTD6B9D3D6CSCTD6 D4CPD6D8CXCPD0 D6CTB9 D7D9D0D8D7BA CBDDD2D8CPCRD8CXCRCPD0D0DDB8 D8CWCXD7 DBD3D9D0CS CQCT CPD2CPD0D3CVD3D9D7 D8D3 CRD3D1B9 D4D3D7CXD2CV CP D5D9CPD2D8CXACCTD6 DBCXD8CW CQD3D8CW CP D2D3D9D2 D4CWD6CPD7CT D6CTD7D8D6CXCRB9 D8D3D6 CPD2CS CP CQD3CSDD D4D6CTCSCXCRCPD8CT B4CTBACVBA CP DACTD6CQ D3D6 DACTD6CQ D4CWD6CPD7CTB5 CPD8 D8CWCT D7CPD1CT D8CXD1CTB8 D8D3 D4D6D3CSD9CRCT CPD2D3D8CWCTD6 ACD6D7D8B9D3D6CSCTD6 D4D6CTCSCXCRCPD8CT B4CTBACVBA CP DACTD6CQ D4CWD6CPD7CT D3D6 D7CTD2D8CTD2CRCTB5BA CBCXD2CRCT CP CVCTD2CTD6CPD0CXDECTCS D5D9CPD2D8CXACCTD6 CUD9D2CRD8CXD3D2 D1CTD6CTD0DD CRD3D9D2D8D7 CPD2CS CRD3D1D4CPD6CTD7 D8CWCT CRCPD6CSCXD2CPD0CXD8CXCTD7 D3CU CXD8D7 CPD6CVD9D1CTD2D8D7 CXD2 CP D0CXD2B9 CTCPD6 D8CXD1CT D3D4CTD6CPD8CXD3D2B8 D8CWCXD7 CPD2CPD0DDD7CXD7 D4D6D3DACXCSCTD7 CP D8D6CPCRD8CPCQD0CT D7CWD3D6D8CRD9D8 D8D3 D8CWCT CTDCD4D3D2CTD2D8CXCPD0 CRCPD0CRD9D0CPD8CXD3D2D7 D6CTD5D9CXD6CTCS CXD2 D8CWCT CRD3D2DACTD2D8CXD3D2CPD0 CPD2CPD0DDD7CXD7BA C6D3D8CT D8CWCPD8 D8CWCXD7 CPD2CPD0DDD7CXD7 CQDD CXD8D7CTD0CU CSD3CTD7 D2D3D8 CPCSD1CXD8 D4D6D3CSD9CRD8CXDACT D1D3CSCXACCRCPD8CXD3D2 D3CU D5D9CPD2D8CXACCTD6D7 B4CQCTCRCPD9D7CT D8CWCTCXD6 CUD9D2CRD8CXD3D2D7 CPD6CT CSD6CPDBD2 CUD6D3D1 D7D3D1CT ACD2CXD8CT D7CTD8B5 D3D6 D3CU D5D9CPD2B9 D8CXACCTCS D2D3D9D2 D4CWD6CPD7CTD7 B4CQCTCRCPD9D7CT D8CWCTDD CPD6CT D2D3 D0D3D2CVCTD6 CSCTB9 D6CXDACTCS CPD7 CP D4CPD6D8CXCPD0 D6CTD7D9D0D8B5BA CCCWCXD7 CRCPD9D7CTD7 D2D3 CSCXD7D6D9D4D8CXD3D2 D8D3 D8CWCT CPD8D8CPCRCWD1CTD2D8 D3CU D2D3D2B9CRD3D2CYD9D2CRD8CXDACT D1D3CSCXACCTD6D7B8 CQCTB9 CRCPD9D7CT D3D6CSCXD2CPD6DD D7DDD2D8CPCRD8CXCR D1D3CSCXACCTD6D7 D3CU D5D9CPD2D8CXACCTD6 CRD3D2B9 D7D8CXD8D9CTD2D8D7 CPD6CT D7CTD0CSD3D1 D4D6D3CSD9CRD8CXDACT B4CXD2 D8CWCT D7CTD2D7CT D8CWCPD8 D8CWCTCXD6 CRD3D1D4D3D7CXD8CXD3D2 CSD3CTD7 D2D3D8 DDCXCTD0CS CUD9D2CRD8CXD3D2D7 D3D9D8D7CXCSCT D7D3D1CT ACD2CXD8CT D7CTD8B5B8 CPD2CS D7DDD2D8CPCRD8CXCR D1D3CSCXACCTD6D7 D3CU C6C8 CRD3D2B9 D7D8CXD8D9CTD2D8D7 D9D7D9CPD0D0DD D3D2D0DD D1D3CSCXCUDD D8CWCT D6CTD7D8D6CXCRD8D3D6 D7CTD8 D3CU D8CWCT D5D9CPD2D8CXACCTD6 D6CPD8CWCTD6 D8CWCPD2 D8CWCT CTD2D8CXD6CT D5D9CPD2D8CXACCTCS CUD9D2CRD8CXD3D2B8 CPD2CS CRCPD2 D8CWCTD6CTCUD3D6CT D7CPCUCTD0DD CQCT D8CPCZCTD2 D8D3 CPD8D8CPCRCW CQCTD0D3DB D8CWCT D5D9CPD2D8CXACCTD6B8 D8D3 D8CWCT D9D2D5D9CPD2D8CXACCTCS C6C8BA BUD9D8 D8CWCXD7 CXD7 D2D3D8 D8D6D9CT CXD2 CRCPD7CTD7 CXD2DAD3D0DACXD2CV CRD3D2CYD9D2CRB9 D8CXD3D2BA BVD3D2CYD3CXD2CTCS D5D9CPD2D8CXACCTD6D7B8 D0CXCZCT COD7D3D1CT CQD9D8 D2D3D8 CPD0D0B8B3 CRCPD2D2D3D8 CPD0DBCPDDD7CQCT CSCTACD2CTCS D9D7CXD2CV CP D7CXD2CVD0CT D7D8CPD2CSCPD6CS D0CTDCB9 CXCRCPD0 CUD9D2CRD8CXD3D2BN CPD2CS CRD3D2CYD9D2CRD8CXD3D2D7 D3CU D5D9CPD2D8CXACCTCS D2D3D9D2 D4CWD6CPD7CTD7B8 D0CXCZCT COD3D2CT D3D6CPD2CVCT CPD2CS D3D2CT D0CTD1D3D2B3B8 CRCPD2D2D3D8 CQCT CPD4D4D0CXCTCS D8D3 D9D2D5D9CPD2D8CXACCTCS D7D9CQCRD3D2D7D8CXD8D9CTD2D8D7 B4D7DDD2D8CPCRB9 D8CXCRCPD0D0DDB8 CQCTCRCPD9D7CT D8CWCXD7 DBD3D9D0CS CUCPCXD0 D8D3 D7D9CQD7D9D1CT D8CWCT D7CTCRB9 D3D2CS D5D9CPD2D8CXACCTD6B8 CPD2CS D7CTD1CPD2D8CXCRCPD0D0DDB8 CQCTCRCPD9D7CT CXD8 CXD7 D2D3D8 D8CWCT D6CTD7D8D6CXCRD8D3D6 D7CTD8D7 DBCWCXCRCW CPD6CT CRD3D2CYD3CXD2CTCSB5BA C3CTCTD2CPD2 CPD2CS CBD8CPDACX B4BDBLBKBIB5 D1D3CSCTD0 CRD3D2CYD9D2CRD8CXD3D2D7 D3CU D5D9CPD2D8CXACCTD6D7 CPD2CS D5D9CPD2D8CXACCTCS D2D3D9D2 D4CWD6CPD7CTD7 D9D7CXD2CV D0CPD8D8CXCRCT D3D4CTD6CPD8CXD3D2D7 D3D2 CWCXCVCWCTD6B9D3D6CSCTD6 D7CTD8D7B8 CQD9D8 CPD7 D4D6CTDACXD3D9D7D0DD D7D8CPD8CTCSB8 D8CWCTD7CT CWCXCVCWCTD6B9D3D6CSCTD6 D7CTD8D7 D4D6CTCRD0D9CSCT D8D6CPCRD8CPCQD0CT CXD2D8CTD6D0CTCPDACXD2CV D3CU D7CTD1CPD2D8CXCR CXD2D8CTD6D4D6CTD8CPD8CXD3D2 DBCXD8CW D4CPD6D7CXD2CVBA CCCWCT D7D3D0D9D8CXD3D2 D4D6D3D4D3D7CTCS CWCTD6CT CXD7 D8D3 D8D6CTCPD8 CTCPCRCW D5D9CPD2B9 D8CXACCTD6 D3D6 D5D9CPD2D8CXACCTCS D2D3D9D2 D4CWD6CPD7CT CRD3D2CYD9D2CRD8CXD3D2 CPD7 CPD2 CTD0B9 D0CXD4D8CXCRCPD0 CRD3D2CYD9D2CRD8CXD3D2 D3CU D8DBD3 CRD3D1D4D0CTD8CT ACD6D7D8B9D3D6CSCTD6 D4D6CTCSB9 CXCRCPD8CTD7 B4CTBACVBA DACTD6CQ D4CWD6CPD7CTD7 D3D6 D7CTD2D8CTD2CRCTD7B5B8 CTCPCRCW D7D9CQD7D9D1B9 CXD2CV CP CSCXABCTD6CTD2D8 D5D9CPD2D8CXACCTD6 CPD2CS D2D3D9D2 D4CWD6CPD7CT D6CTD7D8D6CXCRD8D3D6 B4CXD2 D8CWCT CRCPD7CT D3CU C6C8 CRD3D2CYD9D2CRD8CXD3D2B5B8 CQD9D8 D7CWCPD6CXD2CV D3D6 CSD9B9 D4D0CXCRCPD8CXD2CV CP CRD3D1D1D3D2 CQD3CSDD D4D6CTCSCXCRCPD8CTBA CCCWCXD7 CPD2CPD0DDD7CXD7 D6CTD5D9CXD6CTD7 D1D9D0D8CXD4D0CT CRD3D1D4D3D2CTD2D8D7 D8D3 CZCTCTD4 D8D6CPCRCZ D3CU D8CWCT CSD9D4D0CXCRCPD8CTCS D1CPD8CTD6CXCPD0 CPCQD3DACT D8CWCT CRD3D2CYD9D2CRD8CXD3D2B8 CQD9D8 CPD7 D0D3D2CV CPD7 D8CWCT D2D9D1CQCTD6 D3CU CRD3D1D4D3D2CTD2D8D7 CXD7 CQD3D9D2CSCTCSB8 D8CWCT D4D3D0DDD2D3D1CXCPD0 CRD3D1D4D0CTDCCXD8DD D3CU D8CWCT D4CPD6D7CXD2CV CPD0CVD3D6CXD8CWD1 CXD7 CRD3D2D8CPCXD2CXD2CV B4CSD9D4D0CXCRCPD8CTCSB5 D3D2CT D3D6CPD2CVCT B4D9D2CSD9D4D0CXCRCPD8CTCSB5 CPD2CS D3D2CT D0CTD1D3D2 B4D9D2CSD9D4D0CXCRCPD8CTCSB5 BYCXCVD9D6CT BEBM BWD9D4D0CXCRCPD8CTCS DACTD6CQ CXD2 C6C8 CRD3D2CYD9D2CRD8CXD3D2BA D6CTD8CPCXD2CTCSBA BJ BYCXCVD9D6CT BE D7CWD3DBD7 CP CSD9D4D0CXCRCPD8CTCS DACTD6CQ D4D6CTCSCXCRCPD8CT CXD2 D8CWCT CSCTD6CXDACPD8CXD3D2 D3CU CPD2 C6C8 CRD3D2CYD9D2CRD8CXD3D2BA CCCWCT CRD3D2CYD3CXD2CTCS CRD3D2D7D8CXD8D9CTD2D8D7 B4D8CWCT D7CWCPCSCTCS D6CTCVCXD3D2D7 CXD2 D8CWCT ACCVD9D6CTB5 CPD6CT CTCPCRCW CRD3D1D4D3D7CTCS D3CU D8DBD3 CRD3D1D4D3D2CTD2D8D7BM D3D2CT CUD3D6 D8CWCT C6C8 CXD8D7CTD0CUB8 CRD3D2D8CPCXD2CXD2CV D8CWCT D5D9CPD2D8CXACCTD6 CPD2CS D8CWCT D6CTD7D8D6CXCRD8D3D6 D4D6CTCSCXCRCPD8CTB8 CPD2CS D3D2CT CUD3D6 D8CWCT DACTD6CQ DBCWCXCRCW D7D9D4D4D0CXCTD7 D8CWCT CQD3CSDD D4D6CTCSCXCRCPD8CT D3CU D8CWCT D5D9CPD2D8CXACCTD6BA CBCXD2CRCT D8CWCT CRD3D2CYD3CXD2CTCS CRD3D2D7D8CXD8D9CTD2D8D7 CQD3D8CW CRD3D6D6CTD7D4D3D2CS D8D3 CRD3D1D4D0CTD8CT D5D9CPD2D8CXB9 ACCTD6 CTDCD4D6CTD7D7CXD3D2D7 DBCXD8CW D2D3 D9D2D7CPD8CXD7ACCTCS ACD6D7D8B9D3D6CSCTD6 CPD6CVD9B9 D1CTD2D8D7B8 D8CWCTCXD6 CRCPD8CTCVD3D6CXCTD7 CPD6CT D8CWCPD8 D3CU D7CXD1D4D0CT ACD6D7D8B9D3D6CSCTD6 D4D6CTCSCXCRCPD8CTD7 B4D8CWCTDD CPD6CT CTCPCRCW CRD3D1D4D0CTD8CT DACTD6CQ D4CWD6CPD7CTD7 CXD2 CTD7D7CTD2CRCTBM COCRD3D2D8CPCXD2CXD2CV D3D2CT D3D6CPD2CVCTB3 CPD2CS COCRD3D2D8CPCXD2CXD2CV D3D2CT D0CTD1D3D2B3B5BA CCCWCT CRD3D2CYD9D2CRD8CXD3D2 D8CWCTD2 CUD3D6D1D7 CP D0CPD6CVCTD6 CRD3D2B9 D7D8CXD8D9CTD2D8 D3CU D8CWCT D7CPD1CT CUD3D6D1 B4D8CWCT D9D2D7CWCPCSCTCS D3D9D8D0CXD2CT CXD2 D8CWCT ACCVD9D6CTB5B8 DBCXD8CW CP D0D3DBCTD6 CRD3D1D4D3D2CTD2D8 CRD3D2D8CPCXD2CXD2CV D8CWCT CRD3D2CYD3CXD2CTCS CRD3D2D7D8CXD8D9CTD2D8D7B3 C6C8 CRD3D1D4D3D2CTD2D8D7 CRD3D2CRCPD8CTB9 D2CPD8CTCS CXD2 D8CWCT D9D7D9CPD0 DBCPDDB8 CPD2CS CPD2 D9D4D4CTD6 CRD3D1D4D3D2CTD2D8CXD2 DBCWCXCRCW D8CWCT CRD3D2CYD3CXD2CTCS CRD3D2D7D8CXD8D9CTD2D8D7B3 D2D3D2B9C6C8 CRD3D1D4D3B9 D2CTD2D8D7 CPD6CT CXCSCTD2D8CXACCTCS D3D6 D3DACTD6D0CPD4D4CTCSBA C1CU D8CWCT CSD9D4D0CXCRCPD8CTCS CRD3D1D4D3D2CTD2D8D7 CSD3 D2D3D8 CRD3DACTD6 D8CWCT D7CPD1CT D7D8D6CXD2CV DDCXCTD0CSB8 D8CWCT CRD3D2CYD9D2CRD8CXD3D2 CSD3CTD7 D2D3D8 CPD4D4D0DDBA C6D3D8CT D8CWCPD8B8 D7CXD2CRCT D8CWCTDD CPD6CT D3D2D0DD CPD4D4D0CXCTCS D8D3 D3D6CSCXD2CPD6DD ACD6D7D8B9D3D6CSCTD6 D4D6CTCSCXCRCPD8CTD7 B4CTBACVBA D7CTD2D8CTD2CRCTD7 D3D6 DACTD6CQ D4CWD6CPD7CTD7B5 CXD2 D8CWCXD7 CPD2CPD0DDD7CXD7B8 CRD3D2CYD9D2CRD8CXD3D2D7 CRCPD2 D2D3DB D7CPCUCTD0DD CQCT CPD7B9 D7CXCVD2CTCS D8CWCT CUCPD1CXD0CXCPD6 D8D6D9D8CWB9CUD9D2CRD8CXD3D2CPD0 CSCTD2D3D8CPD8CXD3D2D7 CXD2 CTDACTD6DD CRCPD7CTBA BK BTD0D7D3B8 D7CXD2CRCT D8CWCT D6CTD7D9D0D8CXD2CV CRD3D2D7D8CXD8D9CTD2D8 CWCPD7 D8CWCT D7CPD1CT D2D9D1CQCTD6 D3CU CRD3D1D4D3D2CTD2D8D7 CPD7 D8CWCT CRD3D2CYD3CXD2CTCS CRD3D2D7D8CXD8D9CTD2D8D7B8 D8CWCTD6CT CXD7 D2D3D8CWCXD2CV D8D3 D4D6CTDACTD2D8 CXD8D7 D9D7CT CPD7 CPD2 CPD6CVD9D1CTD2D8 CXD2 D7D9CQD7CTD5D9CTD2D8 CRD3D2CYD9D2CRD8CXD3D2 D3D4CTD6CPD8CXD3D2D7BA BT D7CPD1D4D0CT D1D9D0D8CXB9CRD3D1D4D3D2CTD2D8 CPD2CPD0DDD7CXD7 CUD3D6 D5D9CPD2D8CXACCTD6D7 CXD7 D7CWD3DBD2 CQCTD0D3DBB8 CPD0D0D3DBCXD2CV D1CPD8CTD6CXCPD0 D8D3 CQCT CSD9D4D0CXCRCPD8CTCS CQD3D8CW D8D3 D8CWCT D0CTCUD8 CPD2CS D8D3 D8CWCT D6CXCVCWD8 D3CU CP CRD3D2CYD3CXD2CTCS C6C8BM D7D3D1CTB8CPD0D0B8D2D3B8CTD8CRBA BM CGD2C6C8 D5 A1 C6C8 D5 D2C6C8 D5 A1C6C8 D5 BPC6C8 AF CGBPC6C8 D5 A1 C6C8 D5 D2C6C8 D5 A1C6C8 D5 BPC6C8 AF CCCWCT D0CTDCCXCRCPD0 CTD2D8D6DD CUD3D6 CP D5D9CPD2D8CXACCTD6 CRCPD2 CQCT D7D4D0CXD8 CXD2 D8CWCXD7 BJ BWCPCWD0 CPD2CS C5CRBVD3D6CS B4BDBLBKBFB5 D4D6D3D4D3D7CT CP D7CXD1CXD0CPD6 CSD9D4D0CXCRCPD8CXD3D2 D1CTCRCWCPD2CXD7D1 D8D3 D4D6D3CSD9CRCT CPD4D4D6D3D4D6CXCPD8CT D7CTD1CPD2D8CXCR D6CTD4D6CTD7CTD2D8CPD8CXD3D2D7 CUD3D6 C6C8 CPD2CS D3D8CWCTD6 CRD3D2CYD9D2CRD8CXD3D2D7B8 CQD9D8 CUD3D6 CSCXABCTD6CTD2D8 D6CTCPD7D3D2D7BA BK CTBACVBA CUD3D6 D8CWCT DBD3D6CS COCPD2CSB3BM CUCWBMBMBMCCCACDBXBNBMBMBMCCCACDBXBNBMBMBMCCCACDBXCXBN CWBMBMCCCACDBXBNBMBMBYBTC4CBBXBNBMBMBYBTC4CBBXCXBN CWBMBMBYBTC4CBBXBNBMBMCCCACDBXBNBMBMBYBTC4CBBXCXBN CWBMBMBYBTC4CBBXBNBMBMBYBTC4CBBXBNBMBMBYBTC4CBBXCXCV DBCPDDCXD2D8D3CPD2D9D1CQCTD6 D3CU CRD3D1D4D3D2CTD2D8D7B8 D8CWCT D0CPD7D8 B4D3D6 D0D3DBB9 CTD7D8B5 D3CU DBCWCXCRCW CXD7 D2D3D8 CSD9D4D0CXCRCPD8CTCS CXD2 CRD3D2CYD9D2CRD8CXD3D2 DBCWCXD0CT D3D8CWCTD6D7 D1CPDD D3D6 D1CPDD D2D3D8 CQCTBA CCCWCTD7CT CXD2CRD0D9CSCT CP CRD3D1B9 D4D3D2CTD2D8 CUD3D6 D8CWCT D5D9CPD2D8CXACCTD6 C6C8 D5 BPC6C8 AF B4DBCWCXCRCW DBCXD0D0 D9D0D8CXB9 D1CPD8CTD0DD CPD0D7D3 CRD3D2D8CPCXD2 CP D2D3D9D2 D4CWD6CPD7CT D6CTD7D8D6CXCRD8D3D6 D3CU CRCPD8CTB9 CVD3D6DD C6C8 AF B5B8 CP CRD3D1D4D3D2CTD2D8 CUD3D6 D6CTD7D8D6CXCRD8D3D6 C8C8D7 CPD2CS D6CTD0CPB9 D8CXDACT CRD0CPD9D7CTD7 D3CU CRCPD8CTCVD3D6DD C6C8 D5 D2C6C8 D5 D8CWCPD8 CPD6CT CPD8D8CPCRCWCTCS CPCQD3DACT D8CWCT D5D9CPD2D8CXACCTD6 CPD2CS CSD9D4D0CXCRCPD8CTCS CXD2 D8CWCT CRD3D2CYD9D2CRB9 D8CXD3D2B8 CPD2CS CP CRD3D1D4D3D2CTD2D8 CUD3D6 D8CWCT CQD3CSDD B4CP DACTD6CQ D3D6 DACTD6CQ D4CWD6CPD7CT D3D6 D3D8CWCTD6 D4D6CTCSCXCRCPD8CTB5 D3CU CRCPD8CTCVD3D6DD CGD2C6C8 D5 D3D6 CGBPC6C8 D5 BA CCCWCT D7D9CQD7CRD6CXD4D8 D5 D7D4CTCRCXACCTD7 D3D2CT D3CU CP ACD2CXD8CT D7CTD8 D3CU D5D9CPD2D8CXACCTD6D7B8 CPD2CS D8CWCT D7D9CQD7CRD6CXD4D8 AF CXD2CSCXCRCPD8CTD7 CPD2 D9D2D5D9CPD2D8CXACCTCS C6C8BA CCCWCT CSCTCSD9CRD8CXDACT D4CPD6D7CTD6 D4D6CTD7CTD2D8CTCS CXD2 CBCTCRD8CXD3D2 BE CRCPD2 D2D3DB CQCT CTDCD8CTD2CSCTCS CQDD CXD2CRD3D6D4D3D6CPD8CXD2CV D7CTD5D9CTD2CRCTD7 D3CU D6CTCRB9 D3CVD2CXDECTCS CPD2CS D9D2D6CTCRD3CVD2CXDECTCS CRD3D1D4D3D2CTD2D8D7 CXD2D8D3 D8CWCT CRD3D2B9 D7D8CXD8D9CTD2D8 CRCWCPD6D8 CXD8CTD1D7BA BTD7 CRD3D2D7D8CXD8D9CTD2D8D7 CPD6CT CRD3D1B9 D4D3D7CTCSB8 CRD3D1D4D3D2CTD2D8D7 CPD6CT D7CWCXCUD8CTCS CUD6D3D1 D8CWCT D9D2D6CTCRD3CVB9 D2CXDECTCS D7CTD5D9CTD2CRCT AD BD A1A1A1AD CR D8D3 D8CWCT D6CTCRD3CVD2CXDECTCS D7CTD5D9CTD2CRCT CWCX BD BNCY BD BNAD BD CXA1A1A1CWCX CR BNCY CR BNAD CR CXB8 D9D2D8CXD0 D8CWCT D9D2D6CTCRD3CVD2CXDECTCS D7CTB9 D5D9CTD2CRCT CXD7 CTD1D4D8DDBA CCCWCT CTDCD8CTD2CSCTCS D4CPD6D7CTD6 CXD7 CSCTACD2CTCS DBCXD8CWBM AF CRCWCPD6D8 CXD8CTD1D7 D3CU D8CWCT CUD3D6D1 CJCXBNCYBNA1BNA6CLB8 DBCWCTD6CT A1 CXD7 CP D7CTD5D9CTD2CRCT D3CU D9D2D6CTCRD3CVD2CXDECTCS CRD3D1D4D3D2CTD2D8D7 ADB8A6CXD7 CP D7CTD5D9CTD2CRCT D3CU D6CTCRD3CVD2CXDECTCS CRD3D1D4D3D2CTD2D8D7 CWCPBNCQBNADCXB8 CPD2CS CXBNCYBNCZBNCPBNCQBNCR CPD6CT CXD2CSCXCRCTD7 CXD2 D8CWCT CXD2D4D9D8BA BXCPCRCW CXD8CTD1 CJCXBNCYBNA1A1ADBNCWCX BD BNCY BD BNAD BD CXA1A1A1CWCX CR BNCY CR BNAD CR CXCL CXD2CSCXCRCPD8CTD7 D8CWCPD8 D8CWCT D7D4CPD2 CUD6D3D1 CX D8D3 CY CXD2 D8CWCT CXD2D4D9D8 CRCPD2 CQCT CRCWCPD6CPCRD8CTD6CXDECTCS CQDD D8CWCT CRCPD8CTCVD3D6CXCTD7 AD BD D8CWD6D3D9CVCW AD CR CPD8 D4D3D7CXD8CXD3D2D7CX BD D8D3CY BD D8CWD6D3D9CVCWCX CR D8D3CY CR D6CTD7D4CTCRD8CXDACTD0DDB8D7D3 D8CWCPD8 CXCU D8CWCTD7CT D7D4CPD2D7 CPD6CT CRD3D2CRCPD8CTD2CPD8CTCS CXD2 DBCWCPD8CTDACTD6 D3D6CSCTD6 D8CWCTDD D3CRCRD9D6 CXD2 D8CWCT CXD2D4D9D8 D7D8D6CXD2CVB8 D8CWCTDD CUD3D6D1 CP CVD6CPD1D1CPD8CXCRCPD0 CRD3D2D7D8CXD8D9CTD2D8 D3CU CRCPD8CTCVD3D6DD AD DBCXD8CW D9D2D6CTCRD3CVD2CXDECTCS CRD3D1D4D3D2CTD2D8D7 A1BA AF CP D0CTDCCXCRCPD0 CXD8CTD1 CJCXBNCYBNADBNCWCXBNCYBNADCXCL CUD3D6 CTDACTD6DD D6D9D0CT AD AX DB BE C8 CXCU DB D3CRCRD9D6D7 CQCTD8DBCTCTD2 D4D3D7CXD8CXD3D2D7 CX CPD2CS CY CXD2 D8CWCT CXD2D4D9D8BN AF CP D7CTD8 D3CU D6D9D0CTD7 CUD3D6 CPD0D0 CXBNCYBNCZBNCPBNCQBNCR BE C1 D2 BC CPD7 CQCTD0D3DBBA CCDBD3 D6D9D0CTD7 D8D3 CXD2DAD3CZCT D0CTCUD8 CPD2CS D6CXCVCWD8 CUD9D2CRD8CXD3D2 CPD4B9 D4D0CXCRCPD8CXD3D2 D8D3 CPD2 CTDCCXD7D8CXD2CV CRD3D1D4D3D2CTD2D8BM CJCXBNCZBNADBPBNCWCXBNCZBNADBPCXCLCJCZBNCYBNA1A1BNCWCZBNCQBNBPAYCXA1A6CL CJCXBNCYBNA1A1ADBNCWCXBNCQBNADBPAYCXA1A6CL ADAXADBP BEC8B8 CJCZBNCYBNADD2BNCWCZBNCYBNADD2CXCLCJCXBNCZBNA1A1BNCWCPBNCZBND2AYCXA1A6CL CJCXBNCYBNA1A1ADBNCWCPBNCYBNADD2AYCXA1A6CL ADAXADD2BEC8B8 CCDBD3 D6D9D0CTD7 D8D3 CXD2DAD3CZCT D0CTCUD8 CPD2CS D6CXCVCWD8 CUD9D2CRD8CXD3D2 CPD4B9 D4D0CXCRCPD8CXD3D2 D8D3 CP CUD6CTD7CW CRD3D1D4D3D2CTD2D8BM CJCXBNCZBNADBPBNCWCXBNCZBNADBPCXCLCJCZBNCYBNA1A1ADBPA1BNA6CL CJCXBNCYBNA1A1ADBNCWCXBNCZBNADBPCXA1A6CL ADAXADBP BEC8B8 CJCZBNCYBNADD2BNCWCZBNCYBNADD2CXCLCJCXBNCZBNA1A1ADD2A1BNA6CL CJCXBNCYBNA1A1ADBNCWCZBNCYBNADD2CXA1A6CL ADAXADD2BEC8B8 CCDBD3 D6D9D0CTD7 D8D3 CSCXD7CRCWCPD6CVCT CTD1D4D8DD CRD3D1D4D3D2CTD2D8D7BM CJCXBNCYBNA1A1ADBPA1BNA6CL CJCXBNCYBNA1A1ADBNA6CL CJCXBNCYBNA1A1ADD2A1BNA6CL CJCXBNCYBNA1A1ADBNA6CL CCCWD6CTCT D6D9D0CTD7 D8D3 D7CZCXD4 CRD3D2CYD9D2CRD8CXD3D2D7B8 CQDD CPCSCSCXD2CV CP CVCPD4 CQCTD8DBCTCTD2 D8CWCT CRD3D1D4D3D2CTD2D8D7 CXD2 CP CRD3D2D7D8CXD8D9CTD2D8 B4D8CWCT ACD6D7D8 D6D9D0CT CRD3D2D7D9D1CTD7 D8CWCT CRD3D2CYD9D2CRD8CXD3D2 D8D3 CRD6CTB9 CPD8CT CP D4CPD6D8CXCPD0 D6CTD7D9D0D8 D3CU CRCPD8CTCVD3D6DD BVD3D2CY BC  B8 CPD2CS D8CWCT D0CPD8D8CTD6 D8DBD3 D9D7CT D8CWCXD7 D8D3 D7CZCXD4 D8CWCT D3D4D4D3D7CXD2CV C6C8B5BM CJCZBNCYBNA1A1BNA6CL CJCXBNCYBNA1A1BVD3D2CY BC  BNA6CL CJCXBNCZBNBVD3D2CYBNCWCXBNCZBNBVD3D2CYCXCL CJCZBNCYBNA1A1BVD3D2CY BC  BNA6CL CJCXBNCYBNA1A1BNA6CL CJCXBNCZBNA1A1BN CL CJCXBNCZBNA1A1BNA6CL CJCXBNCYBNA1A1BNA6CL CJCZBNCYBNA1A1BVD3D2CY BC  BN CL CCDBD3 D6D9D0CTD7 D8D3 D6CTCPD7D7CTD1CQD0CT CSCXD7CRD3D2D8CXD2D9D3D9D7 CRD3D2B9 D7D8CXD8D9CTD2D8D7 B4CPCVCPCXD2B8 D9D7CXD2CV CP D4CPD6D8CXCPD0 D6CTD7D9D0D8 BVD3D2CY BC  D8D3 D6CTCSD9CRCT D8CWCT D2D9D1CQCTD6 D3CU D6CPD2CVCXD2CV DACPD6CXCPCQD0CTD7B5BM CJCPBNCRBNBVD3D2CYBNCWCPBNCRBNBVD3D2CYCXCLCJCXBNCYBNADBNA6A1CWCRBNCQBNCXCL CJCXBNCYBNADBNA6A1CWCPBNCQBNBVD3D2CY BC  CXCL CJCXBNCYBNADBNA6A1CWCRBNCQBNBVD3D2CY BC  CXCLCJCXBNCYBNADBNA6A1CWCPBNCRBNCXCL CJCXBNCYBNADBNA6A1CWCPBNCQBNCXCL CCDBD3 D6D9D0CTD7 D8D3 CRD3D1CQCXD2CT CPCSCYCPCRCTD2D8 CRD3D1D4D3D2CTD2D8D7BM CJCXBNCYBNADBNA6A1CWCPBNCRBNBPAYCXA1CWCRBNCQBNAYCXCL CJCXBNCYBNADBNA6A1CWCPBNCQBNCXCL CJCXBNCYBNADBNA6A1CWCRBNCQBND2AYCXA1CWCPBNCRBNAYCXCL CJCXBNCYBNADBNA6A1CWCPBNCQBNCXCL BTD2CS D3D2CT D6D9D0CT D8D3 CPD4D4D0DD D5D9CPD2D8CXACCTD6 CUD9D2CRD8CXD3D2D7BM CJCXBNCYBNADBNA6A1CWCPBNCQBN D5 CXCL CJCXBNCYBNADBNA6A1CWCPBNCQBN AF CXCL CCCWCT D4CPD6D7CXD2CV CPD2CS D7CRD3D6CXD2CV CUD9D2CRD8CXD3D2D7 D6CTD1CPCXD2 CXCSCTD2D8CXB9 CRCPD0 D8D3 D8CWD3D7CT CXD2 CBCTCRD8CXD3D2 BEB8 CQD9D8 CPD2 CPCSCSCXD8CXD3D2CPD0 CZ BP BD CRCPD7CT CRD3D2D8CPCXD2CXD2CV CP D1D3CSCXACCTCS D4D6D3CYCTCRD8CXD3D2 CUD9D2CRD8CXD3D2 AP CXD7 D2D3DB CPCSCSCTCS D8D3 D8CWCT CXD2D8CTD6D4D6CTD8CPD8CXD3D2 CUD9D2CRD8CXD3D2B8 CXD2 D3D6CSCTD6 D8D3 D1CPCZCT D8CWCT CSCTD2D3D8CPD8CXD3D2D7 D3CU D5D9CPD2D8CXACCTCS CRD3D2D7D8CXD8D9CTD2D8D7 CSCTD4CTD2CS D3D2 D8CWCTCXD6 CPD7D7D3CRCXCPD8CTCS D5D9CPD2D8CXACCTD6D7BM BWB4DCB5BP CJ CP BD BMBMBMCP CZ D7BMD8BM CP BD BMBMBMCP CZ DC BK BQ BQ BQ BQ BQ BQ BQ BO BQ BQ BQ BQ BQ BQ BQ BM CAB4DCB5 CXCU CZ BPBC AP D5 BWB4CP BD B5 CXCU CZ BP BD CPD2CS CP BD DC BP CJBMBMBMCWBMBMBM D5 CXCL CJBMBMBMCWBMBMBM AF CXCL CZ D3D2 CXBPBD BWB4CP CX B5 D3D8CWCTD6DBCXD7CT CCCWCT D1D3CSCXACCTCS D4D6D3CYCTCRD8CXD3D2 CUD9D2CRD8CXD3D2 CTDACPD0D9CPD8CTD7 CP D5D9CPD2B9 D8CXACCTD6 CUD9D2CRD8CXD3D2 D5 D3D2 D7D3D1CT CPD6CVD9D1CTD2D8 CSCTD2D3D8CPD8CXD3D2 BTB8 CRD3D1D4CPD6CXD2CV D8CWCT CRCPD6CSCXD2CPD0CXD8DD D3CU D8CWCT CXD1CPCVCT D3CU D8CWCT D6CTB9 D7D8D6CXCRD8D3D6 D7CTD8 CXD2 BT DBCXD8CW D8CWCT D8CWCT CRCPD6CSCXD2CPD0CXD8DD D3CU CXD1CPCVCT D3CU D8CWCT CXD2D8CTD6D7CTCRD8CTCS D6CTD7D8D6CXCRD8D3D6 CPD2CS CQD3CSDD D7CTD8D7 CXD2 BTBM BL AP D5 BT BP CUCWCT BE BMBMBMCT CP BND8CXCYCW BNCT BE BMBMBMCT CP BN CXBEBTBN D8 BP D5B4CYCACYBNCYCBCYB5 CA BP BTD3D2CUCW BNCT BE BMBMBMCT CP BN CXCVBN CB BP BTD3D2CUCW BNCT BE BMBMBMCT CP BNCCCACDBXCXCVCV CCCWCXD7 CPD0CVD3D6CXD8CWD1 D4CPD6D7CTD7 CP CRCPD8CTCVD3D6CXCPD0 CVD6CPD1D1CPD6 CXD2 D8CWCT D9D7D9CPD0 DBCPDD DF CRD3D2D7D8CXD8D9CTD2D8D7 CPD6CT CXD2CXD8CXCPD0D0DD CPCSCSCTCS D8D3 D8CWCT CRCWCPD6D8 CPD7 D7CXD2CVD0CT CRD3D1D4D3D2CTD2D8D7 CRD3DACTD6CXD2CV CP CRCTD6D8CPCXD2 DDCXCTD0CS CXD2 D8CWCT CXD2D4D9D8 D7D8D6CXD2CV B4D8CWCT CXD2CSCXCRCTD7 D3CU D8CWCT CRD3D1D4D3D2CTD2D8 CPD6CT D8CWCT D7CPD1CT CPD7 D8CWCT CXD2CSCXCRCTD7 D3CU D8CWCT CRD3D2D7D8CXD8D9CTD2D8 CXD8D7CTD0CUB5B8 CPD2CS D8CWCTDD CPD6CT CRD3D1CQCXD2CTCS CQDD CRD3D2CRCPD8CTD2CPD8CXD2CV D8CWCT DDCXCTD0CSD7 D3CU D7D1CPD0D0CTD6 CRD3D2D7D8CXD8D9CTD2D8D7 D8D3 D1CPCZCT D0CPD6CVCTD6 D3D2CTD7 DF D9D2D8CXD0 CP CRD3D2CYD9D2CRD8CXD3D2 CXD7 CTD2CRD3D9D2D8CTD6CTCSBA CFCWCTD2 CP CRD3D2CYD9D2CRD8CXD3D2 CXD7 BL BYD3D0D0D3DBCXD2CV C3CTCTD2CPD2 CPD2CS CBD8CPDACX B4BDBLBKBIB5BA BC BD BE BF BG CRD3D2D8CPCXD2CXD2CV D3D2CT D3D6CPD2CVCT CPD2CS D3D2CT D0CTD1D3D2 CJBCBNBDBNCBD2C6C8 D5 BPC6C8 D5 BCBN CJBDBNBEBNCGD2C6C8 BL A1 C6C8 BL D2C6C8 BL A1 C6C8 BL BN CJBEBNBFBNBVD3D2CYBN CJBFBNBGBNCGD2C6C8 BL A1 C6C8 BL D2C6C8 BL A1 C6C8 BL BN CWBCBNBDBNCBD2C6C8 D5 BPC6C8 D5 BCCXCL CWBDBNBEBNC6C8 BL CXCL CUD3 BD BND3 BE BND3 BF BND3 BG CV CWBEBNBFBNBVD3D2CYCXCL CWBFBNBGBNC6C8 BL CXCL CUD0 BD BND0 BE BND0 BF CV CUCWD3 BD BNDC BD CXBNCWD0 BE BNDC BD CXBNCWD0 BF BNDC BF CXCV BABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABA B4BDB5 CJBDBNBGBNCGD2C6C8 BL A1 C6C8 BL D2C6C8 BL A1 C6C8 BL BNCWBDBNBEBNC6C8 BL CXCL CUD3 BD BND3 BE BND3 BF BND3 BG CV BABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABA B4BEB5 CJBDBNBGBNCGD2C6C8 BL A1 C6C8 BL D2C6C8 BL A1 C6C8 BL BNCWBFBNBGBNC6C8 BL CXCL CUD0 BD BND0 BE BND0 BF CV BABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABA B4BFB5 CJBDBNBGBNCGD2C6C8 BL A1 C6C8 BL BNCWBDBNBEBNC6C8 BL CXCL CUD3 BD BND3 BE BND3 BF BND3 BG CV BABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABA B4BGB5 CJBDBNBGBNCGD2C6C8 BL A1 C6C8 BL BNCWBFBNBGBNC6C8 BL CXCL CUD0 BD BND0 BE BND0 BF CV BABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABA B4BHB5 CJBCBNBGBNCBD2C6C8 D5 BNCWBCBNBDBNCBD2C6C8 D5 BPC6C8 BL CXA1CWBDBNBEBNC6C8 BL CXCL CUCWD3 BD BNDC BD CXCV BABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABA B4BIB5 CJBCBNBGBNCBD2C6C8 D5 BNCWBCBNBDBNCBD2C6C8 D5 BPC6C8 BL CXA1CWBFBNBGBNC6C8 BL CXCL CUCWD0 BE BNDC BD CXBNCWD0 BF BNDC BF CXCV BABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABA B4BJB5 CJBCBNBGBNCBD2C6C8 D5 BNCWBCBNBDBNCBD2C6C8 D5 BPC6C8 AF CXA1CWBDBNBEBNC6C8 AF CXCL CUDC BD CV BABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABA B4BKB5 CJBCBNBGBNCBD2C6C8 D5 BNCWBCBNBDBNCBD2C6C8 D5 BPC6C8 AF CXA1CWBFBNBGBNC6C8 AF CXCL CUDC BD BNDC BF CV B4BLB5 CJBCBNBGBNCBD2C6C8 D5 BNCWBCBNBDBNCBD2C6C8 D5 BPC6C8 AF CXA1CWBDBNBGBNC6C8 AF CXCL CUDC BD CV B4BDBCB5 CJBCBNBGBNCBD2C6C8 D5 BNCWBCBNBGBNCBD2C6C8 D5 CXCL CUDC BD CV BYCXCVD9D6CT BFBM CBCPD1D4D0CT CSCTD6CXDACPD8CXD3D2 D3CU CRD3D2CYD3CXD2CTCS C6C8BA CTD2CRD3D9D2D8CTD6CTCS CXD1D1CTCSCXCPD8CTD0DD D8D3 D8CWCT D0CTCUD8 D3D6 D6CXCVCWD8 D3CU CP D6CTCRB9 D3CVD2CXDECTCS CRD3D2D7D8CXD8D9CTD2D8 CRD3D2D7D8CXD8D9CTD2D8 DCB8 CPD2CS CPD2D3D8CWCTD6 CRD3D2B9 D7D8CXD8D9CTD2D8 D3CU D8CWCT D7CPD1CT CRCPD8CTCVD3D6DD CXD7 CUD3D9D2CS CXD1D1CTCSCXCPD8CTD0DD CQCTDDD3D2CS D8CWCPD8 CRD3D2CYD9D2CRD8CXD3D2B8 D8CWCT D4CPD6D7CTD6 CRD6CTCPD8CTD7 CP D2CTDB CRD3D2D7D8CXD8D9CTD2D8 D8CWCPD8 CWCPD7 D8CWCT CRD3D1CQCXD2CTCS DDCXCTD0CS D3CU CQD3D8CW CRD3D2B9 D7D8CXD8D9CTD2D8D7B8 CQD9D8 CRD3D4CXCTD7 DCB3D7 CRD3D1D4D3D2CTD2D8 DDCXCTD0CS B4D8CWCT D7D8D6CXD2CV CXD2CSCXCRCTD7 D3CU DCB3D7 D3D6CXCVCXD2CPD0 CRD3D1D4D3D2CTD2D8D7B5 DBCXD8CW D2D3 CRCWCPD2CVCTBA CCCWCXD7 CWCPD7 D8CWCT CTABCTCRD8 D3CU CRD6CTCPD8CXD2CV D8DBD3 D2CTDB CRD3D2D7D8CXD8D9CTD2D8D7 CTDACTD6DD D8CXD1CT D8DBD3 CTDCCXD7D8CXD2CV CRD3D2D7D8CXD8D9CTD2D8D7 CPD6CT CRD3D2CYD3CXD2CTCSBM CTCPCRCW DBCXD8CW CP CSCXABCTD6CTD2D8 CRD3D1D4D3D2CTD2D8 DDCXCTD0CSB8 CQD9D8 CQD3D8CW DBCXD8CW D8CWCT D7CPD1CT B4CRD3D1CQCXD2CTCSB5 CRD3D2D7D8CXD8D9CTD2D8 DDCXCTD0CSBA CCCWCTD7CT D2CTDB CSCXD7CRD3D2D8CXD2D9D3D9D7 CRD3D2D7D8CXD8D9CTD2D8D7 B4DBCXD8CW CRD3D1D4D3D2CTD2D8 DDCXCTD0CSD7 D8CWCPD8 CSD3 D2D3D8 CTDCCWCPD9D7D8 D8CWCTCXD6 CRD3D2D7D8CXD8D9CTD2D8 DDCXCTD0CSD7B5 CPD6CT D7D8CXD0D0 D8D6CTCPD8CTCS CPD7 D3D6CSCXD2CPD6DDCRD3D2D7D8CXD8D9CTD2D8D7 CQDD D8CWCT D4CPD6D7CTD6B8 DBCWCXCRCW CRD3D1CQCXD2CTD7 D8CWCTD1 DBCXD8CW CPD6CVD9D1CTD2D8D7 CPD2CS D1D3CSCXACCTD6D7 D9D2D8CXD0 CPD0D0 D3CU D8CWCTCXD6 CPD6CVD9D1CTD2D8 D4D3D7CXD8CXD3D2D7 CWCPDACT CQCTCTD2 D7D9CRCRCTD7D7B9 CUD9D0D0DD CSCXD7CRCWCPD6CVCTCSB8 CPD8 DBCWCXCRCW D4D3CXD2D8 D4CPCXD6D7 D3CU CSCXD7CRD3D2D8CXD2D9B9 D3D9D7 CRD3D2D7D8CXD8D9CTD2D8D7 DBCXD8CW D8CWCT D7CPD1CT CRD3D2D7D8CXD8D9CTD2D8 DDCXCTD0CS CRCPD2 CQCT D6CTCPD7D7CTD1CQD0CTCS CXD2D8D3 DBCWD3D0CT DF D3D6 CPD8 D0CTCPD7D8 D0CTD7D7 CSCXD7CRD3D2B9 D8CXD2D9D3D9D7 DF CRD3D2D7D8CXD8D9CTD2D8D7 CPCVCPCXD2BA BT D7CPD1D4D0CT CSCTD6CXDACPD8CXD3D2 CUD3D6 D8CWCT DACTD6CQ D4CWD6CPD7CT COCRD3D2B9 D8CPCXD2CXD2CV D3D2CT D3D6CPD2CVCT CPD2CS D3D2CT D0CTD1D3D2B8B3 CXD2DAD3D0DACXD2CV CRD3D2B9 CYD9D2CRD8CXD3D2 D3CU CTDCCXD7D8CTD2D8CXCPD0D0DD D5D9CPD2D8CXACCTCS D2D3D9D2 D4CWD6CPD7CTD7B8 CXD7 D7CWD3DBD2 CXD2 BYCXCVD9D6CT BFB8 D9D7CXD2CV D8CWCT CPCQD3DACT D4CPD6D7CT D6D9D0CTD7 CPD2CS D8CWCT D0CTDCCXCRCPD0CXDECTCS CVD6CPD1D1CPD6BM CRD3D2D8CPCXD2CXD2CV BM CBD2C6C8 D5 BPC6C8 D5 BC D3D2CT BM CGD2C6C8 D5 A1 C6C8 D5 D2C6C8 D5 A1 C6C8 D5 BPC6C8 AF CGBPC6C8 D5 A1 C6C8 D5 D2C6C8 D5 A1C6C8 D5 BPC6C8 AF D3D6CPD2CVCTB8 D0CTD1D3D2 BM C6C8 AF CPD2CS BM BVD3D2CY BYCXD6D7D8 D8CWCT D4CPD6D7CTD6 CPD4D4D0CXCTD7 D8CWCT D7CZCXD4 CRD3D2CYD9D2CRD8CXD3D2 D6D9D0CTD7 D8D3 D3CQD8CPCXD2 D8CWCT CSCXD7CRD3D2D8CXD2D9D3D9D7 CRD3D2D7D8CXD8D9CTD2D8D7 D7CWD3DBD2 CPCUB9 D8CTD6 D7D8CTD4D7 B4BDB5 CPD2CS B4BEB5B8 CPD2CS CP CRD3D1D4D3D2CTD2D8 CXD7 CSCXD7CRCWCPD6CVCTCS CUD6D3D1 CTCPCRCW D3CU D8CWCT D6CTD7D9D0D8CXD2CV CRD3D2D7D8CXD8D9CTD2D8D7 D9D7CXD2CV D8CWCT CTD1D4D8DD CRD3D1D4D3D2CTD2D8 D6D9D0CT CXD2 D7D8CTD4D7 B4BFB5 CPD2CS B4BGB5BA CCCWCT CRD3D2D7D8CXD8D9CTD2D8D7 D6CTD7D9D0D8CXD2CV CUD6D3D1 B4BFB5 CPD2CS B4BGB5 CPD6CT D8CWCTD2 CRD3D1B9 D4D3D7CTCS DBCXD8CW D8CWCT DACTD6CQ CRD3D2D7D8CXD8D9CTD2D8 CUD3D6 COCRD3D2D8CPCXD2CXD2CVB3 CXD2 D7D8CTD4D7 B4BHB5 CPD2CS B4BIB5B8 D9D7CXD2CV D8CWCT D0CTCUD8 CPD8D8CPCRCWD1CTD2D8 D6D9D0CT CUD3D6 CUD6CTD7CW CRD3D1D4D3D2CTD2D8D7BA CCCWCT D5D9CPD2D8CXACCTD6D7 CPD6CT D8CWCTD2 CPD4D4D0CXCTCS CXD2 D7D8CTD4D7 B4BJB5 CPD2CS B4BKB5B8 CPD2CS D8CWCT D6CTD7D9D0D8CXD2CV CRD3D2D7D8CXD8D9CTD2D8D7 CPD6CT D6CTCPD7D7CTD1CQD0CTCS D9D7CXD2CV D8CWCT CRD3D2CYD9D2CRD8CXD3D2 D6D9D0CTD7 CXD2 D7D8CTD4 B4BLB5BA CCCWCT CPCSCYCPCRCTD2D8 CRD3D1D4D3D2CTD2D8D7 CXD2 D8CWCT CRD3D2D7D8CXD8D9CTD2D8 D6CTD7D9D0D8CXD2CV CUD6D3D1 D7D8CTD4 B4BLB5 CPD6CT D8CWCTD2 D1CTD6CVCTCS D9D7CXD2CV D8CWCT CRD3D1CQCXD2CPD8CXD3D2 D6D9D0CT CXD2 D7D8CTD4 B4BDBCB5B8 D4D6D3CSD9CRCXD2CV CP CRD3D1D4D0CTD8CT CVCPD4D0CTD7D7 CRD3D2D7D8CXD8D9CTD2D8 CUD3D6 D8CWCT CTD2D8CXD6CT CXD2D4D9D8BA CBCXD2CRCT D8CWCT D4CPD6D7CTD6 D6D9D0CTD7 CPD6CT ACDCCTCSB8 CPD2CS D8CWCT D2D9D1CQCTD6 D3CU CRD3D1D4D3D2CTD2D8D7 CXD2 CPD2DDCRCWCPD6D8 CRD3D2D7D8CXD8D9CTD2D8 CXD7 CQD3D9D2CSCTCS CQDD D8CWCT D1CPDCCXD1D9D1 D2D9D1CQCTD6 D3CU CRD3D1D4D3D2CTD2D8D7 CXD2 CP CRCPD8CTCVD3D6DD B4CXD2CPD7D1D9CRCW CPD7 D8CWCT D6D9D0CTD7 CRCPD2 D3D2D0DD CPCSCS CP CRD3D1D4D3D2CTD2D8 D8D3 D8CWCT D6CTCRD3CVD2CXDECTCS D0CXD7D8 CQDD D7D9CQD8D6CPCRD8CXD2CV D3D2CT CUD6D3D1 D8CWCT D9D2D6CTCRD3CVD2CXDECTCS D0CXD7D8B5B8 D8CWCT CPD0CVD3D6CXD8CWD1 D1D9D7D8 D6D9D2 CXD2 D4D3D0DDB9 D2D3D1CXCPD0 D7D4CPCRCT CPD2CS D8CXD1CT D3D2 D8CWCT D0CTD2CVD8CW D3CU D8CWCT CXD2D4D9D8 D7CTD2D8CTD2CRCTBA CBCXD2CRCT D8CWCT CRCPD6CSCXD2CPD0CXD8DD D3CU CTCPCRCW CRD3D2D7D8CXD8D9CTD2D8B3D7 CSCTD2D3D8CPD8CXD3D2 CXD7 CQD3D9D2CSCTCS CQDD CYBXCY DA B4DBCWCTD6CT BX CXD7 D8CWCT D7CTD8 D3CU CTD2D8CXD8CXCTD7 CXD2 D8CWCT CTD2DACXD6D3D2D1CTD2D8 CPD2CS DA CXD7 D8CWCT D1CPDCCXB9 D1D9D1 DACPD0CTD2CRDD D3CU CPD2DD CRCPD8CTCVD3D6DDB5B8 D8CWCT CPD0CVD3D6CXD8CWD1 D6D9D2D7 CXD2 DBD3D6D7D8B9CRCPD7CT D4D3D0DDD2D3D1CXCPD0 D7D4CPCRCT D3D2 CYBXCYBN CPD2CS D7CXD2CRCT D8CWCTD6CT CXD7 D2D3 D1D3D6CT D8CWCPD2 D3D2CT D7CTD8 CRD3D1D4D3D7CXD8CXD3D2 D3D4CTD6CPD8CXD3D2 D4CTD6B9 CUD3D6D1CTCS DBCWCTD2 CP D6D9D0CT CXD7 CPD4D4D0CXCTCSB8 CPD2CS CTCPCRCW CRD3D1D4D3D7CXD8CXD3D2 D3D4CTD6CPD8CXD3D2 D6D9D2D7 CXD2 DBD3D6D7D8B9CRCPD7CT D5D9CPCSD6CPD8CXCR D8CXD1CT D3D2 D8CWCT D7CXDECT D3CU CXD8D7 CRD3D1D4D3D7CTCS D7CTD8D7 B4CSD9CT D8D3 D8CWCT D5D9CPD2D8CXACCTD6 D3D4CTD6B9 CPD8CXD3D2B5B8 D8CWCT CPD0CVD3D6CXD8CWD1 D6D9D2D7 CXD2 DBD3D6D7D8B9CRCPD7CT D4D3D0DDD2D3D1CXCPD0 D8CXD1CT D3D2 CYBXCY CPD7 DBCTD0D0BA BG BXDACPD0D9CPD8CXD3D2 CCCWCT CTDCD8CTD2CSCTCS D4CPD6D7CTD6 CSCTD7CRD6CXCQCTCS CPCQD3DACT CWCPD7 CQCTCTD2 CXD1B9 D4D0CTD1CTD2D8CTCS CPD2CS CTDACPD0D9CPD8CTCS D3D2 CP CRD3D6D4D9D7 D3CU BFBGBC D7D4D3B9 CZCTD2 CXD2D7D8D6D9CRD8CXD3D2D7 D8D3 D7CXD1D9D0CPD8CTCS CWD9D1CPD2B9D0CXCZCT CPCVCTD2D8D7 CXD2 CP CRD3D2D8D6D3D0D0CTCS BFB9BW CTD2DACXD6D3D2D1CTD2D8 B4D8CWCPD8 D3CU CRCWCXD0CSD6CTD2 D6D9D2B9 D2CXD2CV CP D0CTD1D3D2CPCSCT D7D8CPD2CSB8 DBCWCXCRCWDBCPD7 CSCTCTD1CTCS D7D9CXD8CPCQD0DD CUCPD1CXD0CXCPD6 D8D3 D9D2CSCTD6CVD6CPCSD9CPD8CT D7D8D9CSCTD2D8 D7D9CQCYCTCRD8D7B5BA CCCWCT D4CPD6D7CTD6 DBCPD7 D6D9D2 D3D2 D8CWCT DBD3D6CS D0CPD8D8CXCRCT D3D9D8D4D9D8 D3CU CPD2 D3ABB9D8CWCTB9D7CWCTD0CU D7D4CTCTCRCW D6CTCRD3CVD2CXDECTD6 B4BVC5CD CBD4CWCXD2DC C1C1B5 CPD2CS D8CWCT D4CPD6D7CTD6 CRCWCPD6D8 DBCPD7 D7CTCTCSCTCS DBCXD8CW CTDACTD6DD CWDDD4D3D8CWCTD7CXDECTCS DBD3D6CSBA CCCWCT D4CPD6D7CTD6 DBCPD7 CPD0D7D3 CRD3D1D4CPD6CTCS DBCXD8CW D8CWCT D6CTCRB9 D3CVD2CXDECTD6 CQDD CXD8D7CTD0CUB8 CXD2 D3D6CSCTD6 D8D3 CSCTD8CTD6D1CXD2CT D8CWCT CSCTCVD6CTCT D8D3 DBCWCXCRCW CPD2 CTD2DACXD6D3D2D1CTD2D8B9CQCPD7CTCS CPD4D4D6D3CPCRCW CRD3D9D0CS CRD3D1B9 D4D0CTD1CTD2D8 CRD3D6D4D9D7B9CQCPD7CTCS CSCXD7CPD1CQCXCVD9CPD8CXD3D2BA CCCWCT D7DDD7D8CTD1D7 DBCTD6CT CTDACPD0D9CPD8CTCS CPD7 DBD3D6CS D6CTCRD3CVD2CXDECTD6D7 B4CXBACTBA CXCVD2D3D6CXD2CV D8CWCT CQD6CPCRCZCTD8D7 CXD2 D8CWCT D4CPD6D7CTD6 D3D9D8D4D9D8B5 D3D2 D8CWCT ACD6D7D8 BDBCBC D7CTD2B9 D8CTD2CRCTD7 D3CU D8CWCT CRD3D6D4D9D7 B4CRD3D6D6CTD7D4D3D2CSCXD2CV D8D3 D8CWCT ACD6D7D8 D7CTDACTD2 D3CU BFBF D7D9CQCYCTCRD8D7B5BN D8CWCT D0CPD8D8CTD6 BEBGBC D7CTD2D8CTD2CRCTD7 DBCTD6CT D6CTB9 D7CTD6DACTCS CUD3D6 D8D6CPCXD2CXD2CV D8CWCT D6CTCRD3CVD2CXDECTD6 CPD2CS CUD3D6 CSCTDACTD0D3D4CXD2CV D8CWCT CVD6CPD1D1CPD6 CPD2CS D7CTD1CPD2D8CXCR D0CTDCCXCRD3D2BA CCCWCT CPDACTD6CPCVCT D9D8D8CTD6CPD2CRCT D0CTD2CVD8CW DBCPD7 CPD4D4D6D3DCCXD1CPD8CTD0DD D8CWD6CTCT D7CTCRD3D2CSD7 B4D7D9CQD7D9D1CXD2CV CPCQD3D9D8 BFBCBC CUD6CPD1CTD7 D3D6 D4D3D7CXB9 D8CXD3D2D7 CXD2 D8CWCT D4CPD6D7CTD6 CRCWCPD6D8B5B8 CRD3D2D8CPCXD2CXD2CV CPD2 CPDACTD6CPCVCT D3CU D2CXD2CT DBD3D6CSD7BA C8CPD6D7CXD2CV D8CXD1CT CPDACTD6CPCVCTCS D9D2CSCTD6 BGBC D7CTCRD3D2CSD7 D4CTD6 D7CTD2D8CTD2CRCT D3D2 CP C8BGB9BDBHBCBCC5C0DEB8 D1D3D7D8 D3CU DBCWCXCRCWDBCPD7 D7D4CTD2D8 CXD2 CUD3D6CTD7D8 CRD3D2D7D8D6D9CRD8CXD3D2 D6CPD8CWCTD6 D8CWCPD2 CSCTD2D3D8CPD8CXD3D2 CRCPD0CRD9D0CPD8CXD3D2BA BTCRCRD9D6CPCRDD D6CTD7D9D0D8D7 D7CWD3DB D8CWCPD8 D8CWCT D4CPD6D7CTD6 DBCPD7 CPCQD0CT D8D3 CRD3D6D6CTCRD8D0DD CXCSCTD2D8CXCUDD CP D7CXCVD2CXACCRCPD2D8D2D9D1CQCTD6 D3CU DBD3D6CSD7 D8CWCPD8 D8CWCT D6CTCRD3CVD2CXDECTD6 D1CXD7D7CTCS B4CPD2CS DACXCRCT DACTD6D7CPB5B8 D7D9CRCW D8CWCPD8 CP D4CTD6CUCTCRD8 D7DDD2D8CWCTD7CXD7 D3CU D8CWCT D8DBD3 B4CRCWD3D3D7CXD2CV D8CWCT CRD3D6D6CTCRD8 DBD3D6CS CXCU CXD8 CXD7 D6CTCRD3CVD2CXDECTCS CQDD CTCXD8CWCTD6 D7DDD7D8CTD1B5 DBD3D9D0CS D4D6D3B9 CSD9CRCT CPD2 CPDACTD6CPCVCT D3CU BK D4CTD6CRCTD2D8CPCVCT D4D3CXD2D8D7 D1D3D6CT D6CTCRCPD0D0 D8CWCPD2 D8CWCT D6CTCRD3CVD2CXDECTD6 CQDD CXD8D7CTD0CU D3D2 D7D9CRCRCTD7D7CUD9D0 D4CPD6D7CTD7B8 CPD2CS CPD7 D1D9CRCW CPD7 BDBL D4CTD6CRCTD2D8CPCVCT D4D3CXD2D8D7 D1D3D6CT CUD3D6 D7D3D1CT D7D9CQCYCTCRD8D7BM BDBC D6CTCRD3CVD2CXDECTD6 D4CPD6D7CTD6 CYD3CXD2D8 D7D9CQCYCTCRD8 D4D6CTCR D6CTCRCPD0D0 CUCPCXD0 D4D6CTCR D6CTCRCPD0D0 D6CTCRCPD0D0 BC BJBI BJBL BDBK BJBE BJBG BLBE BD BJBJ BJBH BEBK BIBF BHBH BKBF BE BJBC BJBD BFBF BGBL BHBG BIBL BF BJBD BIBJ BGBF BGBL BGBH BIBL BG BIBI BHBG BFBJ BGBG BFBL BIBJ BH BHBF BHBE BHBG BFBI BFBD BJBE BI BKBG BKBG BHBC BHBI BIBF BKBF CPD0D0 BIBK BIBJ BFBJ BHBF BHBC BJBH DBCWCXCRCW CXD2CSCXCRCPD8CTD7 D8CWCPD8 D8CWCT CTD2DACXD6D3D2D1CTD2D8 D1CPDD D3ABCTD6 CP D9D7CTCUD9D0 CPCSCSCXD8CXD3D2CPD0 D7D3D9D6CRCT D3CU CXD2CUD3D6D1CPD8CXD3D2 CUD3D6 CSCXD7CPD1B9 CQCXCVD9CPD8CXD3D2BA CCCWD3D9CVCW CXD8 D1CPDD D2D3D8 CQCT D4D3D7D7CXCQD0CT D8D3 CXD1D4D0CTB9 D1CTD2D8 CP D4CTD6CUCTCRD8 D7DDD2D8CWCTD7CXD7 D3CU D8CWCT CTD2DACXD6D3D2D1CTD2D8B9CQCPD7CTCS BDBC CBD9CRCRCTD7D7CUD9D0 D4CPD6D7CTD7 CPD6CT D8CWD3D7CT D8CWCPD8 D6CTD7D9D0D8 CXD2 D3D2CT D3D6 D1D3D6CT CRD3D1D4D0CTD8CT CPD2CPD0DDD7CTD7 D3CU D8CWCT CXD2D4D9D8B8 CTDACTD2 CXCU D8CWCT CRD3D6D6CTCRD8 D8D6CTCT CXD7 D2D3D8 CPD1D3D2CV D8CWCTD1BA CPD2CS CRD3D6D4D9D7B9CQCPD7CTCS CPD4D4D6D3CPCRCWCTD7B8 CXCU CTDACTD2 CWCPD0CU D3CU D8CWCT CPCQD3DACT CVCPCXD2D7 CRCPD2 CQCT D6CTCPD0CXDECTCSB8 CXD8 DBD3D9D0CS D1CPD6CZ CP D7CXCVB9 D2CXACCRCPD2D8 CPCSDACPD2CRCTBA BH BVD3D2CRD0D9D7CXD3D2 CCCWCXD7 D4CPD4CTD6 CWCPD7 CSCTD7CRD6CXCQCTCS CPD2 CTDCD8CTD2D7CXD3D2 D8D3 CPD2 CTD2DACXD6D3D2D1CTD2D8B9CQCPD7CTCS D4CPD6D7CXD2CV CPD0CVD3D6CXD8CWD1B8 CXD2CRD6CTCPD7CXD2CV CXD8D7 D7CTD1CPD2D8CXCR CRD3DACTD6CPCVCT D8D3 CXD2CRD0D9CSCT D5D9CPD2D8CXACCTD6 CPD2CS CRD3D2CYD9D2CRB9 D8CXD3D2 D3D4CTD6CPD8CXD3D2D7 DBCXD8CWD3D9D8 CSCTD7D8D6D3DDCXD2CV CXD8D7 D4D3D0DDD2D3D1CXCPD0 DBD3D6D7D8B9CRCPD7CT CRD3D1D4D0CTDCCXD8DDBA BXDCD4CTD6CXD1CTD2D8D7 D9D7CXD2CV CPD2 CXD1D4D0CTB9 D1CTD2D8CPD8CXD3D2 D3CU D8CWCXD7 CPD0CVD3D6CXD8CWD1 D3D2 CP CRD3D6D4D9D7 D3CU D7D4D3CZCTD2 CXD2D7D8D6D9CRD8CXD3D2D7 CXD2CSCXCRCPD8CT D8CWCPD8 BDB5 D8CWCT D3CQD7CTD6DACTCS CRD3D1D4D0CTDCB9 CXD8DD D3CU D8CWCT CPD0CVD3D6CXD8CWD1 CXD7 D7D9CXD8CPCQD0CT CUD3D6 D4D6CPCRD8CXCRCPD0 D9D7CTD6 CXD2B9 D8CTD6CUCPCRCT CPD4D4D0CXCRCPD8CXD3D2D7B8 CPD2CS BEB5 D8CWCT CPCQCXD0CXD8DD D8D3 CSD6CPDB D3D2 D8CWCXD7 CZCXD2CS D3CU CTD2DACXD6D3D2D1CTD2D8 CXD2CUD3D6D1CPD8CXD3D2 CXD2 CPD2 CXD2D8CTD6B9 CUCPCRCTCS CPD4D4D0CXCRCPD8CXD3D2 CWCPD7 D8CWCT D4D3D8CTD2D8CXCPD0 D8D3 CVD6CTCPD8D0DD CXD1B9 D4D6D3DACT D6CTCRD3CVD2CXD8CXD3D2 CPCRCRD9D6CPCRDD CXD2 D7D4CTCPCZCTD6B9CXD2CSCTD4CTD2CSCTD2D8 D1CXDCCTCSB9CXD2CXD8CXCPD8CXDACTCXD2D8CTD6CUCPCRCTD7BA CACTCUCTD6CTD2CRCTD7 C3CPDECXD1CXCTD6DE BTCYCSD9CZCXCTDBCXCRDEBA BDBLBFBHBA BWCXCT D7DDD2D8CPCZD8CXD7CRCWCT CZD3D2D2CTDCB9 CXD8CPD8BA C1D2 CBBA C5CRBVCPD0D0B8 CTCSCXD8D3D6B8 C8D3D0CXD7CW C4D3CVCXCR BDBLBEBCB9BDBLBFBLB8 D4CPCVCTD7 BEBCBJDFBEBFBDBA C7DCCUD3D6CS CDD2CXDACTD6D7CXD8DD C8D6CTD7D7BA CCD6CPD2D7D0CPD8CTCS CUD6D3D1 CBD8D9CSCXCP C8CWCXD0D3D7D3D4CWCXCRCP BDBM BDDFBEBJBA CHCTCWD3D7CWD9CP BUCPD6B9C0CXD0D0CTD0BA BDBLBHBFBA BT D5D9CPD7CXB9CPD6CXD8CWD1CTD8CXCRCPD0 D2D3D8CPB9 D8CXD3D2 CUD3D6 D7DDD2D8CPCRD8CXCR CSCTD7CRD6CXD4D8CXD3D2BA C4CPD2CVD9CPCVCTB8 BEBLBMBGBJDFBHBKBA C2D3D2 BUCPD6DBCXD7CT CPD2CS CAD3CQCXD2 BVD3D3D4CTD6BA BDBLBKBDBA BZCTD2CTD6CPD0CXDECTCS D5D9CPD2D8CXACCTD6D7 CPD2CS D2CPD8D9D6CPD0 D0CPD2CVD9CPCVCTBA C4CXD2CVD9CXD7D8CXCRD7 CPD2CS C8CWCXB9 D0D3D7D3D4CWDDB8BGBA CBDDD0DACXCT BUCXD0D0D3D8 CPD2CS BUCTD6D2CPD6CS C4CPD2CVBA BDBLBKBLBA CCCWCT D7D8D6D9CRD8D9D6CT D3CU D7CWCPD6CTCS CUD3D6CTD7D8D7 CXD2 CPD1CQCXCVD9D3D9D7 D4CPD6D7CXD2CVBA C1D2 C8D6D3CRCTCTCSCXD2CVD7 D3CU D8CWCT BEBJ D8CW BTD2D2D9CPD0 C5CTCTD8CXD2CV D3CU D8CWCT BTD7D7D3CRCXCPD8CXD3D2 CUD3D6 BVD3D1B9 D4D9D8CPD8CXD3D2CPD0 C4CXD2CVD9CXD7D8CXCRD7 B4BTBVC4 B3BKBLB5B8 D4CPCVCTD7 BDBGBFDFBDBHBDBA CECTD6AJD3D2CXCRCP BWCPCWD0 CPD2CS C5CXCRCWCPCTD0 BVBA C5CRBVD3D6CSBA BDBLBKBFBA CCD6CTCPD8CXD2CV CRD3D3D6CSCXD2CPD8CXD3D2 CXD2 D0D3CVCXCR CVD6CPD1D1CPD6D7BA BTD1CTD6CXCRCPD2 C2D3D9D6D2CPD0 D3CU BVD3D1D4D9D8CPD8CXD3D2CPD0 C4CXD2CVD9CXD7D8CXCRD7B8 BLB4BEB5BMBIBLDFBLBDBA C2D3CWD2 BWD3DBCSCXD2CVB8 CAD3CQCTD6D8 C5D3D3D6CTB8 BYD6CPD2AOCRD3CXD7 BTD2CSCTD6DDB8 CPD2CS BWD3D9CVD0CPD7 C5D3D6CPD2BA BDBLBLBGBA C1D2D8CTD6D0CTCPDACXD2CV D7DDD2D8CPDC CPD2CS D7CTD1CPD2B9 D8CXCRD7 CXD2 CPD2 CTCRCXCTD2D8 CQD3D8D8D3D1B9D9D4 D4CPD6D7CTD6BA C1D2 C8D6D3CRCTCTCSCXD2CVD7 D3CU D8CWCT BFBED2CS BTD2D2D9CPD0 C5CTCTD8CXD2CV D3CU D8CWCT BTD7D7D3CRCXCPD8CXD3D2 CUD3D6 BVD3D1B9 D4D9D8CPD8CXD3D2CPD0 C4CXD2CVD9CXD7D8CXCRD7 B4BTBVC4B3BLBGB5BA C2D3D7CWD9CP BZD3D3CSD1CPD2BA BDBLBLBLBA CBCTD1CXD6CXD2CV D4CPD6D7CXD2CVBA BVD3D1D4D9D8CPB9 D8CXD3D2CPD0 C4CXD2CVD9CXD7D8CXCRD7B8 BEBHB4BGB5BMBHBJBFDFBIBCBHBA BXBA C3CTCTD2CPD2 CPD2CS C2BA CBD8CPDACXBA BDBLBKBIBA BT D7CTD1CPD2D8CXCR CRCWCPD6CPCRD8CTD6CXDECPB9 D8CXD3D2 D3CU D2CPD8D9D6CPD0 D0CPD2CVD9CPCVCT CSCTD8CTD6D1CXD2CTD6D7BA C4CXD2CVD9CXD7D8CXCRD7 CPD2CS C8CWCXD0D3D7D3D4CWDDB8 BLBMBEBHBFDFBFBEBIBA CACXCRCWCPD6CS C5D3D2D8CPCVD9CTBA BDBLBJBFBA CCCWCT D4D6D3D4CTD6 D8D6CTCPD8D1CTD2D8D3CU D5D9CPD2B9 D8CXACCRCPD8CXD3D2 CXD2 D3D6CSCXD2CPD6DD BXD2CVD0CXD7CWBA C1D2 C2BA C0CXD2D8CXCZCZCPB8 C2BAC5BABXBA C5D3D6CPDACRD7CXCZB8 CPD2CS C8BA CBD9D4D4CTD7B8 CTCSCXD8D3D6D7B8 BTD4D4D6D3CPCRCWCTD7 D8D3 C6CPD8B9 D9D6CPD0 C4CPD2CVCPD9CVCTB8 D4CPCVCTD7 BEBEBDDFBEBGBEBA BWBA CACXCTCSCTD0B8 BWD3D6CSD6CTCRCWD8BA CACTD4D6CXD2D8CTCS CXD2 CABA C0BA CCCWD3D1CPD7D3D2 CTCSBAB8 BYD3D6D1CPD0 C8CWCXD0D3D7D3D4CWDDB8 CHCPD0CT CDD2CXDACTD6D7CXD8DD C8D6CTD7D7B8 BDBLBLBGBA CFCXD0D0CXCPD1 CBCRCWD9D0CTD6BA BEBCBCBDBA BVD3D1D4D9D8CPD8CXD3D2CPD0 D4D6D3D4CTD6D8CXCTD7 D3CU CTD2DACXD6D3D2D1CTD2D8B9CQCPD7CTCS CSCXD7CPD1CQCXCVD9CPD8CXD3D2BA C1D2 C8D6D3CRCTCTCSCXD2CVD7 D3CU D8CWCT BFBLD8CW BTD2D2D9CPD0 C5CTCTD8CXD2CV D3CU D8CWCT BTD7D7D3CRCXCPD8CXD3D2 CUD3D6 BVD3D1B9 D4D9D8CPD8CXD3D2CPD0 C4CXD2CVD9CXD7D8CXCRD7 B4BTBVC4 B3BCBDB5B8CCD3D9D0D3D9D7CTB8 BYD6CPD2CRCTBA CBD8D9CPD6D8 C5BA CBCWCXCTCQCTD6B8 CHDACTD7 CBCRCWCPCQCTD7B8 CPD2CS BYCTD6D2CPD2CSD3 BVBAC6BA C8CTD6CTCXD6CPBA BDBLBLBHBA C8D6CXD2CRCXD4D0CTD7 CPD2CS CXD1D4D0CTD1CTD2D8CPD8CXD3D2 D3CU CSCTB9 CSD9CRD8CXDACT D4CPD6D7CXD2CVBA C2D3D9D6D2CPD0 D3CU C4D3CVCXCR C8D6D3CVD6CPD1D1CXD2CVB8 BEBGBMBFDF BFBIBA References Kazimierz Ajdukiewicz.
1935. Die syntaktische konnexitat.
In S.
McCall, editor, Polish Logic 1920-1939, pages 207231.
Oxford University Press.
Translated from Studia Philosophica 1: 127.
Yehoshua Bar-Hillel.
1953. A quasi-arithmetical notation for syntactic description.
Language, 29:4758.
Jon Barwise and Robin Cooper.
1981. Generalized quanti ers and natural language.
Linguistics and Philosophy, 4.
Sylvie Billot and Bernard Lang.
1989. The structure of shared forests in ambiguous parsing.
In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics (ACL '89), pages 143151.
Veronica Dahl and Michael C.
McCord. 1983.
Treating coordination in logic grammars.
American Journal of Computational Linguistics, 9(2):6991.
John Dowding, Robert Moore, Francois Andery, and Douglas Moran.
1994. Interleaving syntax and semantics in an ecient bottom-up parser.
In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics (ACL'94).
Joshua Goodman.
1999. Semiring parsing.
Computational Linguistics, 25(4):573605.
E. Keenan and J.
Stavi. 1986.
A semantic characterization of natural language determiners.
Linguistics and Philosophy, 9:253326.
Richard Montague.
1973. The proper treatment of quanti cation in ordinary English.
In J.
Hintikka, J.M.E.
Moravcsik, and P.
Suppes, editors, Approaches to Natural Langauge, pages 221242.
D. Riedel, Dordrecht.
Reprinted in R.
H. Thomason ed., Formal Philosophy, Yale University Press, 1994.
William Schuler.
2001. Computational properties of environment-based disambiguation.
In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL '01), Toulouse, France.
Stuart M.
Shieber, Yves Schabes, and Fernando C.N.
Pereira. 1995.
Principles and implementation of deductive parsing.
Journal of Logic Programming, 24:336.
 The Elimination of Grammatical Restrictions M.
Salkoff and in a String Grammar of English N.
Sager Institute for Computer Research in the Humanities New York University, New York i.
Sun~nary of String Theory In writing a grammar of a natural language, one is faced with the problem o f e x p r e s s i n g grammatica NVN number: sequence N1 and N 2 (N, noun: V, verb), the subject The boy eats the meat; Q N1 P N2 N For example, i n t h e s e n t e n c e form and the verb V must agree in Or, in the five feet in length, One of the ~ The boys eats the meat.
e.g., (Q a number; P, preposition), are of particular subclasses: theories of linguistic structure which is particularly relevant to this problem is linguistic string analysis[1].
In this theory, the major syntactic (a string is structures of English are stated as a set of elementary strings a sequence of word categories, e.g., N V____NN, N V P N, eta).
Each sentence of the language consists of one elementary sentence (its center string) plus zero or more elementary adjunct strings which are adjoined either to the right or left or in place of particular elements of other elementary strings in the sentence.
17.~ The elementary strings can be grouped into classes according to how and where they can be inserted into other strings.
an elementary string, X If Y = X 1 X 2. . . Xn is ranging over the category symbols, the following classes of strings are defined: left adjuncts of X: adjoined to a string Y to the left of X in Y, or to the left of an ~X adjoined to Y in this manner.
rX right adjuncts of X: adjoined to a string Y to the right of X in Y, or to the right of an rX adjoined to Y in this manner.
replacement strings of X: adjoined to a string Y, replacing X in Y.
sentences adjuncts of the string Y, adjoined to the left of X 1 or after X i in Y (l~ i ~ n), or to the right of an Sy adjoined to Y in this manner.
Cy, i conjunctional strings of Y, conjoined after X i in Y (i< i < n), or to _ _ the right of a Cy, i adjoined to Y in this manner.
z center strings, not adjoined to any string.
These string-class definitions, with various restrictions on the repetition ~and order of members of the classes, constitute rules of combination on the elementary strings to form sentences.
Roughly speaking, a center string is the skeleton of a sentence and the adjuncts are modifiers.
green we met in in An example of a left adjunct of N is the adjective A right adjunet of N is the clause whom the green blackboard.
the man whom we met.
in the sentence A replacement formula of N is, for example, The same sentence what he said What he said was interesting.
with a noun instead of a noun replacement string might be interesting.
since he left.
An example is are Examples of sentence adjuncts are The lecture was in general, at this time, The c strings have coordinating conjunctions at their head.
but left in He was here but left.
Examples of center strings He understood and also We wondered whether he understood.
The grammatical dependencies are expressed by restrictions on the strings as to the word subcategories which can occur together in a string or in strings related by the rules of combination.
Thus, in the center string N 1 V N2, the figrammatical dependency mentioned above is formulated by the restriction: if N1 is plural, theh V does not carry the singular morpheme -_ss.
The string grammar with restrictions gives a compact representation of the linguistic data of a language, and provides a framework within which it is relatively simple to incorporate more linguistic refinement, restrictions.
J i.e., more detailed One may ask whether it is possible to write such a string grammar without any restrictions at all, i.e., to express the grammatical dependencies (restrictions) in the syntactic structures themselves.
In the resulting restrictionless grammar, any elements which are related by a grammatical dependency w i l L b e e l e m e n t s relations, other of the same elementary string.
No grammatical than those given by the simple rule of string combination, The result of this paper is to obtain between two strings of a sentence.
demonstrate that such a restrictionless grammar can be written [4].
In order to obtain a restrictionless form of a string grammar of English, we take as a point of departure the grammar used by the computer program for string decomposition of sentences, developed at the University of Pennsylvania [2,3].
This gran~nar is somewhat more detailed than the sketch of an English A summary of the form of the computer grammar is In section 3 we show how the restrictions can be string grammar in Ill.
presented below in section 2.
eliminated from the gran~nar.
An example of a typical output obtained for a short sentence from a text of a medical abstract is shown in Figs.
1 and 2.
The decomposition of the sentence into a sequence of nested strings is indicated in the output by the numbering of the strings.
As indicated in line 1., the sentence consists of the two assertion centers in lines 2.and ~ ~ conjoined by and.
The line B  ficontains a sentence adjunct th~_~) on the assertion center as a whole . The assertion center 2 . is of the form N V A : Spikes would be effective . The noun spikes has a left adjunct (such enhanced) in line 5  as indicated by the appearance of 5 . to the left of spikes . The object effective has a left adjunct ~ 9 _ ~ ) in line 6 . and a right adjunct in line 7  In the same wsy, each of the elements of the adjunct strings may have its own left and right adjuncts.
Line IO . contains an assertion center in which the subject and the This zeroing is indicated in the modal verb (woul____dd)have been zeroed.
output by printing the zeroe~ element in parentheses.
The difference between the two analyses in Figs.
decomposition of the sequence in initiating analysis (Fig.
i an~ 2 lies in the In the first synaptlc action.
I), this sequence is taken as a P_~N right adjunct on of the effective, where initiating synaptlc is a left adjunct (onaction) form of a repeated adjective (parallel to escaping toxic in the sequence in eseap.ing toxic gases) . In the second analysis (~ig.
2), this same sequence is taken as a ~ right adjunct of effective, where initiating is the Ving, and synaptic action is the Object of initiating.
The Computer String Grammar.
In representing the string grammar in the computer, a generalized grammar where Y-.
= Y' IS where Y' is a grammar string like Y.
This system of nested gram~nar strings terminates when one of the grammar strings is equal to an atomic string (one of the word-category symbols).
The Y.
are called the options of Y, and each option Y.
consists of the elements Y... l l 13 Not every option of a grammar string Y will be well-formed each time the sentence analysis program finds an instance of Y in the sentence being analyzed.
Associated with each option Yi is a series of zero or more tests, called restrictions.
'If RiP is the set of tests associated with Yi then the grammar A restriction is a test (which will be descrfbed below) so written that if it does not give a positive result its attached option may not be chosen.
All of the restrictions in the grammar fall into two types: TypeA: The restrictions of type A enable one to avoid defining many The options of the grammar string Y similar related sets of grammar strings.
have been chosen so that Y represents a group of strings which have related filinguistic properties.
This allows the grammar to be written very compactly, and each grammar string can be formulated as best suits the linguistic data.
However, when a grammar string Y appears as a Y' ij of some other string Y', some of the options of Y may lead to . non-wellformed sequences.
In order to retain the group of options of Y and yet not allow non-wellformed sequences wherever options of Y which would have that effect are used, we attach a restriction of type A to th0s~ options of Y.
For example, let Y be where and YI = which Z V (e.g., which he chose) Y2 = what E V (e.g., what he chose) Then Y can appear in the subject Z of the linguistic center string CI: Cl = z v n What he chose was impDrtant.
This yields Which he chose was important; As it is defined here, Y can also be used to represent the wh-clauses in the right adjuncts of the noun: but in rN only the which option of Y gives wellformed sequences: 3 the book which he chose the book what he chose Hence a restriction R a is attached to the what option of Y (eq.
5) whose effect is to prevent that option from being used in rN.
Type B: With some given set of rather broadly defined major categories (noun, verb, adjective, etc).
it is always possible to express more detailed linguistic relations by defining sub-categories of the major categories.
These relations then appear as constraints on how the sub-categories may appear together in the grammar strings Y.
If some element Yij of Yi is an atomic string (hence a word-category symbol) representing some major category, say C, then Rb may exclude the subcategory Cj as value of Yij if some other element Yik of Yi has the value Ck.
Y i k m a y also be a grammar string, in which case R b m a y exclude a particular option of Yik when Yij has value C..
The restrictions Rb may be classified into three kinds: (a) Between elements of some string Y.
where the Y..
correspond to elements 1 i~ of a linguistic string.
For example, A noun in the sub-category singular cannot appear with a verb in the sub-category plural.
~ The man agree.
Only a certain sub-category of adjective can appear in the sentence adjunct P__AA : in general, (b) in particular, ~ in ha~py.
Between a Yij and a Yik where Yij corresponds to an element of a linguistic For example, string and Yik corresponds to a set of adjuncts of that element.
In rN, the string to V 2 cannot adjoin a noun of sub-categoryN 2 (proper names): the man to do the job ~ John to do the ~ob.
Only a certain adjective sub-category (e.g., re~/.e~, available) can appear in rN without any left or right adjunct of its own: the people present ; (c) ~ the people happy.
Between Yij and Yik ' where one corresponds to an element of a linguistic string and the other corresponds to an adjunct set which can repeat itself, i.e., which allows 2 or more adjuncts on the same linguistic element.
These restrictions enable one to express the ordering among adjuncts in some adjunct sets.
For example, Q (quantifier) and A (adjective) are both in the set N ' the left adjuncts of the noun.
However, _Q can precede A but A cannot precede _Q when both are adjuncts of the same N in a sentence: 3 Q A N books, but ~AQN e.g., five green e.g., green five books.
The string grammar defined by eqs.
i-3, together with the atomic strings (word-category symbols) have the form of a BNF definition.
The system with eq.
4, however, departs from a BNF definition in two important respects : (a) it contains restrictions (tests) on the options of a definition; (b) the atomic strings (word-categories) of the grammar have sub-classifications.
With the elimination of the restrictions, the computer grammar will again have the form of a BNF definition.
fi3. Elimination of the Restrictions The restrictionless string grammar is obtained from the grammar (in described above by the methods of (A) and (B) below.
Initially this paper), conjunctional restrictionless strings have not been included in the grammar.
We estimate that the addition of conjunctions/ grammar by a strings will increase the size of the restrictionless factor of about 5.
(A) The linguistic strings represented in the computer graz~,ar are reformulated in accordance with the following requirement.
any utterance of a language containing Given grammatical dependency obtains between A and B, the elementary strings of a restrictionless string grammar are defined so that A and B appear together in the same linguistic string, and any iterable sequence between A and B is an adjunct of that string.
Iterable sequences of the type seemed to begin to in It seemed to be~in to surprise him that we in It is said to be known worked seriously, or is said to be known to to surprise him that we worked seriuusly are analyzed as adjuncts.
If we place such sequences among the left adjuncts of the verb, v ' then the sentences above can be put in the form It~_v surprise him that we worked seriously fi~v = seemed to begin to ; However, when the adjunct by definition), surprise verb of ~v is said to be known to ; etc.
takes on the value zero (as can all adjuncts, sequence It then (9) above becomes the non-grammatical him that we worked seriously.
~v (seemed ~ This happens because the first and the latter is__) carries the tense morpheme, disappears when We separate the tense morpheme from the verb, and place it in the center string as one of the required elements.
(i0) C1 = Z t ~ V g; This formulation of the assertion center string C1 (lO), in which the tense morpheme is an independent element and iterable sequences are taken as adjuncts, is necessary in ord@r to preserve, for example, the dependence surprises him that we In the between the particle it and the succeeding sequence worked seriously: grammar~which ~ The book surprises him that we worked seriously.
includes restrictions, this formulation is not necessary because this dependence can be checked by a restriction.
(B) Turning to the computer form of the grammar, all the restrictions of the grammar are eliminated either by defining new grammar strings (for the elimination of the restrictions categories by the particular required by the restriction Ra) ' or by replacing the general wordsubclasses of those categories which are (to eliminate Rb).
The application of this procedure increases the number of strings in the grammar, of course.
The restrictions R a can be eliminated in the following manner.
Suppose the option Yi of Y has a restriction R a on it which prevents it from being chosen in Y' (Y is a Y'ij of Y').
Then define a new grammar string Y ' w h i c h ficontains all the options of Y but Y.
: Then the new gran~nar string Y* replaces Y in Y'.
R a on p.
5, the string Y* = which Z t fv V / ....
Thus, in the example of (in the modified treatment of tense and iterable sequences) would replace Y in r N.
The restrictions R b are eliminated in a different way, according to the types described on p.
6. (a) New strings must be written in which only the wellformed sequences In the example of subject-verb agreement, the of subcategories appear.
where N s and Np are singular and plural nouns, V s and Vp singular and plural verbs.
(b) If an element of a particular subcategory, say Ai, can take only a rAi is defined.
It subset of the adjuncts rA, then a new adjunct s~ring contains those options~_ of rA which can appear only with A i plus all the options of r A which are common to all the sub-categ0ries of A.
When this to rA, : has been done f0r  all A i having some particular behavior w i t h r e s p e c t all the remaining sub-categories A rA ~ AlrA1 of A will have a common adjunct string r a As many new sets rAi must be defined as there were special sub-categories A.
A similar argument holds for ~A and other adjunct sets which depend on A.
A new element corresponding to the/adjunct set must be defined in which the adjuncts appear correctly ordered with respect to each other, and each one must be able to take on the value zero.
This procedure for eliminating restrictions is also the algorithm for introducing further grammatical refinements into the restrictionless grammar.
Such a general procedure can be formulated because of an essential property of a string grammar: In terms of linguistic (elementary) strings, all a) between elements of a string, or b) between an restrictions are either element of the string and its adjunct, or same string.
c) between related adjuncts of the Further, there is no problem with discontinuous elements in a all elements which depend in some way on each other grammaticstring grammar: ally appear in the same string or in strings which are contiguous by adjunction.
The cost of the elimination of all restrictions in this way is about an order of magnitude increase in the number of strings of the grammar.
Instead of about 200 strings of the computer grammar, the grammar presented here has about 2000 strings.
It is interesting that the increase in the size of the This suggests that in a program Also, since grammar is not greater than roughly one order of magnitude.
there may be practical applications for such a grammar, e.g. designed to carry out all analyses of a sentence in real time.
the restrictionless grammar is equivalent to a B.N.F. grammar of English, it may prove useful in adding English-language features to programming languages which are written in B.N.F. fiSENTENCE N E U H I B  SUCE ENHANCED SPIKES WOULD BE MORE E F F E C T I V E IN I N I T I A T I N G SYNAPTIC ACTION AND THUS BE RESPONSIBLE FOR THE OBSERVED POST-TETANIC POTENTIATION  Ol I.
PARSE SENTENCE INTRODUCER CENTER AND Z, AND CI ASSERTION SUBJECT 5 . SPIKES VERB $ OBJECT gOULD BE 6.
EFFECTIVE RV T, ACVERB ADVERB THUS CONJUNCTION LN ARTICLE QUANTIFIER SUCH ADJECTIVE ENHANCED TYPE-NS" NOUN AEVERB PN PREPOSITION IN ACTION lO.
CI ASSERTION BE OBJECT RESPONSIBLE II.
LN ARTICLE QUANTIFIER ADJECTIVE INITIATING TYPE-MS SYNAPTIC NOUN PN POTENTIATION LN GUANTIFIER ADJECTIVE OBSERVED P O S T T E T A N I C TYPE-NS NOUN SENIENCE NEUH-.IB  SUCH ENHANCED SPIKES kOULD BE MORE E F F E C T I V E IN I N I | i A T I N G S Y N A P T I C A C T I O N AND THUS UE R E S P O N S I B L E FOR THE OBSERVED P O S T T E T A N I C P O T E N T I A T I O N  02 = |NTROOUCER CENTER AND Z.
AND 3 6 END MARK  PARSE SENTENCE CI VERB  kOULD BE RV T, ACVERB S ADVERB IHUS LN ARTICLE QUANTIFIER SUCH ADJECTIVE ENHANCED TYPE-NS NOUN lCVERB To P NS V I N G I O F | 0 = PREPOSITION IN SN INIIIATING ACTION CI ASSERTION VERB (WOULD) OBJECT RESPONSIBLE LN QUANTIFIER TYPE-NS NOUN PN = LP P R E P O S I T I C N FOR POTENTIATION QUANTIFIER ADJECTIVE OBSERVEO P O S T T E T A N I C TYPE-NS NOUN NG MCRE PARSES Conclusion 4.
This problem was suggested by Professor J.
Schwartz of the Courant institute of Mathematical Sciences, New York University.
5. The option Yi here corresponds to the linguistic string Y of the previous section.
The symbol / separates the options of a string definition.
Academic Press, REFERENCES 1.
Harris, Z.
S., . String Analysis of Sentence Structure.
Papers on Formal Linguistics, No.
l, Mouton and Co., The Hague, 1962.
2. Sager, N., Salkoff, M., Morris, J., and Raze, C., . Report on the String Analysis Programs.
Department of Linguistics, University of Pennsylvania, March 1966.
3. Sager, N., . Syntactic Analysis of Natural Language.
Advances in Computers (Alt, F.
and Rubinoff, M., eds.), vol.
8, pp.
153-188. New York, 1967 .
CONTEXTUAL GRAMMARS Solomon Marcus InsUtutul de Matematica Str, Mihal Eminescuo 47 BuchareSt 9, ROMANIA In the following, we shall introduce a type of generative grammars, called contextual grammars.
They are not comparable with regular grammarsBut every language generated by a contextual grammar is a context-ree language.
Generalized contextual grammars are introduced, which may generate non-cox,text-free languages.
Let V be a finite non-void set ; V lary.
Every finite sequence of elements in ia called a vocnbuV is said to be a string on V.
Given a string x = ala2...an, the number n is called the length of x.
The string of length zero is called the n tring and is denoted by r~J. Any set of strings on V is called a language on V.
The set of all strings on V (the null-string inclusively)is called the universal language on V.
By a nwe denote the string a...a, where a is iterated n times.
Any ordered pair (u,v~ of strings on V_ is said to be a contex~ on V.
The string x is admitted by the context <u,v> With respect to the language L if u~ G L.
Let .~ be a finite set of strings on the vocabulary V~ and let@be a finite se@ of contexts on V.
The triple (v,~, ~)) (1) is said to be a contextual l~rammar ; V is the vocabulary of the grammar, ~ is the ba_s_e_ of the grammar and ~is the co m~,-2textual ccmoonent of the grammar.
Let us denote by ~ the contextual grammar defined by (1).
Oonsider the smallest language L on Vj fulfilling the following two conditiom8 (~J Iz ~ and <u,v>,(~), th-~=,L.
The language L is said to be the lsmguage generated by the contextual grammar G.
This means that the language generated by G is the intersection of all languages L fulfilling the conai~ions (~) and (pj . A language ~L is said to be a eonteF~ual language if there exists a contextual grammar G which generates L.
Proposition i.
Eyer~ finite language is a cont~ual lanProo__f.
Let V be a vocabulary and let ~ be a finite lan.
guage on V.
It is obvious that the contextual grammar (V,L4jO), where 9 dauotes the void set of contexts, gauerates the language L I.
The same language may be gamerated by means of the .g  contextual grammar (V,I~, where is formed by the nu~ cont ex~ only.
Two contextual grammars are called e~uivalemt if they gemssame language.
The grammars CV,LI, O ) and (V,~,~ rate the are equivalent, since they both generate the language ~ The converse of Proposition 1 is not true.
Indeed, we have Proposition 2.
The universal language is a contextual language.
ProOf. Let V = ~alLa2,...~ ~.
De~ote by I~ the umiver.
Sal language on V.'T.et us put ~" S~.~.~ and t <~''i" " C -3~,a~,,...s(~,ian> ~ It is easy to see that thegrammar . (V,~ generates the universal language on V.
Remarks. If we put, in the proof Of Proposition 2, LI-V instead of h =~' then the grammar (V_,h,@) does not generate the universal language on V, since the language it generates dDes not contain the nu/l-strlng.
In order to illustrate the activity of the grammar (V,~, ~defined in the of let consider the proof proposition 2, US particular case when the vocabulary iS formed by two elements only : V =(a.b~.
The general form of a string x on V is x = a ~b ~a-~b~...a ~b~, where il, Jl, i2,j2,...,~,j ~ are arbitra~non-negative integers.
In order to generate the string x, we start with the null.string @@ and we apply il times the context ~,a~ . The result of this operation is the string a 11,to which we apply Jl times the context <~,b> and obtain the string al~ ~I . Now we apply i~ times the context <~,a>, than J2 ti~es the context ~,b_> and we continue so alternatively.
~hen, ~dter 2p-2 steps, we have obtained the J = ai bJlai b 2ooo LP" ib -i, it is ply .~ ti~es ~ne context ~@,~ and, to the string so obtained, jp times ~:e contex~ ~gb>, in order to generate completely Uhe string Xo Haskell Curry considered Ghe larlg~age L = {abn~ (n=l,2~..o) as a model of ~he set of natural numbers \[5~ o We call L the language of Curry.
Prooosiuion 3.
The language of Curry is a contextual langu~eo proof.
The considered language is generated by the grammar (V,LI,~), where V= ~a.b~, I~ =~a 3 -nd~ ~<~,b~.
We recall that a language is Said to be regular if it may be generated by means of a finite aatoma$on (or, equivalently, by means of a finite state grammar in the sense of Ohomsky).
Proposition 4.
There exis$~ a contextual language which is Proof.
Let us consider the language L = ~a-nb n} (n=l,2,)...
If we put V = {ab}, L 1 = ~ab~ and ~ ~<a.b>}, then it is easy to see that L is generated by the con~extuai grammar (V, LI, ~.
On the other hand, L is n~t a regular language.
This fact was assel~ed by Ghomsky in \[3~ and\[~\], but the proof he gives is wrong.
A correct proof of this assertion and a.discussion of Chomsky' s proof were given in \[~\].
and ~.
Propositions 2,3 and # show that there are many infinite languages w~ioh are oontextual.
This fact may be explained by means of P~posi~ion 5.
If the set ~ is non-void and if the set~ contains at least one non-nu/1 contex~ I ~hen the contextual gramma___r_r (V.Ll, ~ ~enerate s an infinite language.
Proof. Since L A is non-void, we may find a string x be@ longing to ~i o Since contains, at least one non-nu/\] context, @  let ~u,v~ be a non-null context belonging to . l~rom these assumptions, we infer that the strings, u2xv 2, . .,un~,,..
are mutually distinc~ and belong all to the language generated by the grammar (V,I~,).
Thus, ~his language is infinite.
The converse of Proposition 5 is true.
Indeed, we have / / -5Proposition 6.
If the contextual 6rammar (V, LI~ gau~ rates an infillite language, then ~Ll.
is non-void, whereas@ c0nrains a no~-nult context.
proof. Let L be the language generated by ~V)LI~.
If is void, L is void too, hence it cannot be infinite.
If contains no non-null context, we have L = L I.
But ~d is in any ease flni~e ; ~hus, L is finite, in contradictiom With the h vpothesis.
Since there are contextual language which are not regular (see Proposition 4 above), it would be interesting to establish whether all contextual languages are context-free ls~guages.
The amswer is affirmative : Proposition 7.
_Every contextua~ lan?.ua~e is a context-free PrP~oof.
Let b be a contextual language.
If L is finite, it is a regular language.
But i~ is well knowm that every regular language is a context-free language.
Therefore, L is a context-free language.
Nowe let us suppose that L is infinite.
Deao~e by G = (V,,L l, a contextual grammar which generates the language L, In view of Proposition 6, L I is non-void,wheream there exists an integer i, l~ i~p, such that the con~ext ~ui,vi~ is non-nu~ Joe.
at least one of the equalities ui =co, v i =~ is false.
Let us make a choice a~d suppose tha@ ~.i ~ ~ Let L~ = {xcA,xp_, ...
9~a} and (~) ={<ultVl>, ~.,U,.~,V."y} . We define a context-free grammar ~)..@| as follows.
The terminal vocabulary of ~ is V.
The nonterminal vocabulary of ~ contains one element onlydenoted by S which is, of course, the axiom of the grammar ~ . The ter-6minal ~ rul es o f ~' are, S -->_x 1, g--~x a, S.--* _Xn whereas uhe non-terminal rules are S ---> u~5% v-i ' S ---) uqS v~, It is obvious tha~ the number of terminal rules is equal to the number of strings in ~, whereas the number of non-terminal rules is precisely the number of conuexts in ~.
Among the nonterminal rules, there is one at least which is non-trivial : it is the rule S ---> UiS vv~., where '*4 {~ " It is not difficult to nrove that the grammar ~ generates the given language L.
Indeed, the general form of a string in L__ is where yG V and <~i, V~ >E~ for s = 1,2,...,p.
In order to generate the considered string we begin by applying --Jl $imes She rule In this way, we obtain the expression h The next step consists in applying J~2 times the rule v -7-a ~,~2 s vv~.~., which yields the expression J~ Ja -Ja -J~ s Ha N 1 ))t~..., -7! l Oontinuing in this way, we arrive, after pression,~z "ia ~-i s J~-\].
J2 ~a us-1 ~,a .... .'$1 "-%'i "'" ~12 ".% " Vie now apply j.p times the rule and thus we obtain the expression p-1 steps, to the ex--Jp.~ .~!2-1 . Ja ':ll ..J.l.,.i2. u jp-1 ~ S "Ull '~i2 ....
~-l . V~p ~-l "" vi2-V-il where, by applying the terminal rule S---@ Z, the considered string is completely generated, Thus, we have proved that L is contained in the language generated by ~, Conversely, let z be a string generated by ~ . The general form of this generation involves sev(ral consecutive applications of non-terminal rules (the number of these applications may be eventually equal to zero) followed by one and only one application of a terminal rule.
It is easy to see that the result of this generation  is always a string of the form (2).
Thus we have proved that the language generated by ~ is contaiued in L, In view of the precedim~ eonsiderations, L iS precisely the language generated by ~ o Proposition 7 easily permits to obtain simple examples of .....
J -8languages which are not contextual l~guages.
For instance, ~he l~~uage of Kleene~an~ (m=~,2,...), the first example of an infinite language which is not regular, is a very simple example of ~ contextual language.
It is enough to remark that the sequence ~n2} (~ = 1,2,)... contains nO subsequaucewhioh is an infinite ari~hmebio progression ~ (We have (n+l)2-n22~+l and lira (~+i)=~, therefore for every subsequence of ~n2} the difi'erance of two consecutive terms has the limiu equal to +oo wh~ n-@ ~ ).
But a result of\[4\] asserts, among others, that given am infinite contex~-f1~ee lan~ guage L, the set of integers which represent the len~hs of the strings in L contains an infinite arithmetic progression.
It follows uhab b~Je language of Kleene is not context-free and, in view of Prooosition 7, it is not a conbex~ual language.
T~ sa-~ ~a~T ~@low* ~,~ ~h~-,~ :3.A,~.
~ \[g'J, ~,,#.
A natural question now arrises : Do there exist non-contextual languages a~ong context-free languages 7 The affirmative answer follows fro~ the following remark : The converse of Proposition 7 is not true.
Indeed, we have Prooositiou 8.
There exists a.
cont e.~-free language which is not a contextual language.
Proof. Let V = ~a,b~.
In view of a theorem of Gru~Lkl ~ ~__.-----~there exists, for every positive integer _n~ a context-free language I~ on V, such that every context-free grammar of I~ contains at least n non-terminal symbols.
But, as we can see in the proof of Proposition 7, every contextual language may be generated with a context-free grammar containing only one non-~erminal symbol.
Therefore, if _n ~ 2, ~ is not a contex~usl l~guage.
Proposition 8 suggests the natural question whe~bsr ~bere exist regular languages which are not contextual lan~ages.
The I -9answer is affirmative : Pronosition 9There exists a regular language which is not a context ual language.
Proof~ Let us consider the laugaage L = {abm-~c~a.~ n,) ~,n= =1,2,...), which was used b~ H.B.Curry \[5\], in order to descrlbe the set of mathematical (true or not) propositions.
This language is regular, since it can be generated by the rules S--> Abj Ac->Ab, A.--~ Ba, B--> CCC., G_--~ ~, C--~ Db, .D_--~ a, We shall show that .L is not a contextual language, Tndeed, let us admit that the contrary holds and let G = <V,~, ~> be a contextual grammar of L_ 2 Here, the gene_.-al form of a string in L is ~ ""-us ~I x~ -~ ...
vi= (3), wh eas (t = 1,2,...,=) where "'',Pn are arbitrary positive integers.
This means that ul,~2,... .---,_Un, Vl,Y2,-..,v ~ in the expression (3) are formed only by those elements of V whnse number of occurences in the strings of L is unlimited.
Only h satisfies this requirement.
It follows that in any string of .L both occurrences of sand the occurence of ~ are terms of the string x in (3).
But this implies that the intermediate terms between the occurrences of a are terms of x, hence we can find two strings y and, m l The string y is obvioasly .the null-string ~o the form 11~., hence " z such that,whereas z is of But m may be here an arbitrary positive integer.
Therefore, since -loX6~, it follows that ~ is an infinite se~ of mtrimgs.
This fact contradicts the assus~tion concern_tug G ! v, is ~t a contextual language and Proposition 9 is proved.
The contextual grammars may be generalized in order to generate some lauguages which are not context-free.
A generalized contextual ~r~mmar is a quadruple G =~,,L2, ~, where V, L I and ~have the same meal~g as in bhe definition of a contextual grammar, whereas J'2 is a finite set of strings on the vocabulary V.
We define the language L G generabed by G in the following way : Y~ is a language on V a~d xe~ if and only if we may e~press x in the form .where z~, y~Le, <ui,Yi>~for i : 1,2,...,n and pl,P2,...,pn, p are positive integers such that pl+P2..,~n=p. Every language generated by a generalized contextual grammar is said to be a generalized contextual lsnguage.
I~, in the delini~ion of G, we take L~ =~c~}, G is equivalent ~o a contextual grammar ! the lang,.% is then precisely the language generated by the contextual grammar ~V,LI~.In_ deed, the general form of a string in the contextual language generated by ~Y~LA, ~ is l P~a Pn Pa P2 Pl p roy ed ~roposition lo.
\]~ery oontex~ual language ~s a ~eneralized context ual lan~uaKe, llWe may consider a conte~ual grammar as a parbicular case of generalized contextual grammar, .by ideatifyimg the contextual grammar ~'~1~ with the generalized contextual grammam~,V,,~, " It is interesting to point out that somet~imes a cont~ual language may be easy generated by a generalized contextual grammar which is not a contextual grammar.
For instance, let.
us consider the l~.~e L= (~=} (~X,2,)...
. ~ ~is, or the proof of ProDosition A, L is a contextual language.
We map generate L by the generalized contextual grammar (which is not a co=textual ~r~a~) <v, h~>, where v : {~,b}, .Li_\[c~}, ~ = Ibm, ~= \[a,~ . It is known that,~_ is not reS~L%ag.
We ma~ give a similar example, wi~h a language which is regular.
In this respect let us consider the language of G~x~V~.~.
In view of Proposition 5, it is a contextual language.
It is a regular language too~ since it may be generated by the regular gramm~r contain~ ~he following two rules : q--~ Sb and S--> a.
Now let us consider the generalized contextual grammar < ~i' This grammar generates the language of Curlew, but if'is not a cent ext,~ al gran~nar.
~ow let us show that generalized contextual languages are an effective generalization of contextual languages.
Propo .sit ion ii.
Th ere _ exist s a_g en=e=~ iaed_gA~nt ext ua! language which is ~IQ~ a eon~ext~sl language, ~, Let us consider the language T, = an_b.n~..
n} (n:=-l,2,.
4 It
is known that this language is not context-free (see,or instance,66\],p.~).
7n view of Proposition 7, every contextual 12language is a context-free language ; hence~ ~ is not a con.
textual language.
Now let us consider the generalized contextual gr~m~ G = <V,~,~2,~>, .here v = ~,~, ~ ~,,~{~ and~ ~(~a>~ . It is easy to see that G generate the ieaguage L.
Yrom the proof of Proposition ll it follows immediately; Proposition 12.
There exists a ~eneralized contextual lang u_~e which is not a ~.nnteYt-f~ee language.
We may now ask whether the converse of Proposition 12 is true.
The answer is given by Proposition 13.
Th ere exists a cont ext-free~a~e~ even a regular language,~ which is.,not a generalized contextual !~ua~e.-~ P#oof.
We may consider the language L = ~sbmc_abn-} (~,n= =1,2,)... used in the proof of Proposition 9.
It was showed in the proof of Proposition 9 that L is regular.
Let us admit that ~ is a generalized contextual language.
Given a string x in L, its representation is of the form Pl P2 Pn P P~ P2Pl ~m ~ : ui u~ ....~n..~.
y'v". ...
v2v i where ~ui,vi~ ~ (i = 1 .....,n),ZG~, y~L2,pl+...+pn = p end G = ~V, L1,L~, ~ is the grsmmar of L.
By a reasoning similar to "that used in the proof of Proposition 9, we find that for every positive integer m there exists a string z in \]i I such that z = abmcab s~, where s is a non.negative integer, depending mf m.
But thls means that ~ eontain~ infinitely ma~ strips.
This fact con... tradicts the definition of a generalized contextu~ grammar.
It / L 13 follows that L is not a generalized contextual language, It is to be expected ~hat every generalized contextual language is a contex~-s~itive language.
But the construction of the corresponding context-sensitive grammar seems to be very complicated, if we thin~ to the generation of the language ~u.A.~reider has introduced a new type of grammars, called gralamatlkl) and defined i~ neighborhood ~ira~.L~ars (okrestnostnye ' the following way (\[4o); see ~4\].
Our presentation is some what different).
Given a finite set V called vocabulary, two strings x and y on V, and a context <u,v> on V, We say that the pair ~u.v>,y) is a neighborhood of y with respect to x if we can find two strings z and w, such that x=zu~vw.
Every pair of the i or~ ~<u,v>, ~\], where ~u,v> is a context on ~, Whereas y is a string on V, is called a neighborhood on V.
Let us consider an element e which does not belong to V ; G will be called the bo~3dary element.
A neighborhood grammar is a triple of the form ~ V, e,~, where V is a vocabulary, is the boundary element and ~is a finite set of neighborhoods on the vocabulary VU(e} . Let L be a l~aguage on V.
2e say that L is generated by the considered neighborhood grammar if ~i every string x of the form x =~ye (with ymL).and only in such strings there ~ists in ~, far every tera a i of X=~la2...a s, a neighborhood of a i with respect to x.
Neighborhood gray, mrs are closely related to the notion of context, since this notion occurs in the definition of a neighborhood.
There is another notion, due to Ja.p.L.Vasilevski~ and 14 ~.V.Ghom~ak6v (see ~he refermnce in~2\],p,~o), which e~lains this fact.
Following these authors, a grammar of contexts (this name is imp_roper, since no context occurs among its objects) is a triple <V, e,9>, where .V and @ have the s-me meaning as in the definition of a neighborhood grammar, whereas Q is a finite set of strings on the vocabulary Vt3e~  This grammar generates the language _L on V in the following way : x6 if and only if for every string y and a~y strings z and w for which there exist strings u and v such that @ x@ = = uzyuv we have either l) y = rasp, where sE Q, whereas the strings m and p may be eventually or 2) (~x@ = urynt, where qr = z, n t = w mad ryn is a string belong~g to Q.
A string belonging to Q is said to be closed from t~ le~ (from the right) if its first (last) term is @ . A string belonging to Q is said to be ~ if it is closed bosh from the left and f~m the right.
A grammar of contexts is said ~o be k-bounded if every non-closed string of _~ is of length _k, whereas every Clesed string of ~ is of length not greater than _kj An important theorem of Bor~ev asserts the equivalance between languages generated by neighborhood grammars and languages generated by k-bounded grammars of con~s (~3,p.4o).
Since grammars of contexts and contextual grammars have some similarities in their definitions, it is Interesting to establish more ~xac~ly the relation b~een them.
v 15 Proposition 14.
There exists a contextual language ~hioh is regular, but which is not a neighborhood language.
Proof. Let us consider the language L = ~a~n~ (n=l,2,...).
This language is regular, since it is generated by the regular grammar consisting in the rules S ~ ~a, T--->Ua, U--->Ta, --->a, where ~ is the start symbol, La~ is the terminal vocabulary, whereas {S,T,U} is the non-terlainal vocabulary.
Let us consider the contextual gramnu~r G =~ {a},{CO}, {~a,a>~.
I@ is easy to see that G generates the language ~ $ therefore L is a contextual language.
We shall show that L is not a neighborhood language.
In this respect, our method will be the following.
We shall consider all systems of possible neighborhoods of the terms of ~he string 0aae and we shall show t~}at every such sysbem is either a system of aeighborhoods of the ~erms of every string Cane (n= = 2,3,4,)... or it is not a system of nei@\]borhoods of the terms 0t the string ea@e . It is easy to see that the first ~erm of the string @aa@ admits ~he following neighborhoods : 1)e, 2) Ca, 3) @aa, ~) eaa~ . The second term has the neighborhooas : l) G_a,~)-a~) aa, 4) ~e ., 5) e_a_a, 6) e_~ae.
The neighborhoods of the third term are : i) e@a, . 2) aa, ~) a, ~) _ae, 5) eaa8, 6)_aaE) . The lass term has the neighborhoods 1) 8, 2) _a~_, 3) a a@_, 4) @aa~ . The noration _u_xv.
represents hier the neighborh~d {<u,v>,x} . It is easy to see that the fourth neighborhood of the firs~ and of the lass term c~t b'e a neighborhood of e with respect @o @g48 . On She other hand, a is a neighborhood of .aa with respect to ea~@ for every n = 1,2, ....
It follows that no 16 neighborhood grammar of L = ~a2ZX 3 may contain one of She neigh.
borhoods _0a2@, Q a2~ and a.
Thus, if a neighborhood grammar of \]~ exists, it contains at leas~ one neighborhood from every group of the following four groups of neighborhoods : ~) _0, _~a, _ea 2 . ~) e_a, _aa, _aae, G~a, ~.aaO.
b') 6~-~, aa, ..aO, ea_a~, agO.
We shall consider all possible combinations betweau a neighborhood of the group ~ and a neighborhood of the group E . By mn we shall denote the combination formed by the m.th neighborhood of ~ and the n-th neighborhood of ~ . It is easy to see Chat every neighborhood grammar containing one of the combinations 12, 22, 23, 25, 42 generates a language whioh eontain~ every string a n with n $ 2.
On the other hand~ every neighborhood grammar containing one of the combinations ll, 13, 14, 15, 21, 24, 31, 32, 33, ~, 35, 41, 43, 44, 45, 51, 52, 53, 5~, 55 generates a language which either does not contain the string a 4 or contains every string a n with n~ 2  (This depends on the fact if the neighborhoods aa or aa belong or not to the considered neighborhood grammar).
Thus, there exists no neighborhood grammar which generates the language ~2n 3.
But the definition of (generalized) contextual grammars, though adequate to the investigation of the generative power of purely contextual operations, does not correspond to ~he situation existing in real (natural or artificial) l~guages, where every string is admired only by some contexts and every o~u~ / 17admits only some strings.
Let us try to obtain a type of grammar corresponding to this more complex situation.
We define a con___y textual grammar with choice as a system G_ =<V,L,~,~o>, where V, L1 and~are the objects of a contextual grammar, whereas is a mappi~ defined on the universal language on V and havi~ the values in the set of subsets of~.
We define the language generated by G as the smallest language L having the follow1  ~ L l x ~ L 2  ing properties : If x, ! If ye L, <u,y>6 ~(y) and Z&~l, then u~L, z v~L and ~L.
Thus, every strin~ chooses some contexts and every context chooses some strings.
We define a contextual language with choice a language which is generated by a contextual grammar wit~oioe.
The investigation of these grammars and languages would better show the generative power of contextual operations, in a manner which corresponds to the situation existing in real languages.
--~$ References i.
Y.BAR-HILLEL, ~I.PEEL~, E.SHA~R : On formal properties of simole phrase structure grammars.
Zei~schrift fur Phonetik, Sorachwissensc~aft und ~ommunikatio~forechung,vol.14,1961, p.145-172. 2.
V.B.BOP~EV : O krestnostn~e gram~atiki.
Nau~o-Techni~eskaja Informacija, Serija 2, 1967, ~o.ll, p.39-41. 3.
N.GHO~SKY : Three models for the description of language.IRE Transactions on Information Theory, IT-2, 3,1956, p.ll~-12@. 4.
N.~{O~KY : Syntactic Structures ~ Gravenhage,1957.
5. H.CURRY : Some logical aspects of ~ra~matical structure, proeeedings of the Symposium in Applied ~athematics, vol.12, S~ructure of language and its mathematical aspects, Amer~ath.
~oc., 1961, p.56-68. 6.
S.GI~SBURG : The mathe~atical ~heo~f context-free languages.
~cGraw-Hill Book Company, New York, 1966.
7. J~XA : On a classification of context-fre~lauguages.
Kybernetika, vol.3, me.l, 1967, p.22-29. 8.
S.~CUS : Gramatici ~i automate finite.
Editura Academiei R.P.R., Bucure~ti,196@.
9. S.~AR~JS : Su~_ les grammaires & un hombre d' ~tats fini.
Ca. hiers de lin~uistique th@orique et appliqu~%vole~,196~,p,~W~-~&@.
io, Ju,A.~REIDER : 0krestnos~naja model jazyka, Tru~ 8impoziuma pO primenenijam poro~.daju~ioh grammatik, Tar~u,septjabr~1967.
ii. Ju.A.~REIDER : Topologi~eskie mod~ll dazvka.
Vsesojuzz~yi stitu$ nau~noi i techni~eskoi informacii, ~oscou,1968 .,., , SERGE BOISVERT ANDI~ DUGAS DENISE BI'LANGER OBLING: A TESTER.
FOIL TR`ANSFORMATIONAL' GKAMMAKS ~.
INTRODUCTION Transformational grammars have developed with recent research in linguistics.
They appear to be a powerful and explicit device for characterizing the description of sentences; they also meet conditions of adequacy that can be applied to check that a sentence, or a set of rules, is well-formed.
A transformational grammar tester is part of a strategy for the selection of a well-formed grammar matching the data base.
To put it more explicitly, a tester of this sort should provide the linguist a class of the possible grammars which concerns precisely the linguistic theory.
These grammars have the form given by CUOMSKY in Aspects (see bibliograIShy).
2. GENERAL DESCRIPTION OF THE SYSTEM O~UNO is a .program for dealing with the structures of the French language: it performs the verification of phrase structure rules, the derivation of sentences according to the transformational component and the graphic illustration of the intermediate or final structures.
In the program, UNG is the routine that controls all the subroutines and the matching of the input structures with those allowed by the phrase structure rules.
If the matching is impossible, a comment is Acknowledgments.
This work was supported in part by Canada Council of Arts grants @69-0404 and @71-0819.
We are also indebted to the staff of the Computer Center of the Universit4 du Qu4bec ~ Montr4al for providing computing facilities, and giving this project high priority.
David Sankoff, of the Universit4 de Montr6al, is also responsible for the first version of the tree editing program, l~inaUy, Jossdyne G4rard helped debugging linguistic rules.
122 SERGE BOISVERTANDIL~ DUGASDENISE BI~,/ANGER ieeIIIItiIeletlIillllllelIlIlIllll~ : o :.
-*:: Iieeee~ ...........................~> ...........................9= ~z *,0,~o : .....
~Z ...................
~g o D~O o aO OUd 3N1 z o..~ ~ 3z O.
~ Z o k9 bO.
~3J ATd o:r =:~ o 0~3 qD_~  :: u.
0. tk NNN IOH U~ ~ Uj~ (~ ~ZD~-Z NUd ~dO NWZDU~ Fig.
1. Tree for an input sentence OBLING: A TESTER FOR TRANSFORMATIONAL GRAMMARS 123 made.
Otherwise, the output gives the graphic illustration of the tree for this sentence, or the input structures are immediately processed using transformational rules.
For example, General transformational rules are operated by a number of subroutines of which the main are explained hereafter.
3. GENERAL CAPACITIES OF THE SYSTEM The system OBLING is divided into four parts: a main program LING, the service subroutines, the phrase structure grammar tester and the transformational grammar testers.
LING and the service subroutines are stored in the central memory while the two grammars testers operate on disks.
The main program invokes the various linguistic rules and controls the application of these rules to the data base structure(s) or the derived structure(s).
The service subroutines are called by the routines concerning the application of the transformational rules and work in parallel with LING during the processing.
Phrase structure grammar tester " LING Service I Subroutines (processing memories) 4--1~ ' grammar testers Fig.
2. The OBLING system 4.
SPECIFIC CAPABILITIES OF THE PROGRAM LING The program LING will first initialize the working areas.
Then, it loads and operates the program V~rlCATEU~ which, after the reading and the verification of the input data, returns control to LINe.
124 SERGE BOISVERTANDI~ DUGASDENISE BELANGER ZING will then load and execute, using an overlay technique, the small control programs cYcH Q1, CYCLI Q2 ....., cYcLI Qi.
Each of these handles, in conjunction with HNG, the mapping on the input structure of a fixed number of transformation rules.
In the current version of the program, cYctI Q1 deals with the linguistic transformational rules T1 to Ts included, cYcrI Q2 the rules To to T10 included, etc.
The total number of these control programs cYcrI Q depends on the memory space allowed; processing is most efficient if the number of these control programs is as small as possible.
5. INFORMATION PATTERN BETWEEN LING AND VERIFICATEUR When VERI~CATEUR (the phrase structure grammar tester) is in memory, the structure to be analysed is read from the standard input unit (punched cards) and is processed by the subroutine Cr~RB~ to LING v V~ICATBD~ c'zcLi qO  T CKARBRE ARBRE verification printing of syntagmatic of the tree rules Fig.
3. The Vm~L~CAWSUa program (see figure 1 for updated tree and structure) OBLING: A TESTER FOR TRANSFORMATIONAL GRAMMARS 125 be validated.
This subroutine first checks if the phrase structure is consistent, then calls up eke which tests the names of the constituents describing the structure; finally, it compares this structure with those allowed by the phrase structure rules.
When errors are discovered during the processing, various sorts of comments are printed and followed if possible by a partial or full tree of the sentence.
When updating is done, the tree is printed and the program VERrFACATEUR passes control to LING.
The following illustrations concern first, the program VERIHCATEUR and second, an example of an updated tree and structure.
6. INFORMATION PATTERN BETWEEN LING AND THE TRANSFORMATIONAL GRAMMAR TESTERS Each time LING receives the control from VERIFICATEUR, that is, when no further errors have been detected, it loops in order to call successively the monitors CYCLI ql ....., CYCLI Q9 which contain up 45 different rules; we suppose that we are working now with a specific version of a grammar.
The first of these monitors has the following structure.
Transformational rule # 1 Transformational rule # 2 Transformational rule # 3 Transformational rule # 4 Transformational rule # 5 Fig.
4. The cYcLI Q1 program When CYCLI Q1 gets control, it is botmd to the application of 7'1,  .., Ts which correspond to the first five transformational rules; then control is switched to LING which calls cYCLI Q2.
The programs CYCLI qn process cyclic rules and the output structure is the input structure for the following rule.
When all the cyclic rules have been applied to the input structure, LING starts over again at CYCLI Q1.
If no modifications 126 SERGE BOISVERTANDR~ DUGASDENISE BI~LANGER to the already processed structures occur, or if new errors are discovered, control returns to LING.
After all the cyclic rules have been applied, the post-cyclic rules are processed in a similar manner: cYcu qA comprises the first five post-cyclic rules CYCLI Q~, the five following, and so on.
This chart illustrates the general interaction between the programs for the processing of cyclic or post-cyclic rules.
I_ cYcI.t Q1 cYcu Q2 CYCI, I Q9 CYCLI QA I CYCLI QB cYC~i Qi I End Fig.
5. Flow of Control between control programs under the direction of LInG 7.
SERVICE SUBROUTINES They are implemented within the main monitor ZING.
All but a few of these subroutines are called during the execution of the routines corresponding to the 88 rules, that is during the phrase structure analysis or the mapping of n structures.
A short description of the main subroutines follows: ^R~ (tree).
This subroutine is responsible for printing the tree.
At the input, we find a vector D of ND elements which represents the tree.
The horizontal distance for printing is calculated along with the total number and the relative position of these nodes; the vertical one is fixed.
OBLING: A TESTER FOR TRANSFORMATIONAL GRAMMARS For example, fACHE~ 2 # NOMRRE DE NOEUDS NOMBRE DE NOEUDS CHA/NE 1 = $ O(.
I) 2 = LE D( 21 3 = N D(3) 4 = V O( ~) 5 = $ D( 5\] 6 = $ D( 6) 7 = PRP D( 7) 8 = OIJE O( 8) g : LE O( 91 I0 = N D(IO) 11 = V O(ll 12 = $ D(12) 13 : DET D(13) 14 : DET 0(14) 15 : GN D(iS) 16 = GV D(16) 17 = C D(17) 18 = GN O(18) 19 = GV D(lg) ~0 = P O(~O) ~I = P D(21 21 TERMINAUX = 2'0 = 13 = 15 = 16 = 20 = 21 = 17 = 17 = 16 = 18 ) = Ig = 21 = 15 = 18 = 20 = 20 = 21 = 21 = -I ) = -I 12 ARBQRESCENCE NON PRODUITE SUR DEMANDE : AUCU~E REGLES IGNORFES SUR DENANDE t AIJCUNES Fig.
6a. Representation of the tree in memory 127 FIW~ f~ 7.1.
OT~ (Remove).
This subroutine is needed when nodes are erased; another subroutine, NEWTgF_~ will erase the nodes.
In the example below, oxv sets D(6), D(7), 9(13) to zero, and NEWTREE erases nodes numbered 6, 7 and 13.
If node 12 was also erased, OT~ and NEWT~E would have erased node 28 automatically.
The same holds for the node 32, where all the nodes between 6 and 13 would have been erased.
7.2. DFER, DFERX, GFER, GFERX.
Except for a few details, these four subroutines do the same work.
For example, Dr~RX \[I, J\] is applied to a structure J that has to be moved to the right under a dominating node L As illustrated below, Dr~Rx \[31, 30\] moves the structure headed by 30 to the right under the node 128 SERGE BOISVERTANDRE DUGASDENISE B~LANGER ....................~.
0":> ~ IJ-: .....
~g ............. 9~ : : ............
~ m : .....
~g ............
~ ~ ........... ..~z ~ : .....
~ ~w ....................~.
~Z~ ZU wz mz~wz ~ ~z~z m Fig.
6b. Corresponding printed tree OBLING: A TESTER FOR TRANSFORMATIONAL GRAMMARS 129 .......,.......,..,.....~........,.....~.~,     ,cu~ W~ud ~ : ".
...........o...............~ ....................~.
=> .~> g~ .............~z ~z ow .....
'.~ : ~J ......
~ : .....
~ ............. 9~  o ....~......,..................,...~> Fig.
7a. Sample tree before OTE and NEwra~ apply ~z~wz ~z~wz wz wz~o ~ w O~d 3NI II O~Do H~N ~ZD~ ION N~d dOD HN  NH" JI  + o=~Do 06d II ~NN ' I0~ O~DO 3~d dO0 ~AV ~I" ~ ~zz~ dN" c~o 130 SERGE BOISVERTANDP~ DUGASDENISE BELANGER,........,......,.......,................~, 8~ U oo.o o~ .oo*o--~ : ............
~ ...........................~ elost$ eset0 t eQ o o  " "  " o   o   "Z      oJo :     oz      .~ : ..... h~o :e ...... go ............
~ o . ~  --~ o.,' .......
., ...........  ....
.> m .....
0 ....  ........,..*0o2 ~z **.~,..,oo, o,,o-,o,~o.,,..,....o....,.oo.,~ ~ q~L0 tn>uz NgCW {gh.w DX~ 02 ~uu 08d JNr a.=~..=oc :~ o=II cz Frld 0~3 u~ .J m snN ION ado 0.
~--u o.
3Z :~d03 ~ 0.
N, u~-,x~..~h.Z NO" ~cxo..~ -~wl     oliN"  ~NI cc~ x'W-Jcc II Bo.
o_ ngd ~D~>O_:E0C GND r-o.J ~ :g uJ o_ HnN : o.
i.-.~ ~ u.
x: |ON H~d,o 3~d ~dO :g :~ ~ 0 dO3  ~ uJ .J~c EZZo.
~zzo--I + O~D W W Z D bJ .J O: _J,Xb.C~O.
Fig. 7b.
Sample tree where oT\]~ and N~WTREB have applied OBLING: A TESTER FOR TRANSFORMATIONAL GRAMMARS 131 31.
(Node 31 was created by rule T2 and DF~R was applied on the resuiting tree) \[I,/\] DFERX \[;r, j\] GF R \[1, J\] G RX \[I, j\] makes node J the next younger brother of node I makes node J the youngest son of node I makes node J the next older brother of node I makes node J the oldest son in node I The general technique for these four subroutines is the following.
Before modification, the tree is copied in another area of memory.
All the terminal nodes identified with the structure J take the place of the terminal nodes identified with the structure L Then, the terminal nodes of I are requested in the copied structure and parsed with their dominating nonterminal nodes at the right place.
Gr~R,permits the new numbering of the sequence and, if necessary, prunes the tree.
In the example illustrated below (Fig.
9a and 9b), Gr~R \[14, 13\] is applied and node CPL (13) has been attached to node 16, the father of node 14.
If GF~RX \[14, 13\] had been specified, node CPL would have been attached directly to node 14, rather than the father of node 14.
7.3. INTERV, This subroutine is used for the permutation of 2 structures.
For example, INT~RV.
\[I, J\] where I = 24 andJ = 28 gives rise to the structural change illustrated below.
7.4. INSERT.
This subroutine is used for the insertion of a new terminal node; for example, INSERT \[4, 1HE, 1HT, 1HR, 9\] introduces node with name ETR which becomes a new son of node 9.
7.5. Other subroutines.
There is a number of other subroutines concerning conditions specified within a rule, such as the presence or absence of a node or of a feature in the whole structure, the logical restrictions of loose equality or strict equality.
132 SERGE BOISVERTANDR~ DUGASDENISE B\]~LAlqGER ..................................S.
.o*~*~oi~*~*~Z ,~,..~~ ~W ~,*...**~  ~W :~.**~ ..................................~.
..................................~ .-~z : ...................
~.~ oi ............ ....~.,.,.,..~.........>   .  ......
                * .z ......
~_~ ~ : ~>~ ~Z~W 0~td O~ ~ g ~ 3NI II 177d N~N IOH H~d =l~d ~ldO  ~wz~w~ o .-INI  o0N33 flTd IOH un bJ z ~ h2 .-J ~ N~d ~o.o_ Od0 4" c3~ a~ zza-~x :~zo~co dO3 ~..~-a:Q.~ .-e~   j   ...aAV m~w~2w OFig.
8a. Sample tree before GFER applied OBLING~ A TESTER FOR TRANSFORMATIONAL GRAMMARS .........................................~.
d8 ......
~ *~ ...... g~> ............
"~> : .............
~z ......
~g  :.,.,.~..,. o,~ .~ : --z      m(  o      ~o ; .,-w .     . ,   .   ,.j ....................~,~: .~ ............
~.~ ......................0...........~.
.,..,,~.,~,,,,~,~,..,,,~...,..~ ......
~g ~  ,~ 0 , ., , .,., m m ,,,,,,,,,,,,,~ 133 =>~ ~z ~z~z ~z~z~o rOBd  4NI,o II H3.-I + HAN oz~o lOW 3~d ~ 0.
o o.
(~ < P..~ o(J =0 z dOO . ~IAV 30" eU NO" HN ~ * 3NI o~x~o II 0~ Ld z ~ W,J ~' .J  :~ h.
(l. ~ 1~3_4 fVld ~AN ION O~Z~O H~d hJ Z ~ ~J.Ja: ~"~  ~ h.
a.o. 3~d ~dO dO3,O ~AV 30"+ o~-~o NO'* .-I  ~ u.
0. 0.
HN" NH" Fig.
8b. Sample tree where Grsa has applied 134 SERGE BOISVERTANDRI~ DUGASDENISE B~LANGER .....................,,......,..........9~ :...................9~  ~ ~ m ~Z o O~ld JNI II nqd c k~RN e... ozc,~,~.~ zw~g 3aa ~dO 0=~ dO3 . ~ t,.
cL cL ~AV 30" Q.
t-.(.,.~ O.
~2 --)-Z NO" O.
C30. J ~ v.J 0 J ~ ~ HN* NH" ~O_ N  U..~ .:~oAI" e3 oz~ ha~ zh4 ha Fig.
9a. Input structure OBLING: A TESTER FOR TRANSFORMATIONAL GRAMMARS 135 ~i.
i le~eee ee$ lse eee eeeee~ e,e,eee,ee,,,~,,,le,e,~Z ~z ...........-.~z II  -g .......
~ ~ ......
~ , ~ ,1 ,, ,..
 ~ ~, , .,   ,.
1~ ozJ~z 2 08d II ~z~ W33 ~z~wz ~Id WnN 8~d 3~d HdO dO3~AV 30  NO" Fig.
9b. Output structure after cFm~ has applied 136 SERGE BOISVERTANDI~ DUGASDENISE BIILANGER ...................,.....................~, Ii J .~ : ....
~ ....... o~  ~~ ...........
~ ~ : ......
~  ~: .....
~ : .~ ~  ...
.,~.0,~... mz ~,~,,,,,,',,~,~,.,,,,,,,.~ Fig.
lOa. Input structure O~d 4N\] TT nqd HnN zoH HHd * dOC) 0~:~0 ~IAV NO" _~I.
'-)~_ N  b.~o:~ no" O~d JNI II N3J Flqd HnN IOH ~d 3~d ~dO OBLING: A TESTER POR TRANSFORMATIONAL GRAMMARS @ @ i@ Ii @@I@ @@@ @Ii@ I@ i oII iI @ iI i@I @Ii@ kU II~leo~$e~Ioo$oi@$~ ...... g ............. o,.~ ......
~g .....
~ ............
R~ ~d I M! ~ed .,,....,.,,..,,,~z o~ ...~..No .,.,~.,.....,......,,.
..................................~>,~..,,.~z . "-~ "''   "  "''''  "" "" " "" "  ''~ .....,..,,..,...........,...,.-,, .... ..~ 137 ~ ~z~o  m2~Nz m + O~DO  m oz~w zww~ 08d ..4NI II F mqd 083 k(nN IOn k4~d 38d 8dO dO:) SAY 30" NO" 8N  NH" 4IFIO" dN" 08d 3NI II H3../ f)ld 083 HNN ION HSd 38,:t tldO dO3 dAY :)0"4.
NO., NN" NH" AI" NO-~dN" Fig.
lob. Output structure after ~r~v has applied 138 SERGE BOISVERT ANDl~ DUGAS DENISE B\]~LANGER :...................~ u_ ,,*,0,,o$1,$1,,, ............~z >.
m ~Z .,-,,, : ~ "...  . . .............~, + oxx~O D.
Fig. 11a.
Insertion of a node (before) OBLING" A TESTER FOR TRANSFORMATIONAL GRAMMARS 139 .   . . ..
 ,,.........~.,,,,......,.....~.=.
***eeeoeo$ooe~ o e~ x tn mo~ o~ ~Z~WZ ~S~W m   Fig.
1lb. Insertion of a node (after) 140 SERGE BOISVERTANDRI~ DUGASDENISE BELANGER 8.
CONCLUSIONS OBJ-mC is a system which has been implemented in low-level tORTeN IV for the CDC 6400.
It occupies 55,000s 60-bit words of memory.
It has about 7000 lines of comments and instructions.
REFERENCES N.
CHOMSKY, Aspects of the Theory of syntax, Cambridge (Mass.), 1965.
A. Ducas, et al., Description syntaxique ~l/mentaire du franfais inspir~ des th/ories transformationnelles, Montr6al, 1969.
J. FIUEDM~, A Computer Model of Transformational Grammar, New York, 1971.
D. LIEBERMAN, (ed), Specification and Utilization of a Transformational Grammar, Yorktown Heights (N.Y.), 1966.
D. L.
LotrD~, W.
J. SCHO~N~, TOT: A Transformational Grammar Tester, in Proc.
Spring Joint Computer Conference, Part I, 1968, pp.
385-393. R.
PETRICK, A Recognition Procedure for Transformational Grammars, Ph.-D.
Dissertation, Cambridge (Mass.), 1965.
J. R.
Ross, A proposed rule of tree-pruning, NSF-17, Computation Laboratory, Harvard University, IV (1966), pp.
1-18. A.
M. 7.WICKY, J.
FRIEDMAN, \]3.
HALL, D.
E. W.~a.g.F.R, The MrrRE Syntactic Analysis Procedure for Transformational Grammars, in Proc.
Fall Joint Computer Conference, Vol.
27, Pt.
1, 1965, pp. 317-326 .
S$ I:Mct ur a 1 Cor r e AponfJet\]Ge S~ec \[ f I c_a t j O0_#DvLEonmer~ Yongfeng YAN Groupe d'Etudes pour la TradlJctlon Automatlque (GETA) B.P. 68 Unlverslty of Grenoble 38402 Saint Martln d'H~res FRANCE ABSTRACT This article presents tile Structural Correspondence Speclflcatlon Environment (S('SE) being Implemented at GETA The SCSE is designed to help linguists to develop, consult and verify the SCS Gr'alt~nar s (SCSG) which specify I lngulst ic models.
It Integrates the t eclln 1 clues of' data bases, structured edltors and language interpreters.
We argue that formalisms and tools of specification are as Important as the specification itself.
z NT ROD_UCT tqN For quite some time, It has been recognized that tile specification Is very important in tile development of large computer systems as well as the linguistic computer systenls.
But it ls very difficult to make good use of specification without a well defined formalism and convenient tool.
The Structural Correspondence Specification Gran~ar (SCSG) is a powerful linguist ic specification Formalism.
the SCSGs were ftrst studied in S.Chappuy's thesis (1}, under the supervision of Professor B.
VaLIqUOt s.
In their paper presented at Colgate University in 1985 {6} SCSG was called Static Greener, as opposed to dynamic grammars which are executable programs, because the 8CSG aims at specifying WI4AT the linguistic models are rather than IIOW they are calculated, A SCSG describes a llnqulstlc medel by specifying the correspondence between the valid surface strings of words and the multi=level structures or a language.
Thus, from a SCSG, one can obtain at the same tlme valid str lngs, valid structures and the relat ton between them.
A SCSG can be used for the synthesis of' dynamic gralr~}lars (analyser and generator) and as a reference for large linguistic systems.
An SOS Language (SCSL) has been designed at GETA, tn whlcll the SCSG can be \]lnearly written.
The SCS Environment (SCSE) presented here ts a compLIter aided SCSG des lgn system.
I t wl 1 1 al low lhlgulsts to create, modify, consult and verify their granlnars in a convenient way and therefore to augment their productivity.
Sect 1on I gives a outline of the system: Its architecture, pr Inciple, data structure and comdnand syntax.
Section II describes the malrl functions of the system.
We conclude by gtvtng a perspective for luther developments el' the system.
I=.AN OVERVIEW OF TI4E S YSTE_M 1.
ARC HI\]EC T URE The SCSE can be logically divided tn five parts: 1 SCSG base 2.
monitor 3.
input 4.
output 5.
procedures The SCSG base consists of a set of files Contalnlng tile grarrlnars, lhe base has a hlerarchtca\] structure.
A tree form directory describes tile relationship between the data of the base.
The monitor Is the interface between the system and the user.
It reads and analyses colTinands from the input and then calls the procedures to execute the cormlands.
1he input is the support containing the COlrrnands to be executed and the data to update the base.
rhere is a standard input (usually the keyboard) from which the data and cormlands sllould be read unless an Input ls explicitly specified by a conlnand.
The output is a supper t receiving the system's dialogue messages and execution results.
There is a standard output (usual ly the screen) to which tile message and results should be sent unless all output Is explicitly specified by a con~and.
The procedures are the most irnportant part of tl~e systenl.
It ls the execution of procedures that carries out a COn~land.
The procedures can communicate directly wtth the user and with other procedures.
2. THE_E.RJNCU}LE An SCSE session begins by loading the original SCSG base or the one saved from the last Session, Then the monitor reads lines from tile com~nand input and calls the corresponding procedures to execute the COmd~lands found.
When an SCSE session Is ended by the colm~and "QUIT", the current state of tlqe base Is saved.
The SCSG base can only be updated by the execution of c omrlland s, The original SCSG base contains two SCSGs : one describes the syntax of the SCSI_ and the other gives the correspondence between the directory's nodes and the syntactic units of the SCSL.
The first gralmlar ls read-only but the second one can be modified by a user.
This allows a user to have his prefered logical view over the base's physical data.
These two grammars serve also as all Oil-line reference of the system.
Several Interactive levels can be chosen by the user or by the system according to the number of errors in the con~aapd lines.
The system sends a prc~npt message only when a "RETURN" ls met in the CO~nand lines.
So gee carl avoid prompt messages by entering several cen~nands at a time.
;3. DATA S:\[f~UCTURE There are two data structure levels.
The lower one Is linear, supported by the host system.
Tile base Is a set of files containing a llst of strings of characters.
Tile base carl be seen as a single string of characters thai: Is the concatenation of all lines tn the ft\]es of the Llase so that tile structure is said to be llnear.
TIlls structure is the physical structure.
The higher one Is hierarchical, defined by the directory of the base.
Tile base is composed of a number of SCSGs ; each gral~ar contains a declaration sect Ion, a rule (chart) sect Ion...
etc. and the components of a gran~nar (declarat 1Ol1, rules . . . etc, ) have their own structure.
The hierarchical structure ts the logical structure of the base.
The directory has a tree form.
A node In the tree represents a logical data unit that ts its content (for instance a gran~nar).
Every node has a type and a list of attributes characterlslng the node's content, rhe lnternode's content is the composition of those of its descendents, \]he lear's content Is directly associated 81 with a physical data unit (a string o1' characters).
The following figure shows the relation between the two structures.
LOGICAL STRUCTURE (i) 7, 2Y LOGICAL S'\[RUCTURE (2) language date \[Grammar English -----i node type attributes The directory is slmllar to a UNIX directory.
But In our directory, tile leaves do not correspond to Flies but to loglcal data units and Furthermore an attribute list is attached to each node.
The correspondence between two structures is maintained by SCSE.
We shall see later that this organlsatlon allows a more efficient Information retrieval.
It ls possible For" users to have access to the data by means of both structures.
The logical one Is more convenient but the physical one may be more efficient in some cases.
4:~ _COMMANp__SyNTAX The general command format is : <operator> <operand> <options> The "operator" is a word or an abbreviation recalling the operation of tile colmland.
The "operand" is a pattern giving the range OF the operation.
The "options" is a list of optlonal parameters of the COw,land.
For example, the Con~nand : V GRAMMAR ( LANGUAGE = ENGLISH ) visualizes, at the standard output, all the English grammars In the base.
Here V is the operator, GRAMMAR(LANGUAGE=ENGLISti) ls tile operand pattern and no option Is given.
The operand being mostly a node in the directory tree, the pattern is USUally a tree pattern.
When the pattern matches a subtree of the directory, the part that matches a specially marked node Is the effective operand.
The pattern is expressed by a geometric structure and a constraint condition.
The structure ts a tree written in parenthesized form perhaps containing variables eacll representing a tree or a forest.
The coeditlon Is a first order logic predicate In terms of the attributes of the nodes occurring in the geometric structure.
More sophisticated conditions may be expressed by a predicate combined with geometric structure to efficiently select information from the base.
Pattern writing should be reduced to a minimum.
In the abeve example, the geometric structure is shnply a grammar type node and the constraint is the node's language attribote having the value= Erlgllsl\].
The use of a current node tn the directory allows not only the simplification of pattern writing but also the reduction of the pattern matching range.
The effective operand becomes the new current node after the execution of a command.
II. THE MAIN FUNCTIONS We shall Just descrlbe the functions ttiat seem essential, lhe functions may be divided Into four groups= 1.
general 2.
SCSG base updating 3.
SCSG base inquiry 4.
SCSG verification.
_1 ~ _GI~ t>\[E__R A L _F U_N__C__T._I.D_N S These functions Include: SCSE session options setting, the system's miscellaneous lnformatlon inquiry and access to host system's commands.
The following options can'be set by user co,hands: 1.
tnteractivtty 2.
dlalogue language 3.
auto--verlfilcatlon 4.
session trace 5.
standard Input/output.
One of the 4 Following Interactive modes may be chosen: 1.
non-interactive 2.
brief 3.
detalled 4.
system controled.
In non-interactive mode, no question is asked by tile system.
An error con~and Is ignored and a message will be sent but the process continues.
In brief mode, the current accesslble command names are displayed when a corm, and Is completed and a RETURN in the command lines is Found.
In detailed mode, the functton and parameters of the accessible commands are displayed and 1F an error ts Found in the user's Input data, the system will diagnose it and help him to complete the command.
A prompt message ls sent every time RETURN is Found in the COn~nand lines.
In the system controlled mode, the lnteractlvlty Is dynamically chosen by tile system according to the system=user dialogue.
For the tlme being, only French is used as the dialogue language.
But the mu.ltl-langueage dialogue is taken tnto account tn design.
It is simpler In PROLOG to add a new dialogue language.
The auto-verification option Indicates whether the static coherence (see 4.
SCSG verification) of a gra~nar will be verified each time it ls modified.
The trace option is a switch that turns on or off the trace of the session.
The standard Input/output option changes the standard input/output.
Some Inquiries about the system's general Information, such as the current options and directory content, are also ~ncluded in this group of Functions.
The access to host system's co~Ylands without leaving SCSE can augment the efficiency.
But any object modlfted out of SCSE is consided no more coherent.
2. SCSG BASE UPDATING This group of fiuectlons are: CREATE, COPY, CHANGE, LOCATE, DESTROY and MODIFY.
\]hey may be found In all the classic editors or file management systems.
The advantage of our system is that the operand of commands can be specified according to the logical structure of the base.
For example, the col~nand : DESTROY CI4ARTS(TYPE=NP) Destroys all the charts which describe a Noun Phrase.
82 The SCSE has a syntaci Ic editor that knows the logical structure of the texts being edited.
Ihls editor Is used by tile con'Jnands MODIF and CREATE.
The command CREA1 <operand> <options> calls the edltor, creating a logical data unit specified by tile operaod.
If the interactive option ts demanded, the editor will guide the user to write correct ly according to the nature of the data.
Following the same tdea of different interact lye levels, we try to improve on tile classical structural editor, Per instance that of Cornell University \]\[5}, so that one carl enter a piece of text longer than that prompted by the system.
If the interactive option Is not demanded, one Just enters into the editor wlth an empty work space.
The CO~T~nand "MODIF <logical unit>" calls the system's edltor with the logical data untt as the workspace.
The data ill the workspace may be displayed In a legible form which reflects Its logical structure.
The mul t l-w \]ndows facll ity of the editor makes it possible to see simultaneously on tile screen the source text and tile text In structured form.
The SCSE editor inherits the usual editing con~llands from the host editor.
Thus one can change all the occurrences Of a rule's name fn a grarrnlar without cilanglng the strlngs containing the same characters, using a loglcal structure change : C NAME(type=rule) old name new _nan/e, while tile physlcal structure command : C/o 1 d..
name/new .name/* * changes all the strings "old_name" In the workspace by new name.
When an obJect's deflnltloo Is modified, all Its occurrences may need to be revised and vice versa even if the modification does not cause a syntactic error.
A structure location command flndlng the definition and all the occurrences of an object can be used In this case.
Only tile logical units defined in the directory and the SCSL syntax can be manipulated by the structural COrr~land s.
SCSGBA=SI~_INQUIRY These functions allow users to express what they are interested ill and to get the Inquiry results In a legible form.
A part of the on-llne manual of usage in the form of SCSG may also be consulted by them.
The operand patterns discussed above are used to select the relevant data.
The operator and options of co~nands choose the output device and corresponding parameters.
A parametered output form for each logical data unit has been defined.
The data matching the operand pattern are shaped according to their output form.
The data may of course be obtained in their source form.
One may wish to examine an object at different levels (e.g.
Just tile abstract or some comments).
The options of the con~and can specify this.
If one Just wants to change the current node in the directory for factlltatlng the following retrieval, the same locating co~nand as before may be used.
4. SCSG VERIEICAT#ONS.
Two klnds of verifications may be distinguished : static and dynamic.
Tile static verification checks whether a grammar or a part of a gra~nar respects the syntax and semantics of the formalism.
The dynamic verification tests whether a given gran'mnar specifies what we want It to.
S tatlc_ve, r Ifica~ton All internal representation of the analyzed text ts produced and used by the system for structural manipulation, the analyser may produce a list of cross references of = nameable objects and a list of syntaxo-semantlc errors found In the text.
The exemples of nameable objects are the charts, tile macros, the attributes.
The list of cross-references reveals the objects which are used but never defined or those defined but never used.
A chart may refer to other charts.
This reference relation can be represented by an oriented graph where the nodes stand for a set of charts.
A hlerarciltcal reference graph is often given before writing the charts.
A program can calculate the effective graph of a grammar according to the result o analysis and compare It with the given one.
The cornlland options may cancel the output of tllese two llsts and the graph calculat Ion.
The graph calculation may also be executed alone.
One of optlons Indicates whether the analysis wtll be Interactive.
D.y.n ~!# J c.
v. ~gr :1 f i canon Tile dynamic verification Is tile calculatlon of a subset of the st ring-tree relation defined by a gr altrnar.
A member of the relation is a pair <string,tree>.
Ti)e command gives the granYnar and the subset to be calculated.
The subset may be one of the four following forms : I.
a pair with a given string and a given tree (to see whether It belongs to the relation) 2.
pairs with a given string and an arbitrary tree 3.
pairs with an arbitrary string and a given tree 4.
all possible pairs rhe calculation is carried out by all interpreter.
The user may give interpretation parameters Indicating interactive and trace modes, slze o the subset to be calculated and other constraints such as a list of passive (or active) charts during this interpretation, the depth and width of trees and length of the string etc..
As SCSGs are statlc gral~nars, no heuristic strategy wllt be used In the lnterprete's algorithm.
So the interpretation will not be efficient.
Since the goal ts rather to test gramnars than to apply them on a real scale, the efficiency of the interpreter Is of no import ance.
CONCLUS I0N The system presented Is being implemented at GETA.
In thls article, we Put emphasis on the system's design principles and specification rather tilan on the detalis of lmplementatlon.
We have to1 lowed three widely recommended des ign principles: a} early focus on users and tasks, b) empirical measurement and c) Interactive design \]\[2\]\[.
The specification of the functions are checked by the system's future users before implementation.
The user's advice Is taken into account.
This dtalogue continues during lhe implementation.
The top-down and modular programming approaches are followed so ttlat, even 1f the Implementation ls not completly acilieved, the implemented part can still be used.
The system Is designed for being rapidly implemented and east ly modt f led thanks to Its modular lty and especially to a htgh level logic programming language: PROLOG (3\].
We have tried our best to make the system as user-fr lendly as possible.
The system's most remarkable character is that the users manage their data according to the loglcal structure adapted to tile human be I rig.
What ts interesting In our system ls not that it shows sonle very original ideas or the most recent techniques In state-of-the-art but tt shows that tile combination of well-known techniques used orignally In different fields may flnd its application in other fields.
83 Long term perspectives of the system are numerous.
Wlth the evaluation o the SCSG, some strategic and heuristic meta.-rules may be added to a grammar.
Equipped by an expert system of SCSG, SCSE could lnterprete effclently a static grammar and synthetlse from It efficacious dynamic grammars.
It Is also interesting to integrate into SCSE an expert system which could compare two SCSGs of two languages and produce a transfer grammar or' at least glve some advice for constructing it.
Using its logical structure manipulation mechanism, SCSE can be extended to deal with other types of structured texts.
Thanks to Its efficient Interpreter or in Cooperation with a powerful machine translation system such as ARIANE, SCSE could be capable of offering multi-llngual editing facilities (4~.
-O--O--O-O--O-O-O-O84 BIBLIOGRAPHY S.Chappuy, "Formallsatlon de la Description des Niveaux d'Intepretation des Langues Nature\]les.
Etude Men~e en Vue de l'Analyse et de la G6n@ratlon au Moyeo de Transducteur.", Th~se de trotsl~me cycle & I'USMG-INPG, Juillet 1983.
2. John G.
Gould and Clayton Lewis.
"Designing for Useabllity: Key Principles and What Designers Think", Co~nunIcatlon of the ACM, March 1985 Volume 28 N .
Ph. Donz, "PROLOGCRISS, une extention du langage PROLOG", CRISS, Unlverslte II de Grenoble, Verston 4.0, Juillet 1985.
HEIDORN G.E., JENSEN K., MILLER L.A., BYRD R.J.0 CHODOROW M.S., "The EPISTLE text-crltlauing system.", IBM Syst.
Journal, 21/3, 1982.
TEITELBAUM 1.
et al, "The Cornell Program Synthesizer: a syntax directed pr'ogra~ntng environments.
", Co~nunicatlon of ACM, 24(9), Sept.
1981. VAUOOIS & S.
CHAPPUY, "Static Gran~ars : a formalism for the descrlbtion of linguistic models", Proceedings of the conference on theoretical and methodological issues in machtne translation of natural language, Colgate University, Hamilton N.-Y., USA, August 14-16, 1985 .
CondiLioned UnificaLiou for Natural l,an~uage Processinp, A\]\]STV~ACT This paper prescnLs what wc c.all a condiLiol'md unification, a r'm'w meLhod of unificatiol'~ for processing natural languages.
The key idea is to annotate Lhe patterns wiLh a cerLcdn sort of conditions, so that they carry abundant inforrnation.
'\]'his met.bed t.ransmits inforrnaLion frorn one pattern to another more efl'icienLly Lhan proecdurc aLLachmenLs, in which information cortLaincd in the procedure is embcddcd in the progranl rather Lhan dirccl./y aLLachcd Lo paLL(ms Coupled wilt techniques in forrnal linguistics> i\]\]orcovcr, conditiorled unification serves most.
types o1" opcrations for natlu'ai \]ar/guage processil'q~,.
KSiti f/asida \]'\]\]ecLroLechllica\[ 1,abe ' A.ory Ul\]lczorlo I } 4, 7~aktlra MtlFa, Niibari-Gurl, Ibaraki, \[tOb Japan (\[3) ptlt_tllS psrl nrnb(prcsonL, P, N) : notAlrd ~'-tng(P, N) l~ut_Lns, psn nl:nl)(T, l", N) : not_pres(T) noL_3rd sng(lst.,,N).
not pres(past) not_~rd~'-;ng(;~nd, N) not pres(past4~a'rtlclp\]e ) rlotA/rd, sr~g(,'Wd, plural) not pres(basc).
1. Introduction A currcr'd, major t.rcrY.t of naturul la~/guage processing is ehara.cterized by Lhc overall use o\[ unification (Sttiebc~r (198'l), Kay (1985), Proudir:~ and Pollard (1,985), Percira (198b), Shicbcr (1985), etc) reflecLing lhe recent develop merits in nonLra.nsformaLJonal linguistic \[ormalisrns, such as Lexical FuncUonal Grammar (}lrcsnan (198E)), Generalizcd Phrase St.r'tJcl.ur( (\]rarnrnar (GPS(\]) (Gazdar, Klein, Pulhlm and Sag (I 985)), i Icad Grammar (Pol}ard (19f1,1)), and tIcadl)riven Phrase Structure Grammar (lIPS(;) (Pc\]lard (1985a.,b)). These formalisnls dispense wiLt,qlobal opcraLioits sueh as t.ransfornlaLion, alld instead cxp!oit h~cal operations each C'Ollf'lrled wttJ/i\[l a local tree Such local operations ar'c forunulatcd in Lcrms of uni~caLion \]Iowevcr, Lhe ordinary unification as in Prolog is insufficient, seeu rrorn both scientific (here, alias liriguJsl,ic) and cngin(.'ering poilfl'. of vicw '\['he F, robh-'trl is that p,t tc\]~\[\]s Lo bc tl\[li(ie({ wiL\]l each other lack the cape.city rot carrying irfforrnaLion In Lhis papcr we \[)rcscnl a new mcLhod of unificaLion which we (call conditioned unification.
The essence of the method is t.o deal wit.h paLLcrns aimoLated by seine sort of condit, ioils.
These eondiLioi<ls are so cortsl, raincd /-Is Lo 'oe cfficicntly operated on, and yet to be able to carry rich enough informaLion t.o caDLure linguistic gcneralizations.
2. The Problem Ordinary patterns as it/ Pr(;h)g Is.el< cxprcssivc power, t)esatlSC var\[ablcs theFcirl arc Sil)i\[)ly il\](iClCl"tlliltdt(7 alld Ihtis car'ry almost no irffqrrnalion 'l'hercforc, stie}l palL(ms aud unification among thcm arc msuffiei0nt for' capturlng t\]le {~,i<'al/l rYlat ic al <!,>erm r'al ixat ior~ and tim process:n~> effici(ncy.
\],It us look a.t some c:<amph.~s below A ~,,l'anl matical catc>ory is assumed Lo be it llst of features A feature consisLs of a feature nalnc and a w~hic, and rcprcscnLcd as a t.cmn like tt~rn.e (vat,z().
'\['hc \]cxical entry of English vcrb p'u,t, for instance, can not.
be described as a I'roloc patLcrn, bill needs some arlllOLation (i.c.,p~zt Im.s~)s?t.~zmS(T, P, N)) as in (1) (1) k:xicorl(puL, I tensc('I'), p(msorl(\]')> number(N)I) : put_Lns_ psn nrnb(T, P, N) }let(?, fcaLLIICS olincr Lhan ten, se., perso?t, and ?lattnher arc omitLed, arm predicate p~ztmtm.spsTt~z?n.b is dcfinc.d c,s in (2) For a biL morc COIT/I)I i:aLcd instance, conmdcr the relationship between a synLacLic gap and iLs filler.
In GPSG, IIPSC, cte., tiffs relationship is; captured in terms of the SI,ASI\[ fea/.urc, which reprcscnts gaps in the category of \] U~.i~tk is craz U, for cxamplc, thc SI,ASII feature is spcciflcd as \[NI j\] ller'e SI,ASI is assumed to take as its wdue alist of catcgorncs.
SLaLcd below is a simplil3cd principle about the disti'ibution of this featqrc in I.yptca\] cascs (3) lu a local tr'ec, Lhc rllotl;cr catcgor/s S\],ASI\] tea.Lure is obl.aJncd by coneatcr~atir, g from h.fi\[.
Lo rip, ht the S I,ASII fcat,wcs of hey de.,.ightcrs In order to describe: this principle, s('nnetlting :uorc than a nlerc pat t.crn ts lcquir(x\] again: (i) IocalJr'cc.(lslc, sh(X)\], Ismsh()l, Islash(Z)\])append(Y, Z, X) l'eaturcs othcr thanSI,AS}l arc omitted herc.
The so called procedure altachmcnts is the most colnrnon way or conllflclncntJnp, the \])oor clcscriplivc capacM.y or ordinary patterns \["or instance, you may regard Lhc bodies of \]h)rn elaus(s (1) and (4) as at la,_hed procedures The dr'awbacl< of procedure atLachr~lcnl is ut the fact t.hnt the ouly way of using Lhc proccclurcs Is to execute thorn I"or t.his reason, proecdur,}s arc Irmrcly embedded lu programs, rcAhcr than at.lath(x\] to those paLterrls which th(sc itrotu'ams operate on The irtforrnaLion which \[)ro((durcs cantain car/rx~t {U.'nera\]\]y be I~.',rricd aFOlllld a( ross scvc;a/ part ial s\[rtlC\[ tlI( s ci\]ch Of which it pFoocc\]arc dircclly operates on, bccausc> oncc a procc(lurc is cxc cnt.td, the informs.lion whkh it c.ontainc:d is palqially lost For instance, when Icxical entry (1) is cxploiLecl, p~ztJ.n.s pstt.n;/m,6(?', I), /\i) is cxecut.cd and 7' and /~ arc il~stanliatcd Lo be preset~.t and Ist, icsp(cLivc\[y 'l'h'ds Ic\[L bchh~d is the informaLion abotlL the other ways Lo instanLiaLc those wwiablcs.
Actual procedure attaclu'ncrd.s musL be arr-ar<e, ed so that infornlat on shouhl not be it)st whelt procedures arc cxccutcd Freeze of Prolog (Colmcraucr (1982)), for instanc/, is a mcans of tins arr~ingerncnt.
\]\]y exc(tll.i\[\]g freeze(V, "~), atomic formula ~ is frozen; i.c, the exccuLlon of'~ is >-uspcnded until w~riable X is instanttat.cd \]f' contams.'(, thcl'cforc, }lop(fully uot.
so rnuch lrtforrnat.iol~ is test.
whcc  is cxecuLed Ncvc.rthcless, freeze is problematic in two rcspt(ts Virsk, irJorn\]ation cart still be lost when the frozen pro-ccdtll'CS LIFe cxccnted.
Seeond, too nltlCh illforrllatiol3 cat\] be accumulatcd whilc several procedures arc frozen Sup pose, for itlst.ance, thaL freeze IX, t~tet~ber(.Y, \[a, 6 })) and fr<,t~.~<~.
(r,,~.',~,.~.,'(}'. I~'.~ I)) have bccn execut, cd '\['herr, X and Y can be ulfif\]cd with each other witt~ouL awakening ciLbcr procedure.
In that (asc, Lho iifforn/at, ion that X may bc t) is redundant bctwccn Lhc I.wo proccdures, and Litc other part or irlfornmLion those Droecdtlres contain is Ill(Of\]" sistcitL What one might hope here is \[o Jrlstitntiatc )( (and Y) to be b If we had cxectitcd freeze(Y, member(Y, It, cL ) iristcad of freeze (Y, rn.ernber(Y, Ib, c I), computational 85 resources would be wasted as the price for a wrong processint.
After all, it is up to a programmer to Lake a deliberate care so that information should t)e efficiently transmitted across patterns This causes sewral problems interwoven with one another.
First, since those programs reflect the intended order of execution, they fad to straightforwardly capture the nniforrnitJes captured by rules or principles such as (3).
Accordingly, programnnng takes rnuch labor'.
Moreover, the resulting programs work efficiently only along t.he initially inLer~ded order.
3o Conditioned Unification 3.1.
Conditioned Patterns These problems will be.
settled if we earl attach information to patterns, instead of attaching procedures to programs l\[ere wc consider that such information is carried by some conditions on variables Variables are then regarded as carrying some information rather than remain:ing simply indcterminatc I-}y a conditioned pal.tern let.
us refer to a pair o\[a pat tern and a condition on the w~riables contained in that pat.tern.
l~'or simplicity, assume LhaL the condition of a conditioned pattern consists of atomic formulas of Pro/og whose argument positions are filled with variables appearing m tile pattx.'rn, and that the pre(hcates heading those atomic for mulas are defh~ed in l.erms of Horn clauses.
For instance, we would hkc to regard the whole tbing in (\[) or (4) as a condJtioncd pattern.
 3.2.
Modular Conditions The conditions in conditioned patterns must not be executed, or the contained information would be partially lost Tile conditions have to be somehow joined when conditioned patterns are unified, so t.hat the information they contain should be transmitted properly in tile sense that the resulting condit.ion is equivalent \[o the Logical conjunction of tam input renditions and contains nciCrmr rcdnndant nor ineon sistent information.
We call suet a unification a conditioned unification A simple way to reduce redundancy and inconsistency in a ('.ondiLion is to let each part of each possible value of each variable be sLlbjcct to at, most one constraint.
\],eL us formulate this below.
We say that a condition is superficially modular, when no variable appears twice in that rendition For instance, (Sa) is a superficially modular condition, whereas (Sb,c) are not.
(Conditions are some.
times wr'itterr as lists of atomic forrnulas ) (',9 a \[a(X, Y), b(Z), a(U, v)\] b.
l a(X, Y), b(Y)\] e \[a(X,Y,X)\] l,'urther we say that a condition ~I' is modular, when all the relevant renditions are superficially modular, lIere, the relevant, conditions are {I} and the bodies of Horn clauses reached by descending along the definitions of the predicat.es appearing in .
A predicate is said to be modular when its definition contains only those Iiorn clauses whose bodies are modular conditions.
A predicate is potentially modular when it is equivalent to some modular predicate A modular condition does not.
impose two constraints on any one part.
of any variable, and thcrcfore contains ne> kher redundancy nor ineonsistency, ltereaRer we consider that the condition m (.'very conditioned pattern should be modular.
a.a. l'Jxpressive Power Conditioned patterns can carry rich enough information for capturing the linguistic generality.
Obviously, at 86 'st., they can describe any finite set of finite patterns.
\];'or instance, (I) is regarded as a conditioned pattern with modular condition \[pztt_g'ms_pstt~q,r~zb (T, P, N)\].
Moreover, also some recursivc predicates are modular, as is demonstrated below.
(6) a appcnd(\[\], Y, Y): append(\[U I X\], Y, \[U I Z\]) :append(X, Y, Z).
b sublist(\[\], Y).
sub\]ist(\[U I<I, \[U I Y\]) :sublist(X, Y).
sublJst(X, \[U IY\]):-sublist(X, Y).
Thus, (4) is also a conditioned pattern.
\]lowever, some recursive predicates are not potentially modular.
They include reverse (the binary predieate which is satisfied i~r its two arguments are the reversals of each oilier, as in reverse(\[tot, b\], c, all, \[d, c,\[ct b\]\])), .perm (Lbe binary predicate satisfied iff its arguments arc permutat, ions of each other, as inperm(\[i, 2, 3\], \[2, 1, 3\])), subset (the binary predicate which obtains iff the first argument is a subset of the second, as in s~zbset(\[d! b\], to, b, c, all)), etc.
New.'rtheless, t.his causes no problem regarding natural language proeessing, because potentially infinite patterns corne up only out of features such as SLASt\[, which do not require those non ruodular predicates.
3.4. The Unifier Shown below is a'trace of the conditioned unification between conditioned patter'us (7) and (8) (here we use the same notation for eondit.ioned patterns as for IIorn clauses), where the predicates therein have been defied as in (9).
(The definitions of c0 and e3 are not exploited).
First, we unify iX, )2 Z, g/\] and \[A, 7\], C, D\] with one another and get.
XA, Y : /3, Z = C, and W = D \]n the environment under lifts unification, the two conditions are concatenated, resulting in \[c0(X), e I(Y, Z), e2(Z, W)\].
The major task of this conditioned unification is to obtain a modular condition equivalent to this rmn-rnodular conditiorl This is tire job of funcl.ion ~tod~zlayi, ze.
Mod~zla.~tze rails function ~;~ttegrctte, which r'eLtlrns an atomic formula equiwrlent Lo the gives condition.
The Lcrminatior~ of a ?rtodulct,'ize or anir~fegrate is indicated by ~ preceding the reLurn-waluc, with the same amount of indentation as the outset of this functionrail was indicated witb When an {~ztegro, te calls a ~zodula~'ize, the alphabetic identifier of the exploited Ilorn clause is indicated to the h.'ft hand side, and the temporal unification to the right-hand side.
Atomic formulas made in integrate is written following 4.
Each lIorn clause entered into the definition is shown following % and given an alphabetic identifier indicatedto the right-hand side.
(7) IX, Y, Z, W\] :-e0(X), el (Y, Z).
(8) \[A, \]~, C, D\] :e2(C, D).
(9) e*(0, \]).
(a) e ~ (q, e) (b) ca(l, P):-e:Xl').
(c) c~(e, 0).
(d) modularize(\[e0(X), cl(Y, Z), c2(Z, W)\]) integrate(\[e0(X) \]) cO(X) integPate(\[cl(Y, Z), e2(Z, W)\]) c4(Y, Z, W) (a) modularize(\[e2(1, W)\]) Y = 0, Z = 1 integrate(to2(1' W)\]) * eS(W) (c) rnodularize(\[e3(P)\]) W = P integrate(\[e3(P)\]) =~ e3(P) tea(p)\] c~(p) :ca(P).
(0 =:> eS(W) -~ \[c,~)(w)\] 1' o4(o, :, w) :.
~:',~(w).
(j) (b) n:odular:'zc(\[c2(2, W)i)  :q, '/, :~ i:,t.o~ra~,'.(Im<3(a W)\]) * cO(W) (d) nladularizc(l I) w =.: o =-~ \[\] cS(0) (k) =+ c6(W) > I cs(w)J " o,3(q, ~, w) : o6(w).
(I) => c4(Y, Z, W) > \[co(x), <:4(< z, W) l We CaN refine Lhe progra\]'n o\[ "btt~.grcs, ta so that it should avoid ally predicaLe w}iose definiLion coiuLains only one llorn clause.
For instance, Lhe definiLion of cb consisLs only o\[(i) InsLead of (j), LhereR)re, we may }rove cd(0, 1, P) : c3(P) Also (1) can bc replaced by c 4(0, 2> 0), based on (k) We are able Lo work out r'ccursivc condiLions from F, lvor; recursivc coI:dit.iolls, lVor example, considor how X and Z arc unifiod under" t, ha conclit,iol: (10), whore ~rte'n~be.r is defined as in (1 1) (10) \[n,e:nher(X, Y), o0(z)\] (11) n/cinber(A, IA I IJl).
(a) member(A, IC I i~i) :-i ....... her(A, it) (b) The Lracc of this ulfif\]cat.k~n is showl~ b('\]:'w, whc's'c prcdica.l~' c 1 is rccursivcly (\]o/~ll/Cd based on Lhc i'(,ctlrsiv(! dcfillJLioH of 77"~ ( 77}, t) I~, '? modularizc(lmcmbcr(X, Y), cO(X)I) int.cgraLo(I member(X, Y), cO(X)\]), el(X, Y) (a) modularize(\[cO(A)\]) X = A, V -.
\[A Ill\] int.e<~';ratc (I c O(A)I) = > oO(A) =~ 1 <:)(A)\] 1' c I(A, \[A I ~<J) :-c0(A) (1)) n:odularizc(lmcn,bcr(A, 11), c0(A)l) X :: A, Y :: \[c!i;I Jnl.c<qral.e(\[ nlernbcr(A, I~), cO(A)I) :~ el(A, 1~) ::> \[cl(A, 33)1 1'ci(A, Iclt<l): c~(A, 13).
.... > c:(X, Y) :.~ \[o:(x, Y)I IL Js a job of in, tegra, te, Lo handle re,cursive de,hiLton.
The lasL g?l, te.g?,ts, te.
above recognizes Lhat.
the first 4m.tegrate, which is Lrying Lo (\]cNr/c c 1, was called wit.h the same arguITlCrlI.S except, the variable narnes, llencc t.he last "inttegrctte simply reLul'ns c.
I(A, H), because t,hc conLent, or cl is now bring worked ouL tlrldoY Lhc J'\]rsl.
~?ttegro.te arid thus it is rednndanL fol' t, he lasL {~tfegrate Lo \[urLhcr examine c 1.
It is not.
a\]ways possible fro' the above unifier t.~ unify paLL(2i"\[/s tlrl(\](~r roc.
tlr'sivo Col\]d\[Liol/S \["or J//sLalloL', J\[ Cf/illIOL unify X with )" under \[appe~td.(X, Y, Z)\], becal_tsc Lhe result ing condiLion is noL potsnLia\]\]y rnodular, llowcver, such a situaLiol'l CtOCS FioL seoln t.o occur Jn actual \[al:g:lagc proccssir:g.
4. Conclusiori We have prc.'-~er, Led a new nle/hod a\[ umfiealion, wh,ch wc call o.
coildiLior~c(\] tltti(loaLioli, Whcl'e paLLorils to be uniPlc(\] "'re annoLaked by a certain sort.
of corldit.ions on lhe variables wifich octroi" ill those patt.crn.<;.
Theso condiLions are so r'est.ricted t.haL Lhey conlain as lit.Lie redundancy a<'; possible, and d'ms arc always assured to be satisfiable.
This method has Lhe fo/h)wtng welcome characteristics l"h'st, I.he \])attorns to bc unified can carry at)llllda\]'lL infos' mat.ion rcprcscnLcd by t.he conditions han:,~in!,; on t.hClll The expressive capacity of Lhe,<c condiLion,s is sl:fl'Jcen\[ for capt.uring \]JH~U, IIihLJ(: ~sCHCl'a\]i',,iOS ~ccorld, such irfformat.ion is cfreclivcly Ir'ansrnitLed, by h~t.egrat.\[n? the col~dil,ion.~ v;her, pat.'..crl:,<s o.ro unif'ied Unlike procedure aLLacl:ment.s, in thil~ COllne(:lion, Ll/c infornGaLioi~-conveying <.'fficicl:cy of our Colt dilioiued unif'icat.\[on is no afl'c'(gcd by the direct.ion of t.i~c daLa.flow Therefore, O/ll" col'l(\]{lioned unifies.Lion is oo;rnplel/ly r(ver'sJbk< and ',hns is \[n'on:ising its a Los\] for dcscril)h'T> <~{l'all/lllilrs fOF bolh SCllL(Hb':C comprel/ensiol: slid prochl(d toll Owing t.o Lhese cllar'act(!risLics, Otll conditioned unif'caLian l)r'avh|es a now prog, ra.unniiug 1.1aradigtn foi I/illt/l'/tl lar/y,/lag(".
\[)l'OCCSSil/lJ>, rcph~.cing proccd/1Fc aLt, o.ctlI3:ont.s which haw3 tradlLionally el2joyed i.\]lc Llbiq/lity Lhat.
t.hcy do noL descrvc References Bresnan, J.
(ed). (1982) The Mental Representation of Grammatical Relations, MIT Press, Cambridge, Massachusetts.
Cohnerauer, A.
(1982) Prolog II Reference Manual and Theoretical Model ERA CNRS 363, Groupe d'Intelligence Artificielle, Universit de Marseilic, Marselle.
Gazdar, G.
E. Klein, G.
K. Pullum, and J.
A. Sag (1985) Generalized Phrase Structure Grammar; Basil Blackwell.
Oxford. Kay, M.
(1985) "Parsing in Functional Unification Grammar".
Natural Language Parsing, pp.
251--278, Cambridge University Press, Cambridge, England.
Fernando C.
N. Pereira, A structure-sharing representation for unification-based grammar formalisms, Proceedings of the 23rd annual meeting on Association for Computational Linguistics, p.137-144, July 08-12, 1985, Chicago, Illinois Pollard, C.
J. (1984) Generalized Phrase Structure Grammar.
Head Grammars, and Natural Languages.
Doctoral dissertation, Stanford University, Stanford, California.
Pollard, C.
J. (1985a) Lecture Notes on Head-Driven Phrase Structure Grammar.
Center for the Study of Language and Information.
Pollard, C.
J. (1985b) "Phrase Structure Grammar without Metarules," Proceedings of the Fourth West Coast Conference on Formal Linguistics, University of Southern California, Los Angeles, California.
Derek Proudian, Carl Pollard, Parsing Head-driven Phrase Structure Grammar, Proceedings of the 23rd annual meeting on Association for Computational Linguistics, p.167-171, July 08-12, 1985, Chicago, Illinois Stuart M.
Shieber, The design of a computer language for linguistic information, Proceedings of the 22nd annual meeting on Association for Computational Linguistics, p.362-366, July 02-06, 1984, Stanford, California Stuart M.
Shieber, Using restriction to extend parsing algorithms for complex-feature-based formalisms, Proceedings of the 23rd annual meeting on Association for Computational Linguistics, p.145-152, July 08-12, 1985, Chicago, Illinois
LA RESOUTION D'ANAPHORE A PARTIR D'UN LEXIQUEGRAMMAIRE DE45 VERBES ANAPHORIQUES Blandine GEL.AIN & Gelestin SEDOGBO 26 place Ovale BULL cediag 94230 Cachan 78430 Leuveciennes (France) Abstract This paper presents a system which intends to resolve anaphora in the framework of the Discourse Representation Theory, arrd using a lexicon-grammar of anaphoric verbs, through the application of selection criteria for assignment of a referent to an anaphora.
From a semantic representation of text provided by a DRT system implemented in Prolog, the system uses several criteria of selection of referent.
One of these criteria is the anaphoric conditions of verbs described as a lexicon-grammar of anaphoric verbs.
The present paper investigates a transformational analysis of verbs related to their anaphoric behaviour, and the adequacy of extension of the lexicon~grammar of MGROSS to anaphonc conditions on verbs.
1. Introduction La Th~orie de la Representation du Discours (ou DRT 1) propose une approche unifiee de phenom~nes du langage naturel tels que le temps, I'evenernent, I'anaphore Elle se caracterise par sa filiation avec la semantique Iogique, et sa distance d'avec les niveaux de representations basees sur la Iogique des predieats et ses extensions.
Ainsi les notions de consequence Iogique et de validite (\[SEDO 87\]) peuvent s'appliquet naturellement aux structures maniputees par la DR1 Cette theorie propose une explication de la formation de I'anaphore, sans en proposer ta resolution.
Celle-ci passe en general par I'application de orit~res de selection syntaxiques, semantiques et pragmatiques, qui levent les ambiguites engendrees par t'usage de I'anaphore.
\[GUIN 85\], \[CARB 88\], \[RICH 88\] entre autres ont intloduit les notions de foyer et de contramtes successives 8 activer Cependant cos criteres sont paffois insufiisants pour etablir une relation evidente entre un nomet un pronom Dar~s sa theorie du Oouvernement-liage, 1 H.KAMP "A Theory of T\[uth and SemarY0c Interpretation" GroP.nendiik Amsterdam 1-(:JR t NCHOMSKY 2 explique I'anaphore ~ partir d'un mecanisme de liage et s'appuie sur la remarque que le liage d'une anaphore A son referent depend aussi des proprietes anaphoriques du vetbe.
Aucune etude empirique des proprietes anaphoriques n'a ete faite ~) ce jour, alors que toute approche de resolution d'anaphore devrait etre basee sur un texique des verbes et leurs proprietes anaphonques associees.
Le present article decdt une approche de resolution d'anaphore qui repose sur: la mise en oeuvre de ta DRT pour representer la semantique d'un discours; I'elaboration d'un lexique-.grammaire des verbes anaphotique@; un systeme de filtre base stir differents criteres de selection Ce systeme illustte la resolution automatique de certaines anaphores en partant de la representation semantique d'un texte obtenue d'apres la DRT los ptonoms que nous avons etudies font partie d'un type d'anaphore qui represente une relation pouvant s'etablir entre deux phrases saris mettre forcement en jeu une regle syntaxique (le pronom pout identifier un referent dans le discours precedent): Jean croit que Mane IU/ offre un //we Cet article est divise en cinq parties: introduction ~ notre travail, presentation de la DRT, puis de son implementation en Prolog, description des lexique-grammaires et leur extension aux proprietes anaphoriques, presentation generale de I'architecture de notre systeme de resolution, et enfin perspectives de creation systematique d'un lexique-grammaire.
2. La reprdsentatlon s~manticlue La DRT se fonde done sur un ensemble de regtes de construction traduisant un disoours en une representation semantique formelte: la Structure de Representation du Discours.
Pour chaque pattie de discours, une DRS est construite, boite pouvant err contenir d'autres, 2 "Government Binding", notee GB 3 Exlension des tables de verbes d#vetoppt~es au LADL, aux propri~tes anaphonques (cf \[GELA g2\]) Ac:iEs DE COLING-92.
NAtCI~S, 23-28 AO~ t992 90 1 PROC.
OI; COLING=92, NANI'E,'I.
AUG. 23-28, 1992 qui reprL~sente le contenu significatif de cette par'tie Une DRS complete est I'ensemble de plusieurs DRSs apparaissant t~ mesure que le discours continue.
La DRT etudie doric les contraintes sur cette continuation.
La forme d'une DRS consiste en une paire <U,Con> constituant deux z6nes, o0 U (univers) est un ensemble des r~fdrents #u discours representant les entites du disc, ours, et Con un ensemble de conditions qua doivent satisfaire ces referents.
Celles-ci sont des predicats et des relations de referents du discours mais peuvent etre plus complexes.
Notees comme en Iogique des pr~dicats, les conditions de verite sont definies par rapport & la possibilit~ d'incture la DRS dens un modele (pour plus de details, consulter \[GUEN 85\]).
D'autre part, I'extension d'une DRS ne peut changer les valeurs d(~j& assignees: tout Ce qui etait vrai auparavant restera vrai par la suite: Un aamion bansporte une charge u.K=\[xl,x2\], Con.K=\[carnion(Xl),charge(X2),t~'ansporte(Xl,X2)\].
Tout camion b'ansporte une charge U.K=\[ \], Con.K = \[:>,K1,K2\]; U.KI=\[Xl\], Con.K1= \[camion(X1)\]; U.K2=\[X2\], Con.
K2=\[charge(X2),transporte(Xl,X2.)\].
2. I La notion d'accessibilite On peut representer des restrictions configurationnelles sur les relations anaphoriques possibles entre les pronoms et leurs antecedents.
Ces restrictions sont obtenues en reduisant I'accessibilitd des referents.
L'accessibilit~ permet donc de determiner les liens anaphoriques entre un marqueur pronominal et un marqueur de discours.
Toute DRS est accessible d'eltem~me; son univets de marqueurs accessibles est I'unNers du discours de la DRS.
Dans une DRS implicative, la DRS antecedente est accessible de la DRS consequente Enfin, la relation d'accessJbilite est transitive.
Donc pour "tout camion qui transporte une charge la declare", I'antecedent du pronom obJet laest une charge.
Ici, U.K1 est accessible & K2.
Les conditions de continuite d'un discours sont aussi fonotion de I'accessibilite: la phrase: "il va & Berlin" ne peut continuer la precedente puisqu'aucun marqueur de diseours n'est accessible de la DRS K (voir ce sujet \[KASP 86\]) Ceci explique pourquoi une phrase comme: "Cheque chauffeur possC~de un camion, fl te conduit": doit etre formulae: "Chaque chauffeur qui poss~de un camion le conduit* (avec "qui poss~e un cam/on" comma extension de "cheque chauffeur") pour Otre representable.
Mais la DRT a des limites; elle n'explique pas la bonne formation de ce texte, par example: Cheque chauffeur transpo~te une charge.
II n)et plusieurs jours & la tivrer.
Ella ne sere livr~e qu'au bout de 3lOUtS Les cleux dernieres phrases, selon la DRT, ne peuvent suivre la premiere, & cause de la portee du quantificateur cheque.
Par contre la phrase "Cheque chauffeur fransporfe une charge qui ne sere livree qu'au bout de 3 jours" sere parfaitement representee par la DRT.
Ces r~gles obligeraient doric le Iocuteur & d~crire une situation en une seule phrase! 2.2 Imp.~.
mentation de la DRT Notre analyseur semantique demane avec des arbres syntaxiques resultant d'une grammaire de type GPSG, programmee en Prolog La grammaire semantique est bas~e sur les memes principes: unification de structures, augmentation de listes ordonnees, presentee sous forme de regles de reecriture suivies de contraintes, de type: ph -> gn gv <ph drs courante>=<gn drs courante> <gn suite drs>=<gv drs_courante> <gn argument>=<gv arg_sujet> <ph arg sujet>=<gv arg suJet> <ph arg objet>=<gv arg_objet> Ceci donne, 8 partir de regles DCG issues de la compilation des precedentes: traduire(ph(GN, G V), P) :~'aduire(G V, P2), traduire(GN, PI), imerge(P,\[courante\],Pl,\[courante), imerge(P l, \[suite\], F~2, \[courente\]), tmerge(Pl, \[arg\], P2,\[arg sujet\], imerge(P,\[arg sujet\], P2,\[erg_sulet), tmerge(P, \[arg_objet\], P2,\[arg_objet\]) soit la formule semantique: drs (arg sujet(Xl),arg objet(X2), cour(cond( \[imp(drs(cond(\[camion(X l)\]), univ(\[X1\])), drs(cond( \[charge(X2), transporte(X l, X2)\]),univ(\[X2\])) )\] )) ).
correspondant ~ la phrase "tout camion ffansporfe une charge".
C'est 8 partir d'une telle formula que commence la resolution anaphorique 3.Le lexique-clrammaire Le lexique-grammaire, represente sous forme de tables (matrices composees de colonnes ACIT~ DE COLING-92, NAN-IT~, 23-28 AOt~Zr 1992 9 0 2 PRoc.
OF COLING-92, NANTES, AUO.
23-28, 1992 et de rangees), contient les phrases strnples, les diff~rents emplois verbaux et los propri~tes qui leur sent assoei~es: nature semantique des arguments, trallsforrnations possibles et tours conditions, nombre et structure des complements, type de la preposition associ~e etc I/ consid~re la nominalisation comma la transformation d'une phrase contenant un op~rateur verbal, en une autre phrase contenarlt url op~rateur nominal.
On y introduit un verbe predicativement vide -verbe supportdent le r,~le est d'actualiser le substantif qui n'a pas de marques morphologiques susceptibles de le taire: l.uc complimente los acteurs = Luc fail des compliments aux acteuts 3 1 L'entoura~e lexical Nous nous limiterons dans cet article aux possessifs, sur lesquels \[GULL 81\], \[DANL 80\], \[GROS 89\] et \[VtVE 83\] notamment nous ont fourni des informations fort utiles.
L'examen des roots voisins du possessir est important, e.
g; Luci donne # L~.aj son i+ j argent Luc i donne a L~,a son i amour Dens le premier cas, donner est un verbe ordinaire (plein) alors qua dens le second, 41 est un verbe support (V-sup) Seut le substantif N2 change (\[NO donne ~ N1 N2jaoss\]).
Pourtant dens la premiere phrase, son peut ref~rer & trois personnes: si t'argent est & Lea, son est relie ~ L#a; si I'argent est ALuc, son est relie a Luc; si I'argent n'est ni ~ I'un ni ~ I'autre, son est relic,9 une tierce personne du discours pr~o~dent.
Dans la seconde phrase par centre il n'y a qu'un r#ferent: Luc, ~ cause du terme amour qui appartient aux roots "abstFaits" ou de sentiments, pour lesquels on ne peut pas trouver, dans ce type de structure, d'autre relation ~ sot\] qua le sujet, ici l,uc.
II s'agit de cor~f~rence obligatoire au sujet Dens "Luci cheque L~j par sos i id~'es ~ et "Luc i cheque Leajdans sos11rig, as", il y a le verbe chequer Pourtant dens ta pre.miere phrase sos est forcement relic 8 Luc.
II s'agit d'un cas de coreference obligatoire au sujet, induit par la preposition par.
Alors qua dens la seconde, la cor(~f~rence est oblJgMoire au cempl~ment d'objet sos est reli~ ~ L~4a, ~ cause de la preposition dens 3.2 Phrases construites aulour d'un V-sUlq On trouve des expressions verbales figees et d'autres mettant enjeu la paire Vsup/Npred mais sans 6tre des expressions figees.
I.es expressions verbMes fi~es sent de la torme \[NO V Nl-poss/, construites autour de variantes aspectuelles et d'op~.rateur causatif du verbe avozr, dent le N1 est teuioum "pattie du corps" ou "abstrait" ("Luc i rettent sos i tarmes'), et dent la hansformation en gnest impossible.
Elles peuvent aussi etre completees par url troisieme argument (\[NO V N1 Pr#p N2\] avec V support ou non.
L& encore, le nom (N1 ou N2) d~tenniue pal le possessif est "pantie du corps" ou "abstrait", et aucune restructuration n'est possible: L.ucHelte son i d#volu sur L~a l.uci #erda L#a sous sa i protection Si le possessif d~termine I'objet direct dens une structure INO V Nl-poss Prep N2\], \[Prep N2\] pout 0tre remplac# par une compl~tive I'infinitive, donnant \[NO V Nl-poss Pr~p Vlnf\]: Luc i passe son i temps au travail Luc~ passe son i temps ~ travailler Daris toutes ces phrases, la cor~ference est touIours obligatoire au sujet \[.es expressions non #g#es sent construites autaur de verbes support ou de variantes aspectuelles 1NO V N%poss\] ou \[NO V Nl-poss Prep N2\]: Luci a perdu ses i illusions (~ur L~a).
On ne peut pas transformer \[NO V Npred (Prep N1)\] en \[NO V Npred de N2 (Prep N1)\] (* Luc a pe~du les illusions de Paul sur L~a).
En d'autres termes, I'argument NO de Nprep est le sujet du verbe, mais ce verbene pout prendre une expression \[Npred Pr#p NO\] comme complement (ici: los illusions de Paul).
La presence ou non du complement d'objet indirect n'a pas d'incidence sur la relation qui lie le possessif au sujet.
Ces phrases nese construisent pas avec une infinitive.
Certaines expressions non fig~es se construisent avec le possessif comma d(~terminant de N3; dens cecas, los trois arguments sent obligatoires etla cor~f~rence n'est pas obtigatoire au suiet, II s'agit de cas de norPcor~f~rence obligatoire ~ I'objet ("Lucj met Lea ~ sai4 j disposition") On pout restructurer en reliant les complements par ~tre: Lea est ~ ta disposition de (L uc, Paul) Fin resume, parmi les phrases construites autour dt t Vsup, I'adjectif possessif qu'elles contieonent n'est jamals coref~rent ~ I'objet, AcrEs DE COLING-92, NANrHS, 23-28 AO~r 1992 9 0 3 l'rtOC.
Ot; C()I.ING-92, NAN'I'ES, AUG.
23-28, 1992 mais toulours au moins cor(~f~rent a.
sujet En reconnaissant d'emblee ces phrases et teur verbe comma Vsup, on pourra r/esoudre automatiquement I'anaphore Pour cela nous proposons de reperer les autres phrases, pu~s de considerer les phrases non reconnues comme ~tant de carte categorie 33 Phrases construites autour de verbes ordinalres Elles s'articulent autour d'un verbe ~ un ou deux arguments, rNement predicatif de la phrase puisqu'il definit la structure des arguments II est determin#, par: le nombre d'arguments I'articulation syntaxique de ces arguments les traits semantiques de ces aguments Dans la st\[ucture \[NO V Nl-poss\] que ron ne peut poursuivre avec \[&/de N2\], la coreference est obligatoire aun autre nero que le sujet NO (du discours anterieur) Luc approuve son choix Luc apptouve le choix de L#a Par centre, si une phrase a deux arguments et qu'elle peut etre completee par un troJsiCme, la presence ou non de ce dernier fait varier la coref~rence, ou tout au moins la preference entre lee antecedents possibles: Luc i avoue son i depit.
Lucj avoue soni+ j d~p/t a Lea k La relation de coreference existe toujours entre le possessif et le sujet dane ~es phrases simples \[NO V NI\]; ou complet~es par \[Pr~p N2\] oQ la relation peut aussi exister entre le poss et un referent du discours ant~rieur.
n a donc la un cas de non-cor#f~rence obligatoire ~ I'objet Tous les verbes qui donnent ces r~sultats dans une telle structure appartiennent a la table 9.
Quand le syntagme prepositionnel est obligatoire, on distingue les phrases ou I'adjeetif possessif determine le N1 et celles oO il d~terrnine le N2.
Parmi tes premieres, on trouve une cer~fdrence obligatoire au sujet Iorsqu'il y a possibilit~ de pronominalisation: Luc i consacre sa i vie a ta pemture.
Luc se consacre e la peinture OU de verbalisation simple: Luc i accorde son i pardon ~ L#a Luc pardonne a L#.~ Mais pour celles dent la transformation donne une completive ~nfinitive \[nO V N1 VinfJ, la coref~rence est obligatolre ~ I'ebjet: Luc i motive L~ajdans son 1 travail Luc motive Lea g~ travailter Tous ces verbes appartiennent a la table 11 Parmi les phrases de structure \[NO V NI Pr~p N2-poss\], la relation est obligatoire au sujet, ou obIigatoire ~ t'objet Dans les exemples suivants (verbe de la table 4), oQ la 1estructuration est possible en IN2 de NO V N1\], la relation est etablie entre le possessif et le sujet, Iorsque ta preposition est par ou avec: Luc i cheque L&aj paffavec see i idles.
Lea Ides de Luc choquent Lea.
Pa~ centre, la mC~me phrase avec clans ou pout; par exemple, donne \[a transformation \[NO V N2 de N1\], et on etablit alors la relation entre le possessif et le ~:ompldment d'ebjet: LUC I cheque Lea\] darts.~esj Ideas.
Luc cheque tee id#es de L&a.
4.Architecture .qdndralle du s.ystdme Notre systeme se compose donc d'un analyseur sfntaxirlue qui donne des arbres & paftir desquels un analyseur s~mantique produit des ORS.
C'est sur etles qu'oNrera le piogramme de ~euolution anaphorique Ce systerne se complete d'url fichier de verbes par tables, et d'un fichier "fonetionnel", constitue ~ mesure de I'arlalyse semantique, ou sent stockes tousles norns et pronoms, et leur forlction grammaticale.
La procedure de r~solution: apres reperage des pronoms, commence par une recherche des verbee (A chacun est associ~ un trait pour sa table d'appartenance) et de leur structure, dans le fichier lexique-grammaire.
Si cela est trouve, on cherche sila coreference est obligatoire.
Si oui, le traiternent est termine.
Sinon, il faut activer d'autres filtres syntaxicos~mantiques: en partant des listes ordonn~es de pronoms, univers et conditions, on verifie la compatibilite de fonction 4, de genre et de hombre, semantique 5.
II faut parfois chercher te verbe darts plusieurs tables.
Si I'identite de structure entre le texte etles fables n'est pas ~tabtie, on examine I'entuurage substantival du verbe si robjet est concret, il taut activer tes autres filtres.
S'il est abstrait ou "partie du corps", on a affaire ~ une phrase a verbe suppolt (peut-t~tle figee) dont le statut induit la (non) coreference obtigatoire.
&ConclUsion et perspectives \[~emarques sur Des travaux: 4 Un candidat sulet est pr~f~e aun autre pour tre reli~ & un pronom suJet, dans deux phrases dtes paralleles 5 Des traits semanhques sent associes aux roots lexicaux ACRES I)E C()LIN(;-92, NAm'~s, 23-28 aot~rr 1992 9 0 4 Pr~oc.
oJ; COLfNG-92, NANtES, AUrJ.
23-28, 1992 -lous les ver'~;t; donl~uHt lieH ~'~ Hn type de construction partmulier, appadiormuHt ~'~ k~ ~me table.
Sur chaque table on p~;\[IL contraindre la relation de (no~ 0 coiet~rence entre le possessif et Hn argurnent du w~'ibe.
..Tous les uornpl6ments d'obje/ d6termine par le possessif sent "abstraits" uu "pattie du colps".
Avec d'aubes 1~ulns, nun pr~dicatifs, les verbes sotR ordinailt;s et on ne peut resoudre I'ambiguRe anapholique Done en ajoutant ces caract~ristiques dabs la table en question on peat resoudrc, automatiquement I'anaptiofe Ccci ~)11i/iHi~: I'hypath~se que routes les tables peuvent, a priori, tre uinsi complete;as par les sp~.cillcites liees a remploi d'adjectif..
possessifs et permethe ainsi aH syst~me d'viter d'autres filtres plus co0teHx en calClll et pas toujours fiables.
I1 taut done ~.tablir HI~ lexique-grammaim des ve~bes anaphorique'., (pris darts une structure mettant en jeLl till pronom ou, ici, tin adjectif ~x~ssessif) Darts la table 4 du lexiquegran.nu.e, pal exemple, nous repaltissons leS verbes erl: roul~.~__J_: verL~+par (ou avec et pa~ft~isd~;), et cor~f~mnce obligatoite au a~jet (NO): \[ uc i d,~prime L g, aj par son i attilu&e.
r u.q.E~_~e~: verbe+darts, e.t co~ef~len(;u obligatoire ~ I'objet: \[.uc i d~lonne l.g~j dg~tla ~esj propo~.
NOUS avons ajot~te A la table 1me colol,le concernant la pr6sence ou non de #)t~p N2\], divisee en deux colorines: les deux cas,3u cor~ference obligatoile.
Les verbes de la table 11 suHi ~;partis an: ~: V N1 darts \[NO V NI & VOinfl puut etre remplace par pronomi~-~#sation (hi verbe Le possessif est tore.~.ment coref~lent au sujer Luc consacre se vie (e d~.ssme:) au d~s~it~ Luc se consacre (,~ dessJne 0 au dessin Si N1 n'est pas abstrait la pior~t~mirlalisation est imF~ssible:(*Luc con~a:m son hao#~ au dessm): Grou~: M6rne structt.e sans prouontina.lisation.
II peut y avoir simple ve~r-b~l/saiJ~a~ L.e possessit est toujol,s cor~f6rc'r~t au soj~ N1 est toujours abst~ait et pemiet M verbalisation: Luc appolle SOn,sogtie~l ~ c(flh ~.
',dfa#O Luc ~outJent celia ~nb6"prl~e; ~: Verbes qHi, clans la tlarlsknmatiu1~ de \[NO V N1 &/pour VOilffJ eH \[NO V Nf ~/dans N2\] oQ N2 est d,~terniin,} par un poss~ssif, induisent obligatoirement une I t ~~t.,!~fin Ittk; I.(~:? pou; #av~#liu'~ Nuuu avo~ls &t~;Hdll t;~tke tabk; (a~ y ajoutaHt ulle CLtla(..tk')if, ti(\]iit;i CXi'G\[t\](I(;(!
~)11 111011 d'Llll(; Stlucttll~; lllOllOlllilal(~, .~:;tl\[}~iJvik::k'~e {;11 110i14, pol_~r k;,,,,';|ltli;klli~tu i\]hrastiq111:,-,; (,~hldi(~e~ (;t I'oblig~ioil (:lt~ colOi:Or<~i .L, qtli k'H~ c(~lll.~,'.po|ld \[(:Ab',l ~ \[\]~q JCAFd ~,()1\11<l !, I-~ I~ROWN ~_~lq~//h,:v~\] Re.~o!~jti_u_t;: q mul(V :gha2?g~ A#pl~?:~uIE ( .:131 IN(;, l }uduHa.-~t.
AI tj 1988 \[CI IAN,L~TJ \].1 "4 I/\Nll !R, !C,! il All\J, (; SEDf)(;I t ) Ih~, A_~rc,.hu Uni~eyL't~ .SVq/~xe~ ~21 rift M,Mrn_a-f~J~Ll(~J!.{:c;,9 6s,'IFC(k I, Sofia Antipoti~a Nov 1987 \[I)ANL 89\] L,DANL(:),% Rep!~se.~jt~#90.~: d','ni.fm~#i~p~; huguL.4igU~L.~.
COf*St~O*~Ql*S /V t}~rt; t/*e~'p X.
I |10.~,t) de 3~;I m~ cycle, LADI Pail! 7, !i\]\[;U JGt I/~, 91} HGFAAIrq these de ductorat (~'~ p:aHii,t~) I ~aH.'; /, 199>' \[~z,t ~()~ 89\] (:; (~Ro,~.~; lug~t~J~.~ammair~_:.
!AUI.,::L Lh,v.
Paris; ? S\[fMANTICA f>afi:-: Juin igB.q \[GROS /sJ M GhtOSS M~t!ode.u.~jL~A/~{~x P.
L::d.{ k;in|arln, i'ari-u 1!)\[,*; \[k-Jk ILiN 85\] F.GI IENTI tNLR, P,'4ABAIli \]-~ ser~{qtjgp.
F:NS, I )nivel-sit0 t tlbingeu I )(.'-u 1985 \[GIlIN 85\] R GIJIN\[)ON Focusing.
MCC Austin AC.I.i.'185 \[Gt I1\[ 81\] A.Gt IILI.EI ~, CLECI,ERIZ f 3\[n~s.
~ e tpj~_ff;\]&; :;S3ff_(tntiq ~!.'..
\[.angages n"63.
Ed I alottsse Paris "H;WI \[I(AX;t > 8.5\] W KASPER Dtscourse R~#/es~/!t~liun 7hec~E.
Rapport ACORD, Ur~iv.Stuttgart, Mai 1986 \[IdCtl 88\] ERICH, S.LUPERFC>Y tir~ A.
MCC A u sti n, A.
';. l..
f#,v 1988 \[t;I \[)O 871 C SEI )( )GB( ) SEatet~le r~ueJ_.
~OJL-U~t2g)!!s~{. Thbse de DooLorat d'Etat \[J ive~sit#, dr; Ma~seille, 1:)87 \[VlVl/83J I VIVES ::~}rne Cyck; IAI)I Pad:; 7 1983 \[WAI)A 811 tt WAI\]A, N.AsI let4 /~e~o/(~hlo q t hfivn.~.ity Texas, Austin, .l~m 1.987 References 1 Jaime G.
Carbonell, Ralf D.
Brown, Anaphora resolution: a multi-strategy approach, Proceedings of the 12th conference on Computational linguistics, p.96-101, August 22-27, 1988, Budapest, Hungry 2 {CHAN 87} T.
CHAWIER, B.
GELAIN, C.
SEDOGBO Une Approcho Unifice de le Syntaxe et de la Smantigue.
Congios AFCET, Sofia Antipolis Nov 1987 3 {DANL 80} L.
DANLOS Reprsentations d'informations linguistigues, constructions N ire prep X.
These de 3eme cycle, LADL, Paris 7, 1980 4 {GFLA 91} B.
GELAIN Thse de doctorat ( paratre) Paris 7, 1992 5 {GROS 89} G.
GROSS Dsanbiguisation smantique  l'aido d'un lexique grammaire.
LADL et Univ.
Paris 7.
SEMANTICA. Paris Juin 1989 6 {GROS 75} M.
GROSS Mthodes en syntaxe.
Ed. Hermann, Paris 1975 7 {GUEN 85} F.
GUENTHNER, P.
SABATIER Formal Semantics and Knowledge Representation.
FNS, Univ.
Tubingen. Dc 1985 8 {GUIN 85} R.
GUINDON Anaphora Resolution: Short-Term Memory and Focusing.
MCC Austin.
A.C.I., 1985 9 {GUIL 81} A.
GUILLET, C.
LECLERE Formes syntaxiques et prdicats smantiques.
Languages n"63.
Ed Laroussa.
Paris 1981 10 {KASP 85} W.
KASPER Montague Grammar, Situation semantics and Discourse Representation Theory.
Rapport ACORD, Univ.
Stuttgart, Mai 1986 11 {RICH 88} E.
RICH, S.
LUPERFOY An Architeture Program for Anaphora Resolution.
MCC Austin.
A.C.I., fv 1988 12 {SEDO 87} C.
SEDOGBO De la Grammaire en Chaine du Franais  on Systme Question-Rponse.
Thse de Doctorat d'Etat.
Univ. de Marseille, 1987 13 {VIVE 83} R.
VIVES Avoir, prendre, perdre: constructions  verbe support et extensions aspactuelles.
Thse de 3me cycle.
LADI. Paris 7 1983 14 {WADA 87} H.
WADA, N.
ASHER A Computational Account of Syntactic, Semantic and Discourse Principles for Anaphora Resolution.
University Texas, Austin, Jan 1987
Parole et traduction automatique : le module de reconnaissance R A P H A E L
Mohammad AKBAR GEOD, CLIPS/IMAG Universitd Joseph Fourier, BP. 53 38041 Grenoble cedex 9, France Mohammad.Akbar@imag. fr Jean CAELEN GEOD, CLIPS/IMAG Universitd Joseph Fourier, BP. 53 38041 Grenoble cedex 9, France Jean.Caelen@imag.fr

R6sum6

Pour la traduction de parole, il est ndcessaire de disposer d'un syst~me de reconnaissance de la parole spontande grand vocabulaire, tournant en temps rdel. Le module RAPHAEL a dtd congu sur la plateforme logicielle de JANUS-III ddveloppde au laboratoire ISL (Interactive Systems Laboratory) des universitds Karlsruhe et Carnegie Mellon. Le corpus BREF-80 (textes lus extraits du Journal Le Monde) a 6td utilis6 pour le ddveloppement, l'apprentissage et l'dvaluation du module. Les rdsultats obtenus sont de l'ordre de 91% de bonne reconnaissance de mots. L'article ddcrit l'architecture du module de reconnaissance et son int6gration /~ un module de traduction automatique.
Introduction

La traduction des documents dcrits a fait de rdels progrbs pendant ces dcrni6res anndes. Nous pouvons constater l'6mergence de nouveaux syst6mes de traduction de textes qui proposent une traduction soignde en diffdrentes I Synthbse ~ de la parole ] ~

langues[1]. I1 semble envisageable de les adapter pour la traduction de l'oral, /t condition d'en amdliorer le temps de rdponse et la robustesse : c'est le <<challenge >7 pos6 fi ces systbmes mais aussi au module de reconnaissance de la parole. Un syst6me de traduction de l'oral repose sur l'intdgration des modules de reconnaissance et de synth6se de la parole et des modules de traduction, pour obtenir une boucle complbte d'analyse et de synth6se entre les deux interlocuteurs [Fig. 1]. Le projet CSTAR-II [3] est un projet international dans lequel toutes les dquipes travaillent sur tousles aspects de ce mod61e. Pour permettre /t deux personnes de communiquer, il faut deux sdries de processus symdtriques dans les deux langues : un module dc reconnaissance pour acqudrir et transcrire les dnoncds dits par un locuteur dans sa langue puis un module de traduction qui traduit la transcription dans la langue du destinateur ou dans un format d'dchange standard (IF = Interchange Format) et enfin un module de synth6se de la parole (et de gdndration si on utilise le format IF) dans la langue cible du

rReconnaissance Traduction la instantan_._____~ ) ~ ,___~de parole ._J

!/
(Reconnaissance'~_~ T r a d u c t i o n ' ~ ~ ~. instantan6 __) ~. de la p a r o l e )
Fig. 1. L'architecture d'un syst~me de traduction instantan~e.

36

destinateur. Dans le cadre du projet C-STAR II nous avons en charge la conception et la rdalisation du module de reconnaissance de la parole continue h grand vocabulaire pour le fiangais. Nous collaborons avec l'6quipe GETA du laboratoire CLIPS-IMAG et le laboratoire LATL pour la traduction automatique et le laboratoire LAIP pour la synth~se de la parole. Ce consortium s'est fix6 l'objeetif de r6aliser un syst6me de traduction de I'oral pour le frangais. Dans cet article nous allons tout d'abord presenter l'architecture du syst6me de traduction et la plate-forme de ddveloppement JANUS-III [2], puis les diff6rentes 6tapes du d6veloppement du module RAPHAEL et enfin, les premiers rdsultats obtenus. 1 R A P H A E L p o u r la T r a d u e t i o n L'architecture du syst6me de traduction de parole est compos6e de trois modules essentiels (la reconnaissance, la traduction et la synth~se de la parole) [Fig. 2]. Dans ee projet nous utilisons ARIANE et GB [3] pour la traduction ct LAIP-TTS [4] pour la synth6se. Le

point de vue de la robustesse) nous envisageons l'intdgration d'une seconde couche de contr61e pour permettre le <<rescoring >> des hypoth6ses en tenant compte des taux de confiance associds aux diff6rents mots de l'dnoncd reconnu.

1.1

Plate-forme de J A N U S I l l

I

Recolmaissance la Parole ] de RAPIIAEL(CLIPS/IMAG-ISL) Texte
V

Contr61e

Cette plate-forme de traduction a dtd ddvelopp6e dans le laboratoire d'ISL des universitds Carnegie Mellon et Karlsruhe et contient tous les composants ndcessaires au ddveloppement d'un syst6me de reconnaissance phondmique/t grand vocabulaire h base de Cha~nes de Markov Cach6es (CMC) et de rdseaux de neurones. La facilitd d'dcrire un module de reconnaissance en langage Tcl/Tk avec JANUS-III nous permet d'adapter ses capacitds selon les besoins d'application et les caractdristiques du frangais. De cette plate-forme, seul le moteur de reconnaissance est directement exploit& Mais le travail de pr6paration des bases de donndes, l'apprentissage des mod6les de phon6mes, l'dvaluation sont dgalement effectuds dans cet environnement de programmation. Le langage PERL est en grand partie utilisd parall61ement pour traitement du texte du corpus. Les d6tails techniques de JANUS-III sont donnds dans [2], [5], [6]. Cependant nous en prdsentons bri6vement quelques points ci-apr6s. 2 Le Module RAPHAEL L'architecture du module de reconnaissance RAPHAEL est pr6sent6e sur la [Fig. 3]. L'analyse de la parole produit une suite de vecteurs de param6tres acoustiques. Ces vecteurs sont utilis6s par un moteur de recherche base de CMC pour estimer la suite des phon6mes 6nonc6s. Un mod61e de langage stochastique h bigramme et trigramme, et un dictionnaire des variantes phon6tiques sont en parall61e exploit6s pour restreindre le champ de recherche I. ALl cours de la recherche le dictionnaire phon6tique fournit le(s) phon6me(s) suivant(s). Le modble probabiliste de langage base de bigramme et de trigramme est utilis6 Iors de la transition entre deux mots pour fournir un ensemble de roots [Fig. 4]. 1 Avec 45 phon6mes en moyenne une suite de cinq phon6mes se transforme th6oriquement en un arbre de d6cision de 455 = 184,528,125 feuilles ! 37

~_~Traduction Automatique IANE(GETA),GB(I,ATL) J


I Synth~se la Parole de LA1P-~VI'S (LAII') 1 Fig.2. Lescomposantsdu syst~me
d6veloppement du module de reconnaissance RAPHAEL a dt6 effectu6 sur la plate-forme iogicielle de JANUS-Ill. RAPHAEL donne en sortie un treillis de roots sous le protocole TCP/IP. Le traducteur utilise ce rdsultat pour en donner une version traduite. Cette version est ensuite envoyde au synthdtiseur de la parole. Dans cet article nous nous int6resserons seulement au module de reconnaissance RAPHAEL. Pour l'instant la stratdgie d'dchange entre les modules est enti6rement sdquentielle. Afin d'amdliorer le rdsultat final (surtout du

~_~__~ Acquisitionde 1 la parole

I,

Base de donndesdes paramdtres ] des ChaTnesde Markov Cachdes

J
~Z%%? ' o

I Traitementnumdrique,Estimationdes param~tresacoustiques ~ ModUlestochastiquede langage (bigrammeet trigramme)

Chainesde MarkovCachdespour la reconnaissancephondmique Dictionnairephondtique Dict (vocabulaire de reconnaissance)

Fig. 3. Sehdmadu modulede reconnaissance phondmiqueRAPHAEL.
2.1 C
h a i n e d e M a r k o v Cachdes cet alignement l'algorithme de Baum-Welch [5] procdde ~ l'estimation des paramdtres de chaque CMC prdsente dans la cha~ne. Ce procddd est rdpdt6 pour tous les 6noncds du corpus d'apprentissage et cela plusieurs fois. La prdsence des diffdrents contextes phondmiques permet /i ce procdd6 de minimiser le taux d'erreur de reconnaissance. L'dvaluation du taux d'erreur /l la fin de chaque itdration permet d'dtudier l'avancement de l'apprentissage.

Pour utiliser les CMC il faut conduire une phase d'apprentissage prdalable dans laquelle on adapte les probabilitds des transitions et des symboles sortis pour un phondme donnd de manidre ii ce que la probabilit6 du processus associd soit maximale. Les paramdtres des moddles et la transcription phondtique des 6noncds du corpus sont deux 616ments essentiels d'apprentissage. RAPHAEL comporte 45 CMC reprdsentant 42 phondmes de base du fran~ais et 3 moddles pour le silence et le bruit. A quelques exceptions prds les CMC se composent de trois 6tats. Le vecteur de paramdtres d'entrde est de dimension 122. Les CMC ont 16 distributions Gaussiennes pour chaque dtat. Lots de l'apprentissage nous produisons la transcription phondtique correspondante chaque dnoncd (cela se fait ~ l'aide du dictiomaaire phondtique). Pour chaque 6nonc~ les CMC correspondant aux phondmes sont concatdndes pour crder une longue chaine. Ensuite i'algorithme de Viterbi [5] propose un alignement de I'dnoncd avec cette chaine. Avec

2.2

Mod/~lede iangage stochastique

2 Les
coefficients MFCC [5] d'ordre 16 sont calculds sur une trame de 16 ms de parole, avec un pas d'avancement de 10ms. La parole est 6chantillonnde /l 16 kHz et sur 16 bits. Les MFCC, l'dnergie du signal, et leurs premiere et seconde ddrivdes (51 valeurs) subissent ensuite une analyse en composantes principales (ACP) pour rdduire la dimension du vecteur /l 12. La matrice d'ACP est calculde avant la phase d'apprentissage, sur un grand corpus enregistrd. 38

Afin de rdduire le champ de recherche, un moddle de langage doit ~tre utilisd. Bien que dans les systdmes /i commande vocale qui utilisent une syntaxe rdduite les grammaires finies ou rdcurrentes peuvent ~tre utilisdes, ceiles-ci ne sont pas capables de ddcrire tousles phdnomdnes de la langue par[de (ellipses, hdsitations, rdpdtitions, etc.). Pour cette raison il est souhaitable d'utiliser un moddle stochastique qui estime dans un contexte donnd, la probabilit6 de succession des mots. Dans le moddle actuel les contextes gauches d'ordres un et deux (bigramme et trigramme) Sont en marne temps exploitds. Le bigramme est utilisd dans la premidre phase de recherche pour crder un treillis de mots, puis le trigramme est utilisd pour raffiner le rdsultat et ddterminer les N meilleurs phrases plausibles. Le moddle de langage se charge en m~me temps de ia rdsolution de l'aceord en frangais. Le calcui des paramdtres de ce moddle a dt6 effectual /t partir des corpus enregistrds et transcrits. Dans l'6tat actuel un vocabulaire de 7000 mots a dt6 sdlectionnd.

-..

-

L'hypoth6se e mot #1

--:~,.

L'hypoth6se de mot #2

Repr6sentation d'un phoneme

Dans un mot le dictionnaire phon6tique est utilis6 pour trouver et enchMner les phonemes suivants selon les variantes phon6tiques disponibles.

Pour d6terminer les roots et les phon6mes suivants le module stochastique du langage et le vocabulairc transcrit en phon6tique sont en m6me temps utilis6s.

Fig. 4. Representation de I'algorithme de recherche

2.3 Dictionnaire
Phon~tique
La conversion d'une chMne d'hypoth~ses phon6tiques en une chMne orthographique se fair ~t partir d'un dictionnaire phon6tique. Pour couvrir un grand nombre de prononciations diff&entes dues aux diff&ents dialectes de la langue et aux habitudes des locuteurs, ce dictionnaire contient pour chaque mot un ensemble de variantes phon6tiques. A chaque hypoth6se de mot propos6 par le mod61e de langage on associe cet ensemble de variantes. Ind6pendamment donc de la variante utilis6e dans 1'6nonc6, nous obtenons la m6me transcription orthographique. Nous utilisons sp6cifiquement cette technique pour couvrir les variantes produites par ia liaison, par exemple : Je suis parti de la maison. Je suis all~ h la maison. 3 (Z& sHi paRti ...) (Z& sHiz ale ...)

ensemble de BREF-80 comprenant les 6nonc6s de 4 femmes et 4 hommes a 6t6 utilis6 pour l'6valuation 4. Le vocabulaire a 6t6 transcrit soit manuellement, soit ~ partir du dictionnaire phon6tique BDLEX-23000. Le module de langage a 6t6 estim6 fi partir de BREF-80 et un corpus de texte d'~ peu pr6s 10 millions de mots extrait du journal Le Monde. Pour l'initialisation des CMC, au lieu d'utiliser les valeurs al6atoires (technique habituelle), nous avons choisi d'utiliser les mod61es issus du projet GlobalPhone [7]. Pour chaque phon6me de notre module nous avons manuellement choisi un phon6me dans une des langues support6es par GlobalPhone (principalement allemande) et nous avons utilis6 ses param6tres comme valeurs initiales de nos CMC. Ensuite ces mod61es ont 6t6 adapt6s au frangais au moyen de l'algorithme d'apprentissage d6crit en 2.1. A la fin de chaque it6ration et ce pour 3

L'apprentissage
4 Les
sous-corpus de l'apprentissage et de l'6valuation n'ont aucun 6nonc6 et locuteur en commun. En r6alit6, nous avons enlev6 tousles 6nonc6s en communs entre ces deux sous corpus. Ainsi le sous-corpus d'apprentissage comprend 4854 6nonc6s et le sous-corpus d'6valuation 371 6nonc6s. Nous avons retir6 105 6noncds pour assurer la disjonction des deux sous-corpus. 39

Le corpus BREF-80 [8] comportant 5330 6nonc6s par 80 locuteurs (44 femmes et 36 hommes) 3 a 6t6 utilis6 pour les phases d'apprentissage et d'6valuation. Un sous3 BREF-80 contient 3747 textes diff6rents et environ 150,000 mots.

it6rations, le syst~me a 6t~ 6valu6 avec le sous corpus de l'6valuation.

4 R6sultats
Les r6sultats d'dvaluation en terme de taux de reconnaissance sont donn6s dans le [Tableau 1]. Systdmes M0dbles issus de GlobalPhone Premiere it6ration Troisi6me it6ration % mots reconnus 29 88,8 91,1

Tableau 1. Les r6sultats de 1'6valuation

4.1

Commentaires

obtenus. Notre but est d'am61iorer le taux de reconnaissance par l'utilisation des mod61es phon6tiques contextuels et d'61argir le vocabulaire utilis6/t plus de 10000 mots. Pour atteindre ce but nous allons sp6cialiser le vocabulaire dans le domaine du tourisme et utiliser d'autres corpus de la parole spontan6e dans ce domaine avec un nombre plus important de locuteurs. En mfime temps nous d6finirons un protocole d'6change plus 61abor6 avec le module de traduction afin de permettre la communication d'informations linguistiques et statistiques au module de traduction, toujour dans le but d'amdliorer les performances de notre syst6me.

Une tr6s bonne initialisation de certaines consonnes identiques dans des diff6rentes langues (p, t, k, b, d, g, etc.) a rapidement permis d'obtenir un syst6me fonctionnel. On constate une saturation tr6s rapide du taux de reconnaissance d6s la troisi+me it6ration. Nous pouvons distinguer trois types de probl6me qui nous empachent d'atteindre un meilleur taux de reconnaissance :  Fautes de frappe dans le texte du corpus,  Transcription erron6e ou insuffisamment d6taill6e des ~noncds, ,, La couverture partielle de toutes les variantes phon6tiques d'un mot. Ces trois probl~mes sont les causes d'un grand nombre d'erreurs d'alignement qui vont directement influencer le r6sultat final. Nous devons donc effectuer une v6rification compl6te du corpus et du dictionnaire phon6tique. Les mots hors du vocabulaire sont fi l'origine d'un pourcentage important d'erreurs. En effet, dans 371 6noncds du sous-corpus de l'6valuation nous rencontrons environ 300 mots hors vocabulaire. Ces mots repr6sentent environ 3,5 % de la taille du vocabulaire. I I n e sont pas reprdsentds dans le corpus d'apprentissage et leur transcription n'existe pas dans le dictionnaire phon6tique.

Remerciement
Nous remercions Alex Waibel pour la mise /t disposition de JANUS-III et Tanja Schultz pour son support scientifique et technique dans l'utilisation des r6sultats du projet GlobalPhone.

References

1 Hutchins
W. J. (1986) Machine Translation : Past, Present, Future. Ellis Horwood, John Wiley & Sons, Chichester, England, 382 p. 

2 Finke
M., Geutner P., Hild H., Kemp T., Ries K., Westphal M. (1997) : The KarlsruheVerbmobil Speech Recognition Engine, Proc. of ICASSP, Munich, Germany. 
																		
3 Boitet
Ch., (1986) GETA's MTmethodology and a blueprint for its adaptation to speech translation within C-STARI1, ATR International Workshop on Speech Translation, Kyoto, Japan. 

4 Keller, E. (1997). Simplification of TTS architecture versus Operational quality, Proceedings of EuroSpeech'97, Rhodes, Greece. 

5 Rabiner
L., Juang B.H. (1993), Fundamentals of Speech Recognition, Prentice Hall, 507 p. 

6 Haton
J.P., Pierrel J.M., Perennou G., Caelen J., Gauvain J.L. (1991), Reconnaissance automatique de laparole, BORDAS, Paris, 239 p. 

Schultz T. Waibel A., Fast Bootstrapping of LVCSR systems with multilingual phonem sets, Proceedings of EuroSpeech'97, Rhodes, Greece. 

Lamel L.F., Gauvain J.L., Eskenazi M. (1991), BREF, a Large Vocabulary Spoken Colpus for French, Proceedings of. EuroSpeech'91, Genoa, Italy.
References 1 Appelt D.
and Israel D.
(1999). Introduction to Information Extraction Technology.
(IJCAI-99) Tutorial, Stockholm, Sweden (available at: http://www.ai.sri.
com/~appelt/ie-tutorial/) 2 Masayuki Asahara, Yuji Matsumoto, Extended models and tools for high-performance part-of-speech tagger, Proceedings of the 18th conference on Computational linguistics, p.21-27, July 31-August 04, 2000, Saarbrcken, Germany 3 Frdric Bchet, Alexis Nasr, Franck Genet, Tagging unknown proper names using decision trees, Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, p.77-84, October 03-06, 2000, Hong Kong 4 Daniel M.
Bikel, Scott Miller, Richard Schwartz, Ralph Weischedel, Nymble: a high-performance learning name-finder, Proceedings of the fifth conference on Applied natural language processing, p.194-201, March 31-April 03, 1997, Washington, DC 5 Andrew Eliot Borthwick, Ralph Grishman, A maximum entropy approach to named entity recognition, 1999 6 Collins M.
and Singer Y.
(1999) Unsupervised models for named entity classification.
In Proceedings of EMNLP/WVLC, 1999, MA, pp.
189--196. 7 Cucchiarelli A.
and Velardi P.
(1999) Adaptability of linguistic resources to new domains: an experiment with proper noun dictionaries.
In Proceedings of the Vextal Conference, Venice, Italy, pp.
25--30. 8 Andrei Mikheev, Marc Moens, Claire Grover, Named Entity recognition without gazetteers, Proceedings of the ninth conference on European chapter of the Association for Computational Linguistics, June 08-12, 1999, Bergen, Norway 9 Raymond J.
Mooney, Induction Over the Unexplained: Using Overly-General Domain Theories to Aid Concept Learning, Machine Learning, v.10 n.1, p.79-110, Jan.
1993 10 MUC-6 (1995) Proceedings of the Sixth Message Understanding Conference (DARPA), Morgan Kaufmann Publishers, San Francisco.
11 Poibeau
T and Kosseim L.
(2001) Proper-name Extraction from Non-Journalistic Texts.
Proceeding of the 11th Conference Computational Linguistics in the Netherlands, Tilburg.
Netherlands, Rodopi.
12 Satoshi
Sekine, Yoshio Eriguchi, Japanese named entity extraction evaluation: analysis of results, Proceedings of the 18th conference on Computational linguistics, July 31-August 04, 2000, Saarbrcken, Germany 13 Silberztein M.
(1993) Dictionnaires lectroniques.
Masson, Paris.
14 David
Yarowsky, Unsupervised word sense disambiguation rivaling supervised methods, Proceedings of the 33rd annual meeting on Association for Computational Linguistics, p.189-196, June 26-30, 1995, Cambridge, Massachusetts
References 1 Benson, M.
(1990). Collocations and general-purpose dictionaries.
International Journal of Lexicography, 3(1), 23--35.
2 Peter
F.
Brown, Jennifer C.
Lai, Robert L.
Mercer, Aligning sentences in parallel corpora, Proceedings of the 29th annual meeting on Association for Computational Linguistics, p.169-176, June 18-21, 1991, Berkeley, California 3 Catizone R., Russell G., and Warwick S.
(1989). Deriving Translation Data from Bilingual Texts.
In Proceedings of the First International Lexical Acquisition Workshop, Detroit.
4 Church, K., Gale, W., Hanks, P., and Hindle, D.
(1991). Using Statistics in Lexical Analysis.
In Zernick, U.
(ed.), Lexical Acquisition: Exploiting On-Line Resources to Build a Lexicon, Lawrence Erlbaum Associates, pp.
115--164. 5 Ted Dunning, Accurate methods for the statistics of surprise and coincidence, Computational Linguistics, v.19 n.1, March 1993 6 William A.
Gale, Kenneth W.
Church, A program for aligning sentences in bilingual corpora, Computational Linguistics, v.19 n.1, March 1993 7 Gross, G.
(1996). Les expressions figes en franais.
OPHRYS, Paris.
8 Pierre
Isabelle, Marc Dymetman, George Foster, Jean-Marc Jutras, Elliott Macklovitch, Francois Perrault, Xiaobo Ren, Michel Simard, Translation analysis and translation automation, Proceedings of the 1993 conference of the Centre for Advanced Studies on Collaborative research: distributed computing, October 24-28, 1993, Toronto, Ontario, Canada 9 Laenzlinger, C.
and Wehrli, E.
(1991). Fips, un analyseur interactif pour le franais.
TA informations, 32(2): 35--49.
10 Christopher
D.
Manning, Hinrich Schtze, Foundations of statistical natural language processing, MIT Press, Cambridge, MA, 1999 11 Melby A.
(1982). A Bilingual Concordance System and its Use in Linguistic Studies.
In Proceedings of the Eighth LACUS Forum, Columbia, SC, pp.
541--549. 12 Romary L.
and Bonhomme P.
(2000). Parallel alignment of structured documents.
Vronis J.
(Ed.). Parallel Text Processing.
Dordrecht: Kluwer.
13 Michel
Simard, George F.
Foster, Pierre Isabelle, Using cognates to align sentences in bilingual corpora, Proceedings of the 1993 conference of the Centre for Advanced Studies on Collaborative research: distributed computing, October 24-28, 1993, Toronto, Ontario, Canada 14 Frank Smadja, Retrieving collocations from text: Xtract, Computational Linguistics, v.19 n.1, March 1993 15 Eric Wehrli, Parsing and Collocations, Proceedings of the Second International Conference on Natural Language Processing, p.272-282, June 02-04, 2000
Classifying Biological Full-Text Articles for Multi-Database Curation Wen-Juan Hou, Chih Lee and Hsin-Hsi Chen Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan {wjhou, clee}@nlg.csie.ntu.edu.tw; hhchen@csie.ntu.edu.tw Abstract In this paper, we propose an approach for identifying curatable articles from a large document set.
This system considers three parts of an article (title and abstract, MeSH terms, and captions) as its three individual representations and utilizes two domain-specific resources (UMLS and a tumor name list) to reveal the deep knowledge contained in the article.
An SVM classifier is trained and cross-validation is employed to find the best combination of representations.
The experimental results show overall high performance.
Track (http://ir.ohsu.edu/genomics) of TREC 2004 and 2005 organized categorization tasks.
The former focused on simplified GO terms while the latter included the triage for "tumor biology", "embryologic gene expression", "alleles of mutant phenotypes" and "GO" articles.
The increase of the numbers of participants at Genomics Track shows that biological classification problems attracted much attention.
This paper employs the domain-specific knowledge and knowledge learned from full-text articles to classify biological text.
Given a collection of articles, various methods are explored to extract features to represent a document.
We use the experimental data provided by the TREC 2005 Genomics Track to evaluate different methods.
The rest of this paper is organized as follows.
Section 2 sketches the overview of the system architecture.
Section 3 specifies the test bed used to evaluate the proposed methods.
The details of the proposed system are explained in Section 4.
The experimental results are shown and discussed in Section 5.
Finally, we make conclusions and present some further work.
1 Introduction
Organism databases play a crucial role in genomic and proteomic research.
It stores the up-to-date profile of each gene of the species interested.
For example, the Mouse Genome Database (MGD) provides essential integration of experimental knowledge for the mouse system with information annotated from both literature and online sources (Bult et al., 2004).
To provide biomedical scientists with easy access to complete and accurate information, curators have to constantly update databases with new information.
With the rapidly growing rate of publication, it is impossible for curators to read every published article.
Since fully automated curation systems have not met the strict requirement of high accuracy and recall, database curators still have to read some (if not all) of the articles sent to them.
Therefore, it will be very helpful if a classification system can correctly identify the curatable or relevant articles in a large number of biological articles.
Recently, several attempts have been made to classify documents from biomedical domain (Hirschman et al., 2002).
Couto et al.(2004) used the information extracted from related web resources to classify biomedical literature.
Hou et al.(2005) used the reference corpus to help classifying gene annotation.
The Genomics System Overview Figure 1 shows the overall architecture of the proposed system.
At first, we preprocess each training article, and divide it into three parts, including (1) title and abstract, (2) MeSH terms assigned to this article, and (3) captions of figures and tables.
They are denoted as "Abstract", "MeSH", and "Caption" in this paper, respectively.
Each part is considered as a representation of an article.
With the help of domain-specific knowledge, we obtain more detail representations of an article.
In the model selection phase, we perform feature ranking on each representation of an article and employ cross-validation to determine the number of features to be kept.
Moreover, we use cross-validation to obtain the best combination of all the representations.
Finally, a support vector machine (SVM) (Vapnik, 1995; Hsu et al., 2003) classifier is obtained.
Abstract AbsSEM/TM Full-Text Training Articles Preprocessing MeSH MeSHSEM Model Selection Caption CapSEM/TM Domain-Specific Knowledge A New Full-Text Article Preprocessing Multiple Parts PartsSEM/TM SVM Classifier Yes/No Figure 1.
System Architecture Experimental Data We train classifiers for classifying biomedical articles on the Categorization Task of the TREC 2005 Genomics Track.
The task uses data from the Mouse Genome Informatics (MGI) system (http://www.informatics.jax.org/) for four categorization tasks, including tumor biology, embryologic gene expression, alleles of mutant phenotypes and GO annotation.
Given a document and a category, we have to identify whether it is relevant to the given category.
The document set consists of some full-text data obtained from three journals, i.e., Journal of Biological Chemistry, Journal of Cell Biology and Proceedings of the National Academy of Science in 2002 and 2003.
There are 5,837 training documents and 6,043 testing documents.
Methods Document Preprocessing In the preprocessing phase, we perform acronym expansion on the articles, remove the remaining tags from the articles and extract three parts of interest from each article.
Abbreviations are often used to replace long terms in writing articles, but it is possible that several long terms share the same short form, especially for gene/protein names.
To avoid ambiguity and enhance clarity, the acronym expansion operation replaces every tagged abbreviation with its long form followed by itself in a pair of parentheses.
4.2 Employing
Domain-Specific Knowledge can identify the gene names contained in an article.
Moreover, by further consulting organism databases, we can get the properties of the genes.
Two domain-specific resources are exploited in this study.
One is the Unified Medical Language System (UMLS) (Humphreys et al., 1998) and the other is a list of tumor names obtained from Mouse Tumor Biology Database (MTB)1.
UMLS contains a huge dictionary of biomedical terms  the UMLS Metathesaurus and defines a hierarchy of semantic types  the UMLS Semantic Network.
Each concept in the Metathesaurus contains a set of strings, which are variants of each other and belong to one or more semantic types in the Semantic Network.
Therefore, given a string, we can obtain a set of semantic types to which it belongs.
Then we obtain another representation of the article by gathering the semantic types found in the part of the article.
Consequently, we get another three much deeper representations of an article after this step.
They are denoted as "AbstractSEM", "MeSHSEM" and "CaptionSEM".
We use the list of tumor names on the Tumor task.
We first tokenize all the tumor names and stem each unique token.
With the resulting list of unique stemmed tokens, we use it as a filter to remove the tokens not in the list from the "Abstract" and "Caption", which produce "AbstractTM" and "CaptionTM".
4.3 Model
Selection As mentioned above, we generate several representations for an article.
In this section, we explain how feature selection is done and how the best combination of the representations With the help of domain-specific knowledge, we can extract the deeper knowledge in an article.
For example, with a gene name dictionary, we fiof an article is obtained.
For each representation, we first rank all the tokens in the training documents via the chi-square test of independence.
Postulating the ranking perfectly reflects the effectiveness of the tokens in classification, we then decide the number of tokens to be used in SVM classification by 4-fold cross-validation.
In cross-validation, we use the TF*IDF weighting scheme.
Each feature vector is then normalized to a unit vector.
We set C+ to ur* Cbecause of the relatively small number of positive examples, where C+ and Care the penalty constants on positive and negative examples in SVMs.
After that, we obtain the optimal number of tokens and the corresponding SVM parameters Cand gamma, a parameter in the radial basis kernel.
In the rest of this paper, "Abstract30" denotes the "Abstract" representation with top-30 tokens, "CaptionSEM10" denotes "CaptionSEM" with top-10 tokens, and so forth.
After feature selection is done for each representation, we try to find the best combination by the following algorithm.
Given the candidate representations with selected features, we start with an initial set containing some or zero representation.
For each iteration, we add one representation to the set by picking the one that enhances the cross-validation performance the most.
The iteration stops when we have exhausted all the representations or adding more representation to the set doesn't improve the cross-validation performance.
For classifying the documents with better features, we run the algorithm twice.
We first start with an empty set and obtain the best combination of the basic three representations, e.g., "Abstract10", "MeSH30" and "Caption10".
Then, starting with this combination, we attempt to incorporate the three semantic representations, e.g., "Abstract30SEM", "MeSH30SEM" and "Caption10SEM", and obtain the final combination.
Instead of using this algorithm to incorporate the "AbstractTM" and "CaptionTM" representations, we use them to replace their unfiltered counterparts "Abstract" and "Caption" when the cross-validation performance is better.
Results and Discussions Utility (NU)2 measure).
For category Allele, "Caption" and "AbstractSEM" perform the best among the basic and semantic representations, respectively.
For category Expression, "Caption" plays an important role in identifying relevant documents, which agrees with the finding by the winner of KDD CUP 2002 task 1 (Regev et al., 2002).
Similarly, MeSH terms are crucial to the GO category, which are used by top-performing teams (Dayanik et al., 2004; Fujita, 2004) in TREC Genomics 2004.
For category Tumor, MeSH terms are important, but after semantic type extraction, "AbstractSEM" exhibits relatively high cross-validation performance.
Since only 10 features are selected for the "AbstractSEM", using this representation alone may be susceptible to over-fitting.
Finally, by comparing the performance of the "AbstractTM" and "Abstract", we find the list of tumor names helpful for filtering abstracts.
We list the results for the test data in Table 2.
Column "Experiment" identifies our proposed methods.
We show six experiments in Table 2: one for Allele (AL), one for Expression (EX), one for GO (GO) and three for Tumor (TU, TN and TS).
Column "cv NU" shows the cross-validation NU measure, "NU" shows the performance on the test data and column "Combination" lists the combination of the representations used for each experiment.
In this table, "M30" is the abbreviation for "MeSH30", "CS10" is for "CaptionSEM10", and so on.
The combinations for the first 4 experiments, i.e., AL, EX, GO and TU, are obtained by the algorithm described in Section 4.3, while the combination for TN is obtained by substituting "AbstractTM30" for "Abstract30" in the combination for TU.
The experiment TS only uses the "AbstractSEM10" because its cross-validation performance beats all other combinations for the Tumor category.
The combinations of the first 5 experiments illustrate that adding other inferior representations to the best one enhances the performance, which implies that the inferior ones may contain important exclusive information.
The cross-validation performance fairly predicts the performance on the test data, except for the last experiment TS, which relies on only 10 features and is therefore susceptible to over-fitting.
Table 1 lists the cross-validation results of each representation for each category (in Normalized Please refer to the TREC 2005 Genomics Track Protocol (http://ir.ohsu.edu/genomics/2005protocol.html).
Abstract MeSH Caption AbstractSEM MeSHSEM CaptionSEM AbstractTM CaptionTM Table 1.
Partial Cross-validation Results.
Experiment AL (for Allele) EX (for Expression) GO (for GO) TU (for Tumor) TN (for Tumor) TS (for Tumor) cv NU 0.8717 0.7691 0.5402 0.8742 0.8764 0.8814 NU 0.8423 0.7515 0.5332 0.8299 0.8747 0.5699 Recall 0.9488 0.8190 0.8803 0.9000 0.9500 0.6500 Precision 0.3439 0.1593 0.1873 0.0526 0.0518 0.0339 F-score 0.5048 0.2667 0.3089 0.0994 0.0982 0.0645 Combination M30+C10+A10+CS10+AS10+MS10 M10+C10+CS10+MS10 M10+C10+MS10 M30+C30+A30+AS10+CS30 M30+C30+AT30+AS10+CS30 AS10 Table 2.
Evaluation Results.
Subtask Allele Expression GO Annotation Tumor NU (Best/Median) 0.8710/0.7773 0.8711/0.6413 0.5870/0.4575 0.9433/0.7610 Recall (Best/Median) 0.9337/0.8720 0.9333/0.7286 0.8861/0.5656 1.0000/0.9500 Precision (Best/Median) 0.4669/0.3153 0.1899/0.1164 0.2122/0.3223 0.0709/0.0213 F-score (Best/Median) 0.6225/0.5010 0.3156/0.2005 0.3424/0.4107 0.1325/0.0417 Table 3.
Best and Median Results for Each Subtask on TREC 2005 (Hersh et al., 2005).
To compare with our performance, we list the best and median results for each subtask on the genomics classification task of TREC 2005 in Table 3.
Comparing to Tables 2 and 3, it shows our experimental results have overall high performance.
Conclusions and Further Work In this paper, we demonstrate how our system is constructed.
Three parts of an article are extracted to represent its content.
We incorporate two domain-specific resources, i.e., UMLS and a list of tumor names.
For each categorization work, we propose an algorithm to get the best combination of the representations and train an SVM classifier out of this combination.
Evaluation results show overall high performance in this study.
Except for MeSH terms, we can try other sections in the article, e.g., Results, Discussions and Conclusions as targets of feature extraction besides the abstract and captions in the future.
Finally, we will try to make use of other available domain-specific resources in hope of enhancing the performance of this system.
Acknowledgements Research of this paper was partially supported by National Science Council, Taiwan, under the contracts NSC94-2213-E-002-033 and NSC94-2752-E-001-001-PAE.
References Bult, C.
J., Blake, J.
A., Richardson, J.
E., Kadin, J.
A., Eppig, J.
T. and the Mouse Genome Database Group.
The Mouse Genome Database (MGD): Integrating Biology with the Genome.
Nucleic Acids Research, 32, D476D481, 2004.
Couto, F.
M., Martins, B.
and Silva, M.
J . Classifying Biological Articles Using Web Resources.
Proceedings of the 2004 ACM Symposium on Applied Computing, 111-115, 2004.
Dayanik, A., Fradkin, D., Genkin, A., Kantor, P., Lewis, D.
D., Madigan, D.
and Menkov, V . DIMACS at the TREC 2004 Genomics Track.
Proceedings of the Thirteenth Text Retrieval Conference, 2004.
Fujita, S., . Revisiting Again Document Length Hypotheses TREC-2004 Genomics Track Experiments at Patolis.
Proceedings of the Thirteenth Text Retrieval Conference, 2004.
Hersh, W., Cohen, A., Yang, J., Bhuptiraju, R.T., Toberts, P.
and Hearst, M . TREC 2005 Genomics Track Overview.
Proceedings of the Fourteenth Text Retrieval Conference, 2005.
Hirschman, L., Park, J., Tsujii, J., Wong, L.
and Wu, C.
H . Accomplishments and Challenges in Literature Data Mining for Biology.
Bioinformatics, 18(12): 1553-1561, 2002.
Hou, W.
J., Lee, C., Lin, K.
H. Y.
and Chen, H.
H . A Relevance Detection Approach to Gene Annotation.
Proceedings of the First International Symposium on Semantic Mining in Biomedicine, http://ceur-ws.org, 148: 15-23, 2005.
Hsu, C.
W., Chang, C.
C. and Lin, C.
J . A Practical Guide to Support Vector Classification.
http://www.csie.ntu.edu.tw /~cjlin/libsvm/index.html, 2003.
Humphreys, B.
L., Lindberg, D.
A., Schoolman, H.
M. and Barnett, G.
O . The Unified Medical Language System: an Informatics Research Collaboration.
Journal of American Medical Information Association, 5(1):1-11, 1998.
Regev, Y., Finkelstein-Landau, M.
and Feldman, R . Rule-based Extraction of Experimental Evidence in the Biomedical Domain the KDD Cup (Task 1).
SIGKDD Explorations, 4(2):90-92, 2002.
Vapnik, V . The Nature of Statistical Learning Theory, Springer-Verlag, 1995 .
Multidocument Summarization via Information Extraction Michael White and Tanya Korelsky CoGenTex, Inc.
Ithaca, NY Claire Cardie, Vincent Ng, David Pierce, and Kiri Wagstaff Department of Computer Science Cornell University, Ithaca, NY mike,tanya@cogentex.com We present and evaluate the initial version of RIPTIDES, a system that combines information extraction, extraction-based summarization, and natural language generation to support userdirected multidocument summarization.
IE system and the Summarizer in turn.
2.1 IE
System The domain for the initial IE-supported summarization system and its evaluation is natural disasters.
Very briefly, a top-level natural disasters scenario template contains: document-level information (e.g.
docno, date-time); zero or more agent elements denoting each person, group, and organization in the text; and zero or more disaster elements.
Agent elements encode standard information for named entities (e.g.
name, position, geo-political unit).
For the most part, disaster elements also contain standard event-related fields (e.g.
type, number, date, time, location, damage sub-elements).
The final product of the RIPTIDES system, however, is not a set of scenario templates, but a user-directed multidocument summary.
This difference in goals influences a number of template design issues.
First, disaster elements must distinguish different reports or views of the same event from multiple sources.
As a result, the system creates a separate disaster event for each such account.
Disaster elements should also include the reporting agent, date, time, and location whenever possible.
In addition, damage elements (i.e.
human and physical effects) are best grouped according to the reporting event.
Finally, a slight broadening of the IE task was necessary in that extracted text was not constrained to noun phrases.
In particular, adjectival and adverbial phrases that encode reporter confidence, and sentences and clauses denoting relief effort progress appear beneficial for creating informed summaries.
Figure 2 shows the scenario template for one of 25 texts tracking the 1998 earthquake in Afghanistan (TDT2 Topic 89).
The texts were also manually annotated for noun phrase coreference; any phrase involved in a coreference relation appears underlined in the running text.
The RIPTIDES system for the most part employs a traditional IE architecture [4].
In addition, we use an in-house implementation of the TIPSTER architecture [8] to manage all linguistic annotations.
A preprocessor first finds sentences and tokens.
For syntactic analysis, we currently use the Charniak [5] parser, which creates Penn Treebank-style parses [9] rather than the partial parses used in most IE systems.
Output from the parser is converted automatically into TIPSTER parse and part-of-speech annotations, which are added to the set of linguistic annotations for the document.
The extraction phase of the system identifies domain-specific relations among relevant entities in the text.
It relies on Autoslog-XML, an XSLT implementation of the Autoslog-TS system [12], to acquire extraction patterns.
Autoslog-XML is a weakly supervised learning system that requires two sets of texts for training -one set comprises texts relevant to the domain of interest and the other, texts not relevant Although recent years has seen increased and successful research efforts in the areas of single-document summarization, multidocument summarization, and information extraction, very few investigations have explored the potential of merging summarization and information extraction techniques.
This paper presents and evaluates the initial version of RIPTIDES, a system that combines information extraction (IE), extraction-based summarization, and natural language generation to support userdirected multidocument summarization.
(RIPTIDES stands for RapIdly Portable Translingual Information extraction and interactive multiDocumEnt Summarization).
Following [10], we hypothesize that IE-supported summarization will enable the generation of more accurate and targeted summaries in specific domains than is possible with current domain-independent techniques.
In the sections below, we describe the initial implementation and evaluation of the RIPTIDES IE-supported summarization system.
We conclude with a brief discussion of related and ongoing work.
Figure 1 depicts the IE-supported summarization system.
The system first requires that the user select (1) a set of documents in which to search for information, and (2) one or more scenario templates (extraction domains) to activate.
The user optionally provides filters and preferences on the scenario template slots, specifying what information s/he wants to be reported in the summary.
RIPTIDES next applies its Information Extraction subsystem to generate a database of extracted events for the selected domain and then invokes the Summarizer to generate a natural language summary of the extracted information subject to the user's constraints.
In the subsections below, we describe the fiuser information need Summarizer multi-document template merging event-oriented structure text collection IE System slot filler slot slot filler filler slot filler slotslot filler filler slot filler slot filler...
... slot filler slot filler filler slot slot filler slot filler slot filler ...
... slot filler slot filler scenario templates content selection event-oriented structure with slot importance scores A powerful earthquake struck Afghanistan on May 30 at 11:25...
Damage VOA (06/02/1998) estimated that 5,000 were killed by the earthquake, whereas AP (APW, 06/02/1998) instead reported ...
Relief Status NLG of summary CNN (06/02/1998): Food, water, medicine and other supplies have started to arrive.
[...] summary Figure 1.
RIPTIDES System Design to the domain.
Based on these and a small set of extraction pattern templates, the system finds a ranked list of possible extraction patterns, which a user then annotates with the appropriate extraction label (e.g.
victim). Once acquired, the patterns are applied to new documents to extract slot fillers for the domain.
Selectional restrictions on allowable slot fillers are implemented using WordNet [6] and BBN's Identifinder [3] named entity component.
In the current version of the system, no coreference resolution is attempted; instead, we rely on a very simple set of heuristics to guide the creation of output templates.
The disaster scenario templates extracted for each text are provided as input to the summarization component along with all linguistic annotations accrued in the IE phase.
No relief slots are included in the output at present, since there was insufficient annotated data to train a reliable sentence categorizer.
Selected News Excerpts, as shown in the two sample summaries appearing in Figures 3 and 4, and discussed further in Section 2.2.5 below.
2.2.1 Summarization
Stages The Summarizer produces each summary in three main stages.
In the first stage, the output templates are merged into an eventoriented structure, while keeping track of source information.
The merge operation currently relies on simple heuristics to group extracted facts that are comparable; for example, during this phase damage reports are grouped according to whether they pertain to the event as a whole, or instead to damage in the same particular location.
Heuristics are also used in this stage to determine the most relevant damage reports, taking into account specificity, recency and news source.
Towards the same objective but using a more surface-oriented means, simple word-overlap clustering is used to group sentences from different documents into clusters that are likely to report similar content.
In the second stage, a base importance score is first assigned to each slot/sentence based on a combination of document position, document recency and group/cluster membership.
The base importance scores are then adjusted according to user-specified preferences and matching 2.2 The Summarizer In order to include relief and other potentially relevant information not currently found in the scenario templates, the Summarizer extracts selected sentences from the input articles and adds them to the summaries generated from the scenario templates.
The extracted sentences are listed under the heading Document no.: ABC19980530.1830.0342 Date/time: 05/30/1998 18:35:42.49 Disaster Type: earthquake location: Afghanistan date: today magnitude: 6.9 magnitude-confidence: high epicenter: a remote part of the country PAKISTAN MAY BE PREPARING FOR ANOTHER TEST Thousands of people are feared dead following...
(voiceover) ...a powerful earthquake that hit Afghanistan today.
The quake registered 6.9 on the Richter scale, centered in a remote part of the country.
(on camera) Details now hard to come by, but reports say entire villages were buried by the quake.
human-effect: victim: Thousands of people number: Thousands outcome: dead confidence: medium confidence-marker: feared physical-effect: object: entire villages outcome: damaged confidence: medium confidence-marker: Details now hard to come by / reports say Figure 2.
Example scenario template for the natural disasters domain criteria.
The adjusted scores are used to select the most important slots/sentences to include in the summary, subject to the userspecified word limit.
In the third and final stage, the summary is generated from the resulting content pool using a combination of top-down, schema-like text building rules and surface-oriented revisions.
The extracted sentences are simply listed in document order, grouped into blocks of adjacent sentences.
(2) any intermediate estimates that are lower than the maximum estimate.1 In the content determination stage, scores are assigned to the derived information units based on the maximum score of the underlying units.
In the summary generation stage, a handful of text planning rules are used to organize the text for these derived units, highlighting agreement and disagreement across sources.
2.2.2 Specificity
of Numeric Estimates In order to intelligently merge and summarize scenario templates, we found it necessary to explicitly handle numeric estimates of varying specificity.
While we did find specific numbers (such as 3,000) in some damage estimates, we also found cases with no number phrase at all (e.g.
entire villages).
In between these extremes, we found vague estimates (thousands) and ranges of numbers (anywhere from 2,000 to 5,000).
We also found phrases that cannot be easily compared (more than half the region's residents).
To merge related damage information, we first calculate the numeric specificity of the estimate as one of the values NONE, VAGUE, RANGE, SPECIFIC, or INCOMPARABLE, based on the presence of a small set of trigger words and phrases (e.g.
several, as many as, from ...
to). Next, we identify the most specific current estimates by news source, where a later estimate is considered to update an earlier estimate if it is at least as specific.
Finally, we determine two types of derived information units, namely (1) the minimum and maximum estimates across the news sources, and 2.2.3 Improving the Coherence of Extracted Sentences In our initial attempt to include extracted sentences, we simply chose the top ranking sentences that would fit within the word limit, subject to the constraint that no more than one sentence per cluster could be chosen, in order to help avoid redundancy.
We found that this approach often yielded summaries with very poor coherence, as many of the included sentences were difficult to make sense of in isolation.
To improve the coherence of the extracted sentences, we have experimented with trying to boost coherence by favoring sentences in the context of the highest-ranking sentences over those with lower ranking scores, following the hypothesis that it is better to cover fewer topics in more depth than to change topics excessively.
In particular, we assign a score to a set of sentences by summing the base scores plus increasing coherence boosts for adjacent sentences, sentences that precede ones with an initial Less specific estimates such as "hundreds" are considered lower than more specific numbers such as "5000" when they are lower by more than a factor of 10.
Earthquake strikes Afghanistan A powerful earthquake struck Afghanistan last Saturday at 11:25.
The earthquake was centered in a remote part of the country and had a magnitude of 6.9 on the Richter scale.
Earthquake strikes quake-devastated villages in northern Afghanistan A earthquake struck quake-devastated villages in northern Afghanistan Saturday.
The earthquake had a magnitude of 6.9 on the Richter scale on the Richter scale.
Damage Estimates of the death toll varied.
VOA (06/02/1998) provided the highest estimate of 5,000 dead.
CNN (05/31/1998) and CNN (06/02/1998) supplied lower estimates of 3,000 and up to 4,000 dead, whereas APW (06/02/1998) gave the lowest estimate of anywhere from 2,000 to 5,000 dead.
People were injured, while thousands more were missing.
Thousands were homeless.
Quake-devastated villages were damaged.
Estimates of the number of villages destroyed varied.
CNN (05/31/1998) provided the highest estimate of 50 destroyed, whereas VOA (06/04/1998) gave the lowest estimate of at least 25 destroyed.
In Afghanistan, thousands of people were killed.
Damage Estimates of the death toll varied.
CNN (06/02/1998) provided the highest estimate of 4,000 dead, whereas ABC (06/01/1998) gave the lowest estimate of 140 dead.
In capital: Estimates of the number injured varied.
Selected News Excerpts CNN (06/01/98): Thousands are dead and thousands more are still missing.
Red cross officials say the first priority is the injured.
Getting medicine to them is difficult due to the remoteness of the villages affected by the quake.
PRI (06/01/98): We spoke to the head of the international red cross there, Bob McCaro on a satellite phone link.
He says it's difficult to know the full extent of the damage because the region is so remote.
There's very little infrastructure.
PRI (06/01/98): Bob McCaro is the head of the international red cross in the neighboring country of Pakistan.
He's been speaking to us from there on the line.
APW (06/02/98): The United Nations, the Red Cross and other agencies have three borrowed helicopters to deliver medical aid.
Figure 4.
200 word summary of actual IE output, with emphasis on Red Cross Further Details Heavy after shocks shook northern afghanistan.
More homes were destroyed.
More villages were damaged.
Landslides or mud slides hit the area.
Another massive quake struck the same region three months earlier.
Some 2,300 victims were injured.
Selected News Excerpts ABC (05/30/98): PAKISTAN MAY BE PREPARING FOR ANOTHER TEST Thousands of people are feared dead following...
ABC (06/01/98): RESCUE WORKERS CHALLENGED IN AFGHANISTAN There has been serious death and devastation overseas.
In Afghanistan...
CNN (06/02/98): Food, water, medicine and other supplies have started to arrive.
But a U.N. relief coordinator says it's a "scenario from hell".
Figure 3.
200 word summary of simulated IE output, with emphasis on damage cases.
We then perform a randomized local search for a good set of sentences according to these scoring criteria.
2.2.4 Implementation
The Summarizer is implemented using the Apache implementation of XSLT [1] and CoGenTex's Exemplars Framework [13].
The Apache XSLT implementation has provided a convenient way to rapidly develop a prototype implementation of the first two processing stages using a series of XML transformations.
In the first step of the third summary generation stage, the text building component of the Exemplars Framework constructs a "rough draft" of the summary text.
In this rough draft version, XML markup is used to partially encode the rhetorical, referential, semantic and morpho-syntactic structure of the text.
In the second generation step, the Exemplars text polishing component makes use of this markup to trigger surfacepronoun, and sentences that preceded ones with strongly connecting discourse markers such as however, nevertheless, etc.
We have also softened the constraint on multiple sampling from the same cluster, making use of a redundancy penalty in such fioriented revision rules that smooth the text into a more polished form.
A distinguishing feature of our text polishing approach is the use of a bootstrapping tool to partially automate the acquisition of application-specific revision rules from examples.
2.2.5 Sample
Summaries Figures 3 and 4 show two sample summaries that were included in our evaluation (see Section 3 for details).
The summary in Figure 3 was generated from simulated output of the IE system, with preference given to damage information; the summary in Figure 4 was generated from the actual output of the current IE system, with preference given to information including the words Red Cross.
While the summary in Figure 3 does a reasonable job of reporting the various current estimates of the death toll, the estimates of the death toll shown in Figure 4 are less accurate, because the IE system failed to extract some reports, and the Summarizer failed to correctly merge others.
In particular, note that the lowest estimate of 140 dead attributed to ABC is actually a report about the number of school children killed in a particular town.
Since no location was given for this estimate by the IE system, the Summarizer's simple heuristic for localized damaged reports -namely, to consider a damage report to be localized if a location is given that is not in the same sentence as the initial disaster description -did not work here.
The summary in Figure 3 also suffered from some problems with merging: the inclusion of a paragraph about thousands killed in Afghanistan is due to an incorrect classification of this report as a localized one (owing to an error in sentence boundary detection), and the discussion of the number of villages damaged should have included a report of at least 80 towns or villages damaged.
Besides the problems related to slot extraction and merging mentioned above, the summaries shown in Figures 3 and 4 suffer from relatively poor fluency.
In particular, the summaries could benefit from better use of descriptive terms from the original articles, as well as better methods of sentence combination and rhetorical structuring.
Nevertheless, as will be discussed further in Section 4, we suggest that the summaries show the potential for our techniques to intelligently combine information from many articles on the same natural disaster.
earlier version of the Summarizer uses the simulated output of the IE system as its input, including the relief annotations; in the second variant (RIPTIDES-SIM2), the current version of the Summarizer uses the simulated output of the IE system, without the relief annotations; and in the third variant (RIPTIDES-IE), the Summarizer uses the actual output of the IE system as its input.2 Summaries generated by the RIPTIDES variants were compared to a Baseline system consisting of a simple, sentence-extraction multidocument summarizer relying only on document position, recency, and word overlap clustering.
(As explained in the previous section, we have found that word overlap clustering provides a bare bones way to help determine what information is repeated in multiple articles, thereby indicating importance to the document set as a whole, as well as to help reduce redundancy in the resulting summaries).
In addition, the RIPTIDES and Baseline system summaries were compared against the summaries of two human authors.
All of the summaries were graded with respect to content, organization, and readability on an A-F scale by three graduate students, all of whom were unfamiliar with this project.
Note that the grades for RIPTIDES-SIM1, the Baseline system, and the two human authors were assigned during a first evaluation in October, 2000, whereas the grades for RIPTIDESSIM2 and RIPTIDES-IE were assigned by the same graders in an update to this evaluation in April, 2001.
Each system and author was asked to generate four summaries of different lengths and emphases: (1) a 100-word summary of the May 30 and May 31 articles; (2) a 400-word summary of all test articles, emphasizing specific, factual information; (3) a 200-word summary of all test articles, focusing on the damage caused by the quake, and excluding information about relief efforts, and (4) a 200-word summary of all test articles, focusing on the relief efforts, and highlighting the Red Cross's role in these efforts.
The results are shown in Tables 1 and 2.
Table 1 provides the overall grade for each system or author averaged across all graders and summaries, where each assigned grade has first been converted to a number (with A=4.0 and F=0.0) and the average converted back to a letter grade.
Table 2 shows the mean and standard deviations of the overall, content, organization, and readability scores for the RIPTIDES and the Baseline systems averaged across all graders and summaries.
Where the differences vs.
the Baseline system are significant according to the t-test, the p-values are shown.
Given the amount of development effort that has gone into the system to date, we were not surprised that the RIPTIDES variants fared poorly when compared against the manually written summaries, with RIPTIDES-SIM2 receiving an average grade of C, vs.
Aand B+ for the human authors.
Nevertheless, we were pleased to find that RIPTIDES-SIM2 scored a full grade ahead of the Baseline summarizer, which received a D, and that 3.
EVALUATION AND INITIAL RESULTS To evaluate the initial version of the IE-supported summarization system, we used Topic 89 from the TDT2 collection -25 texts on the 1998 Afghanistan earthquake.
Each document was annotated manually with the natural disaster scenario templates that comprise the desired output of the IE system.
In addition, treebank-style syntactic structure annotations were added automatically using the Charniak parser.
Finally, MUC-style noun phrase coreference annotations were supplied manually.
All annotations are in XML.
The manual and automatic annotations were automatically merged, leading to inaccurate annotation extents in some cases.
Next, the Topic 89 texts were split into a development corpus and a test corpus.
The development corpus was used to build the summarization system; the evaluation summaries were generated from the test corpus.
We report on three different variants of the RIPTIDES system here: in the first variant (RIPTIDES-SIM1), an Note that since the summarizers for the second and third variants did not have access to the relief sentence categorizations, we decided to exclude from their input the two articles (one training, one test) classified by TDT2 Topic 89 as only containing brief mentions of the event of interest, as otherwise they would have no means of excluding the largely irrelevant material in these documents.
Table 1 Baseline D RIPTIDES-SIM1 C/CRIPTIDES-SIM2 C RIPTIDES-IE D+ Person 1 APerson 2 B+ RIPTIDES-IE managed a slightly higher grade of D+, despite the immature state of the IE system.
As Table 2 shows, the differences in the overall scores were significant for all three RIPTIDES variants, as were the scores for organization and readability, though not for content in the cases of RIPTIDESSIM1 and RIPTIDES-IE.
our efforts in this area on attempting to balance coherence and informativeness in selecting sets of sentences to include in the summary.
In ongoing work, we are investigating techniques for improving merging accuracy and summary fluency in the context of summarizing the more than 150 news articles we have collected from the web about each of the recent earthquakes in Central America and India (January, 2001).
We also plan to investigate using tables and hypertext drill-down as a means to help the user verify the accuracy of the summarized information.
By perusing the web collections mentioned above, we can see that trying to manually extricate the latest damage estimates from 150+ news articles from multiple sources on the same natural disaster would be very tedious.
Although estimates do usually converge, they often change rapidly at first, and then are gradually dropped from later articles, and thus simply looking at the latest article is not satisfactory.
While significant challenges remain, we suggest that our initial system development and evaluation shows that our approach has the potential to accurately summarize damage estimates, as well as identify other key story items using shallower techniques, and thereby help alleviate information overload in specific domains.
4. RELATED AND ONGOING WORK The RIPTIDES system is most similar to the SUMMONS system of Radev and McKeown [10], which summarized the results of MUC-4 IE systems in the terrorism domain.
As a pioneering effort, the SUMMONS system was the first to suggest the potential of combining IE with NLG in a summarization system, though no evaluation was performed.
In comparison to SUMMONS, RIPTIDES appears to be designed to more completely summarize larger input document sets, since it focuses more on finding the most relevant current information, and since it includes extracted sentences to round out the summaries.
Another important difference is that SUMMONS sidestepped the problem of comparing reported numbers of varying specificity (e.g.
several thousand vs.
anywhere from 2000 to 5000 vs.
up to 4000 vs.
5000), whereas we have implemented rules for doing so.
Finally, we have begun to address some of the difficult issues that arise in merging information from multiple documents into a coherent event-oriented view, though considerable challenges remain to be addressed in this area.
The sentence extraction part of the RIPTIDES system is similar to the domain-independent multidocument summarizers of Goldstein et al.[7] and Radev et al.[11] in the way it clusters sentences across documents to help determine which sentences are central to the collection, as well as to reduce redundancy amongst sentences included in the summary.
It is simpler than these systems insofar as it does not make use of comparisons to the centroid of the document set.
As pointed out in [2], it is difficult in general for multidocument summarizers to produce coherent summaries, since it is less straightforward to rely on the order of sentences in the underlying documents than in the case of single-document summarization.
Having also noted this problem, we have focused We thank Daryl McCullough for implementing the coherence boosting randomized local search, and we thank Ted Caldwell, Daryl McCullough, Corien Bakermans, Elizabeth Conrey, Purnima Menon and Betsy Vick for their participation as authors and graders.
This work has been partially supported by DARPA TIDES contract no.
N66001-00-C-8009. References [1] The Apache XML Project.
2001. "Xalan Java." [2] Barzilay, R., Elhadad, N.
and McKeown, K . 2001.
"Sentence Ordering in Multidocument Summarization".
In Proceedings of HLT 2001.
[3] Bikel, D., Schwartz, R.
and Weischedel, R . 1999.
"An Algorithm that Learns What's in a Name".
Machine Learning 34:1-3, 211-231.
[4] Cardie, C . 1997.
"Empirical Methods in Information [5] Charniak, E . 1999.
"A maximum-entropy-inspired parser".
Brown University Technical Report CS99-12.
[6] Fellbaum, C . 1998.
WordNet: An Electronic Lexical Database.
MIT Press, Cambridge, MA.
[7] Goldstein, J., Mittal, V., Carbonell, J.
and Kantrowitz, M . 2000.
"Multi-document summarization by sentence extraction".
In Proceedings of the ANLP/NAACL Workshop on Automatic Summarization, Seattle, WA.
[8] Grishman, R . 1996.
"TIPSTER Architecture Design Document Version 2.2".
DARPA, available at http://www.tipster.org/.
[9] Marcus, M., Marcinkiewicz, M.
and Santorini, B . 1993.
"Building a Large, Annotated Corpus of English: The Penn Treebank".
Computational Linguistics 19:2, 313-330.
[10] Radev, D.
R. and McKeown, K.
R . 1998.
"Generating natural language summaries from multiple on-line sources".
Computational Linguistics 24(3):469-500.
[11] Radev, D.
R., Jing, H.
and Budzikowska, M . 2000.
"Summarization of multiple documents: clustering, sentence extraction, and evaluation".
In Proceedings of the ANLP/NAACL Workshop on Summarization, Seattle, WA.
[12] Riloff, E . 1996.
"Automatically Generating Extraction Patterns from Untagged Text".
In Proceedings of the Thirteenth National Conference on Artificial Intelligence, Portland, OR, 1044-1049.
AAAI Press / MIT Press.
[13] White, M.
and Caldwell, T . 1998.
"EXEMPLARS: A Practical, Extensible Framework for Dynamic Text Generation".
In Proceedings of the Ninth International Workshop on Natural Language Generation, Niagara-onthe-Lake, Canada, 266-275 .
Structural variation in generated health reports Catalina Hallett and Donia Scott Centre for Research in Computing The Open University Walton Hall Milton Keynes MK7 6AA {c.hallett,d.scott}@open.ac.uk Abstract We present a natural language generator that produces a range of medical reports on the clinical histories of cancer patients, and discuss the problem of conceptual restatement in generating various textual views of the same conceptual content.
We focus on two features of our system: the demand for "loose paraphrases" between the various reports on a given patient, with a high degree of semantic overlap but some necessary amount of distinctive content; and the requirement for paraphrasing at primarily the discourse level.
which aims at providing tools to facilitate easy access to a patient's medical history.
In particular, we describe a natural language generation system that produces a range of summarised reports of patient records from data-encoded views of patient histories which we call chronicles.
Although we are concentrating on cancer patients, we aim to produce good quality reports without the need to construct extensive domain models.
Our typical user is a GP or clinician who uses electronic patient records at the point of care to familiarise themselves with a patient's medical history and current situation.
A number of specific requirements arise from this particular setting:  Reports that provide a quick potted overview of the patient's history are essential; this type of report should not be too long (ideally they should fit entirely on a computer screen) and should take less than a minute to read;  At the same time, a complete view of the medical history must always be available on demand;  Clinicians often need to examine a patient's history from a particular perspective (e.g., tests administered, treatments undertaken, drugs prescribed), and having focussed reports is also a requirement;  Reports should be formatted to enhance readability;  The selection of events for inclusion in a report should follow some basic rules: 1 Introduction Patient records are typically large collections of documents that reflect the medical history of a patient over a period of time.
On average, the electronic patient record of a cancer patient contains information from over 150 documents, representing consult notes, referral letters, letters to and from the patient's GP, hospital admission and discharge notes, laboratory test results, surgery and other treatment descriptions, and drug dispensing notes.
Although each document in this collection will have a specified purpose, there tends to be a high degree of redundancy between documents, but the sheer volume of information makes access extremely difficult.
The work presented in this paper is part of the Clinical E-Science Framework project (CLEF), fi Events that deviate from what is considered to be normal are more important than normal events (for example, an examination of the lymphnodes that reveals lymphadenopathy is more important than an examination that doesn't).
 Some events are more important than others and should not only be included in the report but also highlighted (e.g., through colour coding, graphical timelines or similar display features).
 Less important events should be available on a need-to-know basis These requirements impose important restrictions on the content of the reports and implicitly on the variety of lexical and syntactical devices we can employ: (a) the veracity of the report is essential, therefore we are not at liberty to employ synonymy or lexical paraphrasing that may alter (however slightly) the meaning of the original input, (b) we are required to maintain a certain syntactical ordering throughout a report in order to allow the user to quickly scan through the report with ease, and (c) we have to produce several types of reports from the same input data.
In this paper, we focus on this last requirement, describing the methods we employ for reformulating content according to the type and focus of the generated report.
example displays a fragment of a generated longitudinal report1 : Example 1 The patient is diagnosed with grade 9 invasive medullary carcinoma of the breast.
She was 39 years old when the first cell became malignant.
The history covers 1517 weeks, from week 180 to week 1697.
During this time, the patient attended 38 consults.
YEAR 3: Week 183  Radical mastectomy on the breast was performed to treat primary cancer of the left breast.
 Histopathology revealed primary cancer of the left breast.
Week 191  Examination of the abdomen revealed no enlargement of the liver or of the spleen.
 Examination of the axillary lymphnodes revealed no lymphadenopathy of the left axillary lymphnodes.
 Examination of the breast revealed no recurrent cancer of the left breast.
 Testing of the blood revealed no abnormality of the haemoglobin concentration or of the leucocyte count.
 Radiotherapy was initiated to treat primary cancer of the left breast.
Week 192  First radiotherapy cycle was performed.
...
2 Types
of report In the current implementation, the generator produces two main types of report.
The first is a longitudinal report, which is intended to provide a quick historical overview of the patient's illness, whilst preserving the main events (such as diagnoses, investigations and interventions).
It presents the events in the patient's history ordered chronologically and grouped according to type.
In this type of report, events are fully described (i.e., an event description includes all the attributes of the event) and aggregation is minimal (events with common attributes are aggregated, but there is no aggregation through generalization, for example).
The following The second type of report focusses on a given type of event in a patient's history, such as the history of diagnoses, interventions, investigations or drug prescription.
Under this category fall user-defined reports as well, where the user selects classes of interesting events (for example, Investigations of type CT scan and Interventions of type surgery).
A report of the diagnoses, for example, will focus on the Problem events that are recorded in the chronicle (e.g., cancer, anaemia, lymphadenopathy); other event types will only All the examples presented in this paper are extracted from summaries produced by our Report generator.
fiappear if they are directly related to a Problem.
As it can be seen in Example 2, this type of report is necessarily more condensed, since the events do not have to appear chronologically and can be grouped in larger clusters.
Secondary events are also more highly aggregated.
Example 2  In week 483, primary cancer of the right breast was revealed by the histopathology report.
The cancer was treated with radical mastectomy on the breast.
 In week 491, no abnormality of the leucocyte count or of the haemoglobin concentration, no lymphadenopathy of the right axillary lymphnodes, no enlargement of the spleen or of the liver and no recurrent cancer of the right breast were found.
Radiotherapy was initiated to treat primary cancer of the right breast.
 In the weeks 492 to 496, 5 radiotherapy cycles were performed.
of the right breast.
It is important to note that although the reports are generated from the same input content, they are not exact reformulations of each other, but rather different views of the same content with a large degree of overlap.
This feature is a direct result of the report requirements.
3 Input
As mentioned earlier, the input to our Report Generator is a data-encoded chronicle of the patient's medical history.
Technically, the chronicle is the partial result of information extraction applied on clinical narratives, combined with structured data (such as radiology results or demographic data), and supplemented with inferences.
However, in developing our report generator, we are currently using a Chronicle Simulator, which constructs invented chronicles, allowing us to ignore for the time being some problems that can appear when using an information extraction system (being developed in parallel).
Firstly, the resulting data is complete and correct, thus allowing us to concentrate on the design and testing of the generation and summarisation system without having to take into account at this point errors in the Information Extraction.
Secondly, our data on cancer patients is highly confidential, which makes presentation of the output of the report generator (e.g., for evaluation with real subjects, or dissemination purposes) very difficult.
Using a simulator also means that we can have instant access to a large number of randomly generated chronicles, which at this stage of the project are not yet available.
The Chronicle Simulator simulates the history of a patient's illness, and links the events in the history in a manner that closely resembles the expected output of the real Automatic Chronicler.
The current output format of the simulator is a relational database that stores six types of event2 (interventions, investigations, consults, drugs, problems and loci) and 14 types of relation between events (e.g., Problem 2 The term event is loosely used to denote dynamic (such as interventions) as well as static concepts (such as problems).
If the focus is on Interventions, the same information in the previous example will be presented as: Example 3  In week 483, histopathology revealed primary cancer of the right breast.
Radical mastectomy on the breast was performed to treat the cancer.
Radiotherapy was initiated to treat primary cancer of the right breast.
 In the weeks 492 to 496, 5 radiotherapy cycles were performed.
In an Investigation-focussed report, the intervention will be omitted, since they are not directly relevant: Example 4  In week 483, histopathology revealed primary cancer of the right breast  In week 491, examination revealed no abnormality of the leucocyte count or of the haemoglobin concentration, no lymphadenopathy of the right axillary lymphnodes, no enlargement of the spleen or of the liver and no recurrent cancer Locus, Intervention CAUSED-BY Problem, Intervention SUBPART-OF Intervention, Investigation HAS-INDICATION Problem).
Each event has a variable number of attributes, and each dynamic event is time-stamped with a start date and an end date3 . A typical chronicle contains around 350 events and about 600 relations.
HAS-LOCUS Locus Investigation Problem Intervention Locus Problem Investigation Intervention Locus Problem Drug Problem 4 Architecture The design of the Report Generator follows a classical pipeline architecture, with a content selector, content planner and syntactic realiser.
The Content Planner is tightly coupled to the Content Selector, since part of the discourse structure is already determined in the event selection phase.
Aggregation is mostly conceptual rather than syntactic, and thus it is performed in the content planning stage as well as during realisation (Reape and Mellish, 1999).
4.1 Content
selection Figure 1: Example of a generated spine structure The Content selection process represents the most important component of the Report Generator.
Although in some contexts it may be useful to generate reports containing all the events in a chronicle, the most useful types of report are the focused, summarised ones, for which good selection of important events is essential.
The process of content selection is currently driven by two parameters of a report: type and length.
We define the concept of report spine to represent a list of concepts that are essential to the construction of a given type of report.
For example, in a report of the diagnoses, all events of type Problem will be part of the spine.
Events linked to the spine through some kind of relation may or may not be included in the summary, depending on the type and length of the summary (see Figure 1).
The design of the system does not restrict the spine to containing only events of the same type.
In future extensions to the system where the user will be able to select facts they want in the summary, a spine could contain, for example, problems of type cancer, investigations of type x-ray and interventions of type surgery.
3 In
the current implementation of the chronicle, time stamps are week numbers starting with the date of the first diagnosis.
Spines are not predefined templates, but structures that are constructed dynamically with each request and they depend on the type of request and on the length of the summary.
Important events are selected according to semantic relations.
The first step in the selection process is to cluster related events based on the relations stored in the chronicle.
A cluster of events may tell us, for example, that a patient was diagnosed with cancer following a clinical examination, for which she had a mastectomy to remove the tumour, was given a histopathological test of the removed tumour, which confirmed the cancer, and had a complete radiotherapy course to treat the cancer; the radiotherapy caused an ulcer, which in turn was treated with some drug.
A typical chronicle contains a small number of clusters, typically one or two large clusters and several small ones.
Smaller clusters are generally not related to the main thread of events.
The summarisation process starts with the removal of small clusters, which in the current implementation are defined as clusters containing at most three events4 . This excludes some specified types of information that will be included in the report even when they only appear in short clusters; for example, all reports will contain essential information such as the initial diagnosis and the cause of death (if available).
The next step is the selection of important events, as defined by the type of report.
Each cluster of events is a graph, with some nodes representing spine events.
For each cluster, the spine events are selected, as well as all nodes that are at a distance of less than n from spine events, This threshold was set following a series of experiments.
fiwhere the depth n is a user-defined parameter used to adjust the size of the report.
For example, in the cluster presented in Fig.
2, assuming a depth value of 1, the content selector will choose cancer, left breast and radiotherapy but not radiotherapy cycle or ulcer.
Document planning cancer Ind i d_ cate By Has cus radiotherapy left breast Cau sed radiotherapy cycle ulcer Figure 2: Example of a cluster The first stage in structuring the body of the report is to combine messages linked through attributive relations (e.g., combining messages of type Problem with messages of type Locus if the Problem has a HAS-LOCUS relation pointing to a Locus).
In the second stage, messages are grouped according to specific rules, depending on the type of report.
For longitudinal reports, the rules stipulate that events occurring in the same week should be grouped together, and further grouped into years.
In event-specific reports, patterns of similar events are first identified and then grouped according to the week(s) they occur in.
For example, if in week 1 the patient was examined for enlargement of the liver and of the spleen with negative results and in week 2 the patient was again examined with the same results and had a mastectomy, two groups of events will be constructed: Example 5  In weeks 1 and 2, examination of the abdomen revealed no enlargement of the liver or of the spleen.
 In week 2, the patient underwent a mastectomy.
A document plan is typically a hierarchical structure that contains and combines the messages to be conveyed by the report generator.
Technically, a document plan is an ordered collection of message clusters, where messages within a cluster are combined using rhetorical relations, while individual clusters are ordered and linked according to the type of report.
The construction of document plans is partly performed in the content selection phase, since the content is selected according to the relations between events, which in turn provide information about the structure of the target text.
The actual document planner component is concerned with the construction of complete document plans, according to the type of report and cohesive relations identified in the previous stage.
A report typically consists of three parts: (a) a schematic description of the patient's demographic information (name, age, gender), (b) a two sentence summary of the patient's record (presenting the time span of the illness, the number of consults the patient attended, and the number of investigations and interventions performed) and (c) the actual report of the record produced from the events selected to be part of the content.
We focus here on this last part.
Within groups, messages are structured according to discourse relations that are either deduced from the input database or automatically inferred by applying domain specific rules.
At the moment, the input provides three types of rhetorical relation: Cause, Result and Sequence.
The domain specific rules specify the ordering of messages, and always introduce a Sequence relation.
An example of such a rule is that a histopathology event has to follow a biopsy event, if both of them are present and they start and end at the same time.
These rules facilitate the construction of a partial rhetorical structure tree.
Messages that are not connected in the tree are by default assumed to be in a List relation to other messages in the group, and their position is set arbitrarily.
The document planner also applies aggregation rules between similar messages and employs ellipsis and conjunction in order to create a more fluent text.
Simple aggregation rules state, for example, that two investigations with Figure 3: Aggregation of Investigation messages on the HAS-TARGET field the same name and two different target loci can be collapsed into one investigation with two target loci (Fig.3).
Aggregation rules of this type are designed to make the resulting text more fluent, however they do not always provide the degree of condensation required by the summary.
For example, each clinical examination consists of examinations of the abdomen for enlargement of internal organs (liver and spleen) and examination of the lymphnodes.
Thus, each clinical examination will typically consist of three independent Investigation events.
When fully aggregated according to conceptual and syntactical rules, the three Investigation messages are collapsed into one structure such as: Example 6 Examination revealed no enlargement of the spleen or of the liver and no lymphadenopathy of the axillary nodes.
this can be described as Clinical examination was normal, apart from an enlargement of the spleen.
4.3 Maintaining
the thread of discourse In producing multiple reports on the same patient from different perspectives, or of different types, we operate under the strong assumption that event-focussed reports should be organised in a way that emphasises the importance of the event in focus.
From a document structure viewpoint, this equates to constructing rhetorical structures where the focus event (i.e., the spine event) is expressed in a nuclear unit, and skeleton events are preferably in sattelite units.
Within sentences, spine events are assigned salient syntactical roles that allows them to be kept in focus.
For example, a relation such as Problem CAUSED-BY Intervention is more likely to be expressed as : The patient developed a Problem as a result of an Intervention.
However, this level of aggregation that only takes into account the semantics of individual messages may be not enough, since clinical examinations are performed repeatedly and consist of the same types of investigation.
Two approaches have been implemented in the Report Generator, both of which make use of domain specific rules.
The first is to report only events that deviate from the norm.
In the case of investigations, for example, this equates to reporting only those that have abnormal results.
The second, which produces larger reports, is to produce synthesised descriptions of events.
In the case of clinical examination for example, we could describe a sequence of investigations such as the one in example (5) as Clinical examination was normal.
If the examination deviates from the norm on a restricted numbers of parameters only, when the focus is on Problem events, and, when the focus is on Interventions as: An Intervention caused a Problem.
This kind of variation reflects the different emphasis that is placed on spine events, although the wording in the actual report may be different.
Rhetorical relations holding between simple event descriptions are most often realised as a single sentence (as in the examples above).
Complex individual events are realised in individual clauses or sentences which are connected to other accompanying events through the appropriate rhetorical relation.
For example, a Problem event has a large number of attributes, consisting of name, status, existence, number of nodes counted, number of nodes involved, clinical course, tumour size, genotype, grade, tumour marker and histology, as well as the usual time stamp.
The selection of attributes that are going to be included in a Problem description depends on a number of factors, including whether the Problem is a spine or a skeleton event, and whether the event is mentioned for the first time or is a subsequent mention.
Aditionally, the number of attributes included in the description of a Problem is a decisive factor in realising the Problem as a phrase, a sentence or a group of sentences.
In the following two examples, there are two Problem events (cancer and lymphnode count) linked through an Investigation event (excision biopsy, which is indicated by the first problem and has as a finding the second problem.
In Example 7, the problems are first mentioned spine events, while in Example 8, the problems are skeleton events (the cancer is a subsequent mention and the lymphnode count is a first mention), with the Investigation being the spine event.
Example 7 A 10mm, EGFR +ve, HER-2/neu +ve, oestrogen receptor positive cancer was found in the left breast (histology: invasive tubular adenocarcinoma).
Consequently, an excision biopsy was performed which revealed no metastatic involvement in the 5 nodes sampled.
Example 8 An excision biopsy on the left breast was performed because of cancer.
It revealed no metastatic involvement in the 5 nodes sampled.
5 Evaluation
Automatic evaluation of the generated reports is not possible, as there is no gold standard for such documents.
Additionally, a full-blown quantitative evaluation is not yet feasible, since our users are cancer specialists who cannot easily dedicate time to evaluating large numbers of reports.
However, we have conducted an informal survey with two cancer clinicians to gain feedback on the quality of the current output of the Report Generator.
To do this, we showed them three patient records encoded as chronicles, and, for each patient, two types of report produced from that record: a longitudinal report, and a summarised report of diagnoses.
The three patient records were selected to display a variety of events and sizes (a 6-year history containing 621 events, a 12-year history with 1418 events, and a 9-year history with 717 events).
Although they were (unusually) familiar with the coding scheme of the chronicles, the clinicians found it very difficult to extract a useful overview of the patients' histories from the three chronicles we showed them.
In contrast, they found the generated reports to be much more useful and the quality of the text to be very good.
The clinicians commended the reports for their ability to provide a quick and clear view of data that would be otherwise difficult to access and process.
Most importantly, the various report types were judged to be highly appropriate for use in clinical care.
Whilst this preliminary evaluation was conducted with the aim of finding early shortcomings of the Report Generator and receiving feedback from potential users, we are now embarking on a more extensive formal evaluation with cancer clinicians and medical researchers with specialist knowledge in the area of cancer.
We believe, however, that the true test of utility will be the actual use of the Report Generator in practice.
As can be seen from the examples above, the same basic rhetorical structure consisting of three nodes and two relations (causality and consequence) is realised differently in a Problemfocussed report compared to an Investigationbased report.
The conceptual reformulation is guided by the type of report, which in turn has consequences at syntactical level.
6 Conclusions
We have described a system that generates a range of health reports on individual cancer patients.
At present, our intended readership is composed of clinicians and medical researchers, and the fitype of report will depend on his or her stated needs.
Reports that are required at the point of care (e.g., for a doctor interviewing a newly referred patient, or a team of medics on ward rounds) are likely to be short "30-second" potted histories.
At other times longer, more detailed reports will be required, as will reports that focus on particular aspects of the patient's "journey" through their disease (e.g., from the perspective of the diagnoses that have been made, the drugs they have been prescribed, or surgery they have undergone).
The system is fully implemented in Java and currently generates this full range of reports on-the-fly.
A summarised report based on about 1000 input events is constructed in less than 2 seconds, a speed which is highly appropriate to the demands of clinical practice.
While the various types of generated report all share the same input (i.e., the patient's chronicle), and thus will have a large degree of conceptual overlap, clearly there will be occassions when information that is included in some reports will not be in others.
The range of reports for any given patient at any given point in their illness thus present a special class of paraphrase, with a looser adherance to semantic equivalence between versions than is typically found in other paraphrase generators, for example Kozlowski et al (2003), McKeown et al (1994), Power, Scott and Bouyaad-Agha (2003), Rosner and Stede (1994),(1996), and Scott and Souza (1990).
In this sense, our Report Generator is rather closer in spirit to Hovy's PAULINE system, which generates descriptions of given news events from different perspectives and with different stylistic goals (Hovy, 1988).
However, we achieve our goal with less reliance on terminological variation and more on structural variation at the discourse level.
Syntactic variation, where it does occur, is almost always simply a side-effect of an earlier discourse choice.
Terminological variation is deliberately avoided to prevent false implicatures; however, we are about to introduce a further class of readership, namely patients, at which stage we will make fuller use of our lexical resources.
7 Acknowledgments
CLEF is supported in part by grant G0100852 under the E-Science Initiative.
Thanks are due its clinical collaborators at the Royal Marsden and Royal Free hospitals, to colleagues at the National Cancer Research Institute (NCRI) and NTRAC and to its industrial collaborators.
Special thanks to Dr.
Jeremy Rogers who provided us with the automated Chronicle Simulator that we have used in all our experiments.
References Eduard H.
Hovy. 1988.
Generating natural language under pragmatic constraints.
Lawrence Erlbaum, Hillsdale, New Jersey.
Raymond Kozlowski, Kathleen F.
McCoy, and K.
Vijay-Shanker. 2003.
Generation of singlesentence paraphrases from predicate/argument structure using lexico-grammatical resources.
In Kentaro Inui and Ulf Hermjakob, editors, Proceedings of the Second International Workshop on Paraphrasing, pages 18.
Kathleen McKeown, Karen Kukich, and James Shaw.
1994. Practical issues in automatic document generation.
In Proceedings of the Fourth Conference on Applied Natural-Language Processing (ANLP-1994), pages 714, Stuttgart, Germany.
Richard Power, Donia Scott, and Nadjet BouayadAgha.
2003. Document structure.
Computational Linguistics, 29(2):211260.
Mike Reape and Chris Mellish.
1999. Just what is aggregation anyway?
In Proceedings of the 7th European Workshop on Natural Language Generation (EWNLG'99), pages 2029, Toulouse, France.
Dietmar R sner and Manfred Stede.
1994. Generating multilingual documents from a knowledge base: the TECHDOC project.
In Proceedings of the 15th conference on Computational Linguistics (Coling'94), pages 339343, Kyoto, Japan.
Donia Scott and Clarisse de Souza.
1990. Getting the message across in RST-based text generation.
In R.
Dale C.
Mellish and M.
Zock, editors, Current Research in Natural Language Generation, pages 31  56.
Academic Press.
Manfred Stede.
1996. Lexical paraphrases in multilingual sentence generation.
Machine Translation, 11:75107 .
Automatic Summarization of Open-Domain Multiparty Dialogues in Diverse Genres Klaus Zechner Educational Testing Service Automatic summarization of open-domain spoken dialogues is a relatively new research area.
This article introduces the task and the challenges involved and motivates and presents an approach for obtaining automatic-extract summaries for human transcripts of multiparty dialogues of four different genres, without any restriction on domain.
We address the following issues, which are intrinsic to spoken-dialogue summarization and typically can be ignored when summarizing written text such as news wire data: (1) detection and removal of speech disfluencies; (2) detection and insertion of sentence boundaries; and (3) detection and linking of cross-speaker information units (question-answer pairs).
A system evaluation is performed using a corpus of 23 dialogue excerpts with an average duration of about 10 minutes, comprising 80 topical segments and about 47,000 words total.
The corpus was manually annotated for relevant text spans by six human annotators.
The global evaluation shows that for the two more informal genres, our summarization system using dialoguespecific components significantly outperforms two baselines: (1) a maximum-marginal-relevance ranking algorithm using TF*IDF term weighting, and (2) a LEAD baseline that extracts the first n words from a text.
1. Introduction Although the field of summarizing written texts has been explored for many decades, gaining significantly increased attention in the last five to ten years, summarization of spoken language is a comparatively recent research area.
As the number of spoken audio databases is growing rapidly, however, we predict that the need for high-quality summarization of information contained in this medium will increase substantially.
Summarization of spoken dialogues, in particular, may aid in the archiving, indexing, and retrieval of various records of oral communication, such as corporate meetings, sales interactions, or customer support.
The purpose of this article is to explore the issues of spoken-dialogue summarization and to describe and evaluate an implementation addressing some of the core challenges intrinsic to the task.
We will use an implementation of a state-of-the-art text summarization method (maximum marginal relevance, or MMR) as the main baseline for comparative evaluations, and then add a set of components addressing issues specific to spoken dialogues to this MMR module to create our spoken dialogue summarization system, which we call DIASUMM.
We consider the following dimensions to be relevant for our research; the combination of these dimensions distinguishes our work from most other work in the field Educational Testing Service, Rosedale Road MS 11-R, Princeton, NJ 08541.
E-mail: kzechner@ets.org c 2002 Association for Computational Linguistics Computational Linguistics Volume 28, Number 4 of summarization:     spoken versus written language multiparty dialogues versus texts written by one author unrestricted versus restricted domains diverse genres versus a single genre The main challenges this work has to address, in addition to the challenges of writtentext summarization, are as follows:     coping with speech disfluencies identifying the units for extraction maintaining cross-speaker coherence coping with speech recognition errors We will discuss these challenges in more detail in the following section.
Although we have addressed the issue of speech recognition errors in previous related work (Zechner and Waibel 2000b), for the purpose of this article, we exclusively use human transcripts of spoken dialogues.
Intrinsic evaluations of text summaries usually use sentences as their basic units.
For our data, however, sentence boundaries are typically not available in the first place.
Thus we devise a word-based evaluation metric derived from an average relevance score from human relevance annotations (section 6.2).
The organization of this article is as follows: Section 2 provides the motivation for our research, introducing and discussing the main challenges of spoken-dialogue summarization, followed by a section on related work (section 3).
Section 4 describes the corpus we use to develop and evaluate our system, along with the procedures employed for corpus annotation.
The system architecture and its components are described in detail in section 5, along with evaluations thereof.
Section 6 presents the global evaluation of our approach, before we conclude the article with a discussion of our results, contributions, and directions for future research in this field (sections 7 and 8).
2. Motivation Consider the following example from a phone conversation drawn from the English CALLHOME database (LDC 1996).
It is a transcript of a conversation between two native speakers of American English; one person is in the New York area (speaker a), the other one (speaker b) in Israel.
It was recorded about a month after Yitzhak Rabin's assassination (1995).
This dialogue segment is about one minute of real time.
The audio is segmented into speaker turns using silence heuristics,1 and each turn is marked with a turn number and with the speaker label.
Noises are removed to increase readability.2 1 Therefore, in some cases, we can find several turns of one speaker following each other.
2 Hence
there can be "missing" turns (e.g., turn 37), in case they contain only noises and no actual words.
Zechner Automatic Summarization of Dialogues 28 a: oh 29 b: they didn't know he was going to get shot but it was at a peace rally so i mean it just worked out 30 b: i mean it was a good place for the poor guy to die i mean because it was you know right after the rally and everything was on film and everything 31 a: yeah 32 b: oh the whole country we just finished the thirty days mourning for him now you know it's uh oh everybody's still in shock it's 33 a: oh 34 a: i know 35 b: terrible what's going on over here 36 b: and this guy that killed him they show him on t v smiling he's all happy he did it and everything he isn't even sorry or anything 38 a: there are i 39 b: him him he and his brother you know the two of them were in it together and there's a whole group now it's like a a conspiracy oh it's eh 40 a: mm 41 a: with the kahane chai 42 b: unbelievable 43 b: yeah yeah it's all those people yeah you probably see them running around new york don't you they're all 44 a: yeah 45 a: oh yeah they're here 46 b: new york based yeah 47 a: oh there's 48 a: all those fanatics 49 a: like the extreme 50 b: oh but 51 b: but whwhat's the reaction in america really i mean i mean do people care you know i mean you know do they 52 a: yeah momost pei mean uh 53 a: i don't know what commui mean like the jewish community 54 a: a lot eall of us were 55 a: very upset and there were lots all the 56 b: yeah 57 a: like two days after did it happen like on a sunday 58 b: yeah it hapit happened on it happened on a saturday night By looking at this transcript we can readily identify some of the phenomena that would cause difficulties for conventional summarizers of written texts:  Some turns (e.g., turn 51) contain many disfluencies that (1) make them hard to read and (2) reduce the relevance of the information contained therein.
Some (important) pieces of information are distributed over a sequence of turns (e.g., turns 535455, 45474849); this is due to a silence-based 449 Computational Linguistics Volume 28, Number 4 segmentation algorithm that causes breaks in logically connected clauses.
A traditional summarizer might render these sequences incompletely.
 Some turns are quite long (e.g., 36, 39) and contain several sentences; a within-turn segmentation seems necessary to avoid the extraction of too much extraneous information when only parts of a turn contain relevant information.
Some of the information is constructed interactively by both speakers; the prototypical cases are question-answer pairs (e.g., turns 5152ff., turns 5758).
A traditional text summarizer might miss either question or answer and hence produce a less meaningful summary.
We shall discuss these arising issues along with an indication of our computational remedies in the following subsections.
We want to stress beforehand, though, that the originality of our system should not be seen in the particular implementation of its individual components, but rather in their selection and specific composition to address the issues at hand in an effective and also efficient way.
2.1 Disfluency
Detection The two main negative effects speech disfluencies have on summarization are that they (1) decrease the readability of the summary and (2) increase its noncontent noise.
In particular for informal conversations, the percentage of disfluent words is quite high, typically around 20% of the total words spoken.3 This means that this issue should, in our opinion, be addressed to improve the quality (readability and conciseness) of the generated summaries.
In section 5.3 we shall present three components for identifying most of the major classes of speech disfluencies in the input of the summarization system, such as filled pauses, repetitions, and false starts.
All detected disfluencies are marked in this process and can be selectively excluded during summary generation.
2.2 Sentence
Boundary Detection Unlike written texts, in which punctuation markers clearly indicate clause and sentence boundaries, spoken language is generated as a sequence of streams of words, in which pauses (silences between words) do not always match linguistically meaningful segments: A speaker can pause in the middle of a sentence or even a phrase, or, on the other hand, might not pause at all after the end of a sentence or clause.
This mismatch between acoustic and linguistic segmentation is reflected in the output of a speech recognizer, which typically generates a sequence of speaker turns whose boundaries are marked by periods of silence (or nonspeech).
As a result, one speaker's turn may contain multiple sentences, or, on the other hand, a speaker's sentence might span more than one turn.
In a test corpus of five English CALLHOME dialogues with an average length of 320 turns, we found on average of about 30 such continuations of logical clauses over automatically determined acoustic segments per dialogue.
The main problems for a summarizer would thus be (1) the lack of coherence and readability of the output because of incomplete sentences and (2) extraneous information due to extracted units consisting of more than one sentence.
In section 5.4 we 3 Although other studies have found percentages lower than this figure, we included content-less categories such as discourse markers or rhetorical connectives, which are often not regarded as disfluencies per se.
Zechner Automatic Summarization of Dialogues describe a component for sentence segmentation that addresses both of these problems.
2.3 Distributed
Information Since we have multiparty conversations as opposed to monologues, sometimes the crucial information is found in a sequence of turns from several speakers, the prototypical case of this being a question-answer pair.
If the summarizer were to extract only the question or only the answer, the lack of the corresponding answer or question would often cause a severe reduction of coherence in the summary.
In some cases, either the question or the answer is very short and does not contain any words with high relevance that would yield a substantial weight in the summarizer.
In order not to lose these short sentences at a later stage, when only the most relevant sentences are extracted, we need to identify matching question-answer pairs ahead of time, so that the summarizer can output the matching sentences during summary generation as one unit.
We describe our approach to cross-speaker information linking in section 5.5. 2.4 Other Issues We see the work reported in this article as the first in-depth analysis and evaluation in the area of open-domain spoken-dialogue summarization.
Given the large scope of this undertaking, we had to restrict ourselves to those issues that are, in our opinion, the most salient for the task at hand.
A number of other important issues for summarization in general and for speech summarization in particular are either simplified or not addressed in this article and left for future work in this field.
In the following, we briefly mention some of these issues, indicating their potential relevance and promise.
2.4.1 Topic
Segmentation.
In many cases, spoken dialogues are multitopical.
For the English CALLHOME corpus, we determined an average topic length of about one to two minutes' speaking time (or about 200400 words).
Summarization can be accomplished faster and more concisely if it operates on smaller topical segments rather than on long pieces of input consisting of diverse topics.
Although we have implemented a topic segmentation component as part of our system for these reasons, all of the evaluations are based on the topical segments determined by human annotators.
Therefore, this component will not be discussed in this article.
Furthermore, topical segmentation is not an issue intrinsic to spoken dialogues, which in our opinion justifies this simplification.
2.4.2 Anaphora
Resolution.
An analogous reasoning holds for the issue of anaphora resolution: Although it would certainly be desirable, for the sake of increased coherence and readability, to employ a well-working anaphora resolution component, this issue is not specific to the task at hand, either.
One could argue that particularly for summarization of more informal conversations, in which personal pronouns are rather frequent, anaphora resolution might be more helpful than for, say, summarization of written texts.
But we conjecture that this task might also prove more challenging than written-text anaphora resolution.
In our system, we did not implement a module for anaphora resolution.
2.4.3 Discourse
Structure.
Previous work indicates that information about discourse structure from written texts can help in identifying the more salient and relevant sentences or clauses for summary generation (Marcu 1999; Miike et al.1994). Much 451 Computational Linguistics Volume 28, Number 4 less exploration has been done, however, in the area of automatic analysis of discourse structure for non-task-oriented spoken dialogues in unrestricted domains, such as CALLHOME (LDC 1996).
Research for those kinds of corpora reported in Jurafsky et al.(1998), Stolcke et al.(2000), Levin et al.(1999), and Ries et al.(2000) focuses more on detecting localized phenomena such as speech acts, dialogue games, or functional activities.
We conjecture that there are two reasons for this: (1) free-flowing spontaneous conversations have much less structure than task-oriented dialogues, and (2) the automatic detection of hierarchical structure would be much harder than it is for written texts or dialogues based on a premeditated plan.
Although we believe that in the long run attempts to automatically identify the discourse structure of spoken dialogues may benefit summarization, in this article, we greatly simplify this matter and exclusively look at local contexts in which speakers interactively construct shared information (the question-answer pairs).
2.4.4 Speech
Recognition Errors.
Throughout this article, our simplifying assumption is that our input comes from a perfect speech recognizer; that is, we use human textual transcripts of the dialogues in our corpus.
Although there are cases in which this assumption is justifiable, such as transcripts provided by news services in parallel to the recorded audio data, we believe that in general a spoken dialogue summarizer has to be able to accept corrupted input from an automatic speech recognizer (ASR), as well.
Our system is indeed able to work with ASR output; it is integrated in a larger system (Meeting Browser) that creates, summarizes, and archives meeting records and is connected to a speech recognition engine (Bett et al.2000). Further, we have shown in previous work how we can use ASR confidence scores (1) to reduce the word error rate within the summary and (2) to increase the summary accuracy (Zechner and Waibel 2000b).
2.4.5 Prosodic
Information.
A further simplifying assumption of this work is that prosodic information is not available, with the exception of start and end times of speaker turns.
Considering the results reported by Shriberg et al.(1998) and Shriberg et al.(2000), we conjecture that future work in this field will demonstrate the additional benefit of incorporating prosodic information, such as stress, pitch, and intraturn pauses, into the summarization system.
In particular, we would expect improved system performance when speech recognition hypotheses are used as input: In that case, the prosodic information could compensate to some extent for incorrect word information.
3. Related Work The vast majority of summarization research in the past clearly has focused exclusively on written text.
A good selection of both early seminal papers and more recent work can be found in Mani and Maybury (1999).
In general, most summarization approaches can be classified as either corpus-based, statistical summarization (such as Kupiec, Pedersen, and Chen [1995]), or knowledge-based summarization (such as Reimer and Hahn [1988]) in which the text domain is restricted.
(The MMR method [Carbonell, Geng, and Goldstein 1997], which we are using as the summarization engine for our DIASUMM system, belongs to the first category).
More recently, Marcu (1999) presented work on using automatically detected discourse structure for summarization.
Knight and Marcu (2000) and Berger and Mittal (2000) presented approaches in which summarization can be reformulated as a problem of machine translation: 452 Zechner Automatic Summarization of Dialogues translating a long sentence into a shorter sentence, or translating a Web page into a brief gist, respectively.
Two main areas are exceptions to the focus on text summarization in past work: (1) summarization of task-oriented dialogues in restricted domains and (2) summarization of spoken news in unrestricted domains.
We shall discuss both of these areas in the following subsections, followed by a discussion of prosody-based emphasis detection in spoken language, and finally by a summary of research most closely related to the topic of this work.
3.1 Summarization
of Dialogues in Restricted Domains During the past decade, there has been significant progress in the area of closeddomain spoken-dialogue translation and understanding, even with automatic speech recognition input.
Two examples of systems developed in that time frame are JANUS (Lavie et al.1997) and VERBMOBIL (Wahlster 1993).
In that context, several spoken-dialogue summarization systems have been developed whose goal it is to capture the essence of the task-based dialogues at hand.
The MIMI system (Kameyama and Arima 1994; Kameyama, Kawai, and Arima 1996) deals with the travel reservation domain and uses a cascade of finite-state pattern recognizers to find the desired information.
Within VERBMOBIL, a more knowledge-rich approach is used (Alexandersson and Poller 1998; Reithinger et al.2000). The domain here is travel planning and negotiation of a trip.
In addition to finite-state transducers for content extraction and statistical dialogue act recognition, VERBMOBIL also uses a dialogue processor and a summary generator that have access to a world knowledge database, a domain model, and a semantic database.
The abstract representations built by this summarizer allow for summary generation in multiple languages.
3.2 Summarization
of Spoken News Within the context of the Text Retrieval Conference (TREC) spoken document retrieval (SDR) conferences (Garofolo et al.1997; Garofolo et al.1999) as well as the recent Defense Advanced Research Project Agency (DARPA) broadcast news workshops, a number of research groups have been developing multimedia browsing tools for text, audio, and video data, which should facilitate the access to news data, combining different modalities.
Hirschberg et al.(1999) and Whittaker et al.(1999) present a system that supports local navigation for browsing and information extraction from acoustic databases, using speech recognizer transcripts in tandem with the original audio recording.
Although their interface helps users in the tasks of relevance ranking and fact finding, it is less helpful in the creating of summaries, partly because of imperfect speech recognition.
Valenza et al.(1999) present an audio summarization system that combines acoustic confidence scores with relevance scores to obtain more accurate and reliable summaries.
An evaluation showed that human judges preferred summaries with a compression rate of about 15% (30 words per minute at a speaking rate of about 200 words per minute) and that the summary word error rate was significantly smaller than the word error rate for the full transcript.
Hori and Furui (2000) use salience features in combination with a language model to reduce Japanese broadcast news captions by about 3040% while keeping the meaning of about 72% of all sentences in the test set.
Another speech-related reduction approach was presented recently by Koumpis and Renals (2000), who summarize voice mail in the Small Message format.
453 Computational Linguistics Volume 28, Number 4 3.3 Prosody-Based Emphasis Detection in Spoken Audio Whereas most approaches to summarizing acoustic data rely on the word information (provided by a human or ASR transcript), there have been attempts to generate summaries based on emphasized regions in a discourse, using only prosodic features.
Chen and Withgott (1992) train a hidden Markov model on transcripts of spontaneous speech, labeled for different degrees of emphasis by a panel of listeners.
Their "audio summaries" on an unseen (but rather small) test set achieve a remarkably good agreement with human annotators ( > 0.5).
Stifelman (1995) uses a pitch-based emphasis detection algorithm developed by Arons (1994) to find emphasized passages in a 13-minute discourse.
In her analysis, she finds good agreement between these emphasized regions and the beginnings of manually marked discourse segments (in the framework of Grosz and Sidner [1986]).
Although these are promising results, being suggestive of the role of prosody for determining emphasis, relevance, or salience in spoken discourse, in this work we restrict the use of prosody to the turn length and interturn pause features.
We conjecture, however, that the integration of prosodic and word level information would be a fruitful research area that would have to be explored in future work.
3.4 Spoken
Dialogue Summarization in Unrestricted Domains Waibel, Bett, and Finke (1998) report results of their summarizer on automatically transcribed SWITCHBOARD (SWBD) data (Godfrey, Holliman, and McDaniel 1992), the word error rate being about 30%.
Their implementation used an algorithm inspired by MMR, but they did not address any dialogueor speech-related issues in their summarizer.
In a question-answer test with summaries of five dialogues, participants could identify most of the key concepts using a summary size of only five turns.
These results varied widely (between 20% and 90% accuracy) across the five different dialogues tested in this experiment.
Our own previous work (Zechner and Waibel 2000a) addressed for the first time the combination of challenges of dialogue summarization with summarization of spoken language in unrestricted domains.
We presented a first prototype of DIASUMM that addressed the issues of disfluency detection and removal and sentence boundary detection, as well as cross-speaker information linking.
This work extends and expands these initial attempts substantially, in that we are now focusing on (1) a systematic training of the major components of the DIASUMM system, enabled by the recent availability of a large corpus of disfluency-annotated conversations (LDC 1999b), and (2) the exploration of three more genres of spoken dialogues in addition to the English CALLHOME corpus (NEWSHOUR, CROSSFIRE, GROUP MEETINGS).
Further, the relevance annotations are now performed by a set of six human annotators, which makes the global system evaluation more meaningful, considering the typical divergence among different annotators' relevance judgments.
4. Data Annotation 4.1 Corpus Characteristics Table 1 provides the statistics on the corpus used for the development and evaluation of our system.
We use data from four different genres, two being more informal, two more formal:  English CALLHOME and CALLFRIEND: from the Linguistic Data Consortium (LDC) collections, eight dialogues for the devtest set Zechner Automatic Summarization of Dialogues Table 1 Data characteristics for the corpus (average over dialogues).
8E-CH, 4E-CH: English CallHome; NHOUR: NewsHour; XFIRE: CrossFire; G-MTG: Group Meetings.
Data Set 8E-CH 4E-CH NHOUR formal yes 3 8 2 25 101 4.1 6.3 2.0 1224 12.1 5.1 48 0.48 64.8 17.2 3.4 0.7 13.8 XFIRE G-MTG Formal/informal informal informal Topics predetermined no no Dialogue excerpts (total) 8 4 Topical segments (total) 28 23 Different speakers 2.1 2 Turns 242 276 Sentences 280 366 Sentences per turn 1.2 1.3 Questions (in %) 3.7 6.4 False starts (in %) 12.1 11.0 Words 1685 1905 Words per sentence 6.0 5.2 Disfluent (in %) 16.0 16.3 Disfluencies 222 259 Disfluencies per sentence 0.79 0.71 Empty coordinating conjunctions (in %) 30.3 30.4 Lexicalized filled pauses (in %) 18.8 21.0 Editing terms (in %) 3.6 1.6 Nonlexicalized filled pauses (in %) 20.8 29.9 Repairs (in %) 26.6 17.1 (8E-CH) and four dialogues for the eval set (4E-CH).4 These are recordings of phone conversations between two family members or friends, typically about 30 minutes in length; the excerpts we used were matched with the transcripts, which typically represent 510 minutes of speaking time.
   NEWSHOUR (NHOUR): Excerpts from PBS's NewsHour television show with Jim Lehrer (recorded in 1998).
CROSSFIRE (XFIRE): Excerpts from CNN's CrossFire television show with Bill Press and Robert Novak (recorded in 1998).
GROUP MEETINGS (G-MTG): Excerpts from recordings of project group meetings in the Interactive Systems Labs at Carnegie Mellon University.
Furthermore, we used the Penn Treebank distribution of the SWITCHBOARD corpus, annotated with disfluencies, to train the major components of the system (LDC 1999b).
From Table 1 we can see that the two more formal corpora, NEWSHOUR and CROSSFIRE, have longer sentences, more sentences per turn, and fewer disfluencies (particularly nonlexicalized filled pauses and false starts) than English CALLHOME and the GROUP MEETINGS.
This means that their flavor is more like that of written text and not so close to the conversational speech typically found in the SWITCHBOARD or CALLHOME corpora.
4 We
used the devtest set corpus for system development and tuning and set aside the eval set for the final global system evaluation.
For the other three genres, two dialogue excerpts each were used for the devtest set, the remainder for the eval set.
Computational Linguistics Volume 28, Number 4 4.2 Corpus Annotation 4.2.1 First Annotation Phase.
All the annotations were performed on human-generated transcripts of the dialogues.
The CALLHOME and GROUP MEETINGS dialogues were automatically partitioned into speaker turns (by means of a silence heuristic); the other corpora were segmented manually (based on the contents and flow of the conversation).5 There were six naive human annotators performing the task;6 only four, however, completed the entire set of dialogues.
Thus, the number of annotations available for each dialogue varies from four to six.
Prior to the relevance annotations, the annotators had to mark topical boundaries, because we want to be able to define and then create summaries for each topical segment separately (as opposed to a whole conversation consisting of multiple topics).
The notion of a topic was informally defined as a region in the text that ends, according to the annotation manual, "when the speakers shift their topic of discussion".
Once the topical segments were marked, for each such segment, each annotator had to identify the most relevant information units (IUs), called nucleus IUs, and somewhat relevant IUs, called satellite IUs.
IUs are often equivalent to sentences but can span longer or shorter contiguous segments of text, dependent on the annotator's choice.
The overall goal of this relevance markup was to create a concise and readable summary containing the main information present in the topical segment.
Annotators were also asked to mark the most salient words within their annotated IUs with a +, which would render a summary with a somewhat more telegraphic style (+-marked words).
We also asked that the human annotators stay within a preset target length for their summaries: The +-marked words in all IUs within a topical segment should be 1020% of all the words in the segment.
The guideline was enforced by a checker program that was run during and after annotation of a transcript and that also ensured that no markup errors and no accidental word deletions occurred.
We provide a brief example here (n[, n] mark the beginning and end of a nucleus IU, the phrase they fly to Boston was +-marked as the core content within this IU): B: heck it might turn out that you know n[ if +they +fly in +to +boston i can n] 4.2.2 Creation of Gold-Standard Summaries.
After the first annotation phase, in which each coder worked independently according to the guidelines described above, we devised a second phase, in which two coders from the initial group were asked to create a common-ground annotation, based on the majority opinion of the whole group.
To construct such a majority opinion guideline automatically, we assigned weights to all words in nucleus IUs and satellite IUs and added all weights for all marked words of all coders for every turn.7 The total turn weights were then sorted by decreasing value to provide a guide for the two coders in the second phase as to which turns they should focus their annotations on for the common-ground or gold-standard summaries.
5 This
fact may partially account for NEWSHOUR and CROSSFIRE turns being longer than CALLHOME and GROUP MEETING turns.
6 Naive
in this context means that they were nonexperts in linguistics or discourse analysis.
7 The
weights were set as follows: nucleus IUs: 3.0 if +-marked, 2.0 otherwise; satellite IUs: 1.0 if +-marked, 0.5 otherwise.
Zechner Automatic Summarization of Dialogues Table 2 Nuclei and satellites: Length in tokens and relative frequency (in % of all tokens).
Annotator/ Data Set LB BR SC RW RC JK Gold CALLHOME NEWSHOUR CROSSFIRE MEETINGS All Dialogues Avg.
Nuc. Avg.
Sat. Nuc-Tokens Nuc-+-Marked Sat-Tokens Sat-+-Marked Length Length (in %) (in %) (in %) (in %) 12.993 16.507 20.720 22.899 23.741 39.203 21.763 17.108 25.828 33.923 37.674 23.152 13.732 14.551 14.093 19.576 18.553 9.794 6.462 13.099 16.733 22.132 23.413 16.173 11.646 11.978 29.412 19.352 43.573 26.355 13.934 21.962 29.536 21.705 23.034 22.796 8.558 8.339 18.045 11.332 15.434 11.204 6.573 11.003 13.530 10.615 9.222 10.807 5.363 10.558 6.517 2.757 12.749 0.711 0.179 5.126 4.300 1.853 7.456 4.665 3.818 7.645 4.796 1.718 0.333 0.465 0.000 1.932 2.947 0.976 1.123 1.636 Other than this guideline, the requirements were almost exactly identical to those in phase 1, except that (1) the pair of annotators was required to work together on this task to be able to reach a consensus opinion, and (2) the preset relative word length of the gold summary (1020%) applied only to the nucleus IUs.
As for the topical boundaries, which obviously vary among coders, a list of boundary positions chosen by the majority (at least half) of the coders in the first phase was provided.
In this gold-standard phase, the two coders mostly stayed with these suggestions and changed less than 15% of the suggested topic boundaries, the majority of which were minor (less than two turns' difference in boundary position).
4.2.3 General
Annotation Analysis.
Table 2 provides the statistics on the frequencies of the annotated nucleus and satellite IUs.
We make the following observations:  On average, about 23% of all tokens were assigned to a nucleus IU and 5% to a satellite IU; counting only the +-marked tokens, this reduces to about 11% and 2% of all tokens, respectively.
The average total lengths of nuclei and satellites vary widely across corpora: between 17.1 (13.1) tokens for CALLHOME and 37.7 (23.4) tokens for GROUP MEETINGS data.
This is most likely a reflection on the typical length of turns in the different subcorpora.
A similar variation is also observed across annotators: between 12 and 40 tokens for nucleus-IUs and between 9 and 20 tokens for satellites.
The granularity of IUs is quite different across annotators.
Since some annotators mark a larger number of IUs than others, there is an even larger discrepancy in the relative number of words assigned to nucleus IUs and satellite IUs among the different annotators: 1144% (nucleus IUs) and 013% (satellite IUs).
The ratio of nucleus versus satellite tokens also varies greatly among the annotators: from about 1:1 to 40:1.
457 Computational Linguistics Volume 28, Number 4 The ratio of nucleus and satellite tokens that are +-marked varies greatly: between 36 and 77% for nucleus IUs and between 2 and 80% for satellite IUs.
From these observations, we conclude that merging the nucleus and satellite IUs into one class would yield a more consistent picture than keeping them separate.
A similar argument can be made for the +-marked passages, in which we also find a quite high intercoder variation in relative +-marking.
This led us to the decision of giving equal weight to any word in an IU, irrespective of IU type or marking, for the purpose of global system evaluation.
Finally, we conjecture that the average length of our extraction units should be in the 1040 words range, which roughly corresponds to about 312 seconds of real time, assuming an average word length of 300 milliseconds.
As a comparison, we note that Valenza et al.(1999) found summaries with 30-grams8 working well in their experiments, a finding that is in line with our observations here on typical human IU lengths.
4.2.4 Intercoder
Agreement.
Agreement between coders (and between automatic methods and coders) has been measured in the summarization literature with quite a wide range of methods: Rath, Resnick, and Savage (1961) use Kendall's ; Kupiec, Pedersen, and Chen (1995) (among many others) use percentage agreement; and Aone, Okurowski, and Gorlinsky (1997) (among others) use the notions of precision, recall, and F1 -score, which are commonly employed in the information retrieval community.
Similarly, in the literature on discourse segmentation and labeling, a variety of different agreement measures have been used, including precision and recall (Hearst 1997; Passonneau and Litman 1997), Krippendorff's (1980) (Passonneau and Litman 1997) and Cohen's (1960) (Carletta et al.1997). In this work, we use the two following metrics: (1) the -statistic in its extension for more than two coders (Davies and Fleiss 1982); and (2) precision, recall, and F1 -score.9 We will discuss the -statistic first.
For intercoder agreement with respect to topical boundaries, agreement is found if boundaries fall within the same 50-word bin of a dialogue.
Relevance agreements are computed at the word level.
For relevance markings, we compute both for the three-way case (nucleus IUs, satellite IUs, unmarked) and the two-way case (any IUs, unmarked).10 Topical-boundary agreement was not evaluated for two of the GROUP MEETINGS dialogues, in which only one of four annotators marked any text-internal topic boundary.
We compute agreements for each dialogue separately and report the arithmetic means for the five subcorpora in Table 3.
We observe that agreement for topical boundaries is much higher than for relevance markings.
Furthermore, agreement is generally higher for CALLHOME and comparatively low for the GROUP MEETINGS corpus.
As a second evaluation metric, we compute precision, recall, and F1 -scores for the same four annotators and the same sets of subcorpora as before.
For topical boundaries, a match means that the boundaries fall within 3 turns of each other, and for relevant 8 A 30-gram is a passage of text containing 30 adjacent words.
9 Precision
is the ratio of correctly matched items over all items (boundaries, marked words); recall is the ratio of correctly matched items over all items that need to be matched; and the F1 -score combines 2PR precision (P) and recall (R) in the following way: F1 = P+R. 10 These computations were performed for those four (out of six) annotators who completed the entire corpus markup.
Zechner Automatic Summarization of Dialogues Table 3 Intercoder annotation agreement for topical boundaries and relevance markings.
8E-CH Topical boundaries Relevance markings (3 way) Relevance markings (2 way) 0.503 0.147 0.157 4E-CH 0.402 0.161 0.169 NHOUR 0.256 0.123 0.124 XFIRE 0.331 0.089 0.100 G-MTG 0.174 0.040 0.046 Overall 0.384 0.117 0.126 Table 4 Intercoder annotation F1 -agreement for topical boundaries and relevance markings.
8E-CH Topical boundaries Relevance markings (2 way) .54 .38 4E-CH .44 .39 NHOUR .53 .38 XFIRE .38 .32 G-MTG .18 .32 Overall .45 .36 words a match means that the two words to be compared are both in a nucleus or satellite IU.
The results can be seen in Table 4.
4.2.5 Disfluency
and Sentence Boundary Annotation.
In addition to the annotation for topic boundaries and relevant text spans, the corpus was also annotated for speech disfluencies in the same style as the Penn Treebank SWITCHBOARD corpus (LDC 1999b).
One coder (different from the six annotators mentioned before) manually tagged the corpus for disfluencies and sentence boundaries following the SWITCHBOARD disfluency annotation style book (Meteer et al.1995). 4.2.6 Question-Answer Annotation.
A final type of annotation was performed on the entire corpus to mark all questions and their answers, for the purpose of training and evaluation of the question-answer linking system component.
Questions and answers were annotated in the following way: Every sentence that is a question was marked as either a Yes-No-question or a Wh-question.
Exceptions were back-channel questions, such as "Is that right?"; rhetorical questions, such as "Who would lie in public?"; and other questions that do not refer to a propositional content.
These were not supposed to be marked (even if they have an apparent answer), since we see the latter class of questions as irrelevant for the purpose of increasing the local coherence within summaries.
For each Yes-No-question and Wh-question that has an answer, the answer was marked with its relative offset to the question to which it belongs.
Some answers are continued over several sentences, but only the core answer (which usually consists of a single sentence) was marked.
This decision was made to bias the answer detection module toward brief answers and to avoid the question-answer regions' getting too lengthy, at the expense of summary conciseness.
5. Dialogue Summarization System 5.1 System Architecture The global system architecture of the spoken-dialogue summarization system presented in this article (DIASUMM) is depicted in Figure 1.
The input data are a timeordered sequence of speaker turns with the following quadruple of information: start time, end time, speaker label, and word sequence.
The seven major components are executed sequentially, yielding a pipeline architecture.
459 Computational Linguistics dialogue transcript Volume 28, Number 4 POS Tagger Disfluency Detection Sentence Boundary Detection Extraction Unit Identification False Start Detection (+ Chunk Parser) Question & Answer Detection Repetition Filter Topic Segmentation Sentence Ranking & Selection dialogue summary Figure 1 Global system architecture.
The following subsections describe the components of the system in more detail.
As argued earlier, the topic detection component is not relevant for the way we conduct the global system evaluation and hence is not discussed here.
(We implemented a variant of Hearst's [1997] TextTiling algorithm).
The three components involved in disfluency detection are the part-of-speech (POS) tagger, the false-start detection module, and the repetition filter.
They are discussed in subsection 5.3, followed by a subsection on sentence boundary detection (5.4).
The question-answer pair detection is described in subsection 5.5, and the sentence selection module, performing relevance ranking, in subsection 5.6. 5.2 Input Tokenization We eliminate all human and nonhuman noises and incomplete words from the input transcript.
Further, we eliminate all information on case and punctuation, since 460 Zechner Automatic Summarization of Dialogues we emulate the ASR output in that regard, which does not provide this information.
Contractions such as don't or I'll are divided and treated as separate words--in these examples we would obtain do n't and I 'll.
5.3 Disfluency
Detection 5.3.1 Motivation.
Conversational, informal spoken language is quite different from written language in that a speaker's utterances are typically much less well-formed than a writer's sentences.
We can observe a set of disfluencies such as false starts, hesitations, repetitions, filled pauses, and interruptions.
Additionally, in speech there is no good match between linguistically motivated sentence boundaries and turn boundaries or recognition hypotheses from automatic speech recognition.
5.3.2 Types
of Disfluencies.
The classification of disfluencies in this work follows Shriberg (1994), Meteer et al.(1995), and Rose (1998).
It is worth noting, however, that any disfluency classification will be only an approximation of the assumed real phenomena and that often boundaries between different classes are fuzzy and hard to decide for human annotators (cf.
Meteer et al.[1995] on annotators' problems with the classification of the word so).
 Filled pauses: We follow Rose's (1998) classification of nonlexicalized filled pauses (typically uh, um) and lexicalized filled pauses (e.g., like, you know).
Whereas the former are usually nonambiguous and hence easy to detect, the latter are ambiguous and much harder to detect accurately.
Restarts or repairs: These are fragments that are resumed, but without completely abandoning the first attempt.
We follow the notation in Meteer et al.(1995) and Shriberg (1994), which has these parts: (1) reparandum, (2) interruption point (+), (3) interregnum (editing phase, {.
. . }), and (4) repair.
    Repetition: A restart with a verbatim repetition of a word or a sequence of words: [ she is + she is ] happy.
Insertion: A repetition of the reparandum, with some word(s) inserted: [ she liked + {um} she really liked ] it.
Substitution: The reparandum is not repeated: [ she + {uh} my wife ] liked it.
False starts: These are abandoned, incomplete clauses.
In some cases, they may occur at the end of an utterance, and they can be due to interruption by another speaker.
Example: so we didn't--they have not accepted our proposal.
5.3.3 Related
Work.
The past decade has produced a substantial amount of research in the area of detecting intonational and linguistic boundaries in conversational speech, as well as in the area of detecting and correcting speech disfluencies.
Whereas earlier work tended to look at these phenomena in isolation (Nakatani and Hirschberg 1994; Stolcke and Shriberg 1996), more recent work has attempted to solve several tasks within one framework (Heeman and Allen 1999; Stolcke et al.1998). Most approaches use some kind of prosodic information, such as duration of pauses, stress, and pitch contours, and most of them combine this prosodic information with information about word identity and sequence (n-grams, hidden Markov 461 Computational Linguistics Volume 28, Number 4 models).
In the study of Stolcke et al.(1998), the goal was to detect sentence boundaries and a variety of speech disfluencies on a large portion of the SWITCHBOARD corpus.
An explicit comparison was made between prosodic and word-based models, and the results showed that an n-gram model, enhanced with segmental information about turn boundaries, significantly outperformed the prosodic model.
Model combination improved the overall results, but only to a small extent.
In more recent research, Shriberg et al.(2000) reported that for sentence boundary detection in two different corpora (BROADCAST NEWS and SWITCHBOARD), prosodic models outperform word-based language models and a model combination yields additional performance gains.
5.3.4 Overview.
In the following, we will discuss the three components of the DIASUMM system that perform disfluency detection:  a POS tagger that tags, in addition to the standard SWITCHBOARD Treebank-3 tag set (LDC 1999b), the following disfluent regions or words: 1.
coordinating conjunctions that don't serve their usual connective role, but act more as links between subsequent speech acts of a speaker (e.g., and then; we call these empty coordinating conjunctions in this work) lexicalized filled pauses (labeled as discourse markers in the Treebank-3 corpus; e.g., you know, like) editing terms within speech repairs (e.g., I mean) nonlexicalized filled pauses (e.g., um, uh) a decision tree (supported by a shallow chunk parser) that decides whether to label a particular sentence as a false start a repetition detection script (for repeated sequences of up to four words) 5.3.5 Training Corpus.
For training, we used a part of the SWITCHBOARD transcripts that was manually annotated for sentence boundaries, POS, and the following types of disfluent regions (LDC 1999b):       {A.
. . }: asides (very rare; we ignore them in our experiments) {C.
. . }: empty coordinating conjunctions (e.g., and then) {D.
. . }: discourse markers (i.e., lexicalized filled pauses in our terminology, e.g., you know) {E.
. . }: editing terms (within repairs; e.g., I mean) {F.
. . }: filled pauses (nonlexicalized; e.g., uh) [.
. . + . . .]: repairs: the part before the + is called reparandum (to be removed), the part after the + repair (proper) Sentence boundaries can be at the end of completed sentences (E S) or of noncompleted sentences, such as false starts or abandoned clauses (N S).
462 Zechner Automatic Summarization of Dialogues Table 5 Precision, recall and F1 -scores of the four disfluency tag categories for the SWITCHBOARD test set.
Description Empty coordinating conjunctions Lexicalized filled pauses Editing terms Nonlexicalized filled pauses Count 5,990 5,787 1,004 12,926 Tag CO DM ET UH Precision 0.84 0.95 0.98 0.98 Recall 0.93 0.90 0.94 0.98 F1 0.88 0.93 0.96 0.98 Table 6 POS tagging accuracy on five subcorpora (evaluated on 500-word samples).
8E-CH Known words Unknown words (total) Overall 92.8 48.0 (25) 90.6 4E-CH 90.6 44.4 (9) 89.8 NHOUR 92.7 69.6 (23) 91.6 XFIRE 90.6 86.4 (22) 90.4 G-MTG 93.2 92.6 (27) 93.2 5.3.6 POS Tagger.
We are using Brill's rule-based POS tagger (Brill 1994).
Its basic algorithm at run time (after training) can be described as follows: 1.
2. Tag every word with its most likely tag, predicting tags of unknown words based on rules.
Change every tag according to its right and left context (both words and tags are considered), following a list of rules.
For preprocessing, we replaced the tags in the regions of {C.
. . }, {D.
. . }, and {E.
. . } with the tags CO (coordinating), DM (discourse marker), and ET (editing term), respectively.
(The filler regions {F.
. . } are already tagged with UH in the corpus).
Lines that contain typographical errors were excluded from the training corpus.
We further eliminated all incomplete words (XX tag) and combined multiwords, marked by a GW tag, into a single word (hence eliminating the GW tag).11 The entire resulting new tag set had 42 tags.12 Training of the POS tagger proceeded in three stages, using about 250,000 tagged words for each stage.
The trained POS tagger's performance on an unseen test set of about 185,000 words is 94.1% tag accuracy (untrained baseline: 84.8% accuracy).
Table 5 shows precision, recall, and F1 -scores for the four categories of disfluency tags, measured on the test set after the last training phase.
We see that the nonlexicalized filler words are almost perfectly tagged (F1 = 0.98), whereas the hardest task for the tagger is the empty coordinating conjunctions (F1 = 0.88): There are a few highly ambiguous words in that set, such as and, so, and or.
Table 6 shows the POS tagging accuracy on the five subcorpora of our dialogue corpus, evaluated on a sample of 500 words per subcorpus.
We see that the POStagging accuracy is slightly lower than for the SWITCHBOARD set that was used for 11 The sole function of the GW tag is to label words that are considered to be parts of other words but were transcribed separately, such as: drug/GW testing/NN.
12 For
a description of the POS tags used in that database see Santorini (1990) and LDC (1999a).
Computational Linguistics Volume 28, Number 4 Table 7 Disfluency tag detection (F1 ) for five subcorpora (results in parentheses: less than 10 tags to be detected).
8E-CH CO DM ET UH .89 .93 .95 .56 4E-CH .89 .73 .95 .62 NHOUR .38 .90 (.94) (.14) XFIRE .77 .82 .85 (.28) G-MTG .54 .30 .88 .45 training (approximately 9093%; global average: 91.1%).
Further we observe that with the exception of the CALLHOME corpora, the majority of unknown words were actually tagged correctly.
The most frequent errors were (1) conjunctions tagged as empty coordinated conjunctions, (2) proper names tagged as regular nouns, and (3) adverbs tagged as adjectives.
Finally, we look at the POS tagger's performance for the four disfluency tags CO, DM, ET, and UH in our five subcorpora; the results of this evaluation are presented in Table 7.
We can see that the detection accuracy is generally lower than for the corpus on which we trained the tagger (SWITCHBOARD), but still quite good in general.
The major exceptions are the UH tags, on which the F1 -scores are comparatively low for all subcorpora.
The reason for this can be found mostly in words like yes, no, uh-huh, right, okay, and yeah, which are often tagged with UH in SWITCHBOARD but frequently are not considered to be irrelevant words in our corpus and hence not marked as disfluent (e.g., if they are considered to be the answer to a question or a summary-relevant acknowledgment).
We circumvent potential exclusion from the summary output of these and other words that might be erroneously tagged as nonlexicalized filled pauses (UH) by marking a small set of words as exempt from removal (see section 5.5.6). 5.3.7 False Start Detection.
False starts are quite frequent in spontaneous speech, occurring at a rate of about 1015% of all sentences (SWITCHBOARD, CALLHOME).
They involve less than 10% of the total words of a dialogue; about 34% of the words in these incomplete sentences are part of some other disfluencies, such as filled pauses or repairs.
(In complete sentences, only about 15% of the words are part of these disfluencies).
For CALLHOME, the average length of complete sentences is about 6 words, of incomplete sentences about 4.1 words (including disfluencies).
We trained a C4.5 decision tree (Quinlan 1992) on 8,000 sentences of SWITCHBOARD.
As features we use the first and last four trigger words (words that have a high incidence around sentence boundaries) and POS of every sentence, as well as the first and last four chunks from a POS-based chunk parser.
This chunk parser is based on a simple context-free POS grammar for English.
It outputs a phrasal bracketing of the input string (e.g., noun phrases or prepositional phrases).
Further, we encode the length of the sentence in words and the number of the words not parsed by the chunk parser.
We observed that whereas the chunk information itself does not improve performance over the baseline of using trigger words and POS information only, the derived feature of "number of not parsed words" actually does improve the results.
We ran the decision tree on data with perfect POS tags (for SWITCHBOARD only), disfluency tags (except for repairs), and sentence boundaries.
The evaluations were performed on independent test sets of about 3,000 sentences for SWITCHBOARD and of our complete dialogue corpus.
Table 8 shows the results of these experiments.
Typical errors, where complete sentences were classified as incomplete, are inverted forms or 464 Zechner Automatic Summarization of Dialogues Table 8 False start classification results for different corpora (F1 ).
SWBD False start frequency (in %) False start detection (F1 ) 12.3 .611 8E-CH 12.1 .545 4E-CH 11.0 .640 NHOUR 2.0 .286 XFIRE 7.2 .352 G-MTG 13.9 .557 Table 9 Detection accuracy for repairs on the basis of individual word tokens using the repetition filter.
8E-CH Repair tokens (%) Precision Recall F1 -score 4.7 .88 .41 .56 4E-CH 3.8 .78 .32 .45 NHOUR 2.2 .25 .01 .02 XFIRE 1.3 .35 .04 .08 G-MTG 7.9 .91 .27 .41 ellipsis at the end of a sentence (e.g., neither do I, it seems to).
The performance for the informal corpora (CALLHOME, GROUP MEETINGS) is better than that for the formal corpora (NEWSHOUR, CROSSFIRE); this is related to the fact that the relative frequency of false starts is markedly lower in these latter data sets and that these corpora are more dissimilar to the training corpus (SWITCHBOARD).
5.3.8 Repetition
Detection.
The repetition detection component is concerned with (verbatim) repetitions within a speaker's turn, the most frequently occurring case of all speech repairs for informal dialogues (insertions and substitutions are comparatively less frequent).
Repeated phrases can potentially be interrupted by other disfluencies, such as filled pauses or editing terms.
Repetition detection is performed with a script that can identify repetitions of word/POS sequences of length one to four (longer repetitions are extremely rare: on average, less than 1% of all repetitions).
Words that have been marked as disfluent by the POS tagger are ignored when the repeated sequences are considered, so we can correctly detect repetitions such as [ he said uh to + he said to ] him.
. . . We are evaluating the precision, recall, and F1 -scores for this component at the level of individual words when the POS tagger and the sentence boundary detection component are used.
Table 9 shows the results.
We see that for the informal subcorpora, we get very good precision (only a few repetitions detected are incorrect), and recall is in the 2545% range (since we cannot detect substitution or insertion type of repairs).
The results for the formal subcorpora are considerably worse, so this filter should probably not be used for corpora with as few repetitions as NEWSHOUR or CROSSFIRE.
We checked all of the 95 false positives of this evaluation and observed that in the majority of cases (41%), the repetition was correctly detected but was not marked by the human annotator, since it might be considered a case of emphasis.
We believe that although some nuances of the sentence(s) might be lost, for the purpose of summarization it makes perfect sense to reduce this information.
Sometimes, individual words are repeated for emphasis, sometimes whole sentences (e.g., "Good./ Good./").
In the following example from English CALLHOME, the emphasis is rather extreme: 203 B: [...] How is the new person doing? q/ 204 A: Very very very very very well.
/ [...] Computational Linguistics Volume 28, Number 4 Further, about 19% of false positives were correct but not annotated because they span multiple turns, and about 14% were erroneously missed by the human annotator.
Only the remaining cases (26%) were actual false positives, caused by incorrect POS tags (5%, typically an incorrectly tagged "that/WDT that/DT" sequence at the beginning of a relative clause) or incorrect sentence boundaries (21%).
There have been attempts to get a more complete coverage of detection and correction of all types of speech repairs (Heeman and Allen 1999).
We decided, however, to use a simple method here that works well for a large subset of cases and is very efficient at the same time.
5.3.9 Disfluency
Correction in DIASUMM.
After detection, the correction of disfluencies is straightforward.
When DIASUMM generates its output from the ranked list of sentences, it skips the false starts, the repetitions, and the words that were tagged with CO, DM, ET, or UH by the POS tagger.
5.4 Sentence
Boundary Detection 5.4.1 Introduction.
The purpose of the sentence boundary detection component is to insert linguistically meaningful sentence boundaries in the text, given a POS-tagged input.
We consider all intraturn and interturn boundary positions for every speaker in a conversation.
We use the abbreviations EOS for end of complete sentence (E S in the SWITCHBOARD corpus) and NEOS for end of noncomplete sentence (N S in the SWITCHBOARD corpus).
The frequency of sentence boundaries (with respect to the total number of words) is about 13.3%, most of the boundaries (almost 90%) being end markers of completed sentences (SWITCHBOARD).
5.4.2 Training
and Testing.
We trained a C4.5 decision tree and computed its input features from a context of four words before and after a potential sentence boundary, motivated by the results of Gavald`, Zechner, and Aist (1997).
Also following Gavald`, a a Zechner, and Aist (1997), we used 60 trigger words with high predictive potential, employing the score computation method described in this article.
The decision tree input features for every word position are as follows:     POS tag (42 different tags) trigger word (60 different trigger words) turn boundary before this word? if turn boundary: length of pause after last turn of same speaker Since NEOS boundaries occur very infrequently (only about 10% of all boundaries, which is only about 1% of all potential boundaries), we decided to merge this class with the EOS class and report results for this combined class only (CEOS).
(We relied on the false-start detection module described above to identify the NEOS sentences within this merged class of sentences after the sentence boundary classification).
For training, we used 25,000 words from the Treebank-3 corpus; the test set size was 1,000 words.
Table 10 shows the results in detail for the various parameter combinations.
We see that for good performance we need to know about one of these two features: "is there a turn boundary before this word"? or "pause duration after last turn from same speaker".
466 Zechner Automatic Summarization of Dialogues Table 10 Sentence boundary detection accuracy (F1 -score).
With Interturn Pause Duration?
With Turn Boundary Info?
Training set Test set Yes Yes .904 .887 No .903 .884 Yes .900 .884 No No .884 .825 Table 11 Interand intraturn boundary detection (BD) results on 1,000-word test set.
Occurrence (%) Interturn Interturn Intraturn Intraturn non-BD BD non-BD BD 12 112 809 61 (1.2) (11.3) (81.4) (6.1) Detection Accuracy (F1 ) .56 .95 .99 .77 5.4.3 Effect of Imperfect POS Tagging.
To see how much influence an imperfect POS tagging might have on these results, we POS-tagged the test set data using the POS tagger described above.
For this and the following experiments, we increased the training corpus for the decision tree to 40,000 words.
The POS tagger accuracy for this test set was about 95.3%, and the F1 -score for CEOS was .882, which is 98.9% of .892 on perfect POS-tagged input.
This is encouraging, since it shows that the decision tree is not very sensitive to the majority of POS errors.
5.4.4 Interturn
and Intraturn Boundaries.
In this analysis, we are interested in comparing the detection of sentence boundaries between turns (interturn) to the detection of boundaries within a turn (intraturn).
Table 11 shows the results of this analysis (same test set as above).
As might be expected, the performance is very good for the two frequent classes: sentence boundaries at the end of turns and nonboundaries within turns (F1 > .95), but considerably worse for the two more infrequent cases.
The very rare cases (around 1% only) of nonsentence boundaries at the end of turns (i.e.
turn continuations) show the lowest performance (F1 = .56).
5.4.5 Sentence
Boundary Detection on Dialogue Corpus.
To get a picture of the realistic performance of the sentence boundary detection component, using the (imperfect) POS tagger and a faster, but slightly less accurate, decision tree,13 we evaluate the sentence boundary detection accuracy for all five subcorpora of our dialogue corpus.
Table 12 provides the results of these experiments.
The results reflect a trend very similar to that for the SWITCHBOARD corpus, in that the two more frequent classes (interturn boundaries and intraturn nonboundaries) have high detection scores, whereas the two more infrequent classes are less well detected.
Furthermore, we observe that in cases in which the relative frequency of rare classes is further reduced, the classification accuracy declines overproportionally (particularly for the rarest class of the interturn nonboundaries).
Also, overall boundary detection is better for the two more informal corpora, CALLHOME and GROUP MEETINGS (F1 > .72).
13 This
decision tree uses a different type of encoding, but the same input features.
Computational Linguistics Volume 28, Number 4 5.5 Cross-Speaker Information Linking 5.5.1 Introduction.
One of the properties of multiparty dialogues is that shared information is created between dialogue participants.
The most obvious interactions of this kind are question-answer (Q-A) pairs.
The purpose of this component is to create automatically such coherent pieces of relevant information, which can then be extracted together while generating the summary.
The effects of such linkings on actual summaries can be seen in two dimensions: (1) increased local coherence in the summary and (2) a potentially higher informativeness of the summary.
Since Q-A linking has a side effect in that other information will be lost with respect to a summary of the same length without Q-A linking, the second claim is much less certain to hold than the first.
We investigated these questions in related work (Zechner and Lavie 2001) and found that although Q-A linking does not significantly change the informativeness of summaries on average, it does increase summary coherence (fluency) significantly.
In this section, we will be concerned with the following two intuitive subtasks of Q-A linking: (1) identifying questions (Qs) and (2) finding their corresponding answers.
5.5.2 Related
Work.
Detecting a question and its corresponding answer can be seen as a subtask of the speech act detection and classification task.
Recently, Stolcke et al.(2000) presented a comprehensive approach to dialogue act modeling with statistical techniques.
A good overview and comparison of recent related work can also be found in Stolcke et al.'s article.
Results from their evaluations on SWITCHBOARD data show that word-based speech act classifiers usually perform better than prosody-based classifiers, but that a model combination of the two approaches can yield an improvement in classification accuracy.
5.5.3 Corpus
Statistics.
For training of the question detection module, we used the manually annotated set of about 200,000 SWITCHBOARD speech acts14 (SAs);15 for training of the answer detection component, we used the eight English CALLHOME dialogues (8E-CH), which were manually annotated for Q-A pairs.
Although we were aiming to detect all questions in the question detection module, the answer detection module focuses on Q-A pairs only: We exclude all questions from consideration that are not Yes-No(YN) or Wh-questions (such as rhetorical or back-channel questions), 14 In this work, the notions of speech acts and sentences can be considered equivalent.
15 From
the Johns Hopkins University Large Vocabulary Continuous Speech Recognition (LVCSR) Summer Workshop 1997.
Thanks to Klaus Ries for providing the data, which are also available from http://www.colorado.edu/ling/jurafsky/ws97/.
Zechner Automatic Summarization of Dialogues Table 13 Frequency of different types of questions in the 8E-CH data set.
Sentences Wh-questions total . . . With immediate answers YN-questions total . . . With immediate answers Qs excluded for Q-A detection Questions total 2,211 20 15 (75%) 48 38 (79%) 15 83 (3.75%) as well as those that do not have an answer in the dialogue.
Thus we employ only 68 pf the 83 questions marked in the 8E-CH data set for these evaluations.
Table 13 provides the statistics concerning questions and answers for the 8E-CH subcorpus and shows that for a small but significant number of questions, the answer does not immediately follow the question speech act (delayed answers).
5.5.4 Automatic
Question Detection.
We used two different methods, both trained on SWITCHBOARD data: (1) a speech act tagger16 and (2) a decision tree based on trigger word and part-of-speech information.
Speech act tagger.
The speech act tagger tags one speech act at a time and hence can make use only of speech act unigram information.
Within a speech act, it uses a language model based on POS and the 500 most frequent word/POS pairs.
It was trained on the aforementioned SWITCHBOARD speech act training set.
It was not optimized for the task of question detection.
Its typical run time for speech act classification is about 10 speech acts per second.
Decision tree question classifier.
The decision tree classifier (C4.5) uses the following set of features: (1) POS and trigger word information for the first and last five tokens of each speech act;17 (2) speech act length, and (3) occurrence of POS bigrams.
The set of trigger words is the same as for the sentence boundary detection module.
The POS bigrams were designed to be most discriminative between question speech acts (q-SAs) and nonquestion speech acts (non-q-SAs).
The bigrams were obtained as follows: 1.
For a balanced set of q-SAs and non-q-SAs (about 9,000 SAs each): Count all the POS bigrams in positions 1 . . . 5 and (n 4) . . . n (using START and END for the first and last bigrams, respectively) and memorize position (beginning or end of SA) and type (q-SA vs.
non-q-SA). For all bigrams: (a) (b) (c) (d) 3.
Add one to the count (to prevent division by zero).
Divide the q-SA count by the non-q-SA count.
If the ratio is smaller than one, invert it (ratio := 1/ratio).
Multiply the result of (c) by the sum of q-SA count and non-q-SA count.18 Extract the 100 bigrams with the highest value.
16 Thanks
to Klaus Ries for providing us with the software.
17 Shorter
speech acts are padded with dummies.
18 Leaving
out this step favors low-frequency, high-discriminative bigrams too much and causes a slight reduction in overall Q-detection performance.
Computational Linguistics Volume 28, Number 4 Table 14 Question detection on the 8E-CH corpus using two different classifiers.
SA Tagger Overall error Precision Recall F1 Typical classification time (SAs/sec) 3.2% .57 .61 .59 10 Decision Tree 4.7% .63 .51 .56 1,000 Experiments and results.
The question detection decision tree was trained on a set of about 20,000 speech acts from the SWITCHBOARD corpus.
We first evaluated the speech act tagger and the decision tree classifier on the 8E-CH data set.
Whereas in the later stage of answer detection, questions without answers and nonpropositional questions are ignored, at this point we are interested in the detection of all annotated questions in the corpus.
This also reflects the fact that the training set contains all possible types of questions.
Table 14 reports the results of the question detection experiments with the two classifiers used on the 8E-CH subcorpus.
We note that whereas the decision tree is performing only slightly worse than the speech act tagger, its typical classification time is two orders of magnitude faster.
Based on these observations, we decided to use the question detection decision tree in the Q-A linking component of the DIASUMM system.
5.5.5 Detecting
the Answers.
After identifying which sentences are questions, the next step is to identify the answers to them.
From the 8E-CH statistics of Table 13 we observe that for more than 75% of the YNand Wh-questions, the answer is to be found in the first sentence of the speaker talking after the speaker uttering the question.
In the remainder of cases, the majority of answers are in the second (instead of the first) sentence of the responding speaker.
Further, the speaker who has posed a question usually utters no (or only very few) sentences after the question is asked and before the next speaker starts talking.
In addition to detecting sequential Q-A pairs, we also want to be able to detect simple embedded questions, as shown in this example of a brief clarification dialogue: Q Q 1 2 3 4 A: B: A: B: When are we meeting then?
You mean tomorrow?
Yes. At 4pm.
We devise the following heuristics to detect answers to question speech acts which have been previously identified:  If the first speaker change after the question occurs more than maxChg sentences after the question, the search is stopped and no Q-A pair is returned.
Answer hypotheses are sought for maximally maxSeek sentences after the first speaker change following the question, but not over interruptions by any other speaker; that is, we check within a single speaker region Zechner Automatic Summarization of Dialogues (this is the stopping criterion for the following two heuristics).
An exception occurs if there is an embedded question in the first single speaker region: In that case, we look at the next region where a speaker different from the initial Q-speaker is active.19   Answers have to be minimally minAns words long; if they are shorter, we add the next sentence to the current answer hypothesis.
Even if the minimum answer length is reached, the answer can be optionally extended if at least one word in the answer matches a word from the question (one of two different stop lists (StopShort, StopLong) or no stop list is used to remove function words from consideration).20 We have these further restrictions for the case of embedded questions: 1.
If we detect a potential embedded Q-A pair, the answer to the surrounding question must immediately follow the answer to the embedded question (i.e., the region following the potential answer region of the embedded question--sentence 4 in our above example--must (1) not contain a question itself and (2) be from a different speaker than the surrounding question).
A crossover is prohibited; that is, we eliminate all pairs Qj, Al when a pair Qi, Ak was already detected, with i < j < k < l (k, l being start indices of answer spans).
The output of the algorithm is a list of triples Q, Astart, Aend, where Q is the sentence ID of the question and Astart the first and Aend the last sentence of the answer.
As mentioned above, we use only 68 of the 83 questions marked in the 8ECH data set for these evaluations, since only these are YNor Wh-questions that actually have answers in the dialogue.
There are four possible outcomes for each triple: (1) irrelevant: a Q-A pair with an incorrectly hypothesized question (this is the fault of the question detection module, not of this heuristic); (2) missed: the answer was missed entirely; (3) completely correct: Aend coincides with the correct answer sentence ID; and (4) correct range: the answer is contained in the interval [Astart, Aend ] but does not coincide with Aend . For the calculation of precision, recall, and F1 -score, we count classes (3) and (4) as correct and use the sum of all classes for the denominator of precision and the total number of Q-A pairs (68 in this development set) as the denominator of recall.
To determine the best parameters, we varied them across a reasonable set of values and ran the answer detection script for all combinations of parameters.
The best results (with respect to F1 -score) using questions detected by the speech act tagger and the decision tree are reported in Table 15.
In the DIASUMM system, we use the following optimal parameter settings for the answer detection heuristics: maxChg = 2, maxSeek = 4, minAns = 10, sim = on, stop = no.
Finally, we evaluated the performance of both the Q-detection module and the combined Q-A detection on all five subcorpora, using the decision tree for question detection; the results are reported in Table 16.
Except for the rather small NEWSHOUR 19 This would be sentence 4 in the example above.
20 StopLong contains 571 words, StopShort only 89 words, most of which are auxiliary verbs and filler words.
Computational Linguistics Volume 28, Number 4 Table 15 Q-A detection results using two different classifiers for question detection (68 Q-A pairs to be detected).
SA Tagger All hypothesized Q-A pairs Correct [(3) and (4)] maxChg (15) maxSeek (24) minAns (110) Similarity extension (on/off) Stop list (no/short/long) Precision Recall F1 -score 80 42 4 34 510 on no/short .53 .62 .57 Decision Tree 54 31 2 24 210 on no/short .57 .46 .51 Table 16 Performance comparison for Qand Q-A detection (Q-detection with the decision tree question classifier).
8E-CH Q to detect Q-hypotheses Q-detection (F1 ) Q-A pairs to detect Q-A pair hypotheses Q-A detection (F1 ) 83 67 .56 68 54 .51 4E-CH 94 60 .58 69 54 .60 NHOUR 19 16 .80 18 14 .81 XFIRE 110 71 .60 79 54 .51 G-MTG 49 52 .59 32 33 .51 corpus (with fewer than 20 questions or Q-A pairs to identify), the typical Q-detection F1 -score is around .6 and the Q-A detection F1 -score around .5.
In two cases, the Q-A detection performance is slightly better than the Q-detection performance.
This can be explained by the fact that the answer detection algorithm prunes away a number of Q-hypotheses, reducing the space for potential Q-A hypotheses.
5.5.6 Q-A Detection within DIASUMM.
When we use the Q-A detection module as part of the DIASUMM system, we want to ensure that (1) there are no Q-A pairs containing Q-sentences that are false starts and that (2) the initial part of an answer is not lost in case the disfluency detection component marks some indicative words as disfluencies.
To satisfy the first constraint, we block Q-detection of sentences that have been previously classified as false starts; as for the second constraint, we create a list of indicative words (relevant for YN-questions) that are not to be removed by the summary generator if they appear in the beginning (leading five words) of answers.21 5.6 Sentence Ranking and Selection 5.6.1 Introduction.
The sentence ranking and selection component is an implementation of the MMR algorithm (Carbonell, Geng, and Goldstein 1997), applied to extracting the most relevant sentences from a topical segment of a dialogue.
The component's output in isolation serves as the MMR baseline for the global system evaluation.
Its 21 The current list comprises the following words: no, yes, yeah, yep, sure, uh-huh, mhm, nope.
Zechner Automatic Summarization of Dialogues purpose is to determine weights for terms and sentences, to rank the sentences according to their relevance within each topical segment of the dialogue, and finally to select the sentences for the summary output according to their rank, as well as to other criteria, such as question-answer linkages, established by previous components.
The selected sentences are presented to the user in text order.
5.6.2 Tokenization.
In addition to the tokenization rules for the global system (section 5.2), we apply a simple six-character truncation for stemming and use a stop word list to eliminate frequent noncontent words.
In the experiments, we used the following five different stop word lists:      the original SMART list (Salton 1971) (SMART-O) a manually edited stop list based on SMART (SMART-M) a stop list with all closed-class words from the POS tagger's lexicon (POS-O) a manually edited stop list based on the POS tagger's lexicon and frequent closed-class words in the CALLHOME training corpus (POS-M) an empty stop list (EMPTY) 5.6.3 Term and Sentence Weighting.
The basic idea for determining the most relevant sentences within a topical segment is as follows: First, we compute a vector of word weights for the segment tfq (including all stemmed nonstop words) and do the same for each sentence (tft ), then we compute the similarity between sentence and segment vectors for each sentence.
That way, sentences that have many words in common with the segment vector are rewarded and receive a higher relevance weight.
Whereas we compose the sentence vectors tft using direct term frequency counts, the weights for segment terms are determined according to one of the three formulae in equation (1) (freq, smax, and log), inspired by Cornell University's SMART system (Salton 1971): fi,s or 1 + log fi,s, (1) tfi,s = fi,s or 0.5 + 0.5 fsmax where fi,s are the in-segment frequencies of a stem and fsmax are maximal segment frequencies of any stem in the segment.
Finally, we multiply an inverse document frequency (IDF) weight to tfs to obtain the segment vectors tfq, as shown in equations (2) and (3): tfi,q IDFi,s = tfi,s IDFi,s = 1 + log Nseg iseg or Nseg . iseg (2) (3) IDF values are computed with respect to a collection of topical segments, either the current dialogue (DIALOGUE) or a set of dialogues (CORPUS).
Nseg is the total number of topical segments in the IDF corpus, and iseg is the number of segments in which the token i appears at least once.
The effect of using IDF values is to boost those words that are (relatively) unique to any given segment over those that are more evenly distributed across the corpus.
As stated above, the main algorithm is a version of the MMR algorithm (Carbonell, Geng, and Goldstein 1997; Carbonell and Goldstein 1998), which emphasizes sentences 473 Computational Linguistics Volume 28, Number 4 that contain many highly weighted terms for the current segment (salience) and are sufficiently dissimilar to previously ranked sentences (diversity or antiredundancy).
The MMR formula is given in equation (4): nextsentence = arg max(sim 1 (query, tnr,j ) (1 ) max sim 2 (tnr,j, tr,k )).
The MMR formula describes an iterative algorithm and states that the next sentence to be put in the ranked list will be taken from the sentences that have not yet been ranked (tnr ).
This sentence is (1) maximally similar to a query and (2) maximally dissimilar to the sentences that have already been ranked (tr ).
We use the topical segment word vector tfq as query vector.
The parameter (0.0 1.0) is used to trade off the influence of salience against that of redundancy.
Both similarity metrics (sim 1, sim 2 ) are inner vector products of stemmed-term frequencies (equations (5) and (6)).
sim 1 can be normalized in different ways: (1) to yield a cosine vector product (division by product of vector lengths), (2) division by number of content words,22 and (3) no normalization: sim 1 = tfq tft |tfq ||tft | tft1 tft2 |tft1 ||tft2 | or tfq tft 1 + i tfi,t or tfq tft (5) Emphasis factors.
Every sentence's similarity weight (sim 1 ) can be (de)emphasized, based on a number of its properties.
We implemented optional emphasis factors for:     Lead emphasis: for the leading n% of a segment's sentences: sim 1 = sim 1 l, with l being the lead factor.
Q-A emphasis: for all sentences that belong to a question-answer pair: sim 1 = sim 1 q, with q being the Q-A emphasis factor.
False-start deemphasis: for all sentences that are false starts: sim 1 = sim 1 f, with f being the false-start factor.
Speaker emphasis: for each individual speaker s, an emphasis factor se can be defined: sim 1 = sim 1 se for all sentences of speaker s.23 These parameters can serve to fine-tune the system for particular applications or user preferences.
For example, if the false starts are deemphasized, they are less likely to trigger a question's being linked to them in the linking process.
If questions and answers are emphasized, more of them will show up in the summary, increasing its coherence and readability.
In a situation in which a particular speaker's statements are of higher interest than those of other speakers, his sentences can be emphasized, as well.
Since sim 2 is a cosine vector product and hence in [0,1], we have to normalize sim 1 to [0,1] as well to enable a proper application of the MMR formula.
For this normalization of sim 1, we divide each sim 1 score by the maximum of all sim 1 scores in a segment after initial computation and application of the various emphasis factors described here.
22 To avoid division by zero, we add one to every sentence length.
23 Speaker emphasis is not used in our evaluations.
Zechner Automatic Summarization of Dialogues 5.6.4 Q-A Linking.
While generating the output summary from the MMR-ranked list of sentences, whenever a question or an answer is encountered (detected previously by the Q-A detection module), the corresponding answer/question is linked to it and moved up the relevance ranking list to immediately follow the current question/answer.
If the question-answer pair consists of more than two sentences, the linkages are repeated until no further questions or answers can be added to the current linkage cluster.
5.6.5 Summary
Types.
DIASUMM can generate several different types of summaries, the two main versions being (1) the CLEAN summary, which is based on the output of all DIASUMM components (disfluency detection, sentence boundary detection, Q-A linking), and (2) the TRANS summary, in which all dialogue specific components are ignored (essentially, this is an MMR summary of the original dialogue transcript).
For the purpose of the global system evaluation, we use only these two versions of summaries, as well as LEAD baseline summaries, where the summary is formed by extracting the first n words from a topical segment.24 Furthermore, the system can generate phrasal summaries, which render the sentences in the same ranking order as the CLEAN summary but reduce the output to noun phrases and potentially other phrases, depending on the setting of parameters.25 In Figure 2 we show an example of a set of LEAD, TRANS, CLEAN, and PHRASAL summaries.
The set was generated from the CALLHOME transcript presented in section 2.
5.6.6 System
Tuning.
This section describes how we arrive at an optimal parameter setting for each subcorpus (CALLHOME, NEWSHOUR, CROSSFIRE, GROUP MEETINGS).
We want to establish an MMR baseline for the global system evaluations with which we can then compare the results of the entire DIASUMM system.
Note that for all the tuning experiments described in this subsection, we did not make use of any other DIASUMM components, namely, disfluency detection, sentence boundary detection, and questionanswer linking.
All experiments were based on the human gold standard with respect to topical segments.
We used only the devtest set for the four subcorpora here (8ECH = CALLHOME, DT-NH = NEWSHOUR, DT-XF = CROSSFIRE, and DT-MTG = GROUP MEETINGS).
Since the length of turns varies widely, one could argue that an easy way to increase performance for the MMR baseline (which does not use automatic sentence boundary detection) might be to split overly long turns evenly into shorter chunks.
This was done by Valenza et al.(1999), who experimented with lengths of 1030 words per extract fragment.
We add this option as an additional parameter to the MMR baseline.
If the parameter is set to n words, turns with a length l 1.5n get cut into pieces of lengths n iteratively until the last remaining piece is l < 1.5n.
Evaluation metric.
To evaluate the performance of this component, we use the word-based evaluation metric described in section 6.2, which gives the highest scores to summaries containing words with the highest average relevance scores, as marked by human annotators.
We then average these scores over all topical segment summaries of a particular subcorpus.
24 Note that LEAD summaries are to be distinguished from summaries in which lead emphasis is used, as described above.
In the latter case, the segment-initial sentence weights are increased, whereas in the former case, we strictly extract the leading n words from a given segment.
25 To determine these constituents, we use the output of the chunk parser employed by the false start detection component.
Computational Linguistics Volume 28, Number 4 LEAD: 1 a: Oh 2 b: They didn't know he was going to get shot but it was at a peace rally so I mean it just worked out 3 b: I mean it was a good place for the poor guy to die I mean because it was you know right after the rally and everything was on film and everything [...] TRANS: 2 b: They didn't know he was going to get shot but it was at a peace rally so I mean it just worked out 3 b: I mean it was a good place for the poor guy to die I mean because it was you know right after the rally and everything was on film and everything 11 b: Him [...] CLEAN: 7 b: We just finished the thirty days mourning for him now it's everybody's still in shock it's terrible what's going on over here 31 b: What's the reaction in america really do people care [...] 34 a: Most I don't know what I mean like the jewish community a lot all of us were very upset PHRASAL: 4 b: it just worked ...
it was a good place for the poor guy to die ...
it was [...] 7 b: we just finished the thirty days mourning for him ...
it's ...
everybody's ...
in shock it's ...
going ...
31 b: 's the reaction in america ...
do people care ...
34 a: i don't know ...
mean like the jewish community a lot ...
of us were Note: The turn IDs are just indicating the relative position of the turns within the original text and do not always correspond to the turn numbers of the original or to the turn numbers of the other summaries.
The . . . marks the position in those sentences where the length threshold for a summary was reached.
Figure 2 Example summaries of 20% length: LEAD, TRANS, CLEAN and PHRASAL.
Parameter tuning.
The system tuning proceeded in three phases, in which we held the summary size constant to 15% and optimized the following set of parameters: 1.
2. 3.
4. 5.
476 Term weight type: freq, smax, log Normalization: cos, length, none IDF type: corpus, dialogue, none IDF method: log, mult Extract span: 1030 or original turn (orig) Zechner Automatic Summarization of Dialogues Table 17 Optimally tuned parameters for MMR baseline system (tuning on devtest set subcorpora).
8E-CH Term weight type Normalization IDF type IDF method Extract span MMRStop list Lead factor smax cos corpus log 20 0.85 SMART-M 1.0 DT-NH DT-XF DT-Mtg smax none corpus log orig 0.8 POS-M 1.0 smax cos corpus mult 25 1.0 POS-M 1.0 smax none corpus log orig 0.8 POS-M 2.0 MMR-: 0.81.0 Stop lists: SMART-O, SMART-M, POS-O, POS-M, EMPTY Lead factor: 1.05.0 (applied to first 20% of sentences) Table 17 shows the parameter settings that were determined to be optimal for the MMR baseline system (TRANS summaries).
5.7 System
Performance The majority of the system components are implemented in Perl5, except for the C4.5 decision tree (Quinlan 1992), the chunk parser (Ward 1991), and the POS tagger (Brill 1994), which were implemented in C by the respective authors.
We measured the system runtime on a 300 MHz Sun Ultra60 dual-processor workstation with 1 GB main memory, summarizing all 23 dialogue excerpts from our corpus.
The average runtime for the whole system, including all of its components except for the topic segmentation module, was 17.8 seconds, and for the sentence selection component alone 7.0 seconds (per-dialogue average).
The average ratio of system runtime to dialogue duration was 0.029 (2.9% of real speaking time).
6. Evaluation 6.1 Introduction Traditionally, summarization systems have been evaluated in two major ways: (1) intrinsically, measuring the amount of the core information preserved from the original text (Kupiec, Pedersen, and Chen 1995; Teufel and Moens 1997), and (2) extrinsically, measuring how much the summary can benefit in accomplishing another task (e.g., finding a document relevant to a query or classifying a document into a topical category) (Mani et al.1998). In this work, we focus on intrinsic evaluation exclusively.
That is, we want to assess how well the summaries preserve the essential information contained in the original texts.
As other studies have shown (Rath, Resnick, and Savage 1961; Marcu 1999), the level of agreement between human annotators about which passages to choose to form a good summary is usually quite low.
Our own findings, reported in section 4.2.4, support this in that the intercoder agreement, here measured on a word level, is rather low.
We decided to minimize the bias that would result from selecting either a particular human annotator, or even the manually created gold standard, as a reference 477 Computational Linguistics Volume 28, Number 4 for automatic evaluation; instead, we weigh all annotations from all human coders equally.
Intuitively, we want to reward summaries that contain a high number of words considered to be relevant by most annotators.
We formalize this notion in the following subsection.
6.2 Evaluation
Metric All evaluations are based on topically coherent segments from the dialogue excerpts of our corpus.
As mentioned before, the segment boundaries were chosen from the human gold standard for the purpose of the global system evaluation.
For each segment s, for each annotator a, and for each word position wi, we define a boolean word vector of annotations ws,a, each component ws,a,i being 1 if the word wi is part of a nucleus IU or a satellite IU for that annotator and segment, and 0 otherwise.
We then sum over all annotators' annotation vectors and normalize them by the number of annotators per segment (A) to obtain the average relevance vector for segment s, rs : A ws,a,i rs,i = a=1 . (7) A To obtain the summary accuracy score sas,n for any segment summary with length n, we multiply the boolean summary vector summs 26 by the average relevance vector rs, and then divide this product by the sum of the n highest scores within rs (maximum achievable score), rsorts being the vector rs sorted by relevance weight in descending order: summs rs sas,n = n (8) i=1 rsorts,i It is easy to see that the summary accuracy score always is in the interval [0.0, 1.0]. 6.3 Global System Evaluation Whereas section 5 was concerned with the design and evaluation of the individual system components, the goal here is to describe and analyze the quality of the global system, with all its components combined.
In this section, we compare our DIASUMM system with the MMR baseline system, which operates without any dialogue-specific components, and with the LEAD baseline.
We described the optimization and finetuning of the MMR system in subsection 5.6.6.
The second column of Table 18 presents the average relevance scores for this MMR baseline, averaged over the five summary sizes of 5%, 10%, 15%, 20%, and 25% length, for the four devtest set and the four eval set subcorpora; the first column of this table shows the results for the LEAD baseline.
We used the optimized baseline MMR parameters and varied only the emphasis parameters for (1) false starts, (2) lead factor, and (3) Q-A sentences, to optimize the CLEAN summaries further.
(Again, for this step, we used only the devtest subcorpora).
For each corpus in the devtest set, we determined the optimal parameter settings and report the corresponding results also for the eval set subcorpora.
Column 3 in Table 18 provides the results for this optimized DIASUMM system.
Further, in column 4, we provide the summary accuracy averages for the human gold standard (nucleus IUs only, fixed-length summaries).
Table 19 shows the best emphasis parameter combinations for the DIASUMM summaries used in these evaluations.
We determined the statistical differences between the DIASUMM system and the two baselines for the eval set, using the Wilcoxon rank sum test for each of the four 26 Definition: 1 if summs,i is contained in the summary, 0 otherwise.
Zechner Automatic Summarization of Dialogues Table 18 Average summary accuracy scores: devtest set and eval set subcorpora on optimized parameters, comparing LEAD, MMR baseline, DIASUMM, and the human gold standard.
Subcorpus 8E-CH DT-NH DT-XF DT-MTG EVAL-NH EVAL-XF EVAL-MTG Table 19 Best emphasis parameters for the DIASUMM system, trained on the devtest set.
Corpus CALLHOME NEWSHOUR CROSSFIRE GROUP MEETINGS False Start 0.5 0.5 0.5 0.5 Q-A 1.0 2.0 1.0 1.0 Lead Factor 2.0 1.0 1.0 3.0 Table 20 Average summary accuracy scores for different system configurations for the four different subcorpora.
Corpus 4E-CH EVAL-NH EVAL-XF EVAL-GMTG LEAD .438 .692 .378 .324 MMR .526 .526 .564 .449 DFF-ONLY .599 .551 .528 .488 SB-ONLY .547 .608 .525 .513 NO-QA DIASUMM .614 .506 .566 .583 subcorpora.
Comparisons were made for each of the five summary sizes within each topical segment.
For the CALLHOME and GROUP MEETINGS subcorpora, our DIASUMM system is significantly better than the MMR baseline (p < 0.01); for the two more formal subcorpora, NEWSHOUR and CROSSFIRE, the differences between the performance of the two systems are not significant.
Except for on the NEWSHOUR subcorpus, both the MMR baseline and the DIASUMM system perform significantly better than the LEAD baseline.
6.4 Discussion
Table 20 shows the average performance of the following six system configurations, averaged over all topical segments and all summary sizes (525% length summaries; in configurations 35 below, components used are in addition to the core MMR summarizer): 1.
2. LEAD: using the first n% of the words in a segment MMR: the MMR baseline (tuned; see above) 479 Computational Linguistics Volume 28, Number 4 DFF-ONLY: using the disfluency detection components (POS tagger, false-start detection, repetition detection), but no sentence boundary detection or question-answer linking SB-ONLY: using the sentence boundary detection module, but no other dialogue-specific modules NO-QA: a combination of DFF-ONLY and SB-ONLY (all preprocessing components used except for question-answer linking) DIASUMM: complete system with all components (all disfluency detection components, sentence boundary detection, and Q-A linking) We observe that in all subcorpora, except for CROSSFIRE, the addition of either the disfluency components or the sentence boundary component improves the summary accuracy over that of the MMR baseline.
As we would expect, given the much higher frequency of disfluencies in the two informal subcorpora (CALLHOME, GROUP MEETINGS), the relative performance increase of DFF-ONLY over the MMR baseline is much higher here (about 1015%) than for the two more formal subcorpora (5% and below).
Looking at the performance increase of SB-ONLY, we find marked improvements over the MMR baseline for those two subcorpora that use the true original turn boundaries in the MMR baseline: GROUP MEETINGS and NEWSHOUR (>10%); for the two other subcorpora, the improvement is below 5%.
Furthermore, the combination of the disfluency detection and sentence boundary detection components (NO-QA) improves the results over the configurations DFF-ONLY and SB-ONLY.
The situation is much less uniform when we add the question-answer detection component (this then corresponds to the full DIASUMM system): In the CROSSFIRE corpus, we have the largest performance increase (we also have the highest relative frequency of question speech acts here).
For the two informal corpora, the change is only minor; for NEWSHOUR, the performance decreases substantially.
We showed in Zechner and Lavie (2001), however, that in general, for dialogues with relatively frequent Q-A exchanges, the accuracy of a summary (informativeness) does not change significantly when the Q-A detection component is applied.
On the other hand, the (local) coherence of the summary does increase significantly, but we cannot measure this increase with the evaluation criterion of summary accuracy used here.
To conclude, we have shown that using dialogue-specific components, with the possible exception of the Q-A detection module, can help in creating more accurate summaries for more informal, casual, spontaneous dialogues.
When more formal conversations (which may even be partially scripted), containing relatively few disfluencies, are involved, either a simple LEAD method or a standard MMR summarizer will be much harder to improve upon.
7. Discussion and Directions for Future Work The problem of how to generate readable and concise summaries automatically for spoken dialogues of unrestricted domains involves many challenges that need to be addressed.
Some of the research issues are similar or identical to those faced in summarizing written texts (such as topic segmentation, determining the most salient/relevant information, anaphora resolution, summary evaluation), but other additional dimensions are added on top of this list, including speech disfluency detection, sentence boundary detection, cross-speaker information linking, and coping with imperfect speech recognition.
The line of argument of this article has been that whereas using a 480 Zechner Automatic Summarization of Dialogues traditional approach for written text summarization (such as the MMR-based sentence selection component within DIASUMM) may be a good starting point, addressing the dialogue-specific issues is key for obtaining better summaries for informal genres.
We decided to focus on the three problems of (1) speech disfluency detection, (2) sentence boundary detection, and (3) cross-speaker information linking and implemented trainable system components to address each of these issues.
Both the evaluations of the individual components of our spoken-dialogue summarization system and the global evaluations as well have shown that we can successfully make use of the SWITCHBOARD corpus (LDC 1999b) to train a system that works well on two other genres of informal dialogues, CALLHOME and GROUP MEETINGS.
We conjecture that the reasons why the DIASUMM system was not able to improve over the MMR baseline for the two other corpora, which are more formal, lies in their very nature of being of a quite different genre: the NEWSHOUR and CROSSFIRE corpora have longer turns and sentences, as well as fewer disfluencies.
We would also conjecture that their sentence structures are more complex than what we typically find in the other corpora of more colloquial, spontaneous conversations.
Future work will have to address the issue of whether the availability of training data for more formal dialogues (in size and annotation style comparable to the SWITCHBOARD corpus, though) could lead to an improvement in performance on those data sets, as well, or if even then a standard written-text-based summarizer would be hard to improve upon.
Given the complexity of the task, we had to make a number of simplifying assumptions, most notably about the input data for our system: We use perfect transcripts by humans instead of ASR transcripts, which, for these genres, typically show word error rates (WERs) ranging from 15% to 35%.
Previous related work (Valenza et al.1999; Zechner and Waibel 2000b) demonstrated that the actual WERs in summaries generated from ASR output are usually substantially lower than the full-ASR-transcript WER and can further be reduced by taking acoustically derived confidence scores into account.
We further did not explore the potential improvements of components as well as of the system overall when prosodic information such as stress and pitch is added as an input feature.
Past work in related fields (Shriberg et al.1998; Shriberg et al.2000) suggests that particularly for ASR input, noticeable improvements might be achievable when such input is provided.
Although presegmentation of the input into topically coherent segments certainly is a useful step in summarization for any kind of texts (written or spoken), we have not addressed and discussed this issue in this article.
Finally, we think that there is more work needed in the area of automatically deriving discourse structures for spoken dialogues in unrestricted domains, even if the text spans covered might be only local (because of a lack of global discourse plans).
We believe that a summarizer, in addition to knowing about the interactively constructed and coherent pieces of information (such as in question-answer pairs), could make good use of such structured information and be better guided in making its selections for summary generation.
In addition, this discourse structure might aid modules that perform automatic anaphora detection and resolution.
8. Conclusions We have motivated, implemented, and evaluated an approach for automatically creating extract summaries for open-domain spoken dialogues in informal and formal genres of multiparty conversations.
Our dialogue summarization system DIASUMM 481 Computational Linguistics Volume 28, Number 4 uses trainable components to detect and remove speech disfluencies (making the output more readable and less noisy), to determine sentence boundaries (creating suitable text spans for summary generation), and to link cross-speaker information units (allowing for increased summary coherence).
We used a corpus of 23 dialogue excerpts from four different genres (80 topical segments, about 47,000 words) for system development and evaluation and the disfluencyannotated SWITCHBOARD corpus (LDC 1999b) for training of the dialogue-specific components.
Our corpus was annotated by six human coders for topical boundaries and relevant text spans for summaries.
Additionally, we had annotations made for disfluencies, sentence boundaries, question speech acts, and the corresponding answers to those question speech acts.
In a global system evaluation we compared the MMR-based sentence selection component with the DIASUMM system using all of its components discussed in this article.
The results showed that (1) both a baseline MMR system as well as DIASUMM create better summaries than a LEAD baseline (except for NEWSHOUR) and that (2) DIASUMM performs significantly better than the baseline MMR system for the informal dialogue corpora (CALLHOME and GROUP MEETINGS).
Acknowledgments We are grateful to Alex Waibel, Alon Lavie, Jaime Carbonell, Vibhu Mittal, Jade Goldstein, Klaus Ries, Lori Levin, and Marsal Gavald` for many discussions, a suggestions, and comments regarding this work.
We also want to commend the corpus annotators for their efforts.
Finally, we want to thank the four anonymous reviewers for their detailed feedback on a preliminary draft, which greatly helped improve this article.
This work was performed while the author was affiliated with the Language Technologies Institute at Carnegie Mellon University and was supported in part by grants from the U.S.
Department of Defense.
References Alexandersson, Jan and Peter Poller.
1998. Towards multilingual protocol generation for spontaneous speech dialogues.
In Proceedings of INLG-98, Niagara-on-the-Lake, Canada, August.
Aone, Chinatsu, Mary Ellen Okurowski, and James Gorlinsky.
1997. Trainable, scalable summarization using robust NLP and machine learning.
In ACL/EACL-97 Workshop on Intelligent and Scalable Text Summarization, Madrid.
Arons, Barry.
1994. Pitch-based emphasis detection for segmenting speech.
In Proceedings of ICSLP-94, pages 19311934.
Berger, Adam L.
and Vibhu O.
Mittal. 2000.
OCELOT: A system for summarizing Web pages.
In Proceedings of the 23rd ACM-SIGIR Conference.
Bett, Michael, Ralph Gross, Hua Yu, Xiaojin Zhu, Yue Pan, Jie Yang, and Alex Waibel.
2000. Multimodal meeting tracker.
In Proceedings of the Conference on Content-Based Multimedia Information Access (RIAO-2000), Paris, April.
Brill, Eric.
1994. Some advances in transformation-based part of speech tagging.
In Proceedings of AAAI-94.
Carbonell, Jaime, Yibing Geng, and Jade Goldstein.
1997. Automated query-relevant summarization and diversity-based reranking.
In Proceedings of the IJCAI-97 Workshop on AI and Digital Libraries, Nagoya, Japan.
Carbonell, Jaime and Jade Goldstein.
1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries.
In Proceedings of the 21st ACM-SIGIR International Conference on Research and Development in Information Retrieval, Melbourne, Australia.
Carletta, Jean, Amy Isard, Stephen Isard, Jacqueline C.
Kowtko, Gwyneth Doherty-Sneddon, and Anne H.
Anderson. 1997.
The reliability of a dialogue structure coding scheme.
Computational Linguistics, 23(1):1331.
Chen, Francine R.
and Margaret Withgott.
1992. The use of emphasis to automatically summarize a spoken discourse.
In Proceedings of ICASSP-92, pages 229332.
Cohen, Jacob.
1960. A coefficient of agreement for nominal scales.
Educational and Psychological Measurement, 20(1):3746.
Davies, Mark and Joseph L.
Fleiss. 1982.
Measuring agreement for multinomial data.
Biometrics, 38:10471051, December.
Garofolo, John S., Ellen M.
Voorhees, Cedric G.
P. Auzanne, and Vincent M.
Stanford. Zechner Automatic Summarization of Dialogues 1999.
Spoken document retrieval: 1998 evaluation and investigation of new metrics.
In Proceedings of the ESCA Workshop: Accessing Information in Spoken Audio, pages 17, Cambridge, UK, April.
Garofolo, John S., Ellen M.
Voorhees, Vincent M.
Stanford, and Karen Sparck Jones.
1997. TREC-6 1997 spoken document retrieval track overview and results.
In Proceedings of the 1997 TREC-6 Conference, pages 8391, Gaithersburg, MD, November.
Gavalda, Marsal, Klaus Zechner, and Gregory Aist.
1997. High performance segmentation of spontaneous speech using part of speech and trigger word information.
In Proceedings of the fifth ANLP Conference, Washington, DC, pages 1215.
Godfrey, J.
J., E.
C. Holliman, and J.
McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for research and development.
In Proceedings of ICASSP-92, volume 1, pages 517520.
Grosz, Barbara J.
and Candace L.
Sidner. 1986.
Attention, intentions, and the structure of discourse.
Computational Linguistics, 12(3):175204.
Hearst, Marti A.
1997. TextTiling: Segmenting text into multi-paragraph subtopic passages.
Computational Linguistics, 23(1):3364.
Heeman, Peter A.
and James F.
Allen. 1999.
Speech repairs, intonational phrases, and discourse markers: Modeling speakers' utterances in spoken dialogue.
Computational Linguistics, 25(4):527571.
Hirschberg, Julia, Steve Whittaker, Don Hindle, Fernando Pereira, and Amit Singhal.
1999. Finding information in audio: A new paradigm for audio browsing/retrieval.
In Proceedings of the ESCA Workshop: Accessing Information in Spoken Audio, pages 117122, Cambridge, UK, April.
Hori, Chiori and Sadaoki Furui.
2000. Automatic speech summarization based on word significance and linguistic likelihood.
In Proceedings of ICASSP-00, pages 15791582, Istanbul, Turkey, June.
Jurafsky, Daniel, Rebecca Bates, Noah Coccaro, Rachel Martin, Marie Meteer, Klaus Ries, Elizabeth Shriberg, Andreas Stolcke, Paul Taylor, and Carol Van Ess-Dykema.
1998. SwitchBoard discourse language modeling project: Final report.
Research Note 30, Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD.
Kameyama, Megumi, and I.
Arima. 1994.
Coping with aboutness complexity in information extraction from spoken dialogues.
In Proceedings of ICSLP 94, pages 8790, Yokohama, Japan.
Kameyama, Megumi, Goh Kawai, and Isao Arima.
1996. A real-time system for summarizing human-human spontaneous spoken dialogues.
In Proceedings of ICSLP-96, pages 681684.
Knight, Kevin and Daniel Marcu.
2000. Statistics-based summarization--Step one: Sentence compression.
In Proceedings of the 17th National Conference of the AAAI.
Koumpis, Konstantinos and Steve Renals.
2000. Transcription and summarization of voicemail speech.
In Proceedings of ICSLP-00, pages 688691, Beijing, China, October.
Krippendorff, Klaus.
1980. Content Analysis.
Sage, Beverly Hills, CA.
Kupiec, J., J.
Pedersen, and F.
Chen. 1995.
A trainable document summarizer.
In Proceedings of the 18th ACM-SIGIR Conference, pages 6873.
Lavie, Alon, Alex Waibel, Lori Levin, Michael Finke, Donna Gates, Marsal Gavalda, Torsten Zeppenfeld, and Puming Zhan.
1997. Janus III: Speech-to-speech translation in multiple languages.
In IEEE International Conference on Acoustics, Speech and Signal Processing, Munich.
Levin, Lori, Klaus Ries, Ann Thyme-Gobbel, and Alon Lavie.
1999. Tagging of speech acts and dialogue games in Spanish call home.
In Proceedings of the ACL-99 Workshop on Discourse Tagging, College Park, MD.
Linguistic Data Consortium (LDC).
1996. CallHome and CallFriend LVCSR databases.
Linguistic Data Consortium (LDC).
1999a. Addendum to the part-of-speech tagging guidelines for the Penn Treebank project (Modifications for the SwitchBoard corpus).
LDC CD-ROM LDC99T42.
Linguistic Data Consortium (LDC).
1999b. Treebank-3: Databases of disfluency annotated Switchboard transcripts.
LDC CD-ROM LDC99T42.
Mani, Inderjeet, David House, Gary Klein, Lynette Hirschman, Leo Obrst, Therese Firmin, Michael Chrzanowski, and Beth Sundheim.
1998. The TIPSTER SUMMAC text summarization evaluation.
Technical Report MTR 98W0000138, Mitre Corporation, October 1998.
Mani, Inderjeet and Mark T.
Maybury, editors.
1999. Advances in Automatic Text Summarization.
MIT Press, Cambridge.
Marcu, Daniel.
1999. Discourse trees are good indicators of importance in text.
In I.
Mani and M.
T. Maybury, editors, Computational Linguistics Volume 28, Number 4 Advances in Automatic Text Summarization.
MIT Press, Cambridge, pages 123136.
Meteer, Marie, Ann Taylor, Robert MacIntyre, and Rukmini Iyer.
1995. Dysfluency annotation stylebook for the Switchboard corpus.
Linguistic Data Consortium (LDC) CD-ROM LDC99T42.
Miike, Seiji, Etuso Itoh, Kenji Onon, and Kazuo Sumita.
1994. A full-text retrieval system with a dynamic abstract generation function.
In Proceedings of the 17th ACM-SIGIR Conference, pages 318 327.
Nakatani, Christine H.
and Julia Hirschberg.
1994. A corpus-based study of repair cues in spontaneous speech.
Journal of the Acoustic Society of America, 95(3):16031616.
Passonneau, Rebecca J.
and Diane J.
Litman. 1997.
Discourse segmentation by human and automated means.
Computational Linguistics, 23(1):103139.
Quinlan, J.
Ross. 1992.
C4.5: Programs for Machine Learning.
Morgan Kaufmann, San Mateo, CA.
Rath, G.
J., A.
Resnick, and T.
R. Savage.
1961. The formation of abstracts by the selection of sentences.
American Documentation, 12(2):139143.
Reimer, U.
and U.
Hahn. 1988.
Text condensation as knowledge base abstraction.
In Proceedings of the fourth Conference on Artificial Intelligence Applications, pages 338344, San Diego.
Reithinger, Norbert, Michael Kipp, Ralf Engel, and Jan Alexandersson.
2000. Summarizing multilingual spoken negotiation dialogues.
In Proceedings of the 38th Conference of the Association for Computational Linguistics, pages 310317, Hong Kong, China, October.
Ries, Klaus, Lori Levin, Liza Valle, Alon Lavie, and Alex Waibel.
2000. Shallow discourse genre annotation in CALLHOME Spanish.
In Proceedings of the Second Conference on Language Resources and Evaluation (LREC-2000), Athens, May/June.
Rose, Ralph Leon.
1998. The Communicative Value of Filled Pauses in Spontaneous Speech.
Ph.D. thesis, University of Birmingham, Birmingham, UK.
Salton, Gerard, editor.
1971. The SMART Retrieval System--Experiments in Automatic Text Processing.
Prentice Hall, Englewood Cliffs, NJ.
Santorini, Beatrice.
1990. Part-of-Speech Tagging guidelines for the Penn Treebank project.
Linguistic Data Consortium (LDC) CD-ROM LDC99T42.
Shriberg, Elizabeth E . 1994.
Preliminaries to a Theory of Speech Disfluencies.
Ph.D. thesis, University of Berkeley, Berkeley.
Shriberg, Elizabeth, Rebecca Bates, Andreas Stolcke, Paul Taylor, Daniel Jurafsky, Klaus Ries, Noah Coccaro, Rachel Martin, Marie Meteer, and Carol Van Ess-Dykema.
1998. Can prosody aid the automatic classification of dialog acts in conversational speech?
Language and Speech, 41(34):439487.
Shriberg, Elizabeth, Andreas Stolcke, Dilek Hakkani-Tur, and Gokhan Tur.
2000. Prosody-based automatic segmentation of speech into sentences and topics.
Speech Communication, 32(12):127154.
Stifelman, Lisa J . 1995.
A discourse analysis approach to structured speech.
In AAAI-95 Spring Symposium on Empirical Methods in Discourse Interpretation and Generation, Stanford, March.
Stolcke, Andreas, Klaus Ries, Noah Coccaro, Elizabeth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor, Rachel Martin, Carol Van Ess-Dykema, and Marie Meteer.
2000. Dialogue act modeling for automatic tagging and recognition of conversational speech.
Computational Linguistics, 26(3):339373.
Stolcke, Andreas and Elizabeth Shriberg.
1996. Automatic linguistic segmentation of conversational speech.
In Proceedings of ICSLP-96, pages 10051008.
Stolcke, Andreas, Elizabeth Shriberg, Rebecca Bates, Mari Ostendorf, Dilek Hakkani, Madeleine Plauche, Gokhan Tur, and Yu Lu.
1998. Automatic  detection of sentence boundaries and disfluencies based on recognized words.
In Proceedings of ICSLP-98, volume 5, pages 22472250, Sydney, December.
Teufel, Simone and Marc Moens.
1997. Sentence extraction as a classification task.
In ACL/EACL-97 Workshop on Intelligent and Scalable Text Summarization, Madrid.
Valenza, Robin, Tony Robinson, Marianne Hickey, and Roger Tucker.
1999. Summarisation of spoken audio through information extraction.
In Proceedings of the ESCA Workshop: Accessing Information in Spoken Audio, pages 111116, Cambridge, UK, April.
Wahlster, Wolfgang.
1993. Verbmobil--Translation of face-to-face dialogs.
In Proceedings of MT Summit IV, Kobe, Japan.
Waibel, Alex, Michael Bett, and Michael Finke.
1998. Meeting browser: Tracking and summarizing meetings.
In Proceedings of the DARPA Broadcast News Workshop.
Ward, Wayne.
1991. Understanding spontaneous speech: The PHOENIX system.
In Proceedings of ICASSP-91, Zechner Automatic Summarization of Dialogues pages 365367.
Whittaker, Steve, Julia Hirschberg, John Choi, Don Hindle, Fernando Pereira, and Amit Singhal.
1999. SCAN: Designing and evaluating user interfaces to support retrieval from speech archives.
In Proceedings of the 22nd ACM-SIGIR International Conference on Research and Development in Information Retrieval, pages 2633, Berkeley, August.
Zechner, Klaus and Alon Lavie.
2001. Increasing the coherence of spoken dialogue summaries by cross-speaker information linking.
In Proceedings of the NAACL-01 Workshop on Automatic Summarization, pages 2231, Pittsburgh, June.
Zechner, Klaus and Alex Waibel.
2000a. DIASUMM: Flexible summarization of spontaneous dialogues in unrestricted domains.
In Proceedings of COLING-2000, pages 968974, Saarbrucken, Germany,  July/August.
Zechner, Klaus and Alex Waibel.
2000b. Minimizing word error rate in textual summaries of spoken language.
In Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-2000), pages 186193, Seattle, April/May .
From Conceptual Time to Linguistic Time Michel Gagnon* Machina Sapiens Guy Lapalme t Universit4 de Montr4al In this paper, we present a method for generating French texts conveying temporal information that integrates Discourse Representation Theory (DRT) and Systemic Grammar Theory.
DRT is used to represent temporal information and an intermediate semantic level for the temporal localization expressed by temporal adverbial phrases and verb phrases.
This representation is then translated into a syntactic form using Systemic Grammar Theory.
We have implemented this method in a working prototype called Prdtexte.
1. Introduction In speaker-generated texts, reference is made to facts taking place in time.
To use the same kind of references in automatically generated text, the mechanisms that govern the expression of temporal concepts must be identified.
There is no simple or direct mapping between conceptual time, as it is perceived in the real world, and linguistic time, which refers to the way time is formulated in language.
There may be different ways to present the same temporal concept in a text, and a single linguistic marker can be used to convey different temporal meanings.
For example, the discourse below (Discourse 1) is a text generated by Pr4texte, a system we developed for implementing the expression of temporal localization in French texts 1.
It is a slightly modified version of an example used by Bras (1990) for the extraction of temporal information in text analysis.
The sentences report occurrences that are facts taking place in time.
We have inserted labels in parentheses to distinguish the twelve occurrences reported in the text.
Hier l'avion a effectu6 un vol (ol).
A 8h00 il a quitt6 Paris (02).
Quand il a survol6 Barcelone (o3), le r6acteur fonctionnait (o4).
~, 10h15, un voyant a clignot6 (o5).
Auparavant, il s'6tait allure6 (o6).
Puis il s'6tait 6teint (o7).
Pendant 35 minutes, l'avion a survol6 lamer (o8).
Puis il a atteint la c6te (o9).
Jusqu'a 10h50, il a survol6 l'Alg6rie (o10).
A llh30 il 6tait sur la piste (o11).
A ce moment-la le r6acteur a explos6 (o12).
Yesterday the plane made a flight (ol).
At 8:00 A.M. it left Paris (o2).
When it flew over Barcelona (o3), the engine was working (o4).
At 10:15, a warning light flashed (o5).
Previously it had come on (o6).
Then it had gone out (o7).
For 35 minutes the plane flew over the sea (o8).
Then it reached the coast (o9).
Until 10:50 it flew over Algeria (olo).
At 11:30 it was on the landing runway (o11).
At this moment the engine exploded (o12).
Discourse 1 * 3535 Queen-Mary, Bur.
420, Montr6al (Quebec), Canada H3V 1H8, Tel: (514) 733-3959.
E-maih gagnon@iro.umontreal.ca.
This article was written while the author was at "Laboratoire Langue, Raisonnement et Calcul" of IRIT, Toulouse, France.
JD6partement cl'informatique et de recherche op6rationnelle, C.
P. 6128, Succ.
Centre-Ville, Montr6al (Qu6bec), Canada H3C 3J7.
E-mail: lapalme@iro.umontreal.ca.
1 The
French text shown on the left in Discourse 1 was generated by Pr6texte; we give an English translation on the right.
@ 1996 Association for Computational Linguistics Computational Linguistics Volume 22, Number 1 In Discourse 1, we find two types of temporal markers: verb tense and what we call adverbials of temporal location (ATL).
An ATL is an adjunct, such as yesterday, until 10:50, or when it flew over Barcelona, that provides information about the temporal localization of an occurrence or its duration, or both at the same time.
For verb tense, we distinguish different ways of indicating localization in the past.
Three French verb tenses can be used: pass~ composG imparfait, and plus-que-parfait; their closest equivalents in English are the simple past, past progressive, and past perfect.
The passf compos~ il a survold 'it flew over' presents the occurrence as an event and localizes it in relation to the time of speech.
With the plus-que-parfait il s'dtait allum~ 'it had come on', the occurrence is also presented as an event, but localized in relation to a perspective point other than speech time.
The imparfait le r~acteur fonctionnait 'the engine was working' presents the occurrence as being in progress.
For present and future tenses there are fewer options than for past tense, but more than one form is available for theses tenses as well.
For ATLs, temporal localization can be achieved in many ways; for example, in relation to the time of speech (hier 'yesterday'), by designating an absolute temporal location (~ 8hO0 'at 8:00 A.M.'), or in relation to another fact (puis 'then', ~ ce moment-ld 'at this moment', quand il a survold Barcelone 'when it flew over Barcelona').
To this variety in the semantics of localization we must add the variety of syntactic forms.
Localization can be expressed by an adverb (puis 'then'), a prepositional phrase (jusqu'd 10h50 'until 10:50'), a nominal phrase (le lendemain 'the day after') or a subordinate clause (quand il a survold Barcelone 'when it flew over Barcelona').
No text generator has yet been developed to solve the problem of the expression of time.
The ones that have tackled this problem have focused on the production of verb tenses, without solving the choice of temporal adverbs.
The work presented in this paper addresses the problem of generating the elements that convey temporal localization in French, including both verbs and temporal adverbs.
In a previous paper (Gagnon and Lapalme 1992), we proposed a method of integrating the expression of temporal concepts into the text-generation process.
In particular, we showed how to produce different types of text in French from a single representation of events.
Unfortunately, the method governing the planning process was too determined by temporal concepts, so it was difficult to link this planning process with other frameworks, such as the schema proposed by McKeown (1985) or Rhetorical Structure Theory (RST) (Mann 1991; Hovy 1991).
As we were not really successful in integrating the expression of time in French into the text-generation process, we decided to pursue our research with a different perspective.
We designed a system covering many of the possibilities of expressing time in French, our hypothesis being that the achievement of this task would facilitate the design of a text-planning process.
We believed it would be easier to organize the structure of the discourse with a better understanding of the way temporal information can be expressed by adverbs and verb tenses.
We started from the work of Bras (1990), who proposes a method of extracting the temporal structure of a text, according to Discourse Representation Theory (Kamp 1981), that relies on an analysis of adverbials of temporal location made by Molin6s (1990).
To implement the production of ATLs and verb tenses, we have chosen Systemic Grammar (Halliday 1985; Berry 1975, 1976), which formulates the syntactic structure of a sentence as the result of a sequence of semantic choices.
We developed a grammar interpreter inspired by Nigel (Matthiessen and Bateman 1991), but departing from it in many respects; in particular, our representation of the production of verb tenses and adverbs is quite different.
In this paper, we discuss the elements required to produce a text such as Dis92 Gagnon and Lapalme From Conceptual Time to Linguistic Time / DEEP / GENERATIONN f SURFACE GENERATION", Conceptual representation Discourse representation t Semantic representation 1 Syntactic representation Occurrences as primitive concepts Temporal relations The tempolral Jocaligafion is represented as an overlapmg relation Sellmentafion of the conceptual ret~resentaaon Structured information Rhetorical relations Linearization Choice of aspect Identification of temporal markers Temporal adverbial Verb tense Figure 1 The global process.
course 1.
The process starts from a conceptual representation that encodes the facts to be reported in the text, associated with their position in time.
The information at this objective conceptual level must be translated into a semantic representation where the facts are presented according to a subjective perspective.
The semantic representation is then used to produce the text.
We have concentrated our attention on this last stage, but we cannot avoid the problem of determining how the representation used at this level is obtained from previous levels.
In the following sections, we describe the two stages of the text-generation process.
2. The Global Process It is generally accepted that the generation process requires at least two parts.
The first part, deep generation, is a planning process in which the content and the overall structure of the text are established.
In the second part, surface generation, the words and the syntactic structure of the text are chosen.
Figure 1 summarizes our view of the global process, starting from a conceptual representation that contains occurrences and relations between them.
The fact that an occurrence takes place at a certain time is expressed by an overlapping relation between this occurrence and the object representing this time.
The deep generation process is decomposed into two steps.
In the first step, the conceptual representation is segmented and structured to build a discourse representation.
In our discourse representation, which uses Segmented Discourse Representa93 Computational Linguistics Volume 22, Number 1 tion Theory (SDRT) (Asher 1993), the information is cut into smaller segments each of which contains the information to be expressed by a single sentence.
The structure linking these segments relies on a set of rhetorical relations.
In the second step of the deep generation process, the discourse representation is traversed and, for each segment, rules are applied to identify the feature values needed to translate it into a sentence.
We thus obtain a linear structure in which each element is a set of features that determine the syntactic form of the sentence.
In the surface generation process, the information in the semantic representation is used to select the appropriate syntactic structure for the expression of time: an adverbial of temporal location (ATL) or a verb phrase (VP), or both.
3. The Deep Generation Process Although our work focuses on surface generation, we cannot ignore the issue of deep generation, because the nature of the semantic representation is determined not only by the syntax of the language, but also by the temporal concepts available.
Therefore, in this section, we first present the conceptual representation that induces the semantic representation used by our generator.
We then explain the intermediate discourse level.
We do not know yet in detail how to produce the semantic representation from the conceptual representation, but we do have an idea of what information each level of representation must contain and what choices must be made at each stage of the process.
3.1 Conceptual
Representation To represent temporal concepts in Pr6texte, we chose the principles of Discourse Representation Theory (DRT), which offers one of the most interesting explanations of how temporal notions are conveyed by a text.
DRT was developed to deal with specific problems of discourse understanding: in particular, problems with anaphora and the differences between some verb tenses, with respect to temporal localization.
Our goal is not to show how this theory can be used for generation but rather to use its principles as a convenient formalism for the representation of time.
In DRT, a text is associated with a Discourse Representation Structure (DRS) that is updated incrementally by the processing of each sentence.
A DRS is a structure containing a set of entities and a set of conditions on these entities.
There are different types of entities in DRT: a temporal fact can either be presented as an event (having a punctual aspect), or as a situation (having a certain extent in time, but considered from an internal perspective at a given moment in time); a temporal constant that designates a segment of the temporal axis; entities that participate in the events or situations.
In Pr6texte, conceptual knowledge is represented as a DRS, which we adapted slightly for text generation.
We do not distinguish between events and situations in the conceptual representation, because we want this level to remain independent of the language.
Furthermore, we feel that this distinction should not be encoded at the conceptual level, rather the generation system should choose among these possibilities.
Therefore, at the conceptual level we use the concept of occurrence for either an event or a situation.
94 Gagnon and Lapalme From Conceptual Time to Linguistic Time n tl t2 t3 t4 01 02 03 04 a l p b r plane(a) flight(l) engine(r) city(p) Paris(p) city(b) o20 t2 Barcelona(b) ol < n o1: make(a, 1) o3 < n o 2" leave(a,p) o~ C tl o3: flightover(a, b) o4 < n 04: work(r) o3 C 04 n c t3 03 C) t4 tl = Sept.
10 1992 t2 = Sept.
10 1992 at 8:10 am t3 = Sept.
11 1992 t4 = Sept.
10 1992 at 9:00 am 02 <n Figure 2 Conceptual representation for the first three sentences of Discourse 1.
There are essentially two ways of considering time or, to be more precise, the notion of temporal location: either temporal location is determined using a preexisting time scale, or it is deduced from the occurrence.
Following Kamp (1981), we think that the second possibility, in which temporal location is a relative concept, is more suitable for natural text processing.
Treating occurrences as entities, rather than making them subordinate to temporal intervals or points, has been proposed by Davidson (1967).
An occurrence may be represented in relation to another temporal object, without any reference to its own location in time.
This approach eases the representation of underspecified temporal localizers--an important point for our semantics.
For further discussion of the advantages of taking occurrences as primitives, see (Bras 1990; Kamp 1979, 1981).
In the conceptual representation, we find four types of information:  the description of occurrences;  the description of participants in the occurrences;  the description of temporal localizers, which are called temporal constants (they usually refer to time periods of the calendar);  temporal relations between occurrences and temporal localizers: The relation < represents temporal precedence.
The overlap relation C) indicates that two temporal objects are somehow simultaneous.
Thus, in our representation, "Y happens at time X" is represented by "Y temporally overlaps X".
The relation C expresses the fact that the temporal extent of a temporal object is a subset of the temporal extent of another object.
Figure 2 shows the part of the DRS used to generate the first three sentences of Discourse 1.
It contains five temporal constants: n, tl, t2, t3 and t4.
It is not clear how 95 Computational Linguistics Volume 22, Number 1 these temporal constants are to be described in DRT, so we have proposed elsewhere a formalization of the type of objects designated by these constants (Gagnon and Bras 1994).
In this article, we give only an English description of them: n represents speech time, which, in Figure 2, is included in the time represented by t3 (September 11 1992).
Four occurrences are represented: ol, o2, o3, and o4, all of which take place before n.
Not all temporal relations in the DRS need to be given as input because many relations can be inferred using three kinds of knowledge:  The representation of conventional time to identify a specific period in time; this representation relies on a structure of conventional time, together with reasoning mechanisms to deduce temporal relations (see Gagnon and Bras \[1994\] for an implementation of such a structure).
For example, from this knowledge we can deduce that September 10 must be before September 11, which would be represented as tl < t3.
Similarly, we can deduce t2 C tl, t4 C tl, and t2 < t4.
 World knowledge about the occurrences: knowing that o2, 03, and o4 are part of Ol implies that they are all temporally included in it.
 A reasoning mechanism on the temporal relations, using a set of axioms, such as: Vx, y(xOyvx < yvy < x) Vu, v,x,y(uOxAvOyAx < y~ uOvVu < v) The first axiom states that for any two times, either they overlap or one precedes the other.
The second axiom states that if two other times u, v overlap two times x, y that are in a precedence relation, either u overlaps v, or it precedes it.
So in Figure 2, from the relations 02 O t2, 03 O t4, and t2 < t4, we can infer o2 < 03 V o20 o3.
From world knowledge, we can infer that o2 and 03 cannot overlap (leaving of Paris cannot overlap flying over Barcelona); Therefore, we conclude that 02 ~ 03.
3.2 The
Discourse Representation To generate a text from an input such as Figure 2, we must choose a discourse structure that segments the message into sentences.
Figure 3 illustrates one discourse representation, inspired by the Segmented Discourse Representation Theory (SDRT) proposed by Asher (1993), which extends DRT by adding rhetorical relations such as those found in RST (Mann and Thompson 1987).
A discourse structure that contains the same information as in Figure 2, except that it has been segmented, we call a Segmented Discourse Representation Structure (or SDRS).
The top-level DRS contains three small DRSs that are linked by rhetorical relations: each DRS corresponds to a sentence.
In addition to these three small DRSs, the top-level DRS contains the global text information: the description of participants and the description of speech time.
We do not yet produce this discourse structure, but we are working on this problem, using the results of researchers who have applied SDRT to the analysis process (Lascarides and Asher 1993; Bras and Asher 1994).
In the discourse structure of Figure 3, one sentence is elaborated by two other sentences that constitute a narration.
96 Gagnon and Lapalme From Conceptual Time to Linguistic Time plane(a) fight(l) city(p) Paris(p) n t3 a l p b r Ol tl o1: make(a, 1) tl = Sept.
10 1992 Ol Kn Ol C tl Elaboration Barcelona(b) engine(r) t3 = Sept.
11 1992 n C t3 city(b) 02 t2 o2: leave(a, p) t2 = Sept.
10 1992 at 8:10 am 02 Kn 02 0 t2 Narration 03 04 t4 03: flightover(a,b) 04: work(r) O3 (n 04 <n t4 = Sept.
10 1992 at 9:00 am 03 C 04 03 0 t4 Figure 3 Discourse representation for the first three sentences of Discourse 1.
3.3 Semantic
Representation The discourse structure is then translated into a semantic representation of the form $1, $2,..., Sn where Si designates the i th element of a semantic representation S.
Translation of the SDRS is obtained by a depth-first traversal of the DRSs it contains.
For each DRS, we establish its corresponding feature structure in the semantic representation.
Figure 4 is a semantic representation produced from the SDRS of Figure 3.
Each structure contains five features.
The feature message refers to the occurrence that must be reported by the sentence, and specifies its aspect.
We distinguish, as Kamp does, two aspects that can be used to present an occurrence: event or situation.
Situations can be open or resulting.
A situation is open when the speaker/hearer is located at a time within an occurrence.
A resulting situation is the state following the termination of an occurrence.
In French, the event aspect for a past occurrence results in the use of the pass~ composd (simple past in English).
The imparfait and the plus-que-parfait correspond to open and resulting situations (the closest tenses in English are the past progressive and the past perfect).
In the first two elements of Figure 4, the occurrence is presented as an event, whereas in the last one it is presented as an open situation.
Among the four occurrences contained in the DRS, only three of them constitute the main "message" of the text: ol, o2, and o4.
The four other features in a structure Si give the value of four temporal markers that express the localization of the occurrence.
These markers correspond to the four markers proposed by Kamp and Rohrer (Bras 1990) for the analysis of texts, which we have adapted for text generation.
They can be seen as an extension of Reichenbach's markers (1947).
Essentially, the values of these four features depend on two data:  the DRSs to which the visited DRS is attached in the discourse structure, and  the rhetorical relations.
97 Computational Linguistics Volume 22, Number 1 Figure 4 message = event(ol) N=n REF = nil PERSP = n LOC = tl S1 message = event(o2) N=n R =Ol PERSP = n LOG = t2 $2 message : open_situation ( o 4 ) N=n R=o2 PERSP = n LOC = o3 S3 Semantic representation for the first three sentences of Discourse 1.
O1 :....
~. Elaboration 102 ....
LNa a onL04 ....
INaatinIO5 ....
I a atin ~.
Flashback h 08: ....
I 06: ....
\[ Narration ~, 07: ....
\] Figure 5 Discourse representation for the first seven sentences of Discourse 1.
The marker N represents the time of speech.
This marker is constant in our example, but it could be locally altered in the discourse, in indirect speech for example.
We did not study such cases, but we think that the marker N would be required to deal with them.
Perspective point PERSP refers to an instant from which the occurrence must be considered.
Usually it is the same as the time of speech, but in some cases, such as a flashback, it has a different value.
In Discourse 1, there is one such case.
The fifth and sixth sentences (where occurrences o6 and o7 are reported) constitute a flashback: the perspective point is the occurrence of the fourth sentence (os).
In discourse structure, the flashback is represented with a rhetorical structure.
Consider for example the discourse structure for the first seven sentences of Discourse 1 as sketched in Figure 5.
For the translation of the two DRSs containing 06 and o7, the value of PERSP will be the occurrence o5, since the DRS containing this occurrence dominates the two others with the relation flashback.
For the next DRS, the one containing os, the perspective point will be reset to the value it had before entering the flashback, that is, the value when the DRS of o6 was considered.
The value of PERSP is used for the choice of verb tense.
In Discourse 1, the flashback results in the choice of the plus-que-parfait.
LOC represents the temporal location of the occurrence reported.
If this occurrence overlaps another temporal object, this object may be used as a value for LOC.
In 98 Gagnon and Lapalme From Conceptual Time to Linguistic Time Figure 4, the values of LOC show that tl and t2 are used to localize the first two occurrences.
In the third sentence, the situation corresponding to o4 is presented at the instant where o3 takes place.
If no other temporal object overlaps the one that constitutes the message, the temporal location represented by LOC can be defined in relation to another temporal object.
We will see examples of this in the next section.
LOC represents the information to be expressed by an ATL in the sentence and does not necessarily have a value, because a sentence may not contain an ATL.
In a text, when we want to express a succession of occurrences, we need a way to memorize the occurrence that is used as a reference for the localization of the next one.
This is exactly the role of the marker REF.
The values of REF are used to represent the progression of time in the discourse.
Each time a sentence expresses a new temporal location (which can be an occurrence or a temporal constant), the value of REF is updated to this new value, and the temporal localization in the following sentence may be achieved in relation to this reference.
The following rules are used for identifying the value of REF: 1.
Identify the S-antecedent, the DRS to which the current DRS is attached in the discourse structure, and Sa, the feature structure associated with this DRS in the semantic representation.
2. If the occurrence reported as message in Sa is presented as a situation, it cannot be used as a reference point, since a situation cannot state a progression in time.
3. If LOC has a value that is temporally more precise than the occurrence in the message, REF will take on its value, otherwise REF is bound to the occurrence in the message, if this occurrence is not presented as a situation.
4. If LOC has no value and the occurrence in the message is a situation, the antecedent sentence does not state a progress in time.
Therefore, REF takes the same value as in Sa.
In Figure 4, the context for the first sentence is empty, so no value is given to REE For the second and third sentences, the value of REF is the event presented in the previous sentence.
The occurrence in the third sentence is expressed as a situation, so it cannot be the reference for the fourth sentence (not shown in the figure).
Consequently, REF for the fourth sentence takes the value of LOC in the structure of the third sentence: t4.
We will see in Section 5 how the value of REF is used to produce the temporal adverb.
The choice of aspect in building the semantic structure is achieved by taking into account pragmatic information and the interaction with other choices, such as the type of temporal localizer.
Currently, we first identify the localizer that constrains the selection of aspect, but more study is needed to clarify their interaction.
If an occurrence is presented as a situation, the temporal localizer must be a time included in it; but an event aspect cannot be combined with a localizer.
In Figure 4, the occurrence of $1 must be presented as an event, since the localizer tl includes the occurrence.
In $2, the occurrence is also an event, even if the localizer overlaps the occurrence: the overlapping relation does not prevent the existence of an inclusion relation.
If an inclusion relation between t2 and o2 could be deduced, then the situation aspect could be chosen.
In $3, the localizer is included in the occurrence of the message, so the situation aspect is selected.
99 Computational Linguistics Volume 22, Number 1 message = event (oi) N=n R = nil P=n L = ch message = event(o2) N=n ' R =01 P=n L = ct2 Figure 6 An alternative semantic representation.
message = event(03) N=n ' R =02 P=n L = ct4 message = open_situation ( o 4 ) N=n ' R =03 P=n L = ct4 ) The semantic representation given in Figure 4 is not unique.
Figure 6 shows another semantic representation built from the DRS of Figure 2.
It contains a fourth sentence.
The main difference from the previous representation is that 03, instead of acting as a localizer for o4, is the message of a sentence; t3, referring to a moment located two hours after t2, localizes o3.
Therefore, instead of the third sentence of Discourse 1, we would obtain these two sentences: Deux heures plus tard, il a survol6 Two hours later, it flew over Barcelona (o'3).
At Barcelone (o~).
Ace moment-la, le r6acteur this moment, the engine was working (o'4).
foncfionnait (o~).
Once the semantic representation is produced, the adverbial or temporal location (ATL) and the verb phrase (VP) can be generated independently.
The syntactic form of the verb phrase is determined by the combination of the following information:  temporal relation between localizer LOC and speech time N;  temporal relation between localizer LOC and perspective point PERSP;  aspect of the occurrence.
The choice of the syntactic structure of the ATL depends on the value of LOC, which may refer to N or REF.
The interaction of temporal information conveyed by verb tense and adverbs is taken into account in the process of translation from the conceptual representation to the semantic level, where the choices of aspect, perspective point PERSP and localizer LOC are made.
We still have not entirely solved the problem of choosing among all semantic representations that can be built from a DRS.
In the current implementation of Pr6texte, we have identified a set of rules to produce the semantic representation.
In particular, these rules insure that the values of the four temporal markers are coherent with the aspect chosen to present the occurrences.
What remains to be done, essentially, is to identify the knowledge that governs these rules causing them to select a particular semantic representation.
3.4 Representation
of Localization We have argued in the previous section that four temporal markers are needed to express the temporal location of an occurrence.
In this section, we discuss the marker LOC, the localizer that provides information about the location in time of the occurrence using another entity.
The localizer is usually a temporal constant, but it can also be another occurrence, whose approximate location in time is already known.
An ATL can convey localizers of two types: in the first type, a localizer directly identities the temporal zone of an occurrence using another temporal object that overlaps it; in the second type the temporal zone of an occurrence is conveyed in relation 100 Gagnon and Lapalme From Conceptual Time to Linguistic Time to another localizer.
In Figure 4, all occurrences are localized directly.
Occurrence 04 is localized directly by another occurrence, whereas 01 and o2 are localized directly by a temporal constant.
For $1 and $2, the values for LOC are simply constants tl and t2.
The same temporal localization can usually be expressed in many ways, and we must also specify how these constants are worded.
For example, in Discourse 1, T1 has been translated into hier 'yesterday' but it could also have been translated into le 10 septembre 1992 'September 10th 1992', or mercredi dernier 'last Wednesday'.
Thus, the value of LOC in the semantic representation determines the expression of the localization, giving rise to three main problems:  how to represent the temporal constants in the conceptual representation;  how to determine the link between these conceptual temporal constants and their semantic representation, which specifies how they are to be expressed in the text; and  how to implement the selection mechanism, which relies on pragmatic and stylistic information to choose between the many different ways of expressing the same temporal localization.
In Gagnon and Bras (1994), we gave a solution to the first two problems but the last one still remains to be solved.
Here, we discuss only the second problem: the semantic expression of localization.
First, a few words about temporal context: usually, temporal localizers may be understood only in reference to some time in the context.
For example, in on April 15th, it is assumed that it is clear which year this time is part of.
Thus, we take for granted that all expressions of temporal localization are made in relation to such a contextual time.
Let ti be a temporal constant, taken from the conceptual representation, which is to be used as a localizer in the semantic representation, and tcont the contextual time.
The expression in the semantic representation is based on a term of the following form: \[ti, Type, Naming\] The first argument is the identifier of the constant in the conceptual representation.
The second argument is the type of the temporal localizer (day, month, year, etc.).
Thus, tcont may be decomposed into times of type Type, of which t i is one.
The last argument names the time referred to by the localizer ti.
There is exactly one time in the "real world" that corresponds to the temporal constant ti.
We call it objective time.
We use the notation t* to represent the objective time that corresponds to a localizer ti.
For example, the expression for the temporal localizer en avril 'in April', would be something like this (here t67 is the corresponding temporal constant in the conceptual representation): \[t67, mort th, april\] The naming april is not the syntactic word "April" but an internal keyword that helps distinguish between the time referred to and the other months of the contextual year.
An important distinction is made between a temporal constant ti and a objective time t*.
The constant ti pertains to the way a temporal location is expressed in the discourse, whereas t~ can be considered the corresponding portion of time in the real world.
More than one temporal constant may correspond to a single objective time.
101 Computational Linguistics Volume 22, Number 1 Table 1 Relative localizers.
Localizer Description inclin(\[ti, Ti, Ni\],\[tj, Tj, Nj\]) incl(\[ti, Ti, Ni\],\[tj, Tj, Nj\]) begin(\[ti, Ti, Ni\],\[tj, Tj, Nj\]) end(\[ti, Ti, Ni\],\[tj, Tj, Nj\]) after(\[ti, Ti, Ni\],\[tj, Tj, Nj\],D ) before(\[ti, T,, Ni\],\[tj, Tj, Nj\],D) relpos(X,\[ti, T, Ni\],\[tj, T, Nj\]) extent(\[ti, Ti, Ni\],\[G Tj, Nj\],\[tk, Tk, Nk\]) t~ is a time included in the time t 7 t* is a time that includes the time tj* t~ is a time whose beginning overlaps the time t~ t~ is a time whose ending overlaps the time t; t~ is a time after the time t~ with a temporal distance D (expressed as a duration) t~ is a time before time t 7 with a temporal distance D (expressed as a duration) t~ is the X th (-X th, if X<0) time of type T after (before, if X<0) time tj* t~ is a time period starting at time t7 and ending at time t; Suppose, for example, that the discourse contains two temporal constants, corresponding to yesterday and two days after Robert's departure.
We can imagine a situation where both designate exactly the same day.
But it is not possible for a single temporal constant to correspond to more than one objective time.
If it were, it would mean that an ATL could be ambiguous.
In our computational perspective, we accept some underspecified ATLs (not precisely specifying the temporal location), but not ambiguous ones.
A triplet is the simplest expression of a temporal localization.
Usually, expressing a temporal localization is more complex, because the temporal constant used as localizer cannot be rendered directly in relation to the contextual time.
This is the case, for example, if the localizer is a day, and the contextual time a year, because there is no natural way of decomposing a year into days 2.
In these cases, we must express relations with some intermediate localizers, until we reach one that can be related to the contextual time.
Let \[ti, Ti, Nj\] be the localizer and \[tj, Tj, Nj\], \[tk, Tk, Nk\] be intermediates localizers.
Many kinds of relations can be distinguished.
They are listed in Table 1.
Note that these relations can be combined recursively.
This means that the triplets used as arguments may also be represented using a relation.
We will show examples of this in the following discussion.
Among the arguments of these relations, one pertains to the temporal localizer, and one (two, in the case of the relation extent) pertains to an intermediate localizer to which the temporal localizer is related.
We call this last argument an anchor, since it represents a time to which the relation must be "anchored" in order to deduce the time of localization.
We will now give a short discussion of the relations of Table 1.
In the following examples, hoe designates the temporal constant corresponding to the localizer of the occurrence, and n and trey designate the time of speech and the reference time, respectively.
2 It
is possible to name the day using the religious calendar, using something like the day of St-Andrew, but it is far from usual to do so, except maybe for holidays such as Christmas, Easter, or Thanksgiving.
102 Gagnon and Lapalme From Conceptual Time to Linguistic Time The first relation is the most frequent for expressing temporal localization.
It is used to express localizations like le 3 avril 'on April 3rd'.
In this case, the localizer could be formulated as: inclin ( \[ tto, day, 3\], \[t~, month, april\]) As expected, the intermediate localizer tl is to be interpreted in relation to the contextual year.
The semantics of a more complex localizer like le matin du 3 avril 1994 'on the morning of April 3rd 1994' is an example of using recursivity for the expression: inclin ( \[hoc, momen t-of-day, morning\], inclin ( \[ h, day, 3\], inclin (\[t2, month, april\], It3, year, 1994\] ))) The second relation, incl, is required to deal with adverbials such as le jour oft Paul est parti 'the day when Paul left', aujourd'hui 'today' and ce jour-l~ 'that day'.
All of these refer to a day, but this day is not expressed in relation to a time that includes it.
On the contrary, the other time is included in it: the time when Paul left in the first example, the time of speech in the second example, and the referent time associated with REF in the third one.
Suppose that in the conceptual representation, 023 is the object representing the departure of Paul.
These three examples could be represented, respectively, as: incl(\[hoc, day, _\], \[O23, -, -\]) incl(\[ttoc, day, _\], \[n, _, _\]) incl ( \[ tloc, day, _\], \[ tref, -, -\] ) where "_" is used for arguments whose values are not relevant or unknown.
The relations begin and end represent the case where only one boundary of the localizer is known.
This results in an ATL such as depuis le 3 avril 'since April 3rd' or jusqu'au 3 avril 'until April 3rd' where meaning is ambiguous.
What do we mean exactly, when we write that ti begins at time tj?
That the initial boundary of ti is included in tj or that the ending of tj "meets" the beginning of ti?
If the answer is that tj includes the initial boundary of ti, what is the constraint on the duration of ti?
It is clear that it must be shorter than tj.
We do not have any answers to these crucial questions, and other similar ones.
These are problems that pertain to the deep generation process, which is not the focus of this paper.
We think that at the level of the semantic representation, this relation need not be further clarified, since it corresponds to the way time is expressed in the language.
In fact, all our relations have this underspecified nature.
For the relations before and after, the value of the temporal distance is given as a duration, using an expression of this form: duration (N, Type) The value of the duration is obtained by calculating the period corresponding to N periods of type Type.
For example, the adverbial deuxjours apr~s le ddpart de Paul 'two days after Paul's departure' would be represented as (023 represents the occurrence of Paul's departure): afler(\[hoc, -, -\], \[O23, -, -\], duration(2, day)) If the temporal distance is not known (or irrelevent), it is indicated by indefinite.
Thus, aprbs le ddpart de Paul 'after Paul's departure' would be represented as: after(\[ttoc, -,-\], \[023,-, -\], indefinite) 103 Computational Linguistics Volume 22, Number 1 Now, let's suppose that the localizer ti is of type T.
tn some cases, a good way to express it is by giving its position relative to another time tj of the same type.
For example, the ATL cinq jours plus tard 'five days later' is not used to mean "at a time in the future, five days from the reference time," but rather "the 5th day after the one which included the reference time".
If the reference time is itself a day, the semantics of this ATL could be: relpos( 5, \[hoc, day, _\], \[trey,-,-\]) If trey is not a day, we must express the relation by taking as anchor the day which includes it: relpos (5, \[ Gc, day, _\], incl ( \[ t l, day, _\], \[trey, day, -\])) (This takes for granted that the time trey is not bigger than a day.
If trey were bigger than a day, it would not make any sense to express relative position by specifying the temporal distance in days).
Similarly, hier 'yesterday' would be expressed semantically as "the day that is the first one before the day that includes the time of speech": relpos(-1, \[tloc, day, -\], incl ( \[\[h, day, _\], \[n, _, _\])) We have seen a way of expressing duration, by giving the length as a number of time units.
But there is another way of expressing duration: by indicating the two boundaries of the period.
By using this method, not only the duration of an occurrence is expressed, but so is its temporal location, at least partially.
The relation extent is used to express this kind of duration.
For example, the semantics of du 3 avril au 5 mai 'from April 3rd to May 5th' is formulated as: extent(\[hoe, _, _\], inclin ( \[ h, day, 3\], \[t2, month, april\]), inclin ( \[t3, day, 5\], \[t4, month, may\])) The semantics of du 3 au 10 avril 'from April 3 to 10' should represent the fact that the whole duration is included in the same month: inclin (extent (\[boo -, -\], \[h, daY, 3\], \[t2, day, 10\]), \[t3, month, april\]) The relation extent is also used to represent adverbials like depuis trois jours 'for three days' and pendant trois jours ?z partir du 3 avril 'during three days starting from April 3rd'.
These adverbials explicitly express one of the two boundaries.
In the first example, it is either the time of speech or the reference time (the ATL means "for three days until now" or "for three days until then").
In the second example, it is the time corresponding to April 3rd.
The other boundary is indicated implicitly by giving a temporal distance from the anchor.
The first example, supposing that the explicit boundary is speech time, would be represented as: extent(\[tloc, -, -\], before(It1, -, -\], \[n, _, _\], duration (3, day)), In, _, _\]) This expresses a period whose ending is the time of speech and whose beginning must be calculated by finding the time that is three days before speech time.
The second example would be represented as: extent( \[tloc, _, _\], inclin (\[tl, day, 3\], \[t2, month, april\]), after(It3, -, -\], \[t l, -, -\], duration(3, day))) Note that in both examples, the same temporal constant represents both the explicit boundary and the anchor of the relation after or before used to express the implicit boundary (n and tl, respectively).
104 Gagnon and Lapalrne From Conceptual Time to Linguistic Time Table 2 Relative localizers.
Adverbial Semantics entre le 3 avril et le 10 mai inclin(\[hoo_,d,extent(\[ti,_,d, (between April 3rd and May lOth) inclin(\[t2,day,3\],\[t3,month,april\]), inclin(lt4,day,10\],lts,month,aprill))) jusqu'tt il y a trois jours end(\[h ..... -\], (until three days ago) before(lt2,_,_\],\[n,_,d,duration(3,day))) jusqu'/i mercredi de cette semaine end(\[tloo_,_\],inclin(\[t2,moment-of-day,wednesday\] (until Wednesday of this week) incl(\[tg,week,_\],\[n,_,_\]))) Considering the examples we have just given, one may think that recursivity applies only to the anchor.
This is not the case.
The triplet that gives the location time in the expression can be replaced by a complex expression.
We have such a situation with le 3 avril dernier 'the last April 3rd' represented as: relpos (1, inclin ( \[tlo, day, 3\], \[tl, month, april\]), incl( \[t2, day, _\], \[n, _, _\])) More precisely, this expression means "the April 3rd that is the first one in the past, taking speech time as starting point".
Finally, to illustrate the richness of our semantics for expressing ATLs, in Table 2 we give a list of more adverbials with their semantics.
Note the extensive use of the combination property.
Thus, to specify the localization of an occurrence, we can use another simultaneous object, or use a localizer that is expressed in relation to another localizer.
The list of relations given in Table 1, together with the possibility of combining them, offers a very powerful way of expressing the great diversity in the semantics of temporal localizers.
Of course, not all the combinations may be expressed naturally in the language, but we are convinced that most of the ATLs can be expressed with this semantics.
3 The
problem of representing temporal location has received a lot of attention in the past (Dowty 1979, 1982, 1986; Bach 1986; Verkuyl 1989; and Vlach 1993).
But these works have focused on the aspectual structure of adverbials and their relation to tense and aspect.
We have not found any previous proposals of a recursive semantics like ours for representing the various types of localizations.
More related to our work is Allen (1983) who proposes a set of primitive relations to represent all possible relations between two temporal intervals.
The relations defined in Table I differ in many respects from the relations proposed by Allen.
As mentioned before, ours are less precise.
For example, the moment represented using the relation end in our model corresponds to three relations in Allen's model.
Suppose that t* and tj* are the objective times corresponding to the localizer and the anchor, respectively.
Then, in Allen's model, the end of t~ could coincide with the beginning of tj*, could coincide with the end of tj*, or could be included in t 7.
The main reason for using a different set of primitives is to represent as closely as possible the way temporal localization is dealt with in language.
The result of our 3 In fact, the set of relations described here is not sufficient.
In Gagnon and Bras (1994), we define a more complete set of relations.
105 Gagnon and Lapalme From Conceptual Time to Linguistic Time I t ENVIRONMENT ...... ------o-M"-'--"~-~'........ n-Knowte.l.e.,; I base I structure concepts t I I I I I I ! ! I ! _J P R E T E R I SEMANTIC INTERFACE I -.~ ENGINE <  GRAMMAR t (Systemic I t network) I I Ii I t I t < --.... -*BLACK E I BOARD I R I Figure 8 Implementation of the surface generation process in Prdtexte.
To produce a sentence, the network is processed from left to right.
When a system is entered, a choice is made that may lead to another system or to a conjunction of systems processed concurrently.
The syntactic structure of the phrase results from a set of constraints determined by features selected during the traversal of the entire network.
The choices made in the first traversal of the network determine the overall structure of the sentence, represented as an ordered sequence of functions that must be fulfilled.
The term "function," in this context, refers to the role played by a constituent of a phrase in achieving a communicative goal (Halliday 1985).
For example, a sentence can often be decomposed into three constituents fulfilling the following functions: Subject, Predicate, and Object.
4 Once
the functional structure of the sentence is established, its network is traversed again to determine the syntactic structure, which is then further refined until each function is realized by a single word.
Figure 8 illustrates the organization of the modules in Pr6texte, inspired by Nigel (Mann 1983; Mann 1985; Matthiessen 1985; Matthiessen and Bateman 1991).
To produce a sentence, Pr6texte uses three information components: the environment, containing the information about the message and a knowledge base describing how to achieve lexicalization; the grammar, represented as a systemic network; and the blackboard, used to determine the syntactic structure.
The engine controls the surface generation process and accesses the three information components through three interface modules: semantic interface, interpreter and realizer.
The solver determines the final structure of the constituent, using constraints posted in the blackboard during the traversal of the network.
4 In
this text, names of functions will always be capitalized.
107 Computational Linguistics Volume 22, Number 1 Before starting the surface generation process, the environment is augmented with information that determines the message:  a semantic structure such as the one illustrated in Figure 4;  a set of relevant concepts, which are the elements of the conceptual representation pertaining to the entities in the semantic structure;  some pragmatic information, which specifies how to transmit the message.
The engine starts by posting on the blackboard the description of the realized constituent representing the sentence.
It then activates the traversal of the network by the interpreter.
To select a feature in a system, the interpreter transmits inquiries to the engine.
If the information necessary to respond to the inquiry is in the environment, the engine transmits the inquiry to the semantic interface.
If an inquiry is about a decision previously made in the surface generation process (for example, a question about what features have been selected in a system visited earlier), then the engine transmits the inquiry to the realizer.
Answers to inquiries enable the selection of a feature in the visited system.
The interpreter then extracts a set of realization statements associated with the selected feature.
After the execution of these statements by the realizer, the information about the structure of the realized constituent, contained in the blackboard, is updated.
Three kinds of action may be executed by the realizer:  addition of a new constituent with the appropriate semantic information fulfilling a specific function;  updating of the information pertaining to one constituent;  addition of partial ordering constraints that identify the sequence of functions composing the final structure.
This process goes on until no more system can be visited.
The solver then solves the ordering constraints on the blackboard.
We thus obtain a sequence of functions that constitutes the final structure of the realized constituent.
Each of these functions is associated with semantic information extracted from the environment.
For example, the sentence may contain the function Temp_loc (temporal localizer), whose semantic information will be the expression associated with the temporal marker LOC in the input.
If a function is to be lexicalized as a word, the lexicon is consulted to identify the word, taking into account the features selected during the traversal.
Otherwise the grammar is re-entered using the function as the new realized constituent with some features preselected.
For example, to generate the sentence in 2: (2) Jusqu'a 10h50, il a survol6 l'Alg6rie 'until 10:50, it flew over Algeria' the following semantic structure and relations are posted in the environment: message = event(olo) N=n R =09 PERSP = n LOC = end(\[hl, _, _\], inclin(\[t6, minute, 50\], \[t4, hour, lO\])) (a) o10 < n (b) o10 C) tu 108" Gagnon and Lapalme From Conceptual Time to Linguistic Time Positioner \[ Jusque I A A /\ Reference Zon 4 il \ I 1 h lOh50 a survol4 l'Alg6rie Figure 9 The structure of jusqu'd 10h50, il a survold l'Alg&ie.
where o10 must be expressed as an event and tll is a period terminating at 10:50.
The event o10 is before the time of speech and it coincides more or less with the time used as localizer.
The first traversal of the grammar determines the overall structure of the sentence made of the four functions at the top of Figure 9.
As none of them may be lexicalized directly as a word, they must be realized by re-entering the grammar.
We describe only the realization of the function Tempdoc.
The semantics associated with Temp_loc is the value of LOC in the semantic representation.
Traversal of the network for the realization of Temploc results in a structure with two functions: the Positioner and the Reference Zone.
The first function is lexicalized with the preposition jusque; for the second function, the grammar is re-entered, taking as semantics the anchor of the expression associated with Tempdoc: inclin(\[t6,minute,50\], \[t4,hour, lO\]).
Again, this results in a structure with two functions; in this case, both functions can be lexicalized, ending the realization of the function Tempdoc.
The same kind of processing is done for the other functions in the sentence.
5. The Production of ATLs In Section 3.3, we showed how the semantics of ATLs is represented; in this section, we present how ATLs can be lexicalized.
Table 3 gives a list of semantic representations and their translations into ATLs produced by our generator.
As in Section 3.4, we use ttoc for the temporal constant corresponding to the localizer of the occurrence, n for speech time and tref for the reference time.
5.1 Syntactic
Compositions Some ATLs (1-7, 16) are simple, while others (8-15, 17-18) contain an embedded temporal adverbial.
For example, in (11) jusqu'?l mercredi de cette semaine 'until Wednesday of this week' contains another ATL, which itself contains another embedded ATL cette semaine 'this week'.
109 Computational Linguistics Volume 22, Number 1 Table 3 List of adverbial temporal locations.
Semantics ATL (1) relpos(-1, \[tloc, day, _\], incl(\[t2, day, _\], \[n, _, _\])) (2) relpos(-1, \[hoc, daY,-\], incl(\[t2, day, _\], \[tref, _, _\])) (3) incl(\[tloc, day, _\], \[n, _, -\]) (4) incl(\[tloc, month, _\], \[n, _, _\]) (5) incl(\[tloc, month, _\], \[tref, -, -\]) (6) \[tloc,month,April\] (7) \[01,-,-\] (8) inclin(\[tloc,moment-of-day, morning\], inclin(\[t2,season,summer\],\[t3,year,1995\])) (9) inclin(\[ tloc,half-hour,1\],occurrence(ol ) ) (10) duration(3,day) (11) end(\[tloc,_,_\],inclin(\[t2,day, wednesday\], incl(\[t3,week,_\],\[n,_,_\]))) (12) begin(\[tloc,_,_\],relpos(1,\[t2,month,_\], incl (\[ t 3,month,-\],\[ tref,-,-\]) ) ) (13) begin(\[tloc,_,_\],inclin(\[t2,day, lO\],\[t3,month,may\])) (14) relpos(3,\[tloc,day,_\],occurrence(ol )) (15) after(\[tloc,day,_\],\[tref,_,_\],duration(3,day)) (16) after(\[tloc,_,_\],\[tref,_,_\],indefinite) (17) extent(\[tloc,_,_\], inclin(\[t2,day,3\],\[t3,month,april\]), inclin(\[t4,day, lO\],\[ts,month,may\])) (18) extent(\[tloc,_,_\], before(\[t2,-,-\],\[tre f,-,-\],duration(3,day)), \[tre/,-,-l) hier (yesterday) la veille (the day before) aujourd'hui (today) ce mois-ci (this month) ce mois-la (that month) en avril (in April) quand Robert est parti (when Robert left) le matin du 3 avril 1995 (the morning of April 3rd 1995) la premi6re demi-heure de l'6mission (the first half hour of the program) durant trois jours (during three days) jusqu'a mercredi de cette semaine (until Wednesday of this week) a partir du mois suivant (from the following month) depuis le 10 mai (sinceMay lOth) trois jours apr6s le d6part de Robert (three days after Robert's departure) trois jours plus tard (three days later) puis (then) du 3 avril au 10 mai (from April 3rd to May lOth) depuis trois jours (since three days) Unfortunately, the combination of localizers in the semantics does not always correspond to the combination of adverbials.
For example, if there were such a correspondence, the adverbial in (1) would be something like lejour avant lejour qui contient l'instant d'~nonciation 'the day before the day that contains the time of speech'.
Instead, we get the simple adverbial hier 'yesterday'.
For complex semantic expressions, in all examples except (15), (16) and (18), there is one embedded adverbial corresponding to each anchor.
For example, the anchor relpos(1,\[t2,month,_\], incl(\[tB,month,_\], \[tref,-,-\])) in (12) corresponds to le mois suivant 'the following month' in the adverbial.
5 Examples
(1-5) are special, since the relation and the anchor are combined in the same syntactic structure.
In (15), direct translation of the anchor into an adverbial would produce trois jours apr~s ce mornent-l?l 'three days after that moment', and in (18), pendant trois jours jusqu'?~ ce moment-l?l 'during three days until that moment'.
Since there is not always a direct correspondence between semantic and syntactic 5 Note that in the adverbial, du is a contraction of de le.
110 Gagnon and Lapalme From Conceptual Time to Linguistic Time QTY" ANCHORS I unique double ( Figure 10 Section of Pr4texte's grammar for adverbials.
ANCHOR ANCHORI ANCIIOR2 deictic ~ anaphoric autonomous ~ deictic anaphoric autonomous i deictic anaphoric autonomous forms, which one should be used in the grammar to distinguish among ATLs?
We have chosen the semantic form because adverbials are distinguished not only by the number of anchors but also by their nature.
Examples (4) and (5) are both syntactically simple---the anchor in the semantic form is not expressed--but it is the anchor that explains their difference: the first uses speech time, whereas the second uses reference time.
Figure 10 illustrates the part of the network taking into account the combination property.
We first identify the number of anchors.
If there is only one, the feature unique is selected in the system QTY_ANCHORS.
Otherwise, double is selected.
Then, for each anchor, we must establish if it is deictic, anaphoric, or autonomous.
If the localization represented by the anchor is made in relation to the time of speech, deictic is selected; if the localization is made in relation to the reference time, anaphoric is selected; if the anchor achieves a localization without using either of the two temporal markers, autonomous is selected.
In Table 4, we indicate the features selected in the systems of Figure 10, for the production of the adverbials given in Table 3.
Some adverbials may be distinguished using the systems of Figure 10, but Table 4 shows that these systems are not enough.
The features have a strong influence on the most embedded adverbials.
For example, in (11), the selection of deictic results in cette semaine 'this week' for the most embedded adverbial but if the feature anaphoric had been selected, it would have produced cette semaine-lit 'that week'.
These features alone do not explain the recursive form of adverbials.
Figure 11 shows the structure of two adverbials from Table 3.
The structure of adverbial (11) is given in (a).
It has three levels, each one corresponding to one adverbial.
The simple structure of adverbial (4) is shown in (b).
Their difference is not only due to the number of levels in the structure.
In (a), the structure contains a function, the Positioner, that expresses the relation to the anchor; there is no function in (b).
Sometimes an anchor is not realized syntactically at all.
In Figure 12, we consider (13) and (15): the anchor is expressed in (13), shown in (a), but not in (15), shown in (b).
The structure of (15) contains the Positioner and a function conveying the temporal distance to an implicit anchor.
These examples show that features are not sufficient; to determine the syntactic structure of the adverbial, we need more systems in our grammar, such as the network of Figure 7.
The two networks of Figure 7 and Figure 10 must be traversed in parallel.
The system ZONE_DESIGNATION first distinguishes between adverbials that ex111 Computational Linguistics Volume 22, Number 1 Table 4 Distinction of adverbials using the anchor.
(The numbers correspond to the adverbial's position in Table 3).
Adverbial QTY_ANCHORS ANCHOR ANCHOR1 ANCHOR2 (1) hier (3) aujourd'hui (4) ce mois-ci (11) jusqu'h mercredi de cette semaine (2) la veille (5) ce mois-lh (12) a partir du mois suivant (15) trois jours plus tard (16) puis (6) en avril (7) quand Robert est parti (8) le matin du 3 avril 1995 (9) la premi6re demi-heure de l'6mission (10) durant trois jours (13) depuis le 10 rnai (14) trois jours apr6s le d6part de Robert (17) du 3 avril au 10 mai (18) depuis trois jours unique umque unique umque umque unique unique unique umque umque unique unique umque unique umque umque double double deictic deictic deictic deictic anaphoric anaphonc anaphoric anaphoric anaphoric autonomous autonomous autonomous autonomous autonomous autonomous autonomous autonomous anaphonc autonomous anaphoric Positioner \[ Jusque Until Reference Zon~ \ \[Zone Designator\[ Attributor Reference Zon~ h mercredi de \[,-Zone Designator\] Wednesday of / NNN, N cette semaine this week (a) (b) Figure 11 Difference of structure for (11) and (4).
Zone Designator\] /\ ce mois-ci this month press localization directly, as in (1-9), and adverbials that relate it to other localizers.
Selection of direct includes a function Zone Designator in the structure for the most embedded adverbial of (a) and the adverbial of (b) in Figure 11.
This function is realized by a phrase expressing the temporal location, which may be a temporal constant (feature chronological) or an occurrence (feature occurrential).
If relational is selected in ZONE_DESIGNATION, a function Positioner is inserted in the structure.
This function is realized by a phrase that expresses the relation of the localizer to its anchor.
There are two types of relational localizers: those that express a 112 Gagnon and Lapalme From Conceptual Time to Linguistic Time Positioner \[ RefercnceZonc( /\ depuis le 10 mai Since May lOth (a) Figure 12 Difference of structure for (13) and (15).
I T m ora, sta eol pvs,tionor I trois jours plus tard three days later (b) Table 5 Distinction of adverbials with type of designation.
(The numbers correspond to Table 3).
Adverbial ZONE DESIGNATION TYPE LOC ZONE LOCT ASPECT (1) hier direct chronological -(2) la veille direct chronological -(3) aujourd'hui direct chronological -(4) ce mois-ci direct chronological -(5) ce mois-la direct chronological -(6) en avril direct chronological -(8) le matin du 3 avril 1995 direct chronological -(9) la premi6re demi-heure de l'6mission direct chronological -(7) quand Robert est parti direct occurrential -(10) durant trois jours relational -durative (11) jusqu'~ mercredi de cette semaine relational -durative (12) a partir du mois suivant relational -durative (13) depuis le 10 mai relational -durative (17) du 3 avril au 10 mai relational -durative (18) depuis trois jours relational -durative (14) trois jours apr6s le d6part de Robert relational -punctual (15) trois jours plus tard relational -punctual (16) puis relational -punctual duration (see (a) in Figure 12) and those that designate a punctual temporal location (see (a) in Figure 11 and (b) in Figure 12).
The classification of adverbials using these distinctions is shown in Table 5.
But even by combining this classification with that of Table 4, we cannot distinguish between all adverbials.
For example, (1, 3, and 4) select the same features in both networks, as do (2 and 5), (6, 8, and 9), and (15 and 16).
For each of the four cases of Table 5, we will show how the adverbials can be distinguished.
5.2 Relational
Localizers 5.2.1 Punctual Localizers.
The function Positioner is always present in adverbials for which the feature punctual has been selected; this is a consequence of the selection of the feature relational.
In addition to Positioner, there can be two more functions.
One is the Temporal Reference Zone, which conveys the localizer to which the relation expressed by the Positioner pertains.
In our list, only adverbial (14) contains this function: le ddpart de Robert 'Robert's departure'.
The other function is Temporal Distance, which expresses the length of time from the localizer used as anchor.
This function occurs in 113 Computational Linguistics Volume 22, Number 1 Positioner l Puis Then Auparavant Before (a) \[ Positioner \] Temporal Distance \[ Dans trois jours In three days from now I1 y a trois jours Three days ago (b) \[ T~Zone\[ Positioner \] Trois jours pllas tard Three days tater Trois jours av ant Three -days earlier (c) \[ Positioner \] Temp Ref Zone \[ a~re~s le d61~art de Robert R o berY s departure avant 8h00 before 8:00 \] Temporal Distance \] Positioner \[ Temp Ref Zone\[ /\ /\ T,~trOiS jours al~r~s !e d6partde Robert ee aays after Kooert's aeparture Trois jours avant le d6part de Robert Three ~ays before Robert's departure (d) (e) Figure 13 Structure for punctual adverbials.
RELATION TYPE V before punctual L_ after f explicit REFERENCE ZONE E implicit 3-TEMtK)RALDISTANCE F definite ~-indefinite Figure 14 Grammar section for relational punctual adverbials.
adverbials (14) and (15): trois jours 'three days'.
Thus, (14) contains both functions and (15) contains only the Temporal Distance.
Adverbial (16) is distinguished from (14) and (15) because it contains neither of these functions.
In dans trois jours 'in three days from now' and apr~s le 9 octobre 'after October 9th', we find two different structures.
The elements of the first structure are exactly the same as the structure of adverbial (15), but they occur in a different order: Temporal Distance comes before the Positioner.
In the second structure, there is a Temporal Reference Zone, le 9 octobre, but no Temporal Distance.
Thus, for punctual adverbials, there are five possible structures, illustrated in Figure 12.
To distinguish between these adverbials, we use a network, part of which is shown in Figure 13.
Two features are expressed by the system RELATION TYPE: before and after.
To realize the Positioner, there is no need to re-enter the grammar, since it may be found directly in the lexicon.
The lexical choice depends not only on the selection achieved 114 Gagnon and Lapalme From Conceptual Time to Linguistic Time Table 6 List of punctual adverbials.
Adverbial RELATION REFERENCE TEMPORAL TYPE ZONE DISTANCE (14) trois jours apr6s le d6part de Robert after explicit definite (three days after Robert's departure) apr6s le 9 octobre (after October 9th) after explicit indefinite (15) trois jours plus tard (three days later) after implicit definite dans trois jours (three days from now) after implicit definite (16) puis (then) after implicit indefinite trois jours avant le d6part de Robert before explicit definite (three days before Robert's departure) avant 8h00 (before 8:00) before explicit indefinite trois jours avant (three days earlier) before implicit definite il y a trois jours(three days ago) before implicit definite auparavant (before) before implicit indefinite in RELATION TYPE, but also in the choice made in the system ANCHOR of figure 10.
For example, in the cases where after is chosen, the Positioner could be lexicalized as puis 'then' or plus tard 'later'; if anaphoric is chosen in ANCHOR it can be lexicalized as dans 'in' if deictic is chosen or apr~s 'after' if autonomous is chosen.
The fact that Temporal Distance and Temporal Reference Zone are optional in the structure is represented in the grammar by two parallel systems: REFERENCE ZONE and TEMPORAL DISTANCE.
If, in REFERENCE ZONE, explicit is chosen, the function Temporal Reference Zone is included in the structure.
Since this function represents another localizer, the anchor, it is realized by re-entering the grammar, taking as input the semantic representation of this anchor.
In TEMPORAL DISTANCE, the selection of definite results in the inclusion of the function Temporal Distance.
To realize it, the grammar must be re-entered, and some features must be preselected so that it is realized as a noun phrase.
Table 6 lists all possible adverbials represented by the network of Figure 13 together with their selected features.
The three adverbials taken from Table 3 are preceded by their reference number to ease the comparison of their semantics with the selected features; their relations will be discussed later.
The distinction between structures (b) and (c) in Figure 12 is not explained by the grammar section shown in Figure 13, since the same features are selected for trois jours plus tard and dans trois jours.
However, structure (b) in Figure 12 is found only for deictic localizers.
Therefore, features for deictic localizers selected in the system ANCHOR, will distinguish structure (b) from structure (c).
Let us now see how the features are selected for the production of relational punctual localizers.
In RELATION TYPE, the feature reflects the relation used in the semantics, if this relation is before or after.
Adverbial (14) deserves some explanation.
Since its semantic expression uses the relation relpos, we would expect its syntactic realization to be: le troisi~me jour apr~s le jour du d6part de Robert 'the third day after the day of Robert's departure', but this usage is rare.
Instead we find trois jours apr~s le d6part de Robert 'three days after Robert's departure', which is what we would expect if the semantic expression used the relation after.
This seems to be because if the temporal distance is one unit, a direct localizer is preferred.
So, instead of generating un mois plus tard 'one month later', we produce le mois suivant 'the next month'.
Our intuition 115 Computational Linguistics Volume 22, Number 1 durative DURATION TYPE bound quantified DURATION ANCHOR DURATION PERSPECTWE -anterior posterior double QUANTDURATION E anterior ANCHOR posterior nil Figure 15 Grammar section for relational durative adverbials.
-internal external \[P.~i--, \] D~oo Q~,ty i l I ''ia'= \] Bo.d~, \] ..~e~ ~ays . Jusque Is mercredi de three days Sm.ce for ~rom that moment) A r~oir de le 10 mai may lOth (a) (b) Figure 16 Structure for durative localizers.
Du 3 avril au lOmai From April 3rd to May 10th (c) is that when X is '%ig" we have this equivalence: relpos(X, \[ti, Ti, Ni\], Z) ::~ afler(\[ti, Ti, Ni\], Z, duration(X, Zi) ) and similarly for X negative and the relation before.
More study is needed to determine the threshold at which the two relations become equivalent in the linguistic realization.
We are sure that for X = I or X = -1, they are not equivalent, so in our implementation, we use 2 and -2 as thresholds.
In the system REFERENCE ZONE, the feature implicit is chosen if the anchor is a simple localizer using the reference time or the time of speech, otherwise explicit is chosen.
Feature selection in TEMPORAL DISTANCE depends on whether the third argument in the semantic expression is indefinite or a specified duration.
5.2.2 Durative
Adverbials.
We now show how the durative adverbials of Table 5 can be differentiated.
The part of the network that generates these adverbials is shown in Figure 14.
We give the three kinds of structure identified for these adverbials in Figure 15, and, finally, in Table 7 we list the durative adverbials of Table 5 with their corresponding features according to the systems of Figure 14.
To give a complete illustration of all adverbials generated with the network of Figure 14, we added one adverbial to the list: pendant trois jours 'for three days from now', which is symmetrical to depuis trois jours 'for three days until now'.
In Figure 14, we distinguish two types of durative adverbial phrases: bound, if the duration is expressed by specifying one or two of its boundaries; and quantified, 116 Gagnon and Lapalme From Conceptual Time to Linguistic Time Table 7 List of durative adverbials.
Adverbial DUR.
DUR. DUR.
QUANZ TYPE ANCH.
PERSE DUR.
ANCH. (13) depuis le 10 mai (since May lOth) bound ant.
int. -(12) a partir du rnois suivant bound ant.
ext. -(from next month) (11) jusqu'~ mercredi de cette semaine bound post.
--(until Wednesday of this week) (17) du 3 avril au 10 rnai bound double --(from April 3rd to May lOth) -pendant trois jours quant.
--ant.
(for three days until that moment) (18) depuis trois jours quant.
--post.
(for three days from that moment) (10) durant trois jours (during three days) quant.
--nil if the duration is expressed as a quantity of time units.
In the first case, we get a structure such as (b) or (c) in Figure 15.
In (b) there is only one boundary, and the Positioner indicates which one is used: its lexicalization depends on the feature selected in DURATION ANCHOR.
If anterior is selected, one further distinction is required to lexicalize the Positioner, as represented by the system DURATION PERSPECTIVE.
The Positioner is realized by the phrase ~ partir de if the feature chosen is external, otherwise it is realized as depuis.
The choice depends on the aspect of the occurrence reported: the feature external is chosen if the occurrence is presented as an event, and internal is chosen if the occurrence is presented as a situation.
These two cases are exemplified in the following two sentences: (3) a.
A partir de 1972, il enseigna a l'Universit6 de Montr6al.
'From 1972 on, he taught at Universit6 de Montr6al'.
b. Depuis 1972, il enseignait a l'Universit6 de Montr6al.
'Since 1972 he was teaching at Universit6 de Montr6al'.
In (3a), since the occurrence is presented as an event, the feature external is selected during the determination of the ATL, thus resulting in the form ~ partir de 1972 'from 1972'.
In (3b), the same occurrence is presented as a situation considered from the reference time.
Thus, internal is selected, resulting in the form depuis 1972 'since 1972'.
This is a good example of the interaction of ATLs with the aspect of the occurrence.
In the interpretation of the ATL in (3b), the duration is anchored not only on the year 1972 but also on the reference time included in the occurrence.
But in the semantics of the ATL, which uses the relation begin, as well as ATL (12) in Table 3, there is only one anchor.
Even if the reference time is involved in the understanding of the whole sentence, it is not directly expressed in the semantics of the ATL.
An alternative would be to express the same localization using the relation extent, as in (18) of Table 3.
If the reference time is included in 1982 (the beginning of the duration expressed in (3b) thus being 10 years before), the semantics would be: extent(\[hoc,_,_\],before(\[tl,year, d,\[t2,_,-\],duration(lO,year)), incl(\[ta,year, d,\[tref,_,_\])) 117 Computational Linguistics Volume 22, Number 1 In depuis dix ans 'For 10 years', the meaning of the ATL, which is "since 10 years in the past starting from this moment" requires the use of the reference time.
If, in the system DURATION ANCHOR, the feature double is chosen, we get a structure containing two boundaries, as in (c) in Figure 15.
A boundary, in the structure of a bound localizer, is always realized as a temporal adverbial, by re-entering the grammar.
When the feature quantified is selected, the structure in (a) of Figure 15 is obtained.
To realize the Positioner in this case, another system is required, because the quantity of time that constitutes the duration can be worded in many ways.
We can express the duration of the occurrence without giving any hint about its location in time, as in durant trois jours 'during three days', or we can indicate a duration that begins or ends at some time.
To see how the features are selected in the grammar section of Figure 14, compare the adverbials of Table 7 with their semantics as given in Table 3 (the semantics for pendant trois jours is the same as depuis trois jours, but the relation after is substituted before and the two anchors are reversed).
First, the feature bound in DURATION TYPE is selected if the relation used in the semantic representation is either begin or end, or if it is extent and the two anchors are autonomous.
In DURATION ANCHOR, features corresponding to these three cases are selected.
In DURATION PERSPECTIVE, the selection depends on the aspect of the occurrence reported in the sentence.
In DURATION TYPE, quantified is selected if the semantics uses the relation extent and one anchor is deictic or anaphoric, as in our examples, or if it uses the relation duration.
In the first case, the selection depends on the position of the anaphoric or deictic anchor in the expression.
If the relation duration is used, since there is no anchor, the feature nil is selected in the next system.
5.3 Direct
Localizers We complete our discussion of adverbials by explaining how the direct localizers can be differentiated.
Figure 16 shows the structure of the direct adverbials that constitute the first half of Table 5; there are three possible structures for a direct localizer.
The simplest ones, in (a) and (b), contain only one function, Zone Designator, that expresses the temporal location zone designated by the adverbial.
This function is realized directly by an adverb, in (a), using the lexicon depending on the system ANCHOR of Figure 10.
In (b), the grammar must be re-entered to generate a nominal phrase whose form also depends on the choice in ANCHOR.
In some cases, it is not sufficient to specify a temporal location zone: we must also add what we call a Pointer to relate the occurrence with this zone.
In our examples, the Pointer indicates that the occurrence takes place during the month of April, or when Robert left.
The existence of such a localizer in the structure seems to depend on the level of the adverbial in the embedding structure.
For example, we find a Pointer in the adverbial ~ ShOO 'at 8:00' if it is used alone, but not if it is embedded in another adverbial, like depuis 8hO0 'since 8:00'.
Our approach to this problem may be contrasted with Forster's (1989), who determines the realization of the Pointer by the temporal aspect of the Zone Designator (durative or punctual) using a constraint propagation technique.
Other possible structures for direct localizers are illustrated in (d) and (e) in Figure 16.
One function is the Zone Designator, which designates the direct expression of temporal zone.
If this zone is included in another localizer or in a position relative to another localizer, we must include another function in the structure: the Reference Zone, which corresponds to this second localizer.
The Attributor links these two functions.
In (e), le matin du 3 avril 1995 'the morning of April 3rd 1995' directly expresses 118 Gagnon and Lapalme From Conceptual Time to Linguistic Time Izo=~i~o~l Iz/;n~sign~,rl \[Po~i~ tznem~'a~"q /X /X Aujourd'hui Ce mois-ci En avril Today This month In April l-Iier Ce mois-l~t Quand Robert est parti Yesterday That month When Robert lift La veille The day before (a) Co) (c) I ZoneDesignat~rAttributor \] ReferenceZonb \[Zone Designatot de I Zne DesignatOr l /\ La premi&e demi-heure 1'6mission The first half-hour the program (d) I ZneDesignat1 Attributor Referen.eZorle / Zone Designatoq Auributor I Kelerence z.o~e /\ I d~ | \[ ZoneDestgnator I Attn0utor li~c~crencez-on~ \[ Zone Designator I of I the3rd I avril ~ t ator I ! Le matin I April ~ I j//N'N I The morning ! I 1995 I I=_-I I ............
=_----:-'---(e) Figure 17 Structure for direct localizers.
a morning.
This morning is itself part of another localizer, and so on.
The Attributor is sometimes lexicalized as an empty item.
To these direct localizers, we must add the embedded direct adverbials found in the relational adverbials of table 5.
In Figure 17, we use by dashed-line boxes to indicate those that differ from Figure 16.
The system shown in Figure 18 differentiates among these different forms of direct adverbials.
For occurrential adverbials, for example, the second adverbial of (b) in Figure 16 and the embedded adverbials in (d) in Figure 16 and (b) in Figure 17, the occurrence may be nominalized or not.
We do not have any satisfying answers to the question of how to choose between these two possibilities.
We will state only that when the adverbial is embedded, a nominalized form may be preferred to another embedded adverbial.
For chronological adverbials, the system AUTONOMOUS ZONE distinguishes between those that have an anaphoric or deictic temporal location zone, and those for which the temporal location zone is autonomous.
Adverbials of the first type always have a simple structure: aujourd'hui, hier, demain, ce mois-ci, ce mois-lh, cette semaine, le mois suivant.
The temporal location zone is different from the anchor.
In mercredi de cette semaine "Wednesday of this week', the temporal location zone, expressed by mercredi, is autonomous whereas the anchor expressed by cette semaine is deictic.
When the network is traversed the first time, yes is selected in the system AUTONOMOUS ZONE.
Its only in the second traversal, when cette semaine is generated, that no is selected in this system.
We must further distinguish adverbials with an autonomous temporal location zone, by deciding if their structure contains a Reference Zone or not.
The feature 119 Computational Linguistics Volume 22, Number 1 I Bouna y I Jusque IZoneDesignatorl Attributor I Referencezon4 Until / \[ Pointer \[Zone Designator I /\ a mercredi Wednesday de of ! r ..... .T ......
V \[Zone Designator \] /\ cette semaine this week ........... i ..................................
! (a) \[ Temporal Distance I Positioner \] Temp Ref Zone I /\ Trois jours Three days apr~s after \[Zone Designator \[ /\ le d6part de Robert Robert's departure I Positionerl Boundary \] A partir de \[Zone Designator \[ Fro,.
/~ Depuis From le mois prochain the next month le 10 mai May lOth ...........
_1 (b) (c) Figure 18 Structure for direct localizers.
TYPE_LOC_ZONE AUTONOMOUS no I chronological ZONE E yes hNCLUD1NG TIME implicit E explicit nominalised occurrential NOMINAL not nominalized Figure 19 Grammar section for direct adverbials.
implicit, implying the non-existence of Reference Zone, is selected in the system INCLUDING TIME if the semantic form is a single triplet \[ti Type, Naming\].
There is yet another system that decides if there is a Pointer or not, but as the problem of the existence of the Pointer is not completely solved and not really important to our discussion, we do not consider this system here.
120 Gagnon and Lapalme From Conceptual Time to Linguistic Time Table 8 List of durative adverbials.
Adverbial AUTONOMOUS INCLUDING ZONE TIME aujourd'hui (today) no hier (yesterday) no demain (tomorrow) no ce mois-ci (this month) no ce mois-l& (that month) no le mois suivant (the following month) no en avril (in April) yes mercredi de cette semaine (Wednesday of this week) yes cette semaine (this week) no la premi6re demi-heure de l'6mission yes (the first half-hour of the program) le matin du 3 avril 1995 (the morning of April 3rd 1995) yes le 3 avril 1995 yes avril 1995 yes 1995 yes implicit explicit explicit explicit explicit explicit implicit In Table 8, we present the features selected for direct chronological adverbials, embedded or not.
The systems in Figure 19 do not suffice to distinguish all direct adverbials.
The selections in these systems must be combined with the selections made in the systems of Figure 10.
5.4 Related
Work on the Generation of ATLs The problem of temporal localization has already been studied by many researchers, but most of them have focused on the aspectual interaction of the adverbials with verb tense; the problem of the semantic and syntactic structures of ATLs has been neglected.
Molin6s' (1990) study from a linguistic perspective characterizes the adverbials based on noun phrases.
Our work extends hers because our computational perspective has made us go farther in the formalization.
Bras and Molin6s (1993) made a similar attempt, but from the perspective of discourse understanding.
Since the problems of understanding are very different from the problems of generation, we could not simply use their method in a "reversed mode".
Their method relies on a compositional analysis of the language, where all information units extracted from the semantic structure are combined to select one meaning for the adverbial.
This compositional approach is not easily reversible, and it does not provide any insight into the selection problem inherent to the generation task.
Ehrich (1987) classifies adverbials in the context of generation, but she does not cover all the cases presented in this section.
Concerning the problem of the generation of ATLs, Maybury (1991) shows how the notion of focus as used by McKeown (1985) can be extended to include a temporal focus that corresponds essentially to the reference point in the Reichenbach model (1947).
An operation on the temporal focus, in combination with the value of speech and event times, selects the temporal adverbial and the verb tense.
Since the emphasis in this work was on the planning aspect of the task, the variety of adverbials that can be generated is limited.
Forster (1989) explains how the syntactic structure of a temporal adverbial may be controlled by semantic information such as the durative or punctual nature of the localizer.
Essential134 the final structure is obtained by propagating constraints associated with each syntactic subpart of the structure.
In particular, he focuses on the interaction between prepositional phrases and noun phrases.
For example, the preposition on is 121 Computational Linguistics Volume 22, Number 1 selected in on Sunday because Sunday is identified as a punctual localizer; this rules out in, which implies a durative localizer.
We have already presented one problem with this approach: it is not clear how the choice of these prepositions can be achieved by propagating semantic constraints.
The choice of preposition in French is very different from English and it often appears to be arbitrary or conventional.
Furthermore, many aspects of the problem are neglected, such as the type of reference expressed by the adverbial: it is not clear how Forster's system can represent the distinctions between anaphoric, deictic and autonomous localizers because the link between the semantic and syntactic levels is not fully explained.
Nigel (Matthiessen and Bateman 1991) offers the widest coverage of English but the variety of forms for ATLs is quite limited.
The temporal localization that may be expressed by different types of syntactic structures is represented in Nigel by systems dispersed throughout the whole grammar network.
For the expression of temporal localizers, their grammar is more dependent on the syntactic structure than ours, which is mainly determined by the semantics.
To summarize, our approach departs from previous approaches by covering more types of adverbials, by proposing a semantics for localization, and by explaining in detail how the different syntactic structures may be obtained from this semantics.
6. The Production of Verb Phrases In our work, we have focused on the generation of adverbials because we felt this problem had not received enough attention and because the temporal localization achieved by ATLs is more complex and more diversified than that expressed by verb tenses.
To generate a discourse like Discourse 1, however, we cannot avoid the problem of determining the structure of the verb phrase, because part of the localization is achieved by the verb, and because of the relations between verbs and adverbials.
In our implemention of the expression of temporal localization, the relation between the verb and the adverbial is taken into account mainly in the deep generation process.
In the semantic representation, we find traces of this interaction.
By keeping these decisions in the deep generation process, the verb phrase and the ATL can be generated independently.
Our method for generating the verb phrase takes advantage of the kind of information directly represented in DRT: the relation of the occurrence to speech time, which we call the primary localization, the aspect of the occurrence, and the presence or absence of a perspective point.
It is implemented by the grammar section illustrated in Figure 19.
In Prdtexte, the production of verb phrases requires many traversals of the network.
First, when the structure of the sentence is determined, choices are made regarding localization, aspect, and perspective.
After a first traversal of the network, the sentence's structure contains a function called Predicate, realized as a verb phrase.
The grammar must be re-entered to realize the Predicate.
The systems visited during this second traversal (not shown here) classify verb tenses in French.
Most of the selections during this second traversal were preselected during the first traversal.
For each verb tense, there is one associated structure, which contains a main verb and one or two auxiliaries.
To generate each verb or auxiliary, another traversal is needed.
In the first system of Figure 19, PRIMARY LOC, the selection depends on the temporal relation between the localized occurrence and the speech time.
The features of the systems ASPECT and SIT TYPE reflect the value of aspect in the semantic representation.
If the aspect is event, the system PERSPECTIVE determines if this event is presented using a perspective.
If there is one, another choice must be made regarding its type.
122 Gagnon and Lapalme From Conceptual Time to Linguistic Time,---past /I'~YL~ ~ present situation ASI~CT event ~RS~CTIVE E resultirlg open I.
Figure 20 Generation of verb phrase--sentence level.
perspective no perspective PERSPECTIVfi a n teri ority TYPE posteriority Table 9 Production of VP--examples.
Selections during first traversal Tense selected during Example second traversal past situation resulting past situation open past event perspective anteriority past event perspective posteriority past event no perspective plus-que-parfait pass4 ant6rieur imparfait imparfait conditionnel plus-que-parfait pass6 compos6 A 8h00, il avait terrain4.
(At 8:00, he had finished).
Une fois qu'il eut termin4 (Once he had finished) A 8h00, Robert regardait la t616vision.
(At 8:00, Robert was watching television).
J'ai rencontr4 Robert jeudi dernier.
I1 partait le lendemain.
(I met Robert last Thursday.
He was going to leave the day after).
J'ai rencontr6 Robert jeudi dernier.
I1 m'a dit qu'il partirait le lendemain.
(I met Robert last Thursday.
He told me that he would leave the next day).
J'ai rencontr6 Robert jeudi dernier.
I1 4tait arriv4 la veille.
(I met Robert last Thursday.
He had arrived the day before).
Robert a parl4 ~ Marie.
(Robert talked to Marie).
Table 9 shows examples of verb phrases, including the list of features selected for each example during the two traversals.
The same tense can be used for different feature patterns.
This is the case with the imparfait and the plus-que-parfait: the imparfait expresses an open situation or an anterior perspective, while the plus-que-parfait presents a resulting situation or a posterior perspective.
This may be a problem in an understanding process, since it is a source of ambiguity, but not in a generation process since it does not matter if two different inputs map into the same syntactic structure.
More than one verb tense may be used for the same features.
This means that our grammar is not complete: more systems would be needed to distinguish among these different cases.
For example, to distinguish the two tenses used with the first feature pattern of Table 9, we would have to augment the grammar section of Figure 19 123 Computational Linguistics Volume 22, Number 1 to determine if the verb phrase is part of a temporal adverbial or not.
For the two cases in the third feature pattern, the difference relates to the use of indirect discourse.
Here, not only would the grammar have to be modified, but so would the semantic representation, to take into account indirect speech.
In Discourse 1, this problem is not apparent: all verb tenses used are distinguished in our grammar because we limited ourselves to a subset of the data.
Thus, the production of VP is more complex than what we have implemented and we have not completely identified all the rules for the selection of verb tense: indirect discourse is not implemented and we have not identified how modal information can be used to select forms such as the subjunctive and the conditional.
But our approach is a good start and it could be extended by adding more systems and their selection rules, without changing the overall structure of the network.
We can see from the approximate translations given in Table 9 that the systems for generating French and English verb tenses differ greatly.
For English verb tenses, the method implemented in Nigel resorts to a recursive semantics involving temporal markers, as proposed by Halliday (Matthiessen and Bateman 1991).
The purpose in this approach is to deal correctly with complex structures, such as will have been eating.
Put simply, the idea is that each auxiliary reflects a relation between two temporal markers.
This suggests a network that displays a recursive process.
Thus, the phrase will have been going to eat would be represented semantically as to eat at a time that is in the future relative to another time that is in the past relative to a time that is in the future relative to speech time.
This method may be adequate for English, since it seems to capture the recursive structure of verb tenses; but in French, this recursive structure is not found.
Furthermore, nothing is said about how a deep generation process could produce the corresponding semantic structure with the intermediate temporal markers.
In fact, we are not convinced that this could be easily done.
Rather, we think that it is the overall structure that is selected for a particular usage.
This completes our brief description of the generation of verb tenses.
We have not completely solved the problem.
In particular, we have chosen to put most of the problems pertaining to verb tense in the deep generation process, in order to facilitate their generation at the surface level.
This approach greatly simplifies the process and our grammar could be easily completed to encompass all cases.
Once the semantic demands are better understood, it should be easier to solve the problem of deep generation.
7. Conclusion and Future Work In this paper, we have presented a method that has been successfully used to produce text conveying temporal information.
Our method combines the principles of two theories: Kamp's Discourse Representation Theory, which guides the expression of temporal information, and Halliday's Systemic Functional Grammar, which provides a generation process controlled by a set of semantic choices, with the syntactic form resulting from these choices.
We argued for the use of a conceptual structure, a Discourse Representation Structure, combined with rhetorical principles and pragmatic information, and for its translation into a semantic structure that is easily realized syntactically.
The deep generation process is hard to implement, mainly because of the difficulty in formalizing this information.
Since we assume that the deep generation 124 References 1 James F.
Allen, Maintaining knowledge about temporal intervals, Communications of the ACM, v.26 n.11, p.832-843, Nov.
1983 2 Asher, Nicholas.
(1993). Reference to Abstract Objects in Discourse.
Kluwer Academics, Dordrecht.
3 Bach, Emmon.
(1986). The Algebra of Events.
Linguistics and Philosophy, 9(1): 5--16.
4 Berry, M.
(1975). An Introduction to Systemic Linguistics, vol.
1 Structures
and Systems.
St. Martin Press, New York.
5 Orna
Berry, Mathias Hein, Ellon Littwitz, Introduction to Ethernet Switching, International Thomson Computer Press, Boston, MA, 1995 6 Bras, M.
(1990). Calcul des structures temporelles du discours.
Thse de doctorat, Universit Paul-Sabatier.
7 Bras, Myriam and Nicholas Asher.
(1994). Le raisonnement non monotone dans la construction de la structure temporelle de textes en franais.
In 9me congrs AFCET-RFIA.
Paris. 8 Bras, Myriam and Molins, Frdrique.
(1993). "Adverbials of Temporal Location: Linguistic Description and Automatic Processing".
In Sprache Kommunikation Informatik.
Linguistiche Arbeiten 293, edited by J.
Darski and Z.
Vetulani. Max Niemeyer Verlag, Tubingen.
9 Davidson, D.
(1967). "The Logical Form of Action Sentences".
In Essays on Action and Events, edited by D.
Davidson. Clarendon Press.
10 Dowty, D.
(1979). Word Meaning and Montague Grammar.
Reidel, Dordrecht.
11 Dowty, D.
(1982). Tenses, Time Adverbs, and Compositional Semantic Theory.
Linguistics and Philosophy, 5(1): 23--55.
12 Dowty, D.
(1986). The Effects of Aspectual Class on the Temporal Structure of Discourse: Semantics or Pragramatics?
Linguistics and Philosophy, 9: 37--61.
13 Ehrich, Veronika.
(1987). "The generation of tense".
In Natural Language Generation: New Results in Artificial Intelligence, Psychology and Linguistics, edited by Gerard Kempen.
Martinus Nijhoff Publishers, Boston, Dordrecht, 423--440.
14 Fawcett, R.
(1988). "Language Generation as Choice in Social Interaction".
In Advances in Natural Language Generation---An Interdisciplinary Perspective, edited by Michael Zock and Gerard Sabah.
Pinter Publishers, London, 27--48.
15 Forster, D.
(1989). Generating Temporal Expressions in Natural Language.
In Proceedings, 11th Annual Conference of the Cognitive Science Society.
16 Gagnon, M., and Lapalme, G.
(1992). Un gnrateur de texte exprimant des concepts temporels.
Technique et science informatiques, 11(2): 25--44.
17 Gagnon, Michel and Bras, Myriam.
(1994). "Discourse Interpretation and Time Representation".
Technical report 94/54-r, IRIT.
18 Halliday, M.
(1985). An Introduction to Functional Grammar.
Edward Arnold, London.
19 Hovy, Eduard H.
(1991). "Approaches to Planning of Coherent Text".
In Natural Language Generation in Artificial Intelligence and Computational Linguistics, edited by W.
Swartout and W.
Mann. Kluwer Academics Publishers, Boston, 83--102.
20 Kamp, H.
(1979). "Events, Instants and Temporal Reference".
In Semantics from different points of view, edited by R.
Bauerle, U.
Egli, and A.
von Stechow.
Springer Verlag, Berlin, 376--417.
21 Kamp, H.
(1981). Evnements, reprsentations discursives et rfrence temporelle.
Langages, 64: 34--64.
22 Lascarides, Alex, and Asher, Nicholas.
(1993). Temporal Interpretation, Discourse Relations and Commonsense Entailment.
Linguistics and Philosophy, 16: 437--493.
23 Mann, W.
(1983). "An Overview of the Nigel Generation Grammar".
Technical report ISI-RR-83-113, USC/ISI.
24 Mann, W.
(1985). An Introduction to the Nigel Text Generation System.
In Systemic Perspective on Discourse, vol.
1, edited by James D.
Benson and William S.
Greaves. Ablex, Norwood, 84--95.
25 Mann, W., and Thompson, S.
(1987). "Rhetorical Structure Theory: Description and Construction of Text Structure".
In Natural Language Generation, edited by Gerard Kempen.
Martinus Nijhoff, Dordrecht, 85--95.
26 William C.
Mann, Discourse structures for text generation, Proceedings of the 22nd annual meeting on Association for Computational Linguistics, p.367-375, July 02-06, 1984, Stanford, California 27 Matthiessen, C.
(1985). The Systemic Framework in Text Generation: Nigel.
In Systemic Perspective on Discourse, vol.
1, edited by James D.
Benson and William S.
Greaves. Ablex, Norwood, 96--118.
28 Matthiessen, C., and Bateman, J.
(1991). Text Generation and Systemic-Functional Linguistics.
Pinter, London.
29 Maybury, Mark T.
(1991). Topical, Temporal, and Spatial Constraints on Linguistic Realization.
Computational intelligence, 7(4): 266--275.
30 Kathleen R.
McKeown, Discourse strategies for generating natural-language text, Artificial Intelligence, v.27 n.1, p.1-41, Sept.
1985 31 Molins, F.
(1990). Acceptabilit et interprtation des adverbiaux de localisation temporelle.
Mmorie de D.E.A., Universit de Toulouse---Le Mirail.
32 Reichenbach, H.
(1947). Elements of Symbolic Logic.
McMillan, New York.
33 Verkuyl, H.
J. (1989).
Aspectual Classes and Aspectual Composition.
Linguistics and Philosophy, 12: 39--94.
34 Vlach, Frank.
(1993). Temporal Adverbials, Tenses and the Perfect.
Linguistics and Philosophy, 16: 231--283.
35 Winograd, T.
(1983). Language as a Cognitive Process.
Addison Wesley .
Finite-State Transducers in Language and Speech Processing Mehryar Mohri* AT&T Labs-Research Finite-state machines have been used in various domains of natural language processing.
We consider here the use of a type of transducer that supports very efficient programs: sequential transducers.
We recall classical theorems and give new ones characterizing sequential string-tostring transducers.
Transducers that output weights also play an important role in language and speech processing.
We give a specific study of string-to-weight transducers, including algorithms for determinizing and minimizing these transducers very efficiently, and characterizations of the transducers admitting determinization and the corresponding algorithms.
Some applications of these algorithms in speech recognition are described and illustrated.
1. Introduction Finite-state machines have been used in many areas of computational linguistics.
Their use can be justified by both linguistic and computational arguments.
Linguistically, finite automata are convenient since they allow one to describe easily most of the relevant local phenomena encountered in the empirical study of language.
They often lead to a compact representation of lexical rules, or idioms and cliches, that appears natural to linguists (Gross 1989).
Graphic tools also allow one to visualize and modify automata, which helps in correcting and completing a grammar.
Other more general phenomena, such as parsing context-free grammars, can also be dealt with using finitestate machines such as RTN's (Woods 1970).
Moreover, the underlying mechanisms in most of the methods used in parsing are related to automata.
From the computational point of view, the use of finite-state machines is mainly motivated by considerations of time and space efficiency.
Time efficiency is usually achieved using deterministic automata.
The output of deterministic machines depends, in general linearly, only on the input size and can therefore be considered optimal from this point of view.
Space efficiency is achieved with classical minimization algorithms (Aho, Hopcroft, and Ullman 1974) for deterministic automata.
Applications such as compiler construction have shown deterministic finite automata to be very efficient in practice (Aho, Sethi, and Ullman 1986).
Finite automata now also constitute a rich chapter of theoretical computer science (Perrin 1990).
Their recent applications in natural language processing, which range from the construction of lexical analyzers (Silverztein 1993) and the compilation of morphological and phonological rules (Kaplan and Kay 1994; Karttunen, Kaplan and Zaenen 1992) to speech processing (Mohri, Pereira, and Riley 1996) show the usefulness of finite-state machines in many areas.
In this paper, we provide theoretical and algorithmic bases for the use and application of the devices that support very efficient programs: sequential transducers.
* 600 Mountain Avenue, Murray Hill, NJ 07974, USA.
(~) 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 2 We extend the idea of deterministic automata to transducers with deterministic input, that is, machines that produce output strings or weights in addition to (deterministically) accepting input.
Thus, we describe methods consistent with the initial reasons for using finite-state machines, in particular the time efficiency of deterministic machines, and the space efficiency achievable with new minimization algorithms for sequential transducers.
Both time and space concerns are important when dealing with language.
Indeed, one of the recent trends in language studies is a large increase in the size of data sets.
Lexical approaches have been shown to be the most appropriate in many areas of computational linguistics ranging from large-scale dictionaries in morphology to large lexical grammars in syntax.
The effect of the size increase on time and space efficiency is probably the main computational problem of language processing.
The use of finite-state machines in natural language processing is certainly not new.
The limitations of the corresponding techniques, however, are pointed out more often than their advantages, probably because recent work in this field is not yet described in computer science textbooks.
Sequential finite-state transducers are now used in all areas of computational linguistics.
In the following sections, we give an extended description of these devices.
We first consider string-to-string transducers, which have been successfully used in the representation of large-scale dictionaries, computational morphology, and local grammars and syntax, and describe the theoretical bases for their use.
In particular, we recall classical theorems and provide some new ones characterizing these transducers.
We then consider the case of sequential string-to-weight transducers.
Language models, phone lattices, and word lattices are among the objects that can be represented by these transducers, making them very interesting from the point of view of speech processing.
We give new theorems extending the known characterizations of stringto-string transducers to these transducers.
We define an algorithm for determinizing string-to-weight transducers, characterize the unambiguous transducers admitting determinization, and describe an algorithm to test determinizability.
We also give an algorithm to minimize sequential transducers that has a complexity equivalent to that of classical automata minimization and that is very efficient in practice.
Under certain restrictions, the minimization of sequential string-to-weight transducers can also be performed using the determinization algorithm.
We describe the corresponding algorithm and give the proof of its correctness in the appendix.
We have used most of these algorithms in speech processing.
In the last section, we describe some applications of determinization and minimization of string-to-weight transducers in speech recognition, illustrating them with several results that show them to be very efficient.
Our implementation of the determinization is such that it can be used on the fly: only the necessary part of the transducer needs to be expanded.
This plays an important role in the space and time efficiency of speech recognition.
The reduction in the size of word lattices that these algorithms provide sheds new light on the complexity of the networks involved in speech processing.
2. Sequential String-to-String Transducers Sequential string-to-string transducers are used in various areas of natural language processing.
Both determinization (Mohri 1994c) and minimization algorithms (Mohri 1994b) have been defined for the class of p-subsequential transducers, which includes sequential string-to-string transducers.
In this section, the theoretical basis of the use of sequential transducers is described.
Classical and new theorems help to indicate the usefulness of these devices as well as their characterization.
270 Mohri Transducers in Language and Speech b:E Figure 1 Example of a sequential transducer.
2.1 Sequential
Transducers We consider here sequential transducers, namel3~ transducers with a deterministic input.
At any state of such transducers, at most one outgoing arc is labeled with a given element of the alphabet.
Figure 1 gives an example of a sequential transducer.
Notice that output labels might be strings, including the empty string ~.
The empty string is not allowed on input, however.
The output of a sequential transducer is not necessarily deterministic.
The one in Figure 1 is not since, for instance, two distinct arcs with output labels b leave the state 0.
Sequential transducers are computationally interesting because their use with a given input does not depend on the size of the transducer but only on the size of the input.
Since using a sequential transducer with a given input consists of following the only path corresponding to the input string and in writing consecutive output labels along this path, the total computational time is linear in the size of the input, if we consider that the cost of copying out each output label does not depend on its length.
Definition More formally, a sequential string-to-string transducer T is a 7-tuple (Q, i, F, G, A, 6, or), with:  Q the set of states,  i E Q the initial state,  F c Q the set of final states,  ~ and A finite sets corresponding respectively to the input and output alphabets of the transducer,  6 the state transition function, which maps Q x G to Q,   the output function, which maps Q x G to A'.
The functions 6 and rr are generally partial functions: a state q c Q does not necessarily admit outgoing transitions labeled on the input side with all elements of the alphabet.
These functions can be extended to mappings from Q x G* by the following classical recurrence relations: Vs E Q, Vw E ~*,Va c G, 6(s,) = s, 6(s, wa) = 6(6(s,w),a); = wa) = 271 Computational Linguistics Volume 23, Number 2 Figure 2 Example of a 2-subsequential transducer T1.
Thus, a string w E ~* is accepted by Tiff 6(i, w) C F, and in that case the output of the transducer is or(i, w).
2.2 Subsequential
and p-Subsequential Transducers Sequential transducers can be generalized by introducing the possibility of generating an additional output string at final states (Sch~itzenberger 1977).
The application of the transducer to a string can then possibly finish with the concatenation of such an output string to the usual output.
Such transducers are called subsequential transducers.
Language processing often requires a more general extension.
Indeed, the ambiguities encountered in language--ambiguity of grammars, of morphological analyzers, or that of pronunciation dictionaries, for instance---cannot be taken into account when using sequential or subsequential transducers.
These devices associate at most a single output to a given input.
In order to deal with ambiguities, one can introduce p-subsequential transducers (Mohri 1994a), namely transducers provided with at most p final output strings at each final state.
Figure 2 gives an example of a 2-subsequential transducer.
H~re, the input string w = aa gives two distinct outputs aaa and aab.
Since one cannot find any reasonable case in language in which the number of ambiguities would be infinite, p-subsequential transducers seem to be sufficient for describing linguistic ambiguities.
However, the number of ambiguities could be very large in some cases.
Notice that 1-subsequential transducers are exactly the subsequential transducers.
Transducers can be considered to represent mappings from strings to strings.
As such, they admit the composition operation defined for mappings, a useful operation that allows the construction of more complex transducers from simpler ones.
The result of the application of T 20 T 1 to a string s can be computed by first considering all output strings associated with the input s in the transducer T1, then applying T2 to all of these strings.
The output strings obtained after this application represent the result (T2 o T1)(S).
In fact, instead of waiting for the result of the application of T1 to be completely given, one can gradually apply T2 to the output strings of ~-1 yet to be completed.
This is the basic idea of the composition algorithm, which allows the transducer T 2 o T 1 to be directly constructed given T1 and 72.
We define sequential (resp.
p-subsequential) functions to be those functions that can be represented by sequential (resp.
p-subsequential) transducers.
We noted previously that the result of the composition of two transducers is a transducer that can be directly constructed.
There exists an efficient algorithm for the general case of the composition of transducers (transducers subsequential or not, having e-transitions or not, and with outputs in ~*, or in ~* t2 {oo} x T4+ U {oo}) (Mohri, Pereira, and Riley 1996).
The following theorem gives a more specific result for the case of subsequential and p-subsequential functions, which expresses their closure under composition.
We use the expression p-subsequential in two ways here.
One means that a finite number of 272 Mohri Transducers in Language and Speech Figure 3 Example of a subsequential transducer v2.
a ambiguities is admitted (the closure under composition matches this case), the second indicates that this number equals exactly p.
Theorem 1 Let f: E* --* A* be a sequential (resp.
p-subsequential) and g: A* -, f~* be a sequential (resp.
q-subsequential) function, then g of is sequential (resp.
pq-subsequential). Proof We prove the theorem in the general case of p-subsequential transducers.
The case of sequential transducers, first proved by Choffrut (1978), can be derived from the general case in a trivial way.
Let 7-1 be a p-subsequential transducer representing f, T1 = (Ql, i1,F1, E,A, 61,a1,pl), and 7"2 = (Q2, i2, F2, A,~,62, cr2, P2) a q-subsequential transducer representing g.
pl and p2 denote the final output functions of T1 and r2, which map F1 to (A*)P and F2 to (f~*)q, respectively, pl(r) represents, for instance, the set of final output strings at a final state r.
Define the pq-subsequential transducer T = (Q,i,F,E,f~,6,a,p) by Q = Q1 x Q2, i = (il, i2), F -{(91,92) E Q: 91 c F1,62(q2,p1(q1))f3 F2 ~ 0}, with the following transition and output functions: Va E E, V(ql, q2) E Q,6((ql, q2),a) = (61(ql,a),62(q2,al(ql,a))) o'((ql,q2),a) = cra(q2, crl(ql,a)) and with the final output function defined by: V(ql,q2) E F, p((ql, q2)) =a2(q2, Pl(ql))p2(6(q2, Pl(ql))) Clearly, according to the definition of composition, the transducer ~realizes g of.
The definition of p shows that it admits at most pq distinct output strings for a given input one.
This ends the proof of the theorem.
\[\] Figure 3 gives an example of a 1-subsequential or subsequential transducer T2.
The result of the composition of the transducers rl and T2 is shown in Figure 4.
States in the transducer 73 correspond to pairs of states of ;1 and "r2.
The composition consists essentially of making the intersection of the outputs of T1 with the inputs of ~'2.
Transducers admit another useful operation: union.
Given an input string w, a transducer union of "0 and T2 gives the set union of the strings obtained by application of 71 to w and r2 to w.
We denote by ~-1 + r2 the union of 7"1 and T2.
The following theorem specifies the type of the transducer 71 + ~'2, implying in particular the closure under union of p-subsequential transducers.
It can be proved in a way similar to the composition theorem.
Theorem 2 Let f: E* ~ A* be a sequential (resp.
p-subsequential) and g: E* --* A* be a sequential (resp.
q-subsequential) function, then g + f is 2-subsequential (resp.
(p + q)subsequential).
273 Computational Linguistics Volume 23, Number 2 Figure 4 2-subsequential transducer r3, obtained by composition of T1 and r2.
The union transducer T1 + ~-2 Can be constructed from rl and r2 in a way close to the union of automata.
One can indeed introduce a new initial state connected to the old initial states of rl and "r2 by transitions labeled with the empty string both on input and output.
But the transducer obtained using this construction is not sequential, since it contains c-transitions on the input side.
There exists, however, an algorithm to construct the union of p-subsequential and q-subsequential transducers directly as a p + q-subsequential transducer.
The direct construction consists of considering pairs of states (ql, q2), ql being a state of ~-1 or an additional state that we denote by an underscore, q2 a state of r2 or an additional state that we denote by an underscore.
The transitions leaving (ql, q2) are obtained by taking the union of the transitions leaving ql and q2, or by keeping only those of ql if q2 is the underscore state, similarly by keeping only those of q2 if ql is the underscore state.
The union of the transitions is performed in such a way that if ql and q2 both have transitions labeled with the same input label a, then only one transition labeled with a is associated to (O, q2)The output label of that transition is the longest common prefix of the output transitions labeled with a leaving ql and q2.
See Mohri (1996b) for a full description of this algorithm.
Figure 5 shows the 2-subsequential transducer obtained by constructing the union of the transducers rl and ;2 this way.
Notice that according to the theorem the result could be a priori 3-subsequential, but these two transducers share no common accepted string.
In such cases, the resulting transducer is max(p, q)-subsequential.
2.3 Characterization
and Extensions The linear complexity of their use makes sequential or p-subsequential transducers both mathematically and computationally of particular interest.
However, not all transducers, even when they realize functions (rational functions), admit an equivalent sequential or subsequential transducer.
Consider, for instance, the function f associated with the classical transducer represented in Figure 6; f can be defined by: 1 Vw c {x} +, f(w) = alwl if Iwl is even, (1) = blwl otherwise This function is not sequential, that is, it cannot be realized by any sequential transducer.
Indeed, in order to start writing the output associated to an input string w = x n, a or b according to whether n is even or odd, one needs to finish reading the whole input string w, which can be arbitrarily long.
Sequential functions, namely functions that 1 We denote by \]w\[ the length of a string w.
274 Mohri Transducers in Language and Speech a:E b:a Figure 5 2-subsequential transducer v4, union of 7-1 and r2.
x:a x:a x:b Figure 6 Transducer T with no equivalent sequential representation.
can be represented by sequential transducers do not allow such unbounded delays.
More generally, sequential functions can be characterized among rational functions by the following theorem: Theorem 3 (Ginsburg and Rose 1966) Letf be a rational function mapping G* to A*.
f is sequential iff there exists a positive integer K such that: Vu 6 G*,Va E G, 3w 6 A*, Iwl < K: f(ua) =f(u)w (2) In other words, for any string u and any element of the alphabet a, f(ua) is equal to f(u) concatenated with some bounded string.
Notice that this implies that flu) is always a prefix of f(ua), and more generally that if f is sequential then it preserves prefixes.
275 Computational Linguistics Volume 23, Number 2 Q x:xl Figure 7 Left-to-right sequential transducer L.
x.x2 xl:a Figure 8 Right-to-left sequential transducer R.
The fact that not all rational functions are sequential could reduce the interest of sequential transducers.
The following theorem, due to Elgot and Mezei (1965), shows, however, that transducers are exactly compositions of left and right sequential transducers.
Theorem 4 (Elgot and Mezei 1965) Letf be a partial function mapping G" to A*.f is rational iff there exists a left sequential function h ~* --* f~* and a right sequential function r: f~* --* A ~ such thatf = r o I.
Left sequential functions or transducers are those we previously defined.
Their application to a string proceeds from left to right.
Right sequential functions apply to strings from right to left.
According to the theorem, considering a new sufficiently large alphabet f~ allows one to define two sequential functions I and r that decompose a rational function f.
This result considerably increases the importance of sequential functions in the theory of finite-state machines as well as in the practical use of transducers.
Berstel (1979) gives a constructive proof of this theorem.
Given a finite-state transducer T, one can easily construct a left sequential transducer L and a right sequential transducer R such that R o L = T.
Intuitively, the extended alphabet f~ keeps track of the local ambiguities encountered when applying the transducer from left to right.
A distinct element of the alphabet is assigned to each of these ambiguities.
The right sequential transducer can be constructed in such a way that these ambiguities can then be resolved from right to left.
Figures 7 and 8 give a decomposition of the nonsequential transducer T of Figure 6.
The symbols of the alphabet f~ = {xl, x2} store information about the size of the input string w.
The output of L ends with xl iff Iwl is odd.
The right sequential function R is then easy to construct.
276 Mohri Transducers in Language and Speech Sequential transducers offer other theoretical advantages.
In particular, while several important tests, such as equivalence, are undecidable with general transducers, sequential transducers have the following decidability property.
Theorem 5 Let T be a transducer mapping G* to A ".
It is decidable whether T is sequential.
A constructive proof of this theorem was given by Choffrut (1978).
An efficient polynomial algorithm for testing the sequentiability of transducers based on this proof was given by Weber and Klemm (1995).
Choffrut also gave a characterization of subsequential functions based on the definition of a metric on G*.
Denote by u/~ v the longest common prefix of two strings u and v in G*.
It is easy to verify that the following defines a metric on G*: d(u,v) = tul + Ivl 21u A v I (3) The following theorem describes this characterization of subsequential functions.
Theorem 6 Letf be a partial function mapping G* to A*.
f is subsequential iff: 1.
f has bounded variation (according to the metric defined above).
2. for any rational subset Y of A*,f-I(Y) is rational.
The notion of bounded variation can be roughly understood here as follows: if d(x,y) is small enough, namely if the prefix that x and y share is sufficiently long compared to their lengths, then the same is true of their images by f, f(x) and f(y).
This theorem can be extended to describe the case of p-subsequential functions by defining a metric d~ on (A*)p.
For any u = (u I.....,Up) and v = (vl .....,Vp) C (A*)P, we define: doo(u, v) = max d(ui, vi) (4) 1Ki~p Theorem 7 Let f = (fl ..... fp) be a partial function mapping Dom(f) c G* to (A*)P.
f is psubsequential iff: 1.
f has bounded variation (using the metric d on G* and d~ on (A*)p).
2. for all i (1 < i < p) and any rational subset Y of A*,f/-I(Y) is rational.
Proof Assume f p-subsequential, and let T be a p-subsequential transducer realizing f.
A transducer Ti, 1 ~ i < p, realizing a component fi of f can be obtained from T simply by keeping only one of the p outputs at each final state of T.
Ti is subsequential by construction, hence the component )~ is subsequential.
Then the previous theorem implies that each component fi has bounded variation, and by definition of d~, f has also bounded variation.
Conversely, if the first condition holds, afortiori each)~ has bounded variation.
This combined with the second condition implies that each)~ is subsequential.
A transducer T realizing f can be obtained by taking the union of p subsequential transducers realizing each componentS.
Thus, in view of the theorem 2,f is p-subsequential.
\[\] 277 Computational Linguistics Volume 23, Number 2 One can also give a characterization of p-subsequential transducers irrespective of the choice of their components.
Let d~ be the semimetric defined by: V(u,v) E \[(A*)P\] 2, d'p(U,V) = max d(ui, vj) l<i,j<p (5) The following theorem that follows then gives that characterization.
Theorem 8 Letf be a rational function mapping E* to (A*)P.
f is p-subsequential iff it has bounded variation (using the semimetric d~ on (A*)P).
Proof According to the previous theorem the condition is sufficient since: V(u,v) c < a'p(u,v) Conversely if f is p-subsequential, let T = (Q, i,F, ~, A, 6,or, p) be a p-subsequential transducer representing f, where p = (pl ..... pp) is the output function mapping Q to (A*)P.
Let N and M be defined by: N= max Ipi(q)l and M= max Icffq, a)l (6) qEF, l <id< p aE~,qEQ We denote by Dom(T) the set of strings accepted by T.
Let k > 0 and (ul, U2) E \[Dora(T)\] 2 such that d(ul, u2) _< k.
Then, there exists u E E* such that: U 1 = UVl, U 2 = UV2, and \[vii + Iv2l G k (7) Hence, f(Ul) = {cr(i,u)rr(~(i,u),vl)pj(6(i, ul)): 1 G j < p} f(u2) = {rr(i,u)cr(6(i,u),v2)Pj(~(i, u2)): 1 < j < p} (8) Let K = kM + 2N.
We have: dp(f(ul),f(u2)) M(ivl\[ + Iv2I) + dlp(p(6(i, ul)),p(~(i, u2))) < kM+2N = K Thus, f has bounded variation using d~.
This ends the proof of the theorem.
\[\] 2.4 Application to Language Processing We briefly mentioned several theoretical and computational properties of sequential and p-subsequential transducers.
These devices are used in many areas of computational linguistics.
In all those areas, the determinization algorithm can be used to obtain a p-subsequential transducer (Mohri 1996b), and the minimization algorithm to reduce the size of the p-subsequential transducer used (Mohri 1994b).
The composition, union, and equivalence algorithms for subsequential transducers are also useful in many applications.
278 Mohri Transducers in Language and Speech 2.4.1 Representation of Dictionaries.
Very large-scale dictionaries can be represented by p-subsequential dictionaries because the number of entries and that of the ambiguities they contain are finite.
The corresponding representation offers fast look-up since the recognition does not depend on the size of the dictionary but only on that of the input string considered.
The minimization algorithm for sequential and p-subsequential transducers allows the size of these devices to be reduced to the minimum.
Experiments have shown that these compact and fast look-up representations for large natural language dictionaries can be efficiently obtained.
As an example, a French morphological dictionary of about 21.2 Mb can be compiled into a p-subsequential transducer of 1.3 Mb, in a few minutes (Mohri 1996b).
2.4.2 Compilation
of Morphological and Phonological Rules.
Similarly, context-dependent phonological and morphological rules can be represented by finite-state transducers (Kaplan and Kay 1994).
Most phonological and morphological rules correspond to p-subsequential functions.
The result of the computation described by Kaplan and Kay (1994) is not necessarily a p-subsequential transducer.
But, it can often be determinized using the determinization algorithm for p-subsequentiable transducers.
This considerably increases the time efficiency of the transducer.
It can be further minimized to reduce its size.
These observations can be extended to the case of weighted rewrite rules (Mohri and Sproat 1996).
2.4.3 Syntax.
Finite-state machines are also currently used to represent local syntactic constraints (Silberztein 1993; Roche 1993; Karlsson et al.1995; Mohri 1994d).
Linguists can conveniently introduce local grammar transducers that can be used to disambiguate sentences.
The number of local grammars for a given language and even for a specific domain can be large.
The local grammar transducers are mostly p-subsequential.
Determinization and minimization can then be used to make the use of local grammar transducers more time efficient and to reduce their size.
Since p-subsequential transducers are closed under composition, the result of the composition of all local grammar transducers is a p-subsequential transducer.
The equivalence of local grammars can also be tested using the equivalence algorithm for sequential transducers.
For a more detailed overview of the applications of sequential string to string transducers to language processing, see Mohri (1996a).
Because they are so time and space efficient, sequential transducers will likely be used increasingly often in natural language processing as well as in other connected fields.
In the following, we consider the case of string-to-weight transducers, which are also used in many areas of computational linguistics.
3. Power Series and Subsequential String-to-Weight Transducers We consider string-to-weight transducers, namely transducers with input strings and output weights.
These transducers are used in various domains, such as language modeling, representation of word or phonetic lattices, etc., in the following way: one reads and follows a path corresponding to a given input string and outputs a number obtained by combining the weights along this path.
In most applications to natural language processing, the weights are simply added along the path, since they are interpreted as (negative) logarithms of probabilities.
In case the transducer is not sequential, that is, when it does not have a deterministic input, one proceeds in the same way for all the paths corresponding to the input string.
In natural language processing, specifically in speech processing, one keeps the minimum of the weights associated to 279 Computational Linguistics "Volume 23, Number 2 Figure 9 Example of a string-to-weight transducer.
these paths.
This corresponds to the Viterbi approximation in speech recognition or in other related areas for which hidden Markov models (HMM's) are used.
In all such applications, one looks for the best path, i.e., the path with the minimum weight.
3.1 Definitions
In this section, we give the definition of string-to-weight transducers and other deftnitions useful for the presentation of the theorems of the following sections.
In addition to the output weights of the transitions, string-to-weight transducers are provided with initial and output weights.
For instance, when used with the input string ab, the transducer in Figure 9 outputs: 5 + 1 + 2 + 3 = 11, 5 being the initial and 3 the final weight.
Definition More formally, a string-to-weight transducer T is defined by T = (Q, ~, I, F, E, A, p) with:  Q a finite set of states,  G the input alphabet,  I C Q the set of initial states,  F C Q the set of final states,  E C Q x G x T4+ x Q a finite set of transitions,  A the initial weight function mapping I to 7Z+,  p the final weight function mapping F to 7"4+.
One can define for T a transition (partial) function 6 mapping Q x E to 2 Q by: V(q,a) E Q x G,~(q,a) = {q' I 3x E 7"4+: (q,a,x,q') E E} and an output function cr mapping E to T4+ by: Vt = (p,a,x,q) E E, cr(t) = x A path ~r in T from q E Q to q' E Q is a set of successive transitions from q to q': 7r = ((qo, ao, xo, ql) ....., (qm-l,am-l, Xm-l, qm)), with Vi E \[0, m1\], qi+l E 6(qi, ai).
We can extend the definition of a to paths by: cr(Tr) -XoXl""Xm-1.
We denote by ~E q w q, the set of paths from q to q' labeled with the input string w.
The definition of 6 can be extended to Q x G* by: V(q,w) E Q x ~*,6(q,w) = {q': 3 path 7r in T, Tr E qW q,} 280 Mohri Transducers in Language and Speech and to 2 Q x E*, by: VR G Q, Vw E ~*, 6(R, w) = U t~(q, w) qER For (q, w, q') E Q x ~ x Q such that there exists a path from q to ql labeled with w, we define O(q, w, q') as the minimum of the outputs of all paths from q to q' with input w: O(q, w,q') = min cr(Tr) w IrEq...~q' A successful path in T is a path from an initial state to a final state.
A string w E E* is accepted by Tiff there exists a successful path labeled with w: w E 6(L w) N F.
The output corresponding to an accepted string w is then obtained by taking the minimum of the outputs of all successful paths with input label w: min (A(i) + O(i, w,f) + p(f)) (i,f)EIxF: fE6(i,w) A transducer T is said to be trim if all states of T belong to a successful path.
String-toweight transducers clearly realize functions mapping ~* to 74+.
Since the operations we need to consider are addition and min, and since (74+ U {oo}, min, +, cxD, 0) is a semiring, we call these functions formal power series.
2 We
adopt the terminology and notation used in formal language theory (Berstel and Reutenauer 1988; Kuich and Salomaa 1986; Salomaa and Soittola 1978): the image by a formal power series S of a string w is denoted by (S, w) and called the coefficient of w in S, the notation S = ~,w~.
(S, w)w is then used to define a power series by its coefficients, the support of S is the language defined by: suep(S) = {w (S,w) # The fundamental theorem of Schtitzenberger (1961), analogous to Kleene's theorem for formal languages, states that a formal power series S is rational iff it is recognizable, that is, realizable by a string-to-weight transducer.
The semiring (74+ U {cx~}, rain, +, c~, 0) used in many optimization problems is called the tropical semiring.
3 So, the functions we consider here are more precisely rational power series over the tropical semiring.
A string-to-weight transducer T is said to be unambiguous if for any given string w there exists at most one successful path labeled with w.
In the following, we examine, more specifically, efficient string-to-weight transducers: subsequential transducers.
A transducer is said to be subsequential if its input is 2 Recall that a semiring is essentially a ring that may lack negation, namely in which the first operation does not necessarily admit inversion.
(TZ, +,., 0,1), where 0 and 1 are, respectively, the identity elements for + and., or, for any non-empty set E, (2 E, U, n, 0, E), where 0 and E are, respectively, the identity elements for U and O, are other examples of semirings.
3 This
terminology is often used more specifically when the set is restricted to natural integers (Nu {oo},min, +,~,0).
281 Computational Linguistics Volume 23, Number 2 deterministic, that is if at any state there exists at most one outgoing transition labeled with a given element of the input alphabet G.
Subsequential string-to-weight transducers are sometimes called weighted automata, or weighted acceptors, or probabilistic automata, or distance automata.
Our terminology is meant to favor the functional view of these devices, which is the view that we consider here.
Not all string-to-weight transducers are subsequential but we define an algorithm to determinize nonsubsequential transducers when possible.
Definition More formally a string-to-weight subsequential transducer "r = (Q, i, F, ~, 6, or, )~, p) is an 8-tuple, with:  Q the set of its states,  i E Q its initial state,  F c_ Q the set of final states,  G the input alphabet,  6 the transition function mapping Q x E to Q, 6 can be extended as in the string case to map Q x G* to Q,  cr the output function, which maps Q x G to 7%+, cr can also be extended to Q x ~,*,  ;~ E T4+ the initial weight,  p the final weight function mapping F to T4+.
A string w E ~,* is accepted by a subsequential transducer T if there exists f E F such that 6(i, w) =f.
The output associated to w is then: )~ + or(i, w) + p(f).
We will use the following definition for characterizing the transducers that admit determinization.
Definition Two states q and q' of a string-to-weight transducer T = (Q, I, F, G, 6, rr, )~, p), not necessarily subsequential, are said to be twins if: V(u,v) E (~,)2, ({q,q,} C 6(I,u),q E 6(q,v),q' E 6(q',v)) ~ ~(q,v,q) = ~(q',v,q') (9) In other words, q and q' are twins if, when they can be reached from the initial state by the same string u, the minimum outputs of loops at q and q' labeled with any string v are identical.
We say that T has the twins property when any two states q and q' of T are twins.
Notice that according to the definition, two states that do not have cycles with the same string v are twins.
In particular, two states that do not belong to any cycle are necessarily twins.
Thus, an acyclic transducer has the twins property.
In the following section, we consider subsequential power series in the tropical semiring, that is, functions that can be realized by subsequential string-to-weight transducers.
Many rational power series defined on the tropical semiring considered in practice are subsequential, in particular, acyclic transducers represent subsequential power series.
282 Mohri Transducers in Language and Speech We introduce a theorem giving an intrinsic characterization of subsequential power series irrespective of the transducer realizing them.
We then present an algorithm that allows one to determinize some string-to-weight transducers.
We give a general presentation of the algorithm since it can be used with many other semirings, in particular, with string-to-string transducers and with transducers whose output labels are pairs of strings and weights.
We then use the twins property to define a set of transducers to which the determinization algorithm applies.
We give a characterization of unambiguous transducers admitting determinization, and then use this characterization to define an algorithm to test if a given transducer can be determinized.
We also present a very efficient minimization algorithm that applies to subsequential string-to-weight transducers.
In many cases, the determinization algorithm can also be used to minimize a subsequential transducer; we describe this use of the algorithm and give the related proofs in the appendix.
3.2 Characterization
of Subsequential Power Series Recall that one can define a metric on E* by: d(u,v) = lu\[ + Iv\[ 21u A v\[ (10) where we denote by u A v the longest common prefix of two strings u and v in E*.
The definition we gave for subsequential power series depends on the transducers representing them.
The theorem that follows gives an intrinsic characterization of subsequential power series.
4 Theorem
9 Let S be a rational power series defined on the tropical semiring.
S is subsequential iff it has bounded variation.
Proof Assume that S is subsequential.
Let ~= (Q, i, F, E, 6, or, A, p) be a subsequential transducer.
5 denotes the transition function associated with T, cr its output function, and )~ and p the initial and final weight functions.
Let L be the maximum of the lengths of all output labels of T: L= max \[cr(q,a)\[ (11) (q,a)CQxE and R the upper bound of all output differences at final states: R= max \[p(q)-p(q')\[ (12) (q,q')EF 2 and define M as M = L + R.
Let (Ul, u2) be in (E*) 2.
By definition of d, there exists u E E* such that: Ul = uvl, u2 = uv2, and Iv1\[ + Iv2\[ = d(ul,u2) (13) Hence, cr(i,u,) = cr(i,u) + cr(5(i,u),vl) (i, u2) = cr(i,u) + cr(6(i,u),v2) 4 This is an extension of the characterization theorem of Choffrut (1978) for string-to-string functions.
The extension is not straightforward because the length of an output string is a natural integer.
Here we deal with real numbers.
283 Computational Linguistics Volume 23, Number 2 Since \[(6(i,u),vl) ~(6(i,u),v2)l <_ L.
(Iv1\] + Iv21) = L.d(ul, u2) and \[p(6(i, ul))-p(~(i, u2))l <_ R we have IA+(i, ul) + p(6(i, ul)) A+~(i, u2) + p(6(i, u2))\[ <_ L.d(ul, u2) + a Notice that if ul # u2, R <_ R.
d(ul, u2).
Thus \[A + a(i, ul) + p(6(i, Ul)) )~ + or(i, u2) + p(6(i, u2))\[ <_ (L + n) . d(ul, u2) Therefore: V(Ul, U2) E (E*) 2, \[S(Ul)S(u2)\[ < M'd(Ul, U2) (14) This proves that S is M-Lipschitzian s and afortiori that it has bounded variation.
Conversely, suppose that S has bounded variation.
Since S is rational, according to the theorem of Schtitzenberger (1961) it is recognizable and therefore there exists a string-to-weight transducer ~= (Q,I,F, E, ~, or,,k, p) realizing S.
As in the case of string-to-string transducers, one can show that any transducer admits an equivalent trim unambiguous transducer.
So, without loss of generality we can assume T trim  and unambiguous.
Furthermore, we describe in the next sections a determinization algorithm.
We show that this algorithm applies to any transducer that has the twins property.
Thus, in order to show that S is subsequentiable, it is sufficient to show that ~has the twins property.
Consider two states q and q' of ~and let (u,v) E (E*) 2 be such that: {q, q'} c ~(1, u), q E ~(q, v), q' ~ ~(ql, v) Since ~is trim there exists (w,w') E (~.)2 such that 6(q,w) NF 0 and ~(q, w') NF # 0.
Notice that Vk >_ O, cl(uvkw, uvkw ') = a(w, w') Thus, since S has bounded variation 3K >_ 0,Vk _> 0, IS(uvkw) S(uv~w')l <_ K Since r is unambiguous, there is only one path from I to F corresponding to uvkw (resp.
uvkw'). We have: S(uvkw) = O(I, uw, F) +kO(q,v,q) S(uvkw ') = O(I, uw',F) + kO(q',v,q') 5 This implies in particular that the subsequential power series over the tropical semiring define continuous functions for the topology induced by the metric d on E*.
Also this shows that in the theorem one can replace has bounded variation by is Lipschitzian.
284 Mohri Transducers in Language and Speech 9 10 11 12 13 14 15 Figure 10 Power Series Determinization(T~, T2) 1 F2 *-0 2 )~2 ~ (~E~AI(i) iEI1 3 i2 *-U(i,)~21  ~1(i))} iEI1 4 Q ~ {/2} 5 while Q # 0 6 do q2 ~-head\[Q\] 7 if (there exists (q, x) E q2 such that q E F1) 8 then F2 *--F2 U {q2} /:}2 (q2) *-~ X @ PA (q) qEFl,(q,x) Eq2 for each a such that F(q2,a) # 0 do a2(q2,a) *(~ \[x (~ cq (t)\] (q,x)EI'(q2,a) t=(q,a,crl(t),nl(t))EE1 62(q2,a)*-U {(q" ED \[cr2(q2"a)\]-lx(gal(t)} q'Ev(q2,a) (q,x,t) ET(q2,a),nl(t)=q' if (62(q2,a) is a new state) then ENQUEUE(Q, 62(q2, a)) DEQUEUE(Q) Algorithm for the determinization of a transducer ~-~ representing a power series defined on the semiring (S, E3, , 0, i).
Hence 3K > 0,Vk _> 0, i(O(I, uw, F) O(I, uw',F)) + k(O(q,v,q) O(q',v,q')) I <_ K ==~ #(q,v,q) ~)(q',v,q') = 0 Thus T has the twins property.
This ends the proof of the theorem.
\[\] 3.3 General Determinization Algorithm for Power Series We describe in this section an algorithm for constructing a subsequential transducer "1" 2 = (Q2, i2, F2,~,~2, cr2,,,~2, P2 ) equivalent to a given nonsubsequential one ~-I = (Q1, G, I1, F1, El, A1, pl).
The algorithm extends our determinization algorithm for stringto-string transducers representing p-subsequential functions to the case of transducers outputting weights (Mohri 1994c).
Figure 10 gives the pseudocode of the algorithm.
We present the algorithm in the general case of a semiring (S, ~, , 0,1) on which the transducer T1 is defined.
Indeed, the algorithm we are describing here applies as well to transducers representing power series defined on many other semirings.
6 We
describe the algorithm in the case of the tropical semiring.
For the tropical semiring, one can replace @ by min and  by + in the pseudocode of Figure 10.
7 6 In particular, the algorithm also applies to string subsequentiable transducers and to transducers that output pairs of strings and weights.
We will come back to this point later.
7 Similarly, ~21 should be interpreted as -A, and \[~2(q2,a)\] -1 as -cr2(q2,a ).
285 Computational Linguistics Volume 23, Number 2 The algorithm is similar to the powerset construction used for the determinization of automata.
However, since the outputs of two transitions bearing the same input label might differ, one can only output the minimum of these outputs in the resulting transducer, therefore one needs to keep track of the residual weights.
Hence, the subsets q2 that we consider here are made of pairs (q, x) of states and weights.
The initial weight &2 of T2 is the minimum of all the initial weights of ~-1 (line 2).
The initial state i2 is a subset made of pairs (i, x), where i is an initial state of T1, and x = &l (i) )~2 (line 3).
We use a queue Q to maintain the set of subsets q2 yet to be examined, as in the classical powerset construction, s Initially, Q contains only the subset i2.
The subsets q2 are the states of the resulting transducer, q2 is a final state of T2 iff it contains at least one pair (q, x), with q a final state of ~1 (lines 7-8).
The final output associated to q2 is then the minimum of the final outputs of all the final states in q2 combined with their respective residual weight (line 9).
For each input label a such that there exists at least one state q of the subset q2 admitting an outgoing transition labeled with a, one outgoing transition leaving q2 with the input label a is constructed (lines 10-14).
The output o'2(q2, a) of this transition is the minimum of the outputs of all the transitions with input label a that leave a state in the subset q2, when combined with the residual weight associated to that state (line 11).
The destination state 62(q2, a) of the transition leaving q2 is a subset made of pairs (q', x'), where q' is a state of T1 that can be reached by a transition labeled with a, and x' the corresponding residual weight (line 12).
x' is computed by taking the minimum of all the transitions with input label a that leave a state q of q2 and reach q', when combined with the residual weight of q minus the output weight cr2(q2,a).
Finally, 62(q2,a) is enqueued in Q iff it is a new subset.
We denote by nl (t) the destination state of a transition t E El.
Hence nl (t) = q', if t -(q,a,x,q') E El.
The sets F(q2,a), 7(q2,a), and v(q2,a) used in the algorithm are defined by:  F(q2,a) = {(q,x) E q2: 3t = (q,a, rrl(t),nl(t)) E El}  7(q2,a) = {(q,x,t) E q2 x El: t= (q,a, cq(t),nl(t)) E El}  ~(q2,a) = {q': 3(q,x) E q2,3t = (q,a, rrfft),q') E El} F(q2, a) denotes the set of pairs (q, x), elements of the subset q2, having transitions labeled with the input a.
7(q2, a) denotes the set of triples (q, x, t) where (q, x) is a pair in q2 such that q admits a transition with input label a.
v(q2,a) is the set of states q' that can be reached by transitions labeled with a from the states of the subset q2.
The algorithm is illustrated in Figures 11 and 12.
Notice that the input ab admits several outputs in #1:{1 + 1 = 2,1 + 3 = 4,3 + 3 = 6,3 + 5 = 8}.
Only one of these outputs (2, the smallest) is kept in the determinized transducer 1'2, since in the tropical semiring one is only interested in the minimum outputs for any given string.
Notice that several transitions might reach the same state with a priori different residual weights.
Since one is only interested in the best path, namely the path corresponding to the minimum weight, one can keep the minimum of these weights for a given state element of a subset (line 11 of the algorithm of Figure 10).
In the next section, we give a set of transducers "rl for which the determinization algorithm terminates.
The following theorem shows the correctness of the algorithm when it terminates.
8 The
algorithm works with any queue discipline chosen for Q.
286 Mohri Transducers in Language and Speech Figure 11 Transducer #1 representing a power series defined on (7"4+ U {o0}, min, +).
Figure 12 Transducer #2 obtained by power series determinization of #1.
Theorem 10 Assume that the determinization algorithm terminates, then the resulting transducer ~'2 is equivalent to ~1.
Proof We denote by Offq, w,q') the minimum of the outputs of all paths from q to q'.
By construction we have: )~2 "~-min/~1 (il) il E I1 We define the residual output associated to q in the subset (~2(/2, W) as the weight c(q, w) associated to the pair containing q in ~2(i2, w).
It is not hard to show by induction on Iwl that the subsets constructed by the algorithm are the sets 62(i2, w), w E ~*, such that: Vw E E*, ~2(i2,w) = U {(q,c(q,w)} qE61 (ll,w) c(q,w) = m~1~1(/~1(/1 ) q-Ol(il, w,q)) -cr2(/2,w) --/~2 ff2(i2,w) = min w (/~1(/1) qOl(il, w,q)) -~2 q~1(I1, ) (15) 287 Computational Linguistics Volume 23, Number 2 Notice that the size of a subset never exceeds \[QI\[: card(62(i2,w)) ~ IQI\[.
A state q belongs at most to one pair of a subset, since for all paths reaching q, only the minimum of the residual outputs is kept.
Notice also that, by definition of min, in any subset there exists at least one state q with a residual output c(q, w) equal to 0.
A string w is accepted by ~-1 iff there exists q E F1 such that q c 61 (I1, w).
Using equations 15, it is accepted iff 62(i2,w) contains a pair (q,c(q,w)) with q E F1.
This is exactly the definition of the final states F2 (line 7).
So ~1 and T2 accept the same set of strings.
Let w C E* be a string accepted by ~1 and ~-2.
The definition of p2 in the algorithm of figure 10, line 9, gives: p2(62(i2,w)) = rain pl(q) + m'.m(Al(h) + 81(il, w,q)) a2(i2,w) )~2 (16) qE 61 ( II,w )NF1 ll El1 Thus, if we denote by S the power series realized by "rl, we have: p2(62(i2,w)) = (S,w) cr2(/2,w) )~2 (17) Hence: &2 + cr2(i2, w) + p2(62(/2, w)) = (S, w).
\[\] The power series determinization algorithm is equivalent to the usual determinization of automata when the initial weight, the final weights, and all output labels are equal to 0.
The subsets considered in the algorithm are then exactly those obtained in the powerset determinization of automata, all residual outputs c(q, w) being equal to 0.
Both space and time complexity of the determinization algorithm for automata are exponential.
There are minimal deterministic automata with exponential size with respect to an equivalent nondeterministic one.
A fortiori the complexity of the determinization algorithm in the weighted case we just described is also exponential.
However, in some cases in which the degree of nondeterminism of the initial transducer is high, the determinization algorithm turns out to be fast and the resulting transducer has fewer states than the initial one.
We present examples of such cases, which appear in speech recognition, in the last section.
We also present a minimization algorithm that allows the size of subsequential transducers representing power series to be reduced.
The complexity of the application of subsequential transducers is linear in the size of the string to which it applies.
This property makes it worthwhile to use the power series determinization to speed up the application of transducers.
Not all transducers can be determinized using the power series determinization.
In the following section, we define a set of transducers that admit determinization, and characterize unambiguous transducers that admit the application of the algorithm.
Since determinization does not apply to all transducers, it is important to be able to test the determinizability of a transducer.
We present, in the next section, an algorithm to test this property in the case of unambiguous trim transducers.
The proofs of some of the theorems in the next two sections are complex; they can be skipped on first reading.
3.4 Determinizable
Transducers There are transducers with which determinization does not halt, but rather generates an infinite number of subsets.
We define determinizable transducers as those transducers with which the algorithm terminates.
We first show that a large set of transducers 288 Mohri Transducers in Language and Speech admit determinization, then give a characterization of unambiguous transducers admitting determinization.
In what follows, the states of the transducers considered will be assumed to be accessible from the initial one.
The following lemma will be useful in the proof of the theorems.
Lemma 1 W Let T = (Q, E, I, F, E, A, p) be a string-to-weight transducer, ~r E p "-* q a path in T from the state p ~ Q to q ~ Q, and ~" E p',G q' a path from p' E Q to q' ~ Q both labeled with the input string w ~ E*.
Assume that the lengths of 7r and ~-' are greater than \[Q\[2 _ 1, then there exist strings Ul, u2, u3 in E*, and states pit p2, p~r and p~ such that \[u2\[ > 0, UlU2U3 = w and such that 7r and 7r' be factored in the following way: 7r' E p,,,.,u' P~',,~ p~,,~ q, Proof The proof is based on the use of a cross product of two transducers.
Given two transducers T1 = (Q1, E, I1, F1, El, A1, pl) and T2 = (Q2, G,/2, F2, E2, A2, #2), we define the cross product of T1 and T2 as the transducer: T1 X T2 = (Q1 x Q2,G, I1 x I2,F1 x F2,E,A,#) with outputs in 7"4+ x T4+ such that t = ((ql, q2), a, (Xl, x2), (q~, q~)) E Q1 x E x 14+ x 74+ x Q2 is a transition of T1 x T2, namely t E E, iff (ql,a, xl, q~) E E1 and (q2,a, x2,2) E E2.
We also define A and p by: V(i1,i2) E/1 x I2, A(il, i2) = (Al(i1),A2(i2)), V(fl,f2) E F1 x F2, #(fl,f2) ---(Pl 0Cl ), P20c2)).
Consider the cross product of T with itself, T x T.
Let 7r and 7r' be two paths in T with lengths greater than IQI 2 1, (m > IQI2 _ 1): ~r = ( (p = qo, ao, xo, ql) .....
(qm-l,am-l, Xm-l, qm = q) ) V V z ! a x ! ! 7r' = ((p' = q'o, ao, xo, qO .....
~qm-1, m-I, m-l, qm = q')) then: II = (((q0, q~), a0, (x0, x~), (ql, q~)),  ., ((qm-1, q~-x), am-I, (Xm-l., Xm_ 1),, (qm, Cm))) is a path in T x T with length greater than \[Q\]2 _ 1.
Since T x T has exactly \[Q\[2 states, II admits at least one cycle at some state (pl, p~) labeled with a non-empty input string u2.
This shows the existence of the factorization above and proves the lemma.
\[\] Theorem 11 Let 7-1 -(Q1, E, I1, F1, El, A1, pl) be a string-to-weight transducer defined on the tropical semiring.
If T1 has the twins property then it is determinizable.
Proof Assume that ~has the twins property.
If the determinization algorithm does not halt, there exists at least one subset of 2 Q, {q0 ..... qm}, such that the algorithm generates an infinite number of distinct weighted subsets {(q0, Co) .....
(qm, Cm)}.
289 Computational Linguistics Volume 23, Number 2 Then we have necessarily m > 1.
Indeed, we mentioned previously that in any subset there exists at least one state qi with a residual output ci = 0.
If the subset contains only one state qo, then Co = 0.
So there cannot be an infinite number of distinct subsets ((qo, Co)}.
Let A c G* be the set of strings w such that the states of 62(i2, w) be {qo ..... qm}.
We have: Vw E A, ~2(/2, W) = {(q0, c(qo, w)),..., (qm, C(qm, w))}.
Since A is infinite, and since in each weighted subset there exists a null residual output, there exist i0, 0 ~ i0 ~ m, such that c(q/0, w) -0 for an infinite number of strings w E A.
Without loss of generality we can assume that i0 -0.
Let B C_ A be the infinite set of strings w for which c(q0, w) = 0.
Since the number of subsets ((qo, c(qo, w)) .....
(qm, C(qm, W))}, w E B, is infinite, there exists j, 0 < j _< m, such that c(qj, w) be distinct for an infinite number of strings w E B.
Without loss of generality we can assume j = 1.
Let C c B be an infinite set of strings w with c(ql, w) all distinct.
Define R(qo, ql) to be the finite set of differences of the weights of paths leading to q0 and ql labeled with the same string w, \[w\[ _< \[Qll 2 1: R(qo, q,) = {(A(i,) +~(~-,)) (A(io) + (~'o)): 7to E io w qo, Tr, E i, w q', io E/,il E/,Iw I _( IQ, I2 1} We will show that {c(ql, w): w E C} C_ R(qo, ql).
This will yield a contradiction with the infinity of C, and will therefore prove that the algorithm terminates.
Let w E C, and consider a shortest path 7r0 from a state i0 E I to q0 labeled with the input string w and with total cost c~(zr0).
Similarly consider a shortest path ~rl from il E I to ql labeled with the input string w and with total cost cr(zrl).
By definition of the subset construction we have: (A(h) + ~r(~rl)) (A(/o) + er(~r0)) = c(ql, w).
Assume that Iw\[ > \[Q1\] 2 1.
Using the lemma 1, there exists a factorization of ~r0 and zrl of the type: ~ro E io "~ po "~ po d~ qo 7rl E il d2, pl ~ pl ~ ql with \]u2\] > 0.
Since Po and pl are twins, 01(po, u2,po) = 01(pl, u2,Pl).
Define zr~ and ~r~ by: ~r~ E i0 G p0 "~ q0 7r~ E il G pl "~ ql Since ~r and ~r' are shortest paths, we have: a(Tro) = cr(zr~) + 01(po, u2,po) and a0rl ) = cr(zr~) + 01 (Pl, u2, pl).
Hence: (A(il) + rr0r~)) (A(io) + a(Tr~)) = c(ql, w).
By induction on \]w I, we can therefore find shortest paths Ho and H1 from io to qo (resp.
il to ql) with length less or equal to \]Q112 1 and such that (A(h) +a(H1)) (A(io) +a(Ho)) = c(ql, w).
Since a(H1) a(IIo) E R(qo, ql), c(ql, w) E R(qo, ql) and C is finite.
This ends the proof of the theorem.
\[\] There are transducers that do not have the twins property and that are still determinizable.
To characterize such transducers, more complex conditions that we will not describe here are required.
However, in the case of trim unambiguous transducers, the twins property provides a characterization of determinizable transducers.
290 Mohri Transducers in Language and Speech Theorem 12 Let "rl = (Q1,E, I1,F1, El, ~1,Pl) be a trim unambiguous string-to-weight transducer defined on the tropical semiring.
Then ~rl is determinizable iff it has the twins property.
Proof According to the previous theorem, if ~1 has the twins property, then it is determinizable.
Assume now that T does not have the twins property, then there exist at least two states q and q~ in Q that are not twins.
There exists (u, v) E E* such that: ({q,q'} C 61(I,u),q E 51(q,v),q' E 5ffq',v)) and ~l(q,v,q) ~ ~l(q',v,q').
Consider the weighted subsets 62(/2, uvk), with k E N, constructed by the determinization algorithm.
A subset 62(/2, uv k) contains the pairs (q, c(q, uvk)) and (q', c(q', uvk)).
We will show that these subsets are all distinct.
This will prove that the determinization algorithm does not terminate if ~-1 does not have the twins property.
Since T1 is a trim unambiguous transducer, there exits only one path in ~-1 from I to q or to qt with input string u.
Similarly, the cycles at q and q~ labeled with v are unique.
Thus, there exist i E I and i ~ E I such that: VkEN, c(q, uv k) = )~l(i)+01(i,u,q)+kOl(q,v,q)-~2(i2,uvk)-)~2 (18) Vk E ./V', c(q', uv k) = )~1(/') q81 (i', u, q') + k01(q', v, q') or2(/2, uv k) ~2 Let )~ and 0 be defined by: )~ = (,~1(i') )~1(i)) q( l(i',u,q') 01(i,u,q)) 0 = Ol(q',v,q') Offq, v,q) (19) We have: 'Ok E N, c(q', uv k) c(q, uv k) = ~ + k~) (20) Since (1 ~ 0, equation 20 shows that the subsets 62(i2, uv k) are all distinct.
\[\] 3.5 Test of Determinizability The characterization of determinizable transducers provided by theorem 12 leads to the definition of an algorithm for testing the determinizability of trim unambiguous transducers.
Before describing the algorithm, we introduce a lemma that shows that it suffices to examine a finite number of paths to test the twins property.
Lemma 2 Let "rl = (Q1, E, I1,F1, El, )~1,Pl) be a trim unambiguous string-to-weight transducer defined on the tropical semiring.
7-1 has the twins property iff V(u,v) E (E*) 2, luvl <_ 2IQ112 1, ({q,q'} C 61(I,u),q E 51(q,v),q' E 61(q',v)) ~ 01(q,v,q) = 01(q',v,q') (21) Proof Clearly if "O has the twins property, then (21) holds.
Conversely, we prove that if (21) holds, then it also holds for any (u,v) E (E*) 2, by induction on luvI . Our proof is similar to that of Berstel (1979) for string-to-string transducers.
Consider (u, v) E (E*) 2 and (q,q') E IQll 2 such that: {q,q'} c 61(I,u),q E 61(q,v),q' E 61(q',v).
Assume that luvI > 21Qll 2 1 with Iv I > o.
Then either luI > IQll 2 1 or IvI > IQll 2 1.
Assume that lul > IQ112-1.
Since T1 is a trim unambiguous transducer there exists a unique path ~r in rl from i E I to q labeled with the input string u, and a unique path 291 Computational Linguistics Volume 23, Number 2 7r' from i' E I to q'.
In view of lemma 2, there exist strings ul, u2, u3 in ~*, and states pl, p2, p~, and p~ such that \]u2\] > 0, UlU2U3 -~ U and such that lr and 7r' be factored in the following way: ~r E i u"G~ pl "~ pl,G q ~r' E i',-,,*ul Pl',`% P~ "~ q' Since \]ulu3vl < luv\], by induction 01(q,v,q) = Offq',v,q').
Next, assume that Iv I > \]Qll 2 1.
Then according to lemma 1, there exist strings Vl, v2, v3 in E*, and states ql, q2, q~, and q~ such that Iv21 > 0, vlv2v3 = v and such that lr and lr' be factored in the following way: ~r E q~ ql~ ql ~ q ~r' E q',G q~,~ q~,~ q' Since lUVl V3\] < \]uv\], by induction, 81 ( q, vl v3, q) = 81 ( q', vl v3, q').
Similarly, since luvl v2\] < l uv\], 01 (ql, v2, ql) = 01 (q~, v2, q~).
"rl is a trim unambiguous transducer, so: 01(q,v,q) =01(q, vlv3,q)+Ol(ql, v2,ql) 01(q', v, q') = 01(q', vlv3, q') + 01 (q~, v2, q~) Thus, 01 (q, v, q) = 01(q', v, q').
This completes the proof of the lemma.
\[\] Theorem 13 Let T1 = (Q1, E, 11, F1, El, A1, Pl) be a trim unambiguous string-to-weight transducer defined on the tropical semiring.
There exists an algorithm to test the determinizability of ~1.
Proof According to theorem 12, testing the determinizability of T1 is equivalent to testing for the twins property.
We define an algorithm to test this property.
Our algorithm is close to that of Weber and Klemm (1995) for testing the sequentiability of string-to-string transducers.
It is based on the construction of an automaton A = (Q,/, F, E) similar to the cross product of ~-1 with itself.
Let K C T~ be the finite set of real numbers defined by: K= ((t~)-cr(ti)): l <k <2iQ1\]2-1,Vi<_k(ti, tl) EE We define A by the following:  The set of states of A is defined by Q = Q1 x Q1 x K,  The set of initial states by I = h x/1 x {0},  The set of final states by F = F1 x F1 x K,  The set of transitions by: E = {((ql, q2,c),a,(q~,q~2,c')) E.
Q x E x Q: 3 (ql,a, x, q2) E El, ' ' ' x' x}.
(ql,a,x,q2) EEl, C'-C= 292 Mohri Transducers in Language and Speech By construction, two states ql and q2 of Q can be reached by the same string u, lut < 21Qll 2 1, iff there exists c E K such that (ql, q2,c) can be reached from I in A.
The set of such (ql, q2, c) is exactly the transitive closure of I in A.
The transitive closure of I can be determined in time linear in the size of A, O(IQI + IEI).
Two such states ql and q2 are not twins iff there exists a path in A from (ql, q2, 0) to (ql, q2, c), with c # 0.
Indeed, this is exactly equivalent to the existence of cycles at ql and q2 with the same input label and distinct output weights.
According to lemma 2, it suffices to test the twins property for strings of length less than 21Qll 2 1.
So the following gives an algorithm to test the twins property of a transducer ~-1:, 2.
3. Compute the transitive closure of h T(I).
Determine the set of pairs (ql, q2) of T(I) with distinct states ql # q2For each such {ql, q2}, compute the transitive closure of (ql, q2, 0) in A.
If it contains (ql, q2, c) with c # 0, then ~-1 does not have the twins property.
The operations used in the algorithm (computation of the transitive closure, determination of the set of states) can all be done in polynomial time with respect to the size of A, using classical algorithms (Aho, Hopcroft, and Ullman 1974).
\[\] This provides an algorithm for testing the twins property of an unambiguous trim transducer T.
It is very useful when T is known to be unambiguous.
In many practical cases, the transducer one wishes to determinize is ambiguous.
It is always possible to construct an unambiguous transducer T' from T (Eilenberg 1974-1976).
The complexity of such a construction is exponential in the worst case.
Thus the overall complexity of the test of determinizability is also exponentia ! in the worst case.
Notice that if one wishes to construct the result of the determinization of T for a given input string w, one does not need to expand the whole result of the determinization, but only the necessary part of the determinized transducer.
When restricted to a finite set the function realized by any transducer is subsequentiable, since it has bounded variation?
Acyclic transducers have the twins property, so they are determinizable.
Therefore, it is always possible to expand the result of the determinization algorithm for a finite set of input strings, even if T is not determinizable.
3.6 Determinization
in Other Semirings The determinization algorithm that we previously presented applies as well to transducers mapping strings to other semirings.
We gave the pseudocode of the algorithm in the general case.
The algorithm applies for instance to the real semiring (7"4, +,., 0,1).
One can also verify that (~* U {oc}, A,., cx~, e), where A denotes the longest common prefix operation and  concatenation, o~ a new element such that for any string w E (~* U {~}), w A oo = oo A w = w and woo = eo.
w = oo, defines a left semiring}  We call this semiring the string semiring.
The algorithm of Figure 10 used with the string semiring is exactly the determinization algorithm for subsequentiable string-to-string transducers, as defined by Mohri (1994c).
The cross product of two semirings defines a semiring.
The algorithm also applies when the semiring is the cross product of 9 Using the proof of the theorem of the previous section, it is easy to convince oneself that this assertion can be generalized to any rational subset Y of E* such that the restriction of S, the function T realizes, to Y has bounded variation.
10 A
left semiring is a semiring that may lack right distributivity.
293 Computational Linguistics Volume 23, Number 2 a:b/3 Figure 13 Transducer 7-1 with outputs in ~* x 74.
a:b/3 ~ c:c/5 = b.a/~ d:a/~ Figure 14 Sequential transducer r2 with outputs in ~,,* x 74 obtained from fll by determinization.
(E* Ucx~}, A,., cx~, c) and (T4+ Uoo}, min, +, oo, 0), which allows transducers outputting pairs of strings and weights to be determined.
The determinization algorithm for such transducers is illustrated in Figures 13 and 14.
Subsets in this algorithm are made of triples (q, w, x) E Q x E* u {oo} x 7-4 u {cx~}, where q is a state of the initial transducer, w a residual string, and x a residual output weight.
3.7 Minimization
We here define a minimization algorithm for subsequential power series defined on the tropical semiring, which extends the algorithm defined by Mohri (1994b) in the case of string-to-string transducers.
For any subset L of G* and any string u we define u-lL by: u-IL = {w: uw E L} (22) Recall that L is a regular language iff there exists a finite number of distinct u-lL Nerode (1958).
In a similar way, given a power series S we define a new power series u-is by: n u-iS = y~ (S, uw)w (23) wE~* 11 One can prove that S, a power series defined on a field, is rational if it admits a finite number of independent u-iS (Carlyle and Paz 1971).
This is the equivalent, for power series, of Nerode's theorem for regular languages.
294 Mohri Transducers in Language and Speech For any subsequential power series S we can now define the following relation on ~,*: V(u, v) E ~*, u Rs v 4=~ 3k E T4, (u-lsupp(S) = v-lsupp(S)) and (\[u-iS -1 -V S\]/u-lsupp(S ) = k) (24) It is easy to show that Rs is an equivalence relation.
(u-lsupp(S) = v-lsupp(S)) defines the equivalence relation for regular languages.
Rs is a finer relation.
The additional condition in the definition of Rs is that the restriction of the power series \[u-iS -v-iS\] to u-lsupp(S) = v-lsupp(S) is constant.
The following lemma shows that if there exists a subsequential transducer T computing S with a number of states equal to the number of equivalence classes of Rs, then T is a minimal transducer computing f.
Lemma 3 If S is a subsequential power series defined on the tropical semiring, Rs has a finite number of equivalence classes.
This number is bounded by the number of states of any subsequential transducer realizing S.
Proof Let T -(Q, i, F, ~, 6, or, ~, p) be a subsequential transducer realizing S.
Clearly, ~(U,V) E (~*)2, 6(i,u) = 6(i,v) ~ Vw E u-lsupp(S),6(i, uw) E F ~ 6(i, vw) E F u-lsupp(S) = v-lsupp(S) Also, if u-lsupp(S) = v-lsupp(S), V(u, v) E (~,)2, 6(i, u) = 6(i,v) ~ VW E u-lsupp(S), (S, uw) (S, vw) = or(i, u) (i,v) 4=~ \[u-iS v-lS\]/u-~supp(S) = cr(i,u) cr(i,v) So V(u,v) E (G,)2, 6(i,u) = 6(i,v) ~ (uRsv).
This proves the lemma.
\[\] The following theorem proves the existence of a minimal subsequential transducer representing S.
Theorem 14 For any subsequential function S, there exists a minimal subsequential transducer computing it.
Its number of states is equal to the index of Rs.
Proof Given a subsequential power series S, we define a power series f by: Vu E ~*: u-lsupp(S) = 0, (f,u) = 0 Vu E G*: u-lsupp(S) # O, u) = min (S, uw) wEu-lsupp(S) We then define a subsequential transducer T = (Q, i, F, ~, 6, or, )~, p) by: 12  Q={~: uEG*}; 12 We denote by ~ the equivalence class of u E G*.
295 Computational Linguistics Volume 23, Number 2  i=~;  F = {a: u E ~,* Msupp(S)};  Vu e ~*,Va e ~,6(a,a) = Ca;  vu ~ y,*,va ~ z,~(~,a) = (f,u~) (f,u); ,~ = ff,~);  VqEQ, p(q)=0.
Since the index of Rs is finite, Q and F are well-defined.
The definition of 6 does not depend on the choice of the element u in ~, since for any a E ~, u Rs v implies (ua) Rs (va).
The definition of rr is also independent of this choice, since by definition of Rs, if uRsv, then (ua) Rs (va) and there exists k E T4 such that Vw E ~*, (S, uaw) (S, vaw)= (S, uw) (S, vw) = k.
Notice that the definition of G implies that: Vw ~ s*,(i,w) = (f,w) ff,~) (25) So: Vw E supp(S), A + r(i, w) + p(q) -= (f, w) = rnin (S, ww') w' ew-lsupp(S) S is subsequential, hence: Vw' E w-lsupp(S), (S, ww') < (S, w).
Since Vw E supp(S),  E w-lsupp(S), we have: m:m (S, ww') = (S,w) w' ew-lsupp(S) T realizes S.
This ends the proof of the theorem.
\[\] Given a subsequential transducer T = (Q, i, F, G, 6, cr, A, p), we can define for each state q E Q, d(q) by: d(q) -rnin (er(q,w) + p(6(q,w))) (26) 6(q,w) EF Definition We define a new operation of pushing, which applies to any transducer T.
In particular, if T is subsequential the result of the application of pushing to T is a new subsequential transducer T' --(Q, i, F, ~.., 6, er', A', p') that only differs from T by its output weights in the following way:  ' = ;~ +,~(i);  V(q,a) E Q x Z, G'(q,a) = rr(q,a) +d(6(q,a))-d(q);  Vq E Q,#(q) = O.
According to the definition of d, we have: Vw E G*: 6(q, aw) E F,d(q) < cr(q,a) + er(6(q,a),w) + p(6(6(q,a),w)) This implies that: a(q) <_ o(q,a) + a(6(q,a)) So, r ~ is well-defined: v(q,a) ~ Q x z,~'(q,a) > o 296 Mohri Transducers in Language and Speech Lemma 4 Let T' be the transducer obtained from T by pushing.
T ~ is a subsequential transducer which realizes the same function as T.
Proof That T' is subsequential follows immediately its definition.
Also, Vw E ~',q C Q, cr'(q,w) = rr(q,w) + d(6(q,w) ) -d(q) Since 6(i,w) E F =~ d(6(i,w)) = p(6(i,w)), we have: + o'(i,w) + p'(6(i,w)) = ;, + a(i) + o(q,w) + w)) d(i) + 0 This proves the lemma.
\[\] The following theorem defines the minimization algorithm.
Theorem 15 Let T be a subsequential transducer realizing a power series on the tropical semiring.
Then applying the following two operations: 1.
pushing 2.
automata minimization leads to a minimal transducer.
This minimal transducer is exactly the one defined in the proof of theorem 14.
The automata minimization step in the theorem consists of considering pairs of input labels and associated weights as a single label and of applying classical minimization algorithms for automata (Aho, Hopcroft, and Ullman 1974).
We do not give the proof of the theorem; it can be proved in a way similar to what is indicated in Mohri (1994b).
In general, there are several distinct minimal subsequential transducers realizing the same function.
Pushing introduces an equivalence relation on minimal transducers: T Rp T' iff p(T) = p(T'), where p(T) (resp.
p(T')) denotes the transducer obtained from T (resp.
T t) by pushing.
Indeed, if T and T t are minimal transducers realizing the same function, then p(T) and p(T') are both equal to the unique minimal transducer equivalent to T and T t as defined in theorem 14.
So, two equivalent minimal transducers only differ by their output labels, they have the same topology.
They only differ by the way the output weights are spread along the paths.
Notice that if we introduce a new super final state @ to which each final state q is connected by a transition of weight p(q), then d(q) in the definition of T' is exactly the length of a shortest path from  to q.
Thus, T' can be obtained from T using the classical single-source shortest paths algorithms such as that of Dijkstra (Cormen, Leiserson, and Rivest 1992).
13 In
case the transducer is acyclic, a classical linear time algorithm based on a topological sort of the graph allows one to obtain d.
13 This
algorithm can be extended to the case where weights are negative.
If there is no negative cycle the Bellman-Ford algorithm can be used.
297 Computational Linguistics Volume 23, Number 2 d/O / ~ ~ c/1 e,,Q Figure 15 Transducer ill.
d/O / N ~ c/1 e/O =Q Figure 16 Transducer ~'1 obtained from fll by pushing.
Once the function d is defined, the transformation of T into T' can be done in linear time, namely O(IQ\]+IEI), if we denote by E the set of transitions of T.
The complexity of pushing is therefore linear (O(IQI + IEI)) if the transducer is acyclic.
In the general case, the complexity of pushing is O(IE \] log IQI) if we use classical heaps, O(\]E I + IQI log \]Q\]) if we use Fibonacci heaps, and O(IE I log log IQI) if we use the efficient implementation of priority queues by Thorup (1996).
In case the maximum output weight W is small, we can use the algorithm of Ahuja et al.(1988); the complexity of pushing is then O(IEI + IQIx/fwI).
In case the transducer is acyclic, we can use a specific automata minimization algorithm (Revuz 1992) with linear time complexity, O(\]Q\] + IE\]).
In the general case, an efficient implementation of Hopcroft's algorithm (Aho, Hopcroft, and Ullman 1974) leads to O(\]E\] log \]Q\]).
Thus, the overall complexity of the minimization of subsequential transducers is always as good as that of classical automata minimization: O(IQI + IE\]) in the acyclic case, and O(\]E I log \]Q\[) in the general case.
Figures 15 to 17 illustrate the minimization algorithm.
131 (Figure 15) represents a subsequential string-to-weight transducer.
Notice that the size of fll cannot be reduced using the automata minimization.
71 represents the transducer obtained by pushing, and 51 a minimal transducer realizing the same function as fll in the tropical semiring.
298 Mohri Transducers in Language and Speech d/O c/1 Figure 17 Minimal transducer 61 obtained from "n by automata minimization.
The transducer obtained by this algorithm is the one defined in the proof of theorem 14 and has the minimal number of states.
This raises the question of whether there exists a subsequential transducer with the minimal number of transitions and computing the same function as a given subsequential transducer T.
The following corollary offers an answer.
Corollary 1 A minimal subsequential transducer has also the minimal number of transitions among all subsequential transducers realizing the same function.
Proof This generalizes the analogous theorem that holds in the case of automata.
The proof is similar.
Let T be a subsequential transducer with a minimal number of transitions.
Clearly, pushing does not change the number of transitions of T and automatan minimization, which consists of merging equivalent states, reduces or does not change this number.
Thus, the number of transitions of the minimal transducer equivalent to T as previously defined is less or equal to that of T.
This proves the corollary since, as previously pointed out, equivalent minimal transducers all have the same topology: in particular, they have the same number of states and transitions.
\[\] Given two subsequential transducers, one might wish to test their equivalence.
The importance of this problem was pointed out by Hopcroft and Ullman (1979, 284).
The following corollary addresses this question.
Corollary 2 There exists an algorithm to determine if two subsequential transducers are equivalent.
Proof The algorithm of theorem 15 associates a unique minimal transducer to each subsequential transducer T.
More precisely, this minimal transducer is unique up to a renumbering of the states.
The identity of two subsequential transducers with different numbering of states can be tested in the same way as that of two deterministic automata; for instance, by testing the equivalence of the automata and the equality of their number of states.
An efficient algorithm for testing the equivalence of two deterministic automata is given in Aho, Hopcroft, and Ullman (1974).
14 Since
the min14 The automata minimization step can in fact be omitted if this equivalence algorithm is used, since it does not affect the equivalence of the two subsequential transducers, considered as automata.
299 Mohri Transducers in Language and Speech weight transducer, each path of which corresponds to a sentence.
The weight of the path can be interpreted as a negative log of the probability of that sentence given the sequence of acoustic observations (utterance).
Such acyclic string-to-weight transducers are called word lattices.
4.2 Word
Lattices For a given utterance, the word lattice obtained in such a way contains many paths labeled with the possible sentences and their associated weights.
A word lattice often contains a lot of redundancy: many paths correspond to the same sentence but with different weights.
Word lattices can be directly searched to find the most probable sentences, those which correspond to the best paths, the paths with the smallest weights.
Figure 18 shows a word lattice obtained in speech recognition for the 2,000-word ARPA ATIS Task.
It corresponds to the following utterance: Which flights leave Detroit and arrive at Saint Petersburg around nine am?
Clearly the lattice is complex; it contains about 83 million paths.
Usually, it is not enough to consider the best path of a word lattice.
It is also necessary to correct the best path approximation by considering the n best paths, where the value of n depends on the task considered.
Notice that in case n is very large, one would need to consider, for the lattice in Figure 18, all 83 million paths.
The transducer contains 106 states and 359 transitions.
Determinization applies to this lattice.
The resulting transducer W2 (Figure 19) is sparser.
Recall that it is equivalent to W1, realizing exactly the same function mapping strings to weights.
For a given sentence s recognized by W1, there are many different paths with different total weights.
W2 contains a path labeled with s and with a total weight equal to the minimum of the weights of the paths of W1.
Let us insist on the fact that no pruning, heuristic, or approximation has been used here.
The lattice W2 only contains 18 paths.
Obviously, the search stage in speech recognition is greatly simplified when applied to W2 rather than W1.
W2 admits 38 states and 51 transitions.
The transducer W2 can still be minimized.
The minimization algorithm described in the previous section leads to the transducer W3 shown in Figure 20.
It contains 25 states and 33 transitions and of course the same number of paths as W2, 18.
The effect of minimization appears to be less important.
This is because, in this case, determinizafion includes a large part of the minimization by reducing the size of the first lattice.
This can be explained by the degree of nondeterminism of word lattices such as 14/1.15 Many states can be reached by the same set of strings.
These states are grouped into a single subset during determinization.
Also, the complexity of determinization is exponential in general, but in the case of the lattices considered in speech recognition, it is not.
16 Since
they contain a lot of redundancy, the resulting lattice is smaller than the initial one.
In fact, the time complexity of determinization can be expressed in terms of the initial and resulting lattices, W1 and W2, by O(l~ I log IGl(IWllIW21)2), where IWll and IW21 denote the sizes of W1 and W2.
Clearly if we restrict determinization to the cases where I w21 < I W1 I, its complexity is polynomial in terms of the size of the initial transducer \[Wll.
This also 15 The notion of ambiguity of a finite automaton can be formalized conveniently using the tropical semiring.
Many important studies of the degree of ambiguity of automata have been done in connection with the study of the properties of this semiring (Simon 1987).
16 A
more specific determinization can be used in the cases often encountered in natural language processing where the graph admits a loop at the initial state over the elements of the alphabet (Mohri 1995).
301 Mohri Transducers in Language and Speech Figure 19 Equivalent word lattice W2 obtained by determinization of W1.
@ ....
 .... .@ ~,, @ ~,o ...
@  Figure 20 Equivalent word lattice Wa obtained by minimization from W2.
rescoring l approximate_~J detailed \]._..l~ cde4Pls ~ lattice ~ models Figure 21 Rescoring.
applies to the space complexity of the algorithm.
In practice, the algorithm appears to be very efficient.
As an example, it took about 0.02s on a Silicon Graphics (Indy 100 MHZ Processor, 64 Mb RAM) to determinize the transducer of Figure 18.17 Determinization makes the use of lattices much faster.
Since at any state there exists at most one transition labeled with the word considered, finding the weight associated with a sentence does not depend on the size of the lattice.
The time and space complexity of such an operation is simply linear in the size of the sentence.
When dealing with large tasks, most speech recognition systems use a rescoring method (Figure 21).
This consists of first using a simple acoustic and grammar model to produce a word lattice, and then to reevaluate this word lattice with a more sophisticated model.
The size of the word lattice is then a critical parameter in the time and space efficiency of the system.
The determinization and minimization algorithms we presented allow the size of such word lattices to be considerably reduced, as seen in the examples.
We experimented with both determinization and minimization algorithms in the ATIS task.
Table I illustrates these results.
It shows these algorithms to be very effective in reducing the redundancy of speech networks in this task.
The reduction is also illustrated by an example in the ATIS task.
17 Part
of this time corresponds to I/O's and is therefore independent of the algorithm.
303 Computational Linguistics Volume 23, Number 2 Table 1 Word lattices in the ATIS task.
Determinization Determinization + Minimization Objects reduction factor reduction factor States ~ 3 ~ 5 Transitions,-~ 9 ~ 17 Paths > 232 > 232 Table 2 Subsequential word lattices in the NAB task.
Minimization results Objects reduction factor States ~ 4 Transitions,~ 3 Example 1 Example of a word lattice in the ATIS task.
States: 187 --* 37 Transitions: 993 ~ 59 Paths: > 232 ~ 6,993 The number of paths of the word lattice before determinization was larger than that of the largest integer representable with 32 bit machines.
We also experimented with the minimization algorithm by applying it to several word lattices obtained in the 60,000-word ARPA North American Business News task (NAB).
These lattices were already determinized.
Table 2 shows the average reduction factors we obtained when using the minimization algorithms with several subsequential lattices obtained for utterances of the NAB task.
The reduction factors help to measure the gain of minimization alone, since the lattices are already subsequential.
The numbers in example 2, an example of reduction we obtained, correspond to a typical case.
Example 2 Example of a word lattice in NAB task.
Transitions: 10,8211 ~ 37,563 States: 10,407 --* 2,553 4.3 On-the-fly Implementation of Determinization An important characteristic of the determinization algorithm is that it can be used on-the-fly.
Indeed, the determinization algorithm is such that given a subset representing a state of the resulting transducer, the definition of the transitions leaving that state depends only on that state or, equivalently, on the states of that subset, and on the transducer to determinize.
In particular, the definition and construction of these transitions do not depend directly on the previous subsets constructed.
We have produced an implementation of the determinization that allows one both to completely expand the result or to expand it on demand.
Arcs leaving a state of 304 Mohri Transducers in Language and Speech the determinized transducer are expanded only if necessary.
This characteristic of the implementation is important.
It can then be used, for instance, at any step in an onthe-fly cascade of composition of transducers in speech recognition to expand only the necessary part of a lattice or transducer (Pereira and Riley 1996; Mohri, Pereira, and Riley 1996).
One of the essential implications of the implementation is that it contributes to saving space during the search stage.
It is also very useful in speeding up the n-best decoder in speech recognition.
TM The determinization and minimization algorithms for string-to-weight transducers seem to have other applications in speech processing.
Many new experiments can be done using these algorithms at different stages of speech recognition, which might lead to the reshaping of some of the methods used in this field and create a renewed interest in the theory of automata and transducers.
5. Conclusion We have briefly presented the theoretical bases, algorithmic tools, and practical use of a set of devices that seem to fit the complexity of language and provide efficiency in space and time.
From the theoretical point of view, the understanding of these objects is crucial.
It helps to describe the possibilities they offer and to guide algorithmic choices.
Many new theoretical issues arise when more precision is sought.
The notion of determinization can be generalized to that of E-determinization for instance (Salomaa and Soittola 1978, chapter 3, exercise) requiring more general algorithms.
It can also be extended to local determinization: determinization at only those states of a transducer that admit a predefined property, such as that of having a large number of outgoing transitions.
An important advantage of local determinization is that it can be applied to any transducer without restriction.
Furthermore, local determinization also admits an on-the-fly implementation.
New characterizations of rational functions shed new light on some aspects of the theory of finite-state transducers (Reutenauer and Schiitzenberger 1995).
We have also offered a generalization of the operations we use based on the notions of semiring and power series, which help to simplify problems and algorithms used in various cases.
In particular, the string semiring that we introduced makes it conceptually easier to describe many algorithms and properties.
Subsequential transducers admit very efficient algorithms.
The determinization and minimization algorithms in the case of string-to-weight transducers presented here complete a large series of algorithms that have been shown to give remarkable results in natural language processing.
Sequential machines lead to useful algorithms in many other areas of computational linguistics.
In particular, subsequential power series allow for efficient results in indexation of natural language texts (Crochemore 1986; Mohri 1996b).
We briefly illustrated the application of these algorithms to speech recognition.
More precision in acoustic modeling, finer language models, large lexicon grammars, and a larger vocabulary will lead, in the near future, to networks of much larger sizes in speech recognition.
The determinization and minimization algorithms might help to limit the size of these networks while maintaining their time efficiency.
These algorithms can also be used in text-to-speech synthesis.
In fact, the same operations of composition of transducers (Sproat 1995) and perhaps more important size issues can be found in this field.
18 We
describe this application of determinization elsewhere.
305 Computational Linguistics Volume 23, Number 2 Figure 22 Subsequential power series S nonbisubsequential.
Appendix The determinization algorithm for power series can also be used to minimize transducers in many cases.
Let us first consider the case of automata.
Brzozowski (1962) showed that determinization can be used to minimize automata.
This nice result has also been proved more recently in elegant papers by Bauer (1988) and Urbanek (1989).
These authors refine the method to obtain better complexities.
19 Theorem
16 (Brzozowski 1962) Let A be a nondeterministic automaton.
Then the automaton A' = (Q', i', F', E, 6') obtained by reversing A, applying determinization, rereversing the obtained automaton and determinizing it is the minimal deterministic automaton equivalent to A.
We generalize this theorem to the case of string-to-weight transducers.
We say that a rational power series S is bisubsequential when S is subsequential and the power series S R = Y~w~, (S, wR)w is also subsequential.
2 Not
all subsequential transducers are bisubsequential.
Figure 22 shows a transducer representing a power series S that is not bisubsequential.
S is such that: Vn E.M, (S, ba") = n+l (27) Vn E Af, (S, ca n) = 0 The transducer of Figure 22 is subsequential so S is subsequential.
But the reverse S a is not, because it does not have bounded variation.
Indeed, since: We have: Vn E Af, (S a,anb) = n + l VnE.Af, (Sa, anc) = 0 Vn ~ A/', I(Sa, a"b) (Sa, a"c)l = n + 1 (28) 19 See Watson (1993) for a taxonomy of minimization algorithms for automata; see also Courcelle, Niwinski, and Podelski 1991.
20 For any string w E ~*, we denote by w a its reverse.
306 Mohri Transducers in Language and Speech A characterization similar to that of string-to-string transducers (Choffrut 1978) is possible for bisubsequential power series defined on the tropical semiring.
In particular, the theorem of the previous sections shows that S is bisubsequential iff S and S n have bounded variation.
We similarly define bideterminizable transducers as the transducers T defined on the tropical semiring admitting two applications of determinization, as follows: . . The reverse of T, T a can be determinized.
We denote by det(T a) the resulting transducer.
The reverse of det(TR), \[det(Ta)\] R can also be determinized.
We denote by det(\[det(Ta)\] ~) the resulting transducer.
In this definition, we assume that the reverse operation is performed simply by reversing the direction of the transitions and exchanging initial and final states.
Given this definition, we can present the extension of the theorem of Brzozowski (1962) to bideterminizable transducers.
21 Theorem 17 Let T be a bideterminizable transducer defined on the tropical semiring.
Then the transducer det(\[det(TR)\] R) obtained by reversing T, applying determinization, rereversing the obtained transducer and determinizing it is a minimal subsequential transducer equivalent to T.
Proof We denote by:  T1 = (Q1,il,F1,G,61,Crl,)~l, p1) det(Ta),  T'= (Q',i',F',E,6',',&',p') det(\[det(Ta)\] a)  T" = (Q', i', F', G, ~', rr', &', p') the transducer obtained from T by pushing.
The double reverse and determinization algorithms clearly do not change the function that T realizes.
So T' is a subsequential transducer equivalent to T.
We only need to prove that T ~ is minimal.
This is equivalent to showing that T" is minimal, since T' and T" have the same number of states.
T1 is the result of a determinization, hence it is a trim subsequential transducer.
We show that T' = det(T~) is minimal if T1 is a trim subsequential transducer.
Notice that the theorem does not require that T be subsequential.
Let $1 and $2 be two states of T" equivalent in the sense of automata.
We prove that $1 = $2, namely that no two distinct states of T" can be merged.
This will prove that T" is minimal.
Since pushing only affects the output labels, T' and T" have the same set of states: Q' = Q".
Hence $1 and $2 are also states of T'.
The states of T' can be viewed as weighted subsets whose state elements belong to T1, because T' is obtained by determinization of T~.
Let (q, c) E Q1 x T4 be a pair of the subset $1.
Since T1 is trim there exists w c G* such that 61(il, w) = q, so 6~($1,w) E F'.
Since $1 and S 2 are equivalent, we also have: 21 The theorem also holds in the case of string-to-string bideterminizable transducers.
We give the proof in the more complex case of string-to-weight transducers.
307 Computational Linguistics Volume 23, Number 2 b/1 Figure 23 Transducer f12 obtained by reversing ill.
a/O c/l a/4 ~b/1 ~ Figure 24 Transducer/33 obtained by determinizafion of/32.
d/0 a/4 Figure 25 Minimal transducer f14 obtained by reversing fig and applying determinization.
6~($2, w) E F ~.
Since T1 is subsequential, there exists only one state of T~ admitting a path labeled with w to il; that state is q.
Thus, q E $2.
Therefore any state q member of a pair of $1 is also member of a pair of $2.
By symmetry the reverse is also true.
Thus exactly the same states are members of the pairs of $1 and $2.
There exists k > 0 such that: Sl = {(qo, clo), (ql, c11) .....
(qk, clk)} S 2 ~{(qo, c20), (ql, c21) .....
(qk, C2k)} (29) We prove that weights are also the same in $1 and $2.
Let IIj, (0 > j > k), be the set of strings labeling the paths from il to qi in T1.
Crl(il, w) is the weight output corresponding to a string w E IIj.
Consider the accumulated weights cq, 1 < i < 2, 0 < j < k, in determinization of T~.
Each Clj for instance corresponds to the weight not yet output in the paths reaching $1.
It needs to be added to the weights of any path from qj c $1 to a final state in rev(T1).
In other terms, the determinization algorithm will assign the weight Clj qff1(/1, W) q.X1 to a path labeled with w a reaching a final state of T ~ from $1.
T" is obtained by pushing from T ~.
Therefore the weight of such 308 References 1 Alfred V.
Aho, John E.
Hopcroft, The Design and Analysis of Computer Algorithms, Addison-Wesley Longman Publishing Co., Inc., Boston, MA, 1974 2 Alfred V.
Aho, Ravi Sethi, Jeffrey D.
Ullman, Compilers: principles, techniques, and tools, Addison-Wesley Longman Publishing Co., Inc., Boston, MA, 1986 3 Ahuja, Ravindra K., Kurt Mehlhorn, James B.
Orlin, and Robert Tarjan.
1988. Faster algorithms for the shortest path problem.
Technical Report 193, MIT Operations Research Center.
4 Bauer, W.
1988. On minimizing finite automata.
EATCS bULLETIN, 35.
5 Berstel, Jean.
1979. Transductions and Context-Free Languages.
Teubner Studienbucher, Stuttgart.
6 Jean
Berstel, JJr., Christophe Reutenauer, Rational series and their languages, Springer-Verlag New York, Inc., New York, NY, 1988 7 Brzozowski, J.
A. 1962.
Canonical regular expressions and minimal state graphs for definite events.
Methematical Theory of Automata, 12:529--561.
8 Carlyle, J.
W. and A.
Paz. 1971.
Realizations by stochastic finite automaton.
Journal of Computer and System Sciences, 5:26--40.
9 Choffrut, Christian.
1978. Contributions  l'tude de quelques familles remarquables de functions rationnelles.
Ph.D. thesis, (thse de doctorat d'Etat), Universit Paris 7, LITP, Paris.
10 Thomas
T.
Cormen, Charles E.
Leiserson, Ronald L.
Rivest, Introduction to algorithms, MIT Press, Cambridge, MA, 1990 11 Courcelle, Bruno, Damian Niwinski, and Andreas Podelski.
1991. A geometrical view of the determinization and minimization of finite-state utomata.
Mathematical Systems Theory, 24:117--146.
12 Maxine
Crochemore, Transducers and repetitions, Theoretical Computer Science, v.45 n.1, p.63-86, Sept.
1986 13 Samuel Eilenberg, Automata, Languages, and Machines, Academic Press, Inc., Orlando, FL, 1976 14 Elgot, C.
C. and J.
E. Mezei.
1965. On relations defined by generalized finite automata.
IBM Journal of Research and Development, 9.
15 Ginsburg, S.
and G.
F. Rose.
1966. A characterization of machine mappings.
Canadian Journal of Mathematics, 18.
16 Gross, Maurice.
1989. The use of finite automata in the lexical representation of natural language.
Lecture Notes in Computer Science, 377.
17 John
E.
Hopcroft, Jeffrey D.
Ullman, Introduction To Automata Theory, Languages, And Computation, Addison-Wesley Longman Publishing Co., Inc., Boston, MA, 1990 18 Ronald M.
Kaplan, Martin Kay, Regular models of phonological rule systems, Computational Linguistics, v.20 n.3, September 1994 19 Fred Karlsson, Atro Voutilainen, Juha Heikkila, Arto Anttila, Constraint Grammar: A Language-Independent System for Parsing Unrestricted Text, Mouton de Gruyter, 1995 20 Lauri Karttunen, Ronald M.
Kaplan, Annie Zaenen, Two-level morphology with composition, Proceedings of the 14th conference on Computational linguistics, August 23-28, 1992, Nantes, France 21 Krob, daniel.
1994. The equality problem for rational series with multiplicities in the tropical semiring is undecidable.
Journal of Algebra and Computation, 4.
22 Werner Kuich, Arto Salomaa, Semirings, automata, languages, Springer-Verlag, London, 1985 23 Mehryar Mohri, Compact representations by finite-state transducers, Proceedings of the 32nd annual meeting on Association for Computational Linguistics, p.204-209, June 27-30, 1994, Las Cruces, New Mexico 24 Mehryar Mohri, Minimization of Sequential Transducers, Proceedings of the 5th Annual Symposium on Combinatorial Pattern Matching, p.151-163, June 05-08, 1994 25 Mohri, Mehryar.
1994c. On some applications of finite-state automata theory to natural language processing: Representation of morphological dictionaries, compaction, and indexation.
Technical Report IGM 94--22, Institut Gaspard Monge, Noisy-le-Grand.
26 Mohri, Mehryar.
1994b. Syntactic analysis by local grammars automata: An efficient algorithm.
In Proceedings of the International Conference on Computational Lexicography (COMPLEX94).
Linguistic Institute, Hungarian Academy of Science, Budapest, Hungary.
27 Mohri, Mehryar.
1995. Matching patterns of an automaton.
Lecture Notes in Computer Science, 937.
28 Mohri, Mehryar, 1996a.
On The Use of Sequential Transducers in Natural Language Processing.
In Yves Shabes, editor, Finite State Devices in Natural Language Processing.
MIT Press, Cambridge, MA.
To appear.
29 Mehryar Mohri, On some applications of finite-state automata theory to natural language processing, Natural Language Engineering, v.2 n.1, p.61-80, March 1996 30 Morhi, Mehryar, Fernando C.
N. Pereira, and Michael Riley.
1996. Weighted automata in text and speech processing.
In ECAI-96 Workshop, Budapest, Hungary.
ECAI. 31 Mehryar Mohri, Richard Sproat, An efficient compiler for weighted rewrite rules, Proceedings of the 34th annual meeting on Association for Computational Linguistics, p.231-238, June 24-27, 1996, Santa Cruz, California 32 Nerode, Anil.
1958. Linear automaton transformations.
In Proceedings of AMS, volume 9.
33 Pereira, Fernando C.
N. and Michael Riley, 1996.
Weighted Rational Transductions and their Application to Human Language Processing.
In Yves Shabes, editor, Finite State Devices in Natural Language Processing.
MIT Press, Cambridge, MA.
To appear.
34 Dominique Perrin, Finite automata, Handbook of theoretical computer science (vol.
B): formal models and semantics, MIT Press, Cambridge, MA, 1991 35 Christophe Reutenauer, Marcel Paul Schtzenberger, Varieties and rational functions, Theoretical Computer Science, v.145 n.1-2, p.229-240, July 10, 1995 36 Dominique Revuz, Minimisation of acyclic deterministic automata in linear time, Theoretical Computer Science, v.92 n.1, p.181-189, Jan.
6, 1992 37 Roche, Emmanuel.
1993. Analyse syntaxique transformationnelle du franais par transducteurs et lexique-grammaire.
Ph.D. thesis, Universit Paris 7, Paris.
38 Arto Salomaa, M.
Soittola, Automata: Theoretic Aspects of Formal Power Series, Springer-Verlag New York, Inc., Secaucus, NJ, 1978 39 Schtzenberger, Marcel Paul.
1961. On the definition of a family of automata.
Information and Control, 4.
40 Schtzenberger, Marcel Paul.
1977. Sur une variante des fonctions squentielles.
Theoretical Computer Science.
41 Schtzenberger, Marcel Paul.
1987. Polynomial decomposition of rational functions.
In Lecture Notes in Computer Science, volume 386.
Springer-Verlag, Berlin, Heidelberg, and New York.
42 Silberztein Max.
1993. Dictionnaires lectroniques et analyse automatique de textes: le systme INTEX.
Masson, Paris.
43 Simon, Imre.
1987. The nondeterministic complexity of finite automata.
technical Report RT-MAP-8073, Instituto de Matemtica e Estatistica da Universidade de So Paulo.
44 Sproat, Richard.
1995. A finite-state architecture for tokenization and grapheme-to-phoneme conversion in multilingual text analysis.
In Proceedings of the ACL SIGDAT Workshop, Dublin, Ireland.
ACL. 45 Mikkel Thorup, On RAM priority queues, Proceedings of the seventh annual ACM-SIAM symposium on Discrete algorithms, p.59-67, January 28-30, 1996, Atlanta, Georgia, United States 46 Urbanek, F.
1989. On minimizing finite automata.
EATCS Bulletin, 39.
47 Watson, Bruce W.
1993. A taxonomy of finite automata minimization algorithms.
Technical Report 93/44, Eindhoven University of Technology, The Netherlands.
48 Weber, Andreas and Reinhard Klemm.
1995. Economyof description for single-valued transducers.
Information and Computation, 119.
49 W.
A. Woods, Transition network grammars for natural language analysis, Communications of the ACM, v.13 n.10, p.591-606, Oct. 1970
Automatic Word Sense Discrimination Hinrich Schitze* Xerox Palo Alto Research Center This paper presents context-group discrimination, a disambiguation algorithm based on clustering.
Senses are interpreted as groups (or clusters) of similar contexts of the ambiguous word.
Words, contexts, and senses are represented in Word Space, a high-dimensional, real-valued space in which closeness corresponds to semantic similarity.
Similarity in Word Space is based on second-order co-occurrence: two tokens (or contexts) of the ambiguous word are assigned to the same sense cluster if the words they co-occur with in turn occur with similar words in a training corpus.
The algorithm is automatic and unsupervised in both training and application: senses are induced from a corpus without labeled training instances or other external knowledge sources.
The paper demonstrates good performance of context-group discrimination for a sample of natural and artificial ambiguous words.
1. Introduction Word sense disambiguation is the task of assigning sense labels to occurrences of an ambiguous word.
This problem can be divided into two subproblems: sense discrimination and sense labeling.
Sense discrimination divides the occurrences of a word into a number of classes by determining for any two occurrences whether they belong to the same sense or not.
Sense labeling assigns a sense to each class, and, in combination with sense discrimination, to each occurrence of the ambiguous word.
This view of disambiguation as a two-stage process may not be completely general (for example, it may not be appropriate for the iterative process by which a lexicographer arrives at the sense divisions of a dictionary entry), but it seems applicable to most work on disambiguation in computational linguistics.
In this paper, we will address the problem of sense discrimination as defined above.
That is, we will not be concerned with the sense-labeling component of word sense disambiguation.
Word sense discrimination is easier than full disambiguation since we need only determine which occurrences have the same meaning and not what the meaning actually is.
Focusing solely on word sense discrimination also liberates us of a serious constraint common to other work on word sense disambiguation.
If sense labeling is part of the task, an outside source of knowledge is necessary to define the senses.
Regardless of whether it takes the form of dictionaries (Lesk 1986; Guthrie et al.1991; Dagan, Itai, and Schwall 1991; Karov and Edelman 1996), thesauri (Yarowsky 1992; Walker and Amsler 1986), bilingual corpora (Brown et al.1991; Church and Gale 1991), or hand-labeled training sets (Hearst 1991; Leacock, Towell, and Voorhees 1993; Niwa and Nitta 1994; Bruce and Wiebe 1994), providing information for sense definitions can be a considerable burden.
What makes our approach unique is that, since we narrow the problem to sense discrimination, we can dispense of an outside source of knowledge for defining senses.
* Xerox Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CA 94304 Q 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 1 We therefore call our approach automatic word sense discrimination, since we do not require manually constructed sources of knowledge.
In many applications, word sense disambiguation must both discriminate and label occurrences; for example, in order to find the correct translation of an ambiguous word in machine translation or the right pronunciation in a text-to-speech system.
The application of interest to us is information access, i.e., making sense of and finding information in large text databases.
For many problems in information access, it is sufficient to solve the discrimination problem only.
In one study, we measured document-query similarity based on word senses rather than words and achieved a considerable improvement in ranking relevant documents ahead of nonrelevant documents (Schi.itze and Pedersen 1995).
Since the measurement of similarity is a systeminternal process, no reference to externally defined senses need be made.
Another potentially beneficial application of word sense discrimination in information access is the design of interfaces that take account of ambiguity.
If a user enters a query that contains an ambiguous word, a system capable of discrimination can give examples of the different senses of the word in the text database.
The user can then decide which sense was intended and only documents with the intended sense would be retrieved.
Again, a reference to external sense definitions is not required for this task.
The algorithm we propose in this paper is context-group discrimination.
1 Contextgroup
discrimination groups the occurrences of an ambiguous word into clusters, where clusters consist of contextually similar occurrences.
Words, contexts, and clusters are represented in a high-dimensional, real-valued vector space.
Context vectors capture the information present in second-order co-occurrence.
Instead of forming a context representation from the words that the ambiguous word directly occurs with in a particular context (first-order co-occurrence), we form the context representation from the words that these words in turn co-occur with in the training corpus.
Second-order co-occurrence information is less sparse and more robust than first-order information.
In context-group discrimination, the context of each occurrence of the ambiguous word in the training corpus is represented as a context vector formed from secondorder co-occurrence information.
The context vectors are then clustered into coherent groups such that occurrences judged similar according to second-order co-occurrence are assigned to the same cluster.
Clusters are represented by their centroids, the average of their elements.
An occurrence in a test text is disambiguated by computing the second-order representation of the relevant context, and assigning it to the cluster whose centroid is closest to that representation.
Since the choice of representation influences the formation of clusters, we will experiment with several representations in this paper, some involving a dimensionality reduction using singular value decomposition (SVD).
Context-group discrimination can be generalized to do a discrimination task that goes beyond the notion of sense that underlies many other contributions to the disambiguation literature.
If the ambiguous word's occurrences are clustered into a large number n of clusters (e.g., n = 10), then the clusters can capture fine contextual distinctions.
Consider the example of space.
For a small number of clusters, only the senses "outer space" and "limited extent in one, two, or three dimensions" are separated.
If the word's occurrences are clustered into more clusters, then finer distinctions such as the one between "office space" and "exhibition space" are also discovered.
Note that differences between sense entries in dictionaries are often similarly fine-grained.
1 The
basic idea of the algorithm was first described in Schi~tze (1992b).
98 Schiitze Automatic Word Sense Discrimination I WORD I TRAINING TEXT \[VECTORS I WORD SPACE ----T:xxOx!
~x \[\] xi/ L.
Xx~". '......x.
.......... ", TEST CONTEXT / Figure 1 The basic design of context-group discrimination.
Contexts of the ambiguous word in the training set are mapped to context vectors in Word Space (upper dashed arrow) by summing the vectors of the words in the context.
The context vectors are grouped into clusters (dotted lines) and represented by sense vectors, their centroids (squares).
A context of the ambiguous word ("test context") is disambiguated by mapping it to a context vector in Word Space (lower dashed arrow ending in circle).
The context is assigned to the sense with the closest sense vector (solid arrow).
Even if the contextual distinctions captured by generalized context-group discrimination do not line up perfectly with finer distinctions made in dictionaries, they still help characterize the contextual meaning in which the ambiguous word is used in a particular instance.
Such a characterization is useful for the information-access applications described above, among others.
The basic idea of context-group discrimination is to induce senses from contextual similarity.
There is some evidence that contextual similarity also plays a crucial role in human semantic categorization.
Miller and Charles (1991) found evidence in several experiments that humans determine the semantic similarity of words from the similarity of the contexts they are used in.
We hypothesize that, by extension, senses are also based on contextual similarity: a sense is a group of contextually similar occurrences of a word.
The following sections describe the disambiguation algorithm, our evaluation, and the results of the algorithm for a test set drawn from the New York Times News Wire, and discuss the relevance of our approach in the context of other work on word sense disambiguation.
2. Context-Group Discrimination Context-group discrimination groups a set of contextually similar occurrences of an ambiguous word into a cluster, which is then interpreted as a sense.
The particular implementation of this idea described here makes use of a high-dimensional, real-valued vector space.
Context-group discrimination is a corpus-based method: all representations are derived from a large text corpus.
The basic design of context-group discrimination is shown in Figure 1.
Each occurrence of the ambiguous word in the training set is mapped to a point in Word Space (shown for one example occurrence: see dashed line from training text to Word Space).
The mapping is based on word vectors that are looked up in Word Space (to be described below).
Once all training-text contexts have been mapped to Word Space, the resulting point cloud is clustered into groups of points such that points are close to each other in each group and that groups are as distant from each other as 99 Computational Linguistics Volume 24, Number 1 possible.
The resulting clusters are delimited by dotted lines in the figure.
Each cluster is assumed to correspond to a sense of the ambiguous word (an assumption to be evaluated later).
The representative of each group is its centroid, depicted as a square.
After training, a new occurrence of the ambiguous word (labeled "test context" in the figure) is disambiguated by mapping its context to Word Space (see lower dashed line; the context's point is depicted as a circle).
The context is then assigned to the context group whose centroid is closest (solid arrow).
Finally, the context is categorized as being a use of the sense corresponding to this context group.
There are three types of entities that we need to represent: words, contexts, and senses.
They are represented as word vectors, context vectors, and sense vectors, respectively.
Word vectors are derived from neighbors in the corpus, context vectors are derived from word vectors, and sense vectors are derived by way of clustering from the distribution of context vectors.
The representational medium of a vector space was chosen because of its wide acceptance in information retrieval (IR) (see, e.g., Salton and McGill \[1983\]).
The vectorspace model is arguably the most common framework in IR.
Systems based on it have ranked among the best in many evaluations of IR performance (Harman 1993).
The success of the vector-space model motivates us to use it for the representation of words.
We represent words in a space in which each dimension corresponds to a word, just as documents and queries are commonly represented in this space in IR.
Another approach to computing word similarity is the representation of words in a document space in which each dimension corresponds to a document (Lesk 1969; Salton 1971; Qiu and Frei 1993).
There are fewer occurrence-in-document than wordco-occurrence events, so these word representations tend to be more sparse and, arguably, less informative than word-based representations.
Word vectors have also been based on hand-encoded features (Gallant 1991) and dictionaries (Sparck-Jones 1986; Wilks et al.1990). Corpus-based methods like the one proposed here have the advantage that no manual labor is required and that a possible mismatch between a general dictionary and a specialized text (e.g., on chemistry) is avoided.
Finally, word similarity can be computed from structural features like head-modifier relationships (Grefenstette 1994b; Ruge 1992; Dagan, Marcus, and Markovitch 1993; Pereira, Tishby, and Lee 1993; Dagan, Pereira, and Lee 1994).
Like document-based representations, structure-based representations are sparser than those based on co-occurrence.
It is debatable whether structural features are more informative than associational features (Grefenstette 1992, 1996) or not (Schtitze and Pedersen 1997).
Approaches to word representation closely related to ours were proposed by Niwa and Nitta (1994) and Burgess and Lund (1997).
Instead of co-occurrence counts, vector entries are mutual information scores between the word that is to be represented and the dimension words, in Niwa and Nitta's approach.
The algorithms for vector derivation and sense discrimination are described in what follows.
2.1 Word
Vectors A vector for word w is derived from the close neighbors of w in the corpus.
Close neighbors are all words that co-occur with w in a sentence or a larger context.
In the simplest case, the vector has an entry for each word that occurs in the corpus.
The entry for word v in the vector for w records the number of times that word v occurs close to w in the corpus.
It is this representational vector space that we refer to as WOrd Space.
Figure 2 gives a schematic example of two words being represented in a twodimensional space.
The representation is based on the co-occurrence counts of a hypo100 Schiitze Automatic Word Sense Discrimination Table 1 Co-occurrence counts for four words in a hypothetical corpus.
The words legal and clothes are interpreted as dimensions in Figure 2, judge and robe as vectors.
Vector Dimension judge robe legal 300 133 clothes 75 200 LEGAL 300 JUDGE / 133 f f/ ROBE I I 75 200 CLOTHES Figure 2 The derivation of word vectors, judge and robe are represented as word vectors in a two-dimensional space with the dimensions 'legal' and 'clothes'.
Co-occurrence data are from Table 1.
thetical corpus in Table 1.
The word judge has a value of 300 on the dimension "legal" because judge and legal co-occur 300 times with each other (see below for which words are selected as dimensions; a word can be a dimension of Word Space and represented as a word vector in Word Space at the same time).
This vector representation captures the typical topic or subject matter of a word.
For example, words like judge and law are closer to the "legal" dimension; words like robe and tailor are closer to the "clothes" dimension.
By looking at the amount of overlap between two vectors, one can roughly determine how closely they are related semantically.
This is because related meanings are often expressed by similar sets of words.
Semantically related words will therefore co-occur with similar neighbors and their vectors will have considerable overlap.
This similarity can be measured by the cosine between two vectors.
The cosine is equivalent to the normalized correlation coefficient: corr( fi, ~ ) = ~iN=l ViWi where ff and ~ are vectors and N is the dimension of the vector space.
The value of the cosine is higher, the more overlap there is between the neighbors of the two words whose vectors are compared.
If two words occur with exactly the same neighbors 101 Computational Linguistics Volume 24, Number 1 (perfect overlap), then the value of the cosine is 1.0.
If there is no overlap at all, then the value of the cosine is 0.0.
The cosine can therefore be used as a rough measure of semantic relatedness between words.
What words should serve as the dimensions of Word Space?
We will experiment with two strategies: a global and a local one.
The local strategy focuses on the contexts of the ambiguous words and ignores the rest of the corpus.
The global strategy is to select the n most frequent words of the corpus as features and use them regardless of the word that is to be disambiguated.
(See Karov and Edelman \[1996\] for a different approach that selects features according to a combination of global frequency and local salience).
For local selection, we can also use a frequency cutoff.
As an alternative, we will test selection according to a X 2 test.
For the frequency-based selection criterion, the neighbors of the ambiguous word in the corpus are counted.
A neighbor is any word that occurs at a distance of at most 25 words from the ambiguous word (that is, in a 50word window centered on the ambiguous word).
The 1,000 most frequent neighbors are chosen as the dimensions of the space.
For the x2-based criterion, a x2-measure of dependence is applied to a contingency table containing the number of contexts of the ambiguous word in which the candidate word occurs (N++) and does not occur (N+_), and the number of contexts without an occurrence of the ambiguous word in which the candidate word occurs (N_+) and does not occur (N__).
X2 = N(N++N__ N+_N_+) 2 (N++ + N+_)(N_+ + N__)(N++ + N_+)(N+_ + N__) The underlying assumption in using the x2-test is that candidate words whose occurrence depends on whether the ambiguous word occurs will be indicative of one of the senses of the ambiguous word and hence useful for disambiguation.
2 After
1,000 words have been selected in local selection, word vectors are formed by collecting a 1,000-by-I, 000 matrix C, such that element cq records the number of times that words i and j co-occur in a window of size k.
Column n (or, equivalently, row n) of matrix C represents word n.
Note that C is symmetric since the words that are represented as word vectors are also those that form the dimensions of the 1,000dimensional space.
We chose a window size of k = 50 because no improvement of discrimination performance was found in Schfitze (1997) for k > 50.
For global selection, we choose the 20,000 most frequent words as features and the 2,000 most frequent words as dimensions of Word Space.
A global 20, 000-by-2, 000 co-occurrence matrix is derived from the corpus.
Association data were extracted from the training set consisting of 17 months of the New York Times News Service, June 1989 through October 1990.
The size of this set is about 435 megabytes and 60.5 million words.
Two months (November 1990 and May 1989; 46 megabytes, 5.4 million words) were set aside as a test set.
2.2 Context
Vectors The representation for words derived above conflates senses.
For example, both senses of the word suit ('lawsuit' and 'garment') are summed in its word vector, which will therefore be positioned somewhere between the 'legal' and 'clothes' dimensions in Figure 2.
We need to go back to individual contexts in the corpus to acquire information about sense distinctions.
Contexts are represented as context vectors in Word Space.
2 Candidate
words are selected after a list of 930 stopwords has been removed.
This stop list was based on the one used in the Text Data Base system (Cutting, Pedersen, and Halvorsen 1991).
102 Sch~tze Automatic Word Sense Discrimination LEGAL CENTROID LAW / ~JUDGE /iJ~ISTATUTE / I I /// /#SUIT /// I/~// I/1/ CLOTHES Figure 3 The derivation of context vectors.
A context vector is computed as the centroid of the words occurring in the context.
The words in this example context are law, judge, statute, and suit.
A context vector is the centroid (or sum) of the vectors of the words occurring in the context.
Figure 3 shows the context vector of an example context of suit containing the words law, judge, statute, and suit.
Note that the context vector is closer to the "legal' than to the 'clothes' dimension, thus capturing that the context is a 'legal' use of suit.
(The true sum of the four vectors is longer than shown.
Since all correlation coefficients are normalized, the length of a vector does not play a role in the computations).
The centroid "averages" the direction of a set of vectors.
If many of the words in a context have a strong component for one of the topics (like 'legal' in Figure 3), then the average of the vectors, the context vector, will also have a strong component for the topic.
Conversely, if only one or two words represent a particular topic, then the context vector will be weak on this component.
The context vector hence represents the strength of different topical or semantic components in a context.
In the computation of the context vector, we will weight a word vector according to its discriminating potential.
A rough measure of how well word wi discriminates between different topics is the log inverse document frequency used in information retrieval (Salton and Buckley 1990): ai = lg/~ / where ni is the number of documents that wi occurs in and N is the total number of documents.
Poor discriminators of topics are words such as idea or help that are relatively uniformly distributed and therefore have a high document frequency.
Good content discriminators like automobile or China have a bursty distribution (they have several occurrences in a short interval if they occur at all \[Church and Gale 1995\]), and therefore a low document frequency relative to their absolute frequency.
Other algorithms for computing context vectors have been proposed by Wilks et al.(1990) (based on dictionary entries), Gallant (1991) (based on hand-encoded semantic features), Grefenstette (1994b) (based on light parsing), and Niwa and Nitta (1994) (a comparison of dictionary-based and corpus-based context vectors).
103 Computational Linguistics Volume 24, Number 1 LEGAL SENSE 1 Cl..,,,7,s ~SENSE CLOTHES Figure 4 The derivation of sense vectors.
Sense vectors are derived by clustering the context vectors of an ambiguous word (here, cl, c2, c3, c4, c5, c6, c7, and cs), and computing sense vectors as the centroids of the resulting clusters.
The vectors SENSE 1 and SENSE 2 are the sense vectors of clusters {cl, c2, c3, c4} and {cs, c6, c7, cs}, respectively.
2.3 Sense
Vectors Sense representations are computed as groups of similar contexts.
All contexts of the ambiguous word are collected from the corpus.
For each context, a context vector is computed.
This set of context vectors is then clustered into a predetermined number of coherent clusters or context groups using Buckshot (Cutting et al.1992), a combination of the EM algorithm and agglomerative clustering.
The representation of a sense is simply the centroid of its cluster.
It marks the portion of the multidimensional space that is occupied by the cluster.
We chose the EM algorithm for clustering since it is guaranteed to converge on a locally optimal solution of the clustering problem.
In our case, the solution is optimal in that the sum of the squared distances between context vectors and their centroids will be minimal.
In other words, the centroids are optimal representatives for the context vectors in their cluster.
One problem with the EM algorithm is that it finds a solution that is only locally optimal.
It is therefore important to find a good starting point since a bad starting point will lead to a local minimum that is not globally optimal.
Some experimental evidence given below shows that cluster quality varies considerably depending on the initial parameters.
In order to find a good starting point, we use group-average agglomerative clustering (GAAC) on a sample of context vectors.
For each of the 2,000 clustering experiments described below, we first choose a random sample of 50.
This size is roughly equal to v'~, the number of context vectors to be clustered.
Since GAAC is of time complexity O(n2), this guarantees overall linear time complexity of the clustering procedure.
If the training set has more than 2,000 instances of the ambiguous word, 2,000 context vectors are selected randomly.
The centroids of the resulting clusters are then the parameters for the first iteration of EM.
We compute five iterations of the EM algorithm for all experiments since in most cases only a few, if any, context vectors were reassigned in the fifth iteration.
Both the EM algorithm and group-average agglomerative clustering are described in more detail in the appendix.
104 Schfitze Automatic Word Sense Discrimination An example is shown in Figure 4.
The clustering step has grouped context vectors cl, c2, c3, and c4 in the first group and c5, c6, c7, and c8 in the second group.
The sense vector of the first group is the centroid labeled SENSE 1, the sense vector of the second group the centroid labeled SENSE 2.
The result of clustering depends on the representation of context vectors.
For this reason, we also investigate a transformation of the multidimensional space via a singular value decomposition (SVD) (Golub and van Loan 1989).
SVD is a form of dimensionality reduction that finds the major axes of variation in Word Space.
Context vectors can then be represented by their values on these principal dimensions.
The motivation for applying SVD here is much the same as the use of Latent Semantic Indexing (LSI) in information retrieval (Deerwester et al.1990). LSI abstracts away from the surface word-based representation and detects underlying features.
When similarity is computed on these features (via cosine between SVD-reduced context vectors), contextual similarity can be, potentially, better measured than via cosine between unreduced context vectors.
The appendix defines SVD and gives an example matrix decomposition.
In this paper, the word vectors will be reduced to 100 dimensions.
The experiments reported in Schfitze (1992b, 1997) give evidence that reduction to this dimensionality does not decrease accuracy of sense discrimination.
Space requirements for context vectors are reduced to about 1/10 and 1/20 for a 1,000-dimensional and a 2,000dimensional Word Space, respectively.
Although most word vectors are sparse, context vectors are dense, since they are the sum of many word vectors.
Time efficiency is increased on the same order of magnitude when the correlation of context vectors and sense vectors is computed.
The computation of the SVD's in this paper took from a few minutes per word for the local feature set to about three hours for the global feature set.
2.4 Application
of Context-Group Discrimination Context-group discrimination uses word vectors and sense vectors as follows to discriminate occurrences of the ambiguous word.
For an occurrence t of the ambiguous word v:  Map t into its corresponding context vector ~ in Word Space using the vectors of the words in t's context (the lower dashed line in Figure 1).
 Retrieve all sense vectors ~j of v (the two points marked as squares in the figure).
 Assign t to the sense j whose sense vector ~j is closest to ~ (assignment shown as a solid arrow).
This algorithm selects the context group whose sense vector is closest to the context vector of the occurrence of the word that is to be disambiguated.
Context vectors and sense vectors capture semantic characteristics of the corresponding context and sense, respectively.
Consequently, the sense vector that is closest to the context vector has the best semantic match with the context.
Therefore, context-group discrimination categorizes the occurrence as belonging to that sense.
3. Evaluation We test context-group discrimination on the 10 natural ambiguous words that formed the test set in Schfitze (1992b) and on 10 artificial ambiguous words.
Table 2 glosses the major senses of the 20 words.
105 Computational Linguistics Volume 24, Number 1 Table 2 Number of occurrences of test words in training and test set, percent rare senses in test set, baseline performance (all occurrences assigned to most frequent sense), and the two main senses of each of the 20 artificial and natural ambiguous words used in the experiment.
Word Training Test.
Rare Senses Baseline Frequent Senses wide range/ consulting firm 1,422 149 0% 62% heart disease/ reserve board 1,197 115 0% 54% urban development/ cease fire 1,582 101 0% 50% drug administration / fernando valley 1,465 122 0% wide range consulting firm heart disease reserve board urban development cease fire 52% drug administration fernando valley economic development / right field 1,030 88 0% 68% national park/ judiciary committee 1,279 122 0% 70% japanese companies / city hall 1,569 208 0% 58% drug dealers / paine webber 1,183 104 0% 55% league baseball/ square feet 1,097 143 0% 66% pete rose/ nuclear power 1,245 103 0% 52% capital/s 13,015 200 2% 64% interest/s 21,374 200 4% 58% motion/s 2,705 200 0% 55% plant/s 12,833 200 0% 54% economic development right field national park judiciary committee japanese companies city hall drug dealers paine webber league baseball square feet pete rose nuclear power stock of goods seat of government a feeling of special attention a charge for borrowed money movement proposal for action a factory living being 106 Schiitze Automatic Word Sense Discrimination Table 2 Continued.
Word Training Test Rare Senses Baseline Frequent Senses ruling 5,482 200 3.5% 60% an authoritative decision to exert control, or influence space 9,136 200 0% 56% area, volume outer space suit/s 7,467 200 12.5% 57% an action or process in a court a set of garments tank/s 3,909 200 4.5% 90% a combat vehicle a receptacle for liquids train/s 4,271 200 1.5% 74% a line of railroad cars to teach vessel/s 1,618 144 13.9% 69% a ship or plane a tube or canal (as an artery) Artificial ambiguous words or pseudowords are a convenient means of testing disambiguation algorithms (Schtitze 1992a; Gale, Church, and Yarowsky 1992).
It is time-consuming to hand-label a large number of instances of an ambiguous word for evaluating the performance of a disambiguation algorithm.
Pseudowords circumvent this need: Two or more words, e.g., banana and door, are conflated into a new type: banana~door.
All occurrences of either word in the corpus are then replaced by the new type.
It is easy to evaluate disambiguation performance for pseudowords since one can go back to the original text to decide whether a correct decision was made.
To create the pseudowords shown in Table 2, all word pairs were extracted from the corpus, i.e., all pairs of words that occurred adjacent to each other in the corpus in a particular order.
All numbers were discarded, since numbers do not seem to involve sense ambiguity.
Pseudowords were then created by randomly drawing two pairs from those that had a frequency between 500 and 1,000 in the corpus.
Pseudowords were generated from pairs rather than simple words because pairs are less likely than words to be ambiguous themselves.
Pair-based pseudowords are therefore good examples of ambiguous words with two clearly distinct senses.
Table 2 indicates how often the ambiguous word occurred in the training and test sets, how many instances were instances of rare senses, and the baseline performance that is achieved by assigning all occurrences to the most frequent sense.
In the evaluation given here, only senses that account for at least 15% of the occurrences of the ambiguous word are taken into account.
Rare senses are those that account for fewer than 15% of the occurrences.
The words in Table 2 each had two frequent senses.
The frequency of rare senses ranges from 0% to 13.9%, with an average of 2.1%.
Rare senses are not eliminated from the training set.
The training and test sets were taken from the New York Times News Service as described above (training set: June 1989-October 1990; test set: November 1990, May 1989).
If a word had more than 200 occurrences in the test set, then only the first 200 occurrences were included in the evaluation.
The labeling of words in the test corpus was performed by the author.
The distinc107 Computational Linguistics Volume 24, Number 1 tions between the senses in Table 2 are intuitively clear.
For example, the probability of a context in which suit could at the same time refer to a set of garments and an action in court is very low.
Consequently, there were fewer than five instances where the appropriate sense was not obvious from the immediate context.
In these cases, the sense that seemed more plausible to the author was assigned.
It is important to evaluate on a test set that is separate from the training set.
Context-group discrimination is based on the distribution of context vectors in the training set.
The distribution in the training set is often a bad model for the distribution in the test set.
In practice, the intended text of application will be from a time period not covered in the training set (for example, newswire text from after the date of training).
Word distributions can change considerably over time.
The test set was therefore constructed to be from a time period different from the time period of the training set.
This is also the reason that we do not do cross-validation.
Cross-validation respecting the constraint that test and training sets be from different time periods would have required a test set several times larger than the one that was available.
Clustering and evaluating on the same set is also problematic because of sampling variation.
Consider the following example.
We have a set of three context vectors C : {C 1 : (1), c2 = (2), C 3 : (3)} in a one-dimensional space.
Contexts 1 and 2 are uses of sense 1, context 3 is a use of sense 2.
If C is used as both training and evaluation set, then average performance is 83% (with probability 0.5, we get centroids 1.5 and 3 and 100% accuracy, with probability 0.5, we get centroids 1 and 2.5 and 67% accuracy).
If we split C into a training set T of size 2 and a test set E of size 1, we get an average performance of 67% (100% for E = {cl}, 50% for E = {c2}, 50% for E = {3}), which is lower than 83%.
This example shows that conflating training and test set can result in artificially high performance.
An advantage of context-group discrimination is that the granularity of sense distinctions is an adjustable parameter of the algorithm.
Experiments run directly for the senses in Table 2 will test the algorithm's ability to discriminate coarse sense distinctions.
To test performance for fine-grained sense distinctions (e.g., 'office space' vs.
'exhibition space'), we will run two experiments, one that evaluates performance for clustering the context vectors of a word into ten clusters and an information retrieval experiment in which the number of clusters is also large for sufficiently frequent words.
The goal of the 10-cluster experiments is to induce more fine-grained sense distinctions than in the 2-cluster experiments.
However, it is harder to determine the ground truth for fine sense distinctions.
When it comes to fine distinctions, a large number of occurrences are indeterminate or compatible with several of the more finely individuated senses (cf.
Kilgarriff \[1993\]).
For this reason, experiments with a large number of clusters were evaluated using two indirect measures.
The first measure is accuracy for two-way discriminations, i.e., the degree to which each of the ten clusters contained only one of the two "coarse" senses.
This evaluation is indirect because a cluster that contains, say, only 'limited extent in one, two, or three dimensions' uses of space would be deemed 100% correct, yet it could be randomly mixed as far as fine sense distinctions are concerned (e.g., 'office space' vs.
'exhibition space').
The author inspected the data and found good separation of fine-grained senses in the 10-cluster experiments to the extent that the evaluation measure indicated good performance on the two-way discrimination task.
However, because of the above-mentioned subjectivity of judgements for fine sense distinctions, this is hard to quantify.
Results from a second evaluation on an information retrieval task will be presented in Section 4.2 below.
We will show that sense-based information retrieval (in which the relevance of documents to a query is determined using context-group discrimination) 108 Schiitze Automatic Word Sense Discrimination improves the performance of an IR system considerably.
Since the success of sensebased retrieval depends on the accuracy of context-group discrimination, we can infer that the algorithm reliably assigns ambiguous instances to induced senses even in the fine-grained case.
4. Experiments 4.1 Word Sense Discrimination Table 3 shows experimental results for context-group discrimination.
There were four conditions that were varied in the experiments (as described in Section 2):  local vs.
global feature selection  feature selection according to frequency vs.
X 2  term representations vs.
SVD-reduced representations  number of clusters (2 vs.
10) For local feature selection, the other three parameters are varied systematically (the first eight columns of Table 3).
For global feature selection, selection according to X 2 is not possible, since the X 2 test presupposes an event (like the occurrence of an ambiguous word) that the occurrence of candidate words depends on.
There is no such event for global feature selection.
A larger number of dimensions (2,000) is used for the global variant of the algorithm in order to get coverage of a large range of topics that might be relevant for disambiguation.
We therefore apply SVD in the global feature selection case.
Even if word vectors are sparse, context vectors are usually not.
Clustering 2,000dimensional vectors is computationally expensive, so that we only ran experiments with SVD-reduced vectors for the global variant.
Ten experiments with different randomly chosen initial parameters were run for each of the 200 combinations of the different levels of Word, Representation, and Clustering.
The mean percentage correctness and the standard deviation for each such set of 10 experiments is shown in the cells of Table 3.
We give mean and deviation of the percentage of correctly labeled occurrences of all instances in the training set ("total" = "t.'), of the instances of sense 1 ("$1") and of the instances of sense 2 ("$2").
The bottom row of the table gives averages of the total percentage correct numbers over the 20 words covered.
The rightmost column gives averages of the means over the 10 experiments.
We analyzed the results in Table 3 via analysis of variance (ANOVA, see, for example, Ott \[1992\]).
An ANOVA was performed for a 20 x 5 x 2 design with 10 replicates.
The factors were Word, Representation (local, frequency-based, terms; local, frequency-based, SVD; local, Xa-based, terms; local, x2-based, SVD; global, frequencybased, SVD), and Clustering (coarse = 2 clusters, fine = 10 clusters).
Percentages were transformed using the functionf(X) = 2 x sin -1 (v/X) as recommended by Winer (1971).
The transformed percentages have a distribution that is close to a normal distribution as required for the application of ANOVA.
We found that the effects of all three factors and all interactions was significant at the 0.001 level.
These effects are discussed in what follows.
Factor Word.
In general, performance for pseudowords is better than for natural words.
This can be explained by the fact that pseudowords have two focussed senses--the two word pairs they are composed of.
In contrast, some of the senses of natural ambiguous 109 Computational Linguistics Volume 24, Number 1 Table 3 Results of disambiguation experiments.
Rows give total accuracy for each word ("t').
as well as accuracy for the two senses separately ("$1", "$2").
The average in the bottom row is an average over total ("t').
accuracy numbers only.
Columns describe experimental conditions and the mean ("\]~") and standard deviation ("a") of 10 replications of each experiment.
The rightmost column contains an average over the mean values of the 10 experiments.
Pseudowords are abbreviated to the first words of pairs.
Local Global wide~consul.
$1 $2 55 16 100 0 69 31 92 9 74 25 92 13 69 24 82 10 89 6 94 4 t.
51 4 62 0 60 4 66 3 56 6 64 3 65 8 66 2 87 3 87 3 heart~reserve $1 66 0 78 11 100 0 99 4.
72 0 75 12 100 0 98 2 100 0 100 0 $2 100 0 90 7 100 0 100 0 100 0 94 5 98 0 100 1 100 0 100 0 t.
84 0:85 2 100 0 99 287 0 85 3 99 0 99 1 100 0 100 0 urban~cease $1 86 1 87 2 96 0 97 191 4 90 8 98 0 98 1 100 0 98 2 ~rugffern.
.ocon./right nat./jud.
iap./city $1 71rug/paine $1 !eague/square $1 pete~nuclear $1 :apital $1 interest $1,wtion $1 ~lant $1 ruling $1 ;pace $1 ~uit S1 ~ank $1 ~rain $1 vessel $1 Average X 2 Frequency Frequency Terms I SVD Terms I SVD SVD 2 10 2 10 2 10 2 10 2 10 45 16 0 0 45 47 22 22 25 26 19 30 59 37 39 17 84 2 76 8 41.4 81.6 66.4 88.8 98.2 93.8 94.1 $2 78 l J 70 7 100 0 100 1 73 24 80 11 100 0 96 5 100 0 100 0 89.7 t.
82 0i 79 3 98 0 99 1 I 82 10 85 3 99 0 97 2 100 0 99 1 92.0 S1 89 l i 87 7 98 0 100 1 I 94 4 88 5 98 0 95 1 100 0 100 0 94.9 $2 78 1 77 12 95 0 100 1 60 35 90 7 59 8 96 2 100 0 100 1 85.5 t.
84 I 82 3 97 0 100 0 78 15 89 2 80 4 96 1 100 0 100 0 90.6 $1 72 2 89 6 92 1 95 1 92 0 87 5 98 0 98 3 100 0 100 0 92.3 $2 89 0 67 13 96 0 96 2 87 2 91 5 96 0 97 2 100 0 100 0 91.9 t.
78 1 82 1 93 1 95 1 90 1 88 2 98 0 97 1 100 0 100 0 92.1 S1 91 1 96 3 98 0 97 0 99 0 99 0 98 01 97 1 100 0 100 0 97.5 $2 73 0 53 14 100 0 100 1 70 0 61 9 92 0i 96 4 100 0 98 2 84.3 t.
85 1 83 3 98 0 98 0 90 0 87 3 96 0i 97 1 100 0 99 1 93.3 84 18 90 7 96 1 95 1 94 2 91 4 97 2 93 2 99 0 99 1 93.8 i $2 56 10 63 15 71 23 87 4 66 17 71 10 88 5 90 5 99 0 99 1 79.0 t.
73 12 79 3 86 9 92 1 82 6 83 2 93 1 92 1 99 0 99 0 87.8 68 6 76 9 86 1 81 9 70 18 81 14 95 0 85 4 100 0 97 3 83.9 $2 86 13 86 8 100 0 99 1 68 23 87 14 100 0 98 3 100 0 100 0 92.4 t.
76 9 80 2 93 0 89 5 69 19 83 3 97 0 91 2 100 0 98 2 87.6 54 8 77 8 66 41 96 3 32 31 77 10 56 32 90 4 100 0 100 1 74.-----8 $2 60 20 94 3 100 0 99 1 91 18 94 5 100 0 96 4 100 0 99 2 93.3 t.
58 16 88 1 88 14 98 2 71 13 88 1 85 11 94 3 100 0 99 li 86.9 91 0 78 10 94 1 98 2 72 21 90 10 86 19 95 6 100 0 99 1 90.3 $2 78 0 80 8 94 0 91 2 96 1 81 13 88 20 91 7 100 0 99 1 I 89.8 t.
84 0 79 2 94 0 95 2 83 11 86 3 87 19 93 4 100 0 99 1 90.0 88 16 97 3 91 4 96 2 91 3 97 3 93 1 93 2 92 1 93 1 93.1 $2 27 23 36 11 23 34 87 7 36 34 57 9 80 27 88 6 96 1 89 5 61.9 t.
66 7 75 3 66 13 93 2 71 13 82 2 88 10 91 1 94 0 91 1; 81.7 82 18 77 8 95 1 86 5 96 0 93 3 94 1 91 4 96 0 89 3 89.9 $2 43 37 87 4 90 6 96 2 83 1 85 3 71 35 91 4 88 1 93 31 82.7 t.
66 14 81 4 93 2 90 2 90 0 90 1 84 15 91 2 93 0 91 li 86.9 57 14 72 6 58 1 84 1 61 17 88 6 90 15 93 4 85 1 91 5 I 77.-----------9 $2 60 15 70 10 97 0 91 8 58 20 63 16 51 24 77 7 88 13 71 151 72.6 t.
58 10 71 3 76 1 87 3 59 12 77 4 73 12 86 2 86 5 82 5 i 75.5 i 73 20 0 0 92 4 " 0 0 91 16 0 0 54 46 2 5 70 37 0 0' 38.-----2 $2 47 12 100 0 37 5 100 0 41 30 100 0 59 36 100 0 70 26 100 0 75.4 t.
59 8 54 0 63 4 54 0 64 11 54 0 56 7 55 2 70 13 54 0 58.3 75 1 61 13 84 2 71 14 81 1 65 15 79 7 79 13 85 0 82 3 76.2 $2 86 1 90 4 93 1 96 3 87 1 93 4 93 5 95 2 95 0 95 1 92.3 t.
82 0 78 3 90 1 86 4 84 0 82 4 88 1 89 4 91 0 90 1 86.0 10 25 48 30 0 0 48 22 15 25 38 24 16 25 51 15 8 25 54 16 28.8 $2 87 7 91 7 96 0 95 3 97 1 96 2 96 2 96 2 94 10 93 3 94.1 t.
53 7 72 9 54 0 74 8 61 11 71 10 60 12 76 6 56 5 75 6 65.2 83 1 77 5 80 2 85 6 81 2 84 7 94 2 88 8 95 0 83 6 85.0 $2 80 0 i 84 4 93 0 94 2 92 2 88 6 86 29 97 2 96 0 97 2 90.7 t.
82 1 I 80 2 85 1 89 3 86 1 86 2 91 12\] 92 4 95 0 89 3 87.5 29 91 7 6 80 8 32 13 88 5 12 14 86 29i~ 31 22 92 3 28 19 48.5 $2 94 15 100 0 92 95 1 100 0 87 5\] 99 2 84 1 99 2 94.9 4 99 0 t.
87 13 I 90 1 90 3 92 1 95 1 91 1 87 2i 92 1 85 1 92 2 90.1 60 21 100 0 74 16 100 0 89 20 100 0 95 81100 1 79 19 100 0 89.7 $2 40 21 0 0 12 20 0 0 18 29 0 0 8 21! 1 3 55 31 0 0 13.4 t.
55 10' 74 0 58 7 74 0 69 11 74 0 72 1' 74 0 73 8 74 0 69.7 84 18 86 14 100 0 99 1 85 30 90 7 20 42 94 2 30 48 79 5 76.7 $2 76 14 84 9 100 0 100 0 89 3 92 5 79 17 100 0 81 9 100 0 90.1 t.
79 15\] 85 2 100 0 100 0 88 11 91 2 61 14\] 98 1 65 13 93 1 ..
86.O i\[ 72.1 I 77.9 \[ 84.1 i 88.5 i 77.8 i 81.8 i 82.9 \[ 88.3 i 89.7 i 90.6 I\] 110 Schfitze Automatic Word Sense Discrimination Table 4 The Tukey W test shows significantly different performance for the five representations.
Proportions are transformed using fiX) = 2 x sin -1 (v/X).
The rightmost column contains the accuracy A in percent that would correspond to the average value Y in the second column (i.e., f(A) = Y).
Significant difference for a = 0.01:0.034 Average of Difference Corresponding Level 2 x sin-l(V'X) from Closest Accuracy local, )/2, terms 2.11 0.13 76% local, frequency, terms 2.24 0.13 81% local, frequency, SVD 2.44 0.06 88% local, X 2, SVD 2.50 0.06 90% global, frequency, SVD 2.66 0.16 94% words (for example, space and interest) are composed of many different subsenses that are hard to identify for both people and computers.
The only pseudoword with poor performance is wide range/consultingfirm.
This is an illustrative example of a weakness of the particular implementation of contextgroup discrimination chosen here.
Since we only rely on topical information, a word composed of a nontopical sense, like wide range, that can occur in almost any subject area is disambiguated poorly.
The 'area, volume' sense of space and the 'teaching' sense of train are similarly topically amorphous and therefore hard if only topical information is considered.
The poor performance for 'plant' in the 10-cluster experiments is probably due to the way training-set clusters were assigned to senses.
The training set was clustered into 20 clusters and each cluster was given a sense label.
This procedure introduces many misclassifications of individual instances in the training set.
In contrast, a performance of 92% was achieved in Schiitze (1992b) by hand-categorizing the training set, instance by instance.
Note that for some experimental conditions and for some words, performance of two-group clustering is below baseline.
In a completely unsupervised setting, we have to make the assumption that the two induced clusters correspond to two different senses.
In the worst case, we will get, two clusters with identical proportions of the two senses and an accuracy of 50%, below the baseline of assigning all occurrences to a sense that occurs in more than 50% of all cases.
For example, for vessel the worst case would be two clusters each with 69% 'ship' instances and 31% 'tube' instances.
Overall accuracy would be 0.5 x .69 + 0.5 x .31 = 0.5.
It could be argued that the true baseline for unsupervised two-group clustering is 50%, not the proportion of the most frequent sense.
Factor Representation.
A Tukey W test (Ott 1992) was performed to evaluate the factor Representation.
The Tukey W test determines the least significant difference between sample means.
That is, it yields a threshold such that if two levels of a factor differ by more than the threshold, then they are significantly different.
For the factor Representation, in our case, this least significant difference is 0.034 for a = 0.01.
Table 4 shows that all differences are significant.
This is evidence that SVD representations perform better than term representations and that global representations perform better than local representations.
The advantage of SVD representations is partly due to the use of a normality assumption in clustering.
This is a poor approximation for term 111 Computational Linguistics Volume 24, Number 1 Table 5 Occurrence of selected term features in the test set.
The table shows number of words occurring in the test set (averaged over the 20 ambiguous words); number of words occurring per context (averaged over contexts); proportion of words from one representation occurring in another (averaged first over contexts, then over ambiguous words; e.g., on average 91% of X2-selected terms were also in the set selected by local frequency); average number of contexts that a selected term occurred in (e.g., on average a xa-selected term occurred in 8.7 contexts of the artificial ambiguous words, averaged over the words in a context).
~2 Local Frequency Global Frequency Words Occurring in Test Set 283.0 571.2 489.6 Words per Context 6.1 11.1 9.2 Term Overlap X 2 100% 91% 53% local frequency 51% 100% 68% global frequency 34% 78% 100% Average Frequency of terms artificial words 8.7 6.7 16.7 natural words 39.5 22.4 17.5 representations, but is more accurate for SVD-reduced representations.
Why do globally selected features perform better?
Table 5 presents data on the occurrence of selected terms in the test set that are relevant to this question.
Note first that locally selected features seem to do better than globally selected ones on several measures.
More locally selected features occur in the test set ("words occurring in test set": 571.2 vs.
489.6), more local features occur in the individual contexts ("words per context": 11.1 vs.
9.2), and more global features are also local features than vice versa (on a per-context basis, 78% of global features are also local features, but only 68% of local features are also global features), suggesting that local features capture more information than global features.
The first two measures also show that X2-selected features suffer from sparseness.
Both the total number of features that occur in the training set and the number of words per context are small.
This evidence explains why SVD representations that address sparseness do better than term representations for X 2.
To explain the difference in performance between local and global frequency features, we have to break down average accuracy according to artificial and natural ambiguous words.
Average accuracy for artificial ambiguous words is 89.9% (2 clusters) and 92.2% (10 clusters) for local features and 98.6% (2 clusters) and 98.0% (10 clusters) for global features.
Average accuracy for natural ambiguous words is 76.0% (2 clusters) and 84.4% (10 clusters) for local features and 80.8% (2 clusters) and 83.1% (10 clusters) for global features.
These data show a clear split.
Performance of local and global features is comparable for natural ambiguous words.
Global features perform clearly better for artificial ambiguous words.
The last two rows of Table 5 explain this difference in behavior.
The numbers correspond to the average number of contexts that the selected features occur in (averaged first over the words in a context, then over contexts; e.g., a context with three selected terms occurring in 10, 3, and 15 contexts of the ambiguous word in the training set would have an average number of contexts of (10+3+15)/3 = 9.3).
These averages are 11.2 Schi~tze Automatic Word Sense Discrimination small for X 2 and local frequency in the case of artificial ambiguous words.
Clustering can only work well if contexts have enough elements in common so that similarity can be determined robustly.
Apparently, there were too few elements in common for X 2 and local frequency in the case of artificial ambiguous word (and the patterns were so sparse that even SVD was not an effective remedy).
The problem is that artificial ambiguous words are much less frequent in the training set than natural ambiguous words (average frequencies of 1,306.9 vs.
8,231.0), so that reliable feature selection is harder for artificial ambiguous words.
With ample information on natural ambiguous words available in the training set, features can be selected that will occur densely in the test set.
The quality of feature selection for artificial ambiguous words was less successful due to smaller training set sizes.
This analysis reiterates the importance of a clear separation of training and test sets.
Performance numbers will be artificially high if feature selection is done on both training and test sets, avoiding the problems with feature coverage demonstrated in Table 5.
Since global feature selection is simpler and as effective as local approaches, global feature selection is the preferred implementation of context-group discrimination in the general case.
Note, however, that different words may have different optimal representations.
For example, local features work best for vessel.
There are similar individual differences for frequency vs.
x2-based selection.
Frequency-based selection is best for suit, but x2-based selection is better for vessel, at least for SVD-reduced representations.
Factor Clustering.
Fine clustering is generally better than coarse clustering.
The one case for which coarse clustering comes close to the performance of fine clustering is global feature selection.
But this small difference is almost entirely due to the bad performance of fine clustering for plant, which is likely to be due to insufficient handcategorization of the training set, as explained above.
That fine clustering performs better than coarse clustering is not surprising, since more information is used in the evaluation of fine clustering: the labeling of clusters in the training set.
Only coarse clustering is evaluated as strictly unsupervised disambiguation, since we do not have an evaluation set for fine sense distinctions.
Variance. In general, the variance of discrimination accuracy is higher for coarse clustering than for fine clustering.
This is not surprising, given the fact that we evaluate both types of clustering on how well they do on a two-way distinction.
There may be several quite different ways of dividing a set of context vectors into two groups.
But if we first cluster into ten groups and assign these groups to two senses, then the resulting two-way partitions are more likely to resemble each other (even if the initial 10-group clusterings are not very similar).
The experiments indicate that context-group discrimination based on globally selected features is the best implementation in the general case.
The algorithm achieves above-baseline performance (with a small number of exceptions for certain parameter settings).
The average performance of the SVD-based representations of 83% to 91% is satisfactory, although inferior by about 5% to 10%, to disambiguation with minimal manual intervention (e.g., Yarowsky \[1995\]).
3 3 Manually supplied priming information about senses is not the only difference between context-group discrimination and other disambiguation algorithms.
Could one of the other differences be responsible for the difference in performance?
The fact that the error rate more than doubles when the seeds in Yarowsky's (1995) experiments are reduced from a sense's best collocations to just one word per sense suggests that the error rate would increase further if no seeds were provided.
113 Computational Linguistics Volume 24, Number 1 4.2 Application to Information Retrieval Our principal motivation for concentrating on the discrimination subtask is to apply disambiguation to information retrieval.
While there is evidence that ambiguity resolution improves the performance of IR systems (Krovetz and Croft 1992), several researchers have failed to achieve consistent experimental improvements for practically realistic rates of disambiguation accuracy.
Voorhees (1993) compared two term-expansion methods for information retrieval queries, one in which each term was expanded with all related terms and one in which it was only expanded with terms related to the sense used in the query.
She found that disambiguation did not improve the performance of term expansion.
In our study, we will use disambiguation to eliminate document-query matches that are due to sense mismatches (that is, the word in question is used in different types of context in the query and the document).
This approach decreases the number of documents that a query matches with whereas term expansion increases it.
Another important difference in this study is that longer queries are used.
Long queries (as they may arise in an IR system after relevance feedback) provide more context than the short queries Voorhees worked with in her experiments.
Sanderson (1994) modified a test collection by creating pseudowords similar to the ones used in this study.
He found that even unrealistically high rates of disambiguation accuracy had little or no effect on retrieval performance.
An analysis presented in Schfitze and Pedersen (1995) suggests that the main reason for the minor effect of disambiguation is that most of the pseudowords created in the study had a major sense that accounted for almost all occurrences of the pseudoword.
Creating this type of pseudoword amounts to adding a small amount of noise to an unambiguous word, which is not expbcted to have a large effect on retrieval performance.
To some extent, actual dictionary senses have the same property: one sense often accounts for a large proportion of occurrences.
However, this is not necessarily true when rare senses are not taken into account and when high-frequency senses are broken up into smaller groups (the example of 'office space' vs.
'exhibition space').
Large dictionaries tend to break up high-frequency senses into such more narrowly defined subsenses.
The successful use of disambiguation in our study may be due to the fact that rare senses, which are less likely to be useful in IR, are not taken into account and that frequent senses are further subdivided.
Good evidence for the potential utility of disambiguation in information retrieval was provided by Krovetz and Croft (1992).
They showed that there is a considerable amount of ambiguity even in technical text (which is often assumed to be less ambiguous than nonspecialized writing).
Many technical terms have nontechnical meanings that are used in addition to more specialized senses even in technical text (e.g., window and application in computer magazines, convertible in automobile magazines \[Krovetz 1997\]).
Krovetz and Croft also showed that sense mismatches (i.e., spurious matching words that were used in different senses in query and document) occurred significantly more often in nonrelevant than in relevant documents.
This suggests that eliminating spurious matches could improve the separation between nonrelevant and relevant documents and hence the overall quality of retrieval results.
In order to show that context-group discrimination is an approach to disambiguation that is beneficial in information retrieval, we will now summarize the experiment presented in Schfitze and Pedersen (1995).
That experiment evaluates sense-based retrieval, a modification of the standard vector-space model in information retrieval.
(We refer to the standard vector-space model as word-based retrieval).
In word-based retrieval, documents and queries are represented as vectors in a multidimensional space in which each dimension corresponds to a word (similar to the way that we repre114 Schi~tze Automatic Word Sense Discrimination sent word vectors in Word Space).
In sense-based retrieval, documents and queries are also represented in a multidimensional space, but its dimensions are senses, not words.
Words are disambiguated using context-group discrimination.
Documents and queries that contain a word assigned to a particular sense have a nonzero value on the corresponding dimension.
The test corpus in Sch~tze and Pedersen (1995) is the Category B TREC-1 collection (about 170,000 documents from the Wall Street Journal) in conjunction with its queries 51-75 (Harman 1993).
Sense-based retrieval improved average precision by 7.4% when compared to word-based retrieval.
A combination of word-based and sense-based retrieval increased performance by 14.4%.
The greater improvement of the combination is probably due to discrimination errors (i.e., the fact that discrimination is less than 100% correct), which are partially undone by combining sense and word evidence.
Improvement was particularly high when small sets of documents were requested, for example, 16.5% (sense-based) and 19.4% (wordand sense-based combined) for a recall level of 10% of relevant documents.
This experiment suggests a high utility of sense discrimination for information retrieval.
At first sight, sense-based retrieval may seem related to term expansion.
Both sense-based retrieval and term expansion take individual terms as the starting point for modifying the similarity measure that determines which documents are deemed most closely related to the query.
However, the two approaches are actually opposites of each other in the following sense.
Term expansion increases the number of matching documents for a query.
For example, if the query contains cosmonaut and expansion adds astronaut, then documents containing astronaut become additional nonzero matches.
Sense-based retrieval decreases the number of matches.
For example, if the word suit occurs in the query and is disambiguated as being used in the 'legal' sense, then documents that contain suit in a different sense will no longer match with the query.
5. Discussion What distinguishes context-group discrimination from other work on disambiguation is that no outside source of information need be supplied as input to the algorithm.
Other disambiguation algorithms employ various sources of information.
Kelly and Stone (1975) consider hand-constructed disambiguation rules; Lesk (1986), Krovetz and Croft (1989), Guthrie et al.(1991), and Karov and Edelman (1996) use on-line dictionaries; Hirst (1987) constructs knowledge bases; Cottrell (1989) uses syntactic and semantic structure encoded in a connectionist net; Brown et al.(1991) and Church and Gale (1991) exploit bilingual corpora; Dagan, Itai, and Schwall (1991) use a bilingual dictionary; Hearst (1991), Leacock, Towell, and Voorhees (1993), Niwa and Nitta (1994), and Bruce and Wiebe (1994) exploit a hand-labeled training set; and Yarowsky (1992) and Walker and Amsler (1986) perform computations based on a hand-constructed semantic categorization of words (Roget's Thesaurus and Longman's subject codes, respectively).
For some of these algorithms, the expense of supplying information to the disambiguation algorithm is relatively small.
For example, in many of the methods using hand-labeled training sets (e.g., Hearst \[1991\]), a relatively small number of training examples is sufficient.
Yarowsky has proposed an algorithm that requires as little user input as one seed word per sense to start the training process (Yarowsky 1995).
Such minimal user input will be a negligible burden for users in some situations.
However, consider the interactive information-access application described above.
When asked to improve their initial ambiguous information request many users will be reluctant to 115 Computational Linguistics Volume 24, Number 1 give a seed word or a set of good features for each sense of the word.
They are more likely to satisfy a request by the system to choose the correct sense (e.g., by mouse click), if example contexts corresponding to different senses are presented without the requirement of additional user interaction.
In an application like this, it is of great advantage that context-group discrimination does not require any manual intervention to induce senses.
Another body of related work is the literature on word clustering in computational linguistics (Brown et al.1992; Finch 1993; Pereira, Tishby, and Lee 1993; Grefenstette 1994a) and document clustering in information retrieval (van Rijsbergen 1979; Willett 1988; Sparck-Jones 1991; Cutting et al.1992). In contrast to this earlier work, we cluster contexts or, equivalently, word tokens here, not words (or, more precisely, word types) or documents.
The straightforward extension of word-type clustering and document clustering to word-token clustering would be to represent a token by all words it cooccurs with in its context and cluster these representations.
Such an approach based on first-order co-occurrence is used, for example, by Hearst and Plaunt (1993) for the representation of tiles or document subunits that are similar to our notion of context.
Instead, we use second-order co-occurrence to represent the tokens of ambiguous words: the words that occur with the token are in turn looked up in the training corpus and the words they co-occur with are used to represent the token.
Secondorder representations are less sparse and more robust than first-order representations.
In a cluster-based approach, the subdivision of the universe of elements into clusters depends on the representation.
If the representation does not capture the information crucial for distinguishing senses, then context-group discrimination performs poorly.
The clearest such example in the above experiments is the pseudoword wide range~consulting firm.
The algorithm does not do better than the baseline of always choosing the most frequent sense.
The reason is that the representation captures only topic information.
So a cluster will contain a group of contexts that are about the same topic.
Unfortunately, the pair wide range can come up in text about almost any topic.
Since there is no clear topical characterization of one sense of the pseudoword, context-group discrimination performs poorly.
The reliance on topical similarity may also be the reason that performance for pseudowords is generally better than performance for natural ambiguous words.
All pseudowords except for wide range/consultingJirm are composed of two pairs from different topics.
For example, heart disease and reserve board pertain to biology and finance, respectively, two clearly distinct topics.
On the other hand, the senses of some of the ambiguous words have less clear associations with particular topics.
For example, one can be trained to perform a wide variety of activities, so the 'teaching' sense of train can be invoked in many different topics.
Part of the superior performance for pseudowords is due to this different topic sensitivity of natural and artificial ambiguous words.
The limitation to topical distinctions is not so much a flaw of context-group discrimination as a flaw of the particular implementation we have presented here.
It is possible to integrate information in the context vectors that reflect syntactic or subcategorization behavior of different senses, such as the output of a shallow parser as used in Pereira, Tishby, and Lee (1993).
For example, one good indicator of the two senses of the word interest is a preposition occurring to its right.
The phrase interest in invokes the 'feeling of attention' sense, the phrase interest on, the sense 'charge on borrowed money'.
It seems plausible that performance could be improved for words whose senses are less sensitive to topical distinctions if such "proximity" information is integrated.
In some recent experiments, Pedersen and Bruce (1997) have used proximity features (tags of close words and the presence or absence of close functions words 116 Schfttze Automatic Word Sense Discrimination and content words) with some promising results.
This suggests that a combination of the topical features used here and proximity features may give optimal performance of context-group discrimination.
4 We
have used only one source of information (topical features) in the interest of simplicity, not because we see any inherent advantage of topical features compared to a combination of multiple sources of evidence.
Our justification for the basic idea of context-group discrimination, inducing senses from contextual similarity, has been that its results seem to align well with the ground truth of senses defined in dictionaries.
However, there is also some evidence that contextual similarity plays a crucial role in human semantic categorization.
In one study, Miller and Charles (1991) found evidence that human subjects determine the semantic similarity of words from the similarity of the contexts they are used in.
They summarized this result in the following hypothesis: Strong Contextual Hypothesis: Two words are semantically similar to the extent that their contextual representations are similar.
(p. 8) A contextual representation of a word is knowledge of how that word is used.
The hypothesis states that semantic similarity is determined by the degree of similarity of the sets of contexts that the two words can be used in.
The hypothesis that underlies context-group discrimination is an extension of the Strong Contextual Hypothesis to senses: Contextual Hypothesis for Senses: Two occurrences of an ambiguous word belong to the same sense to the extent that their contextual representations are similar.
So a sense is simply a group of occurrence tokens with similar contexts.
The analogy between the contextual hypotheses for words and senses is that both word types and word tokens are semantically similar to the extent that their contexts are semantically similar.
A group of contextually similar word tokens is a sense.
Miller and Charles's work thus provides a justification for our framework, the induction of senses from contextual similarity.
There are several issues that need to be addressed in future work on context-group discrimination.
First, our experiments only considered words with two major senses.
The algorithm also needs to be tested for words with more than two frequent senses and for infrequent senses.
Second, our test set consisted of a relatively small number of natural ambiguous words.
This is a flaw of almost all contemporary work on word sense disambiguation, but in the future more extensive test sets will be required to establish the general applicability of disambiguation algorithms.
Finally, the implementation of context-group discrimination proposed here is based on topical similarity only.
It will be necessary to incorporate other, more structural constraints (such as the interest in vs.
interest on case discussed above) to achieve adequate performance for a wide variety of ambiguous words.
Appendix A: Singular Value Decomposition A singular value decomposition factors an m-by-n matrix A into a product of three matrices: (,)A = U diag (o1,...,o.p)V T 4 See Leacock (1993) for a discussion of proximity and topical features in supervised disambiguation.
117 Computational Linguistics Volume 24, Number 1 Table 6 Co-occurrence counts for eight words in a five-dimensional word space.
judge suit robe gangster criminal police gun bail legal 300 210 133 30 200 160 120 150 clothes 75 182 200 10 5 10 20 15 cop 100 75 25 250 10 140 200 160 fashion 5 100 200 5 5 5 5 5 pants 5 110 190 5 5 5 5 5 Table 7 SVD reduction to two dimensions of the matrix in Table 6.
judge suit robe gangster criminal police gun bail dim1 -0.47 -0.46 -0.41 -0.22 -0.31 -0.30 -0.30 -0.30 dim2 0.13 -0.31 -0.69 0.41 0.05 0.25 0.33 0.28 Table 8 Correlation coefficients of three words before and after SVD dimensionality reduction.
criminal robe Word Space SVD Space Word Space SVD Space gangster 0.17 0.61 0.15 -0.52 criminal 0.41 0.37 where p = minm, n}, U (the left matrix) is an orthonormal m-by-p matrix, V (the right matrix) is an orthonormal n-by-p matrix and diag(o.1 ....., o.p) is a matrix with the diagonal elements o.1 _> o'2 > "" _> o.p ~_ 0 (and the value zero for nondiagonal elements) (Golub and van Loan 1989).
Dimensionality reduction can be based on SVD by keeping only the first k singular values o.1    c~k and setting the remaining ones to zero.
It can be shown that the product A' = U diag(o'l ..... o.k)V T is the closest approximation to A in a k-dimensional space (that is, there is no matrix of rank k with a smaller least-square distance to A than A').
See Golub and van Loan (1989) and Berry (1992) for a detailed description of SVD and efficient algorithms to compute it.
The benefits of dimensionality reduction for our purposes can best be explained using an example.
Table 6 shows co-occurrence counts from a hypothetical corpus (e.g., legal and robe co-occur 133 times with each other).
Note that two semantically similar words, gangster and criminal, have a low correlation in the words they co-occur with because they belong to different registers (this is one of reasons that topically similar words can have few neighbors in common).
Table 7 shows the two columns of the right matrix V of the SVD of the matrix in Table 6.
Table 7 is therefore a dimensionality reduction of Table 6 to two dimensions.
The advantage of the reduced space is that it directly represents the similar topicality of gangster and criminal: their vectors are close to each other in the space, as shown in Figure 5.
On the other hand, both words' vectors 118 Schiitze Automatic Word Sense Discrimination DIMENSION 2 GANGSTER CRIMINA L ROBE r DIMENSION 1 Figure 5 The vectors for robe, gangster, and criminal in the reduced SVD space.
The words gangster and criminal are represented as semantically similar.
Both are represented as semantically dissimilar from robe.
are less correlated with a topically dissimilar word like robe in the reduced space.
The correlation coefficients of the three words are shown in Table 8 for the unreduced and the reduced space.
The correlation of the topically related words (gangster and criminal) increases from 0.17 to 0.61, whereas the correlation of both words with robe decreases.
This example demonstrates the effect of SVD dimensionality reduction: topically similar words are projected closer to each other in the reduced space; topically dissimilar words are projected to distant locations.
Part of the motivation for using SVD for word vectors is the success of latent semantic indexing (LSI) in information retrieval (Deerwester et al.1990). LSI projects topically similar documents to close locations in the reduced space, just as we project topically similar words to close locations.
Appendix B: The EM Algorithm The clustering algorithm used in this paper is the EM algorithm.
The observed data (context vectors in our case) are interpreted as being generated by hidden causes, the clusters.
The EM algorithm is an iterative procedure that, starting from an initial hypothesis of the cluster parameters, improves the estimates of the parameters in each iteration.
We follow here the discussion and notation in Dempster, Laird, and Rubin (1977) and Ghahramani (1994).
We make the assumption that each cluster j is a Gaussian source with density ~j: ~j(~) exp\[ where \]/j is the mean and Gj the covariance matrix of a;j.
We write Oj = (fij, Gj) for the parameters of cluster j.
119 Computational Linguistics Volume 24, Number 1 Assume that we have N d-dimensional context vectors,g = {Xl ...
XN} C T4 d generated by M Gaussians COl...
CVM. The EM algorithm iteratively applies the Expectation step (E step) and the Maximization step (M step).
The E step is the estimation of parameters hq where hq is the probability of event zij, the event that cluster j generated Xi (context vector i).
hij = E(zij I ~i; O k) = O k is 0 at iteration k.
P(~ I ~J ;0~) .p(~l~j;o k) -~j(~;)P(~j) G~ P(~ I ~,; o~) The M step computes the most likely parameters of the distribution given the cluster membership probabilities: ~\]i=1 /j k+l E/N1 hq(2:i lif)(Y:i 11~) T ~j = ~N=lhq These are the well-known maximum-likelihood estimates for mean and variance of a Gaussian.
Recomputed means and variances are the parameters for the next iteration k+l.
For reasons of computational efficiency, we chose the implementation of the EM clustering known as k-means or hard clustering (Duda and Hart 1973).
In each iteration, context vectors are first assigned to the cluster with the closest mean; then cluster means are recomputed as the centroid of all members of the cluster.
This amounts to assuming a very small fixed variance for all clusters and only re-estimating the means in each step.
The initial cluster parameters are computed by applying group-average agglomerative clustering to a sample of size v'N.
Appendix C: Agglomerative Clustering Agglomerative clustering is a clustering technique that starts by assigning each element to a different cluster and then iteratively merges clusters according to a goodness criterion until the desired number of clusters has been reached.
Two such goodness measures give rise to single-link clustering and complete-link clustering.
Single-link clustering in each step merges the two clusters that have two elements with the smallest distance of any two clusters.
Complete-link clustering in each step executes the merger whose resulting cluster has the smallest diameter of all possible mergers.
Single-link clustering has been found in practice to produce elongated clusters (e.g., two parallel lines) that do not correspond well to the intuitive notion of a cluster as a mass of points with a center.
Complete-link clustering is strongly affected by outliers and has a time complexity cubic in the number of points to be merged and, hence, is less efficient than single-link clustering (which can be computed in quadratic time).
In this paper, we chose group-average agglomerative clustering (GAAC) as our clustering algorithm, a hybrid of single-link and complete-link clustering.
GAAC in each iteration executes the merger that gives rise to the cluster F with the largest average correlation C(P): 1 1 C(P) 2 IPl(\[rl1) ~ ~ corr(~,7~) ~cF/~cP 120 References 1 Berry, Michael W.
1992. Large-scale sparse singular value computations.
The International Journal of Supercomputer Applications, 6(1):13--49.
2 Peter
F.
Brown, Stephen A.
Della Pietra, Vincent J.
Della Pietra, Robert L.
Mercer, Word-sense disambiguation using statistical methods, Proceedings of the 29th annual meeting on Association for Computational Linguistics, p.264-270, June 18-21, 1991, Berkeley, California 3 Peter F.
Brown, Peter V.
deSouza, Robert L.
Mercer, Vincent J.
Della Pietra, Jenifer C.
Lai, Class-based n-gram models of natural language, Computational Linguistics, v.18 n.4, p.467-479, December 1992 4 Rebecca Bruce, Janyce Wiebe, Word-sense disambiguation using decomposable models, Proceedings of the 32nd annual meeting on Association for Computational Linguistics, p.139-146, June 27-30, 1994, Las Cruces, New Mexico 5 Burgess, Curt and Kevin Lund.
1997. Modelling parsing constraints with high-dimensional context space.
Language and Cognitive Processes, 12.
To appear.
6 Church, Kenneth W.
and William A.
Gale. 1991.
Concordances for parallel text.
In Proceedings of the Seventh Annual Conference of the UW Centre for the New OED and Text Research, pages 40--62, Oxford, England.
7 Church, Kenneth and William Gale.
1995. Poisson mixtures.
Journal of Natural Language Engineering, 1(2):163--190.
8 Garrison
W.
Cottrell, A connectionist approach to word sense disambiguation, Morgan Kaufmann Publishers Inc., San Francisco, CA, 1989 9 Douglass R.
Cutting, David R.
Karger, Jan O.
Pedersen, Constant interaction-time scatter/gather browsing of very large document collections, Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval, p.126-134, June 27-July 01, 1993, Pittsburgh, Pennsylvania, United States 10 Cutting, Douglass R., Jan O.
Pedersen, and Per-Kristian Halvorsen.
1991. An object-oriented architecture for text retrieval.
In Proceedings of RIAO'91, pages 285--298, Barcelona, Spain.
11 Douglass
R.
Cutting, David R.
Karger, Jan O.
Pedersen, John W.
Tukey, Scatter/Gather: a cluster-based approach to browsing large document collections, Proceedings of the 15th annual international ACM SIGIR conference on Research and development in information retrieval, p.318-329, June 21-24, 1992, Copenhagen, Denmark 12 Ido Dagan, Alon Itai, Ulrike Schwall, Two languages are more informative than one, Proceedings of the 29th annual meeting on Association for Computational Linguistics, p.130-137, June 18-21, 1991, Berkeley, California 13 Ido Dagan, Shaul Marcus, Shaul Markovitch, Contextual word similarity and estimation from sparse data, Proceedings of the 31st annual meeting on Association for Computational Linguistics, p.164-171, June 22-26, 1993, Columbus, Ohio 14 Ido Dagan, Fernando Pereira, Lillian Lee, Similarity-based estimation of word cooccurrence probabilities, Proceedings of the 32nd annual meeting on Association for Computational Linguistics, p.272-278, June 27-30, 1994, Las Cruces, New Mexico 15 Deerwester, Scott, Susan T.
Dumais, George W.
Furnas, Thomas K.
Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis.
Journal of the American Society for Information Science, 41(6):391--407.
16 Dempster, A.
P., N.
M. Laird, and D.
B. Rubin.
1977. Maximum likelihood from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society, Series B, 39:1--38.
17 Richard
O.
Duda, Peter E.
Hart, David G.
Stork, Pattern Classification (2nd Edition), Wiley-Interscience, 2000 18 Finch, Steven Paul.
1993. Finding Structure in Language.
Ph.D. thesis, University of Edinburgh.
19 Gale, William A., Kenneth W.
Church, and David Yarowsky.
1992. Work on statistical methods for word sense disambiguation.
In Robert Goldman, Peter Norvig, Eugene Charniak, and Bill Gale, editors, Working Notes of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language, pages 54--60, AAAI Press, Menlo Park, CA.
20 Stephen I.
Gallant, A practical approach for representing context and for performing word sense disambiguation using neural networks, Neural Computation, v.3 n.3, p.293-309, Fall 1991 21 Ghahramani, Zoubin.
1994. Solving inverse problems using an EM approach to density estimation.
In Michael C.
Mozer, Paul Smolensky, David S.
Touretzky, and Andreas S.
Weigend, editors, Proceedings of the 1993 Connectionist Models Summer School, Erlbaum Associates, Hillsdale, NJ.
22 Golub, Gene H.
and Charles F.
van Loan.
1989. Matrix Computations.
The Johns Hopkins University Press, Baltimore and London.
23 Gregory Grefenstette, Use of syntactic context to produce term association lists for text retrieval, Proceedings of the 15th annual international ACM SIGIR conference on Research and development in information retrieval, p.89-97, June 21-24, 1992, Copenhagen, Denmark 24 Grefenstette, Gregory.
1994a. Corpus-derived first, second and third-order word affinities.
In Proceedings of the Sixth Euralex International Congress, Amsterdam.
25 Gregory Grefenstette, Explorations in Automatic Thesaurus Discovery, Kluwer Academic Publishers, Norwell, MA, 1994 26 Gregory Grefenstetti, Evaluation techniques for automatic semantic extraction: comparing syntactic and window based approaches, Corpus processing for lexical acquisition, MIT Press, Cambridge, MA, 1996 27 Joe A.
Guthrie, Louise Guthrie, Yorick Wilks, Homa Aidinejad, Subject-dependent co-occurrence and word sense disambiguation, Proceedings of the 29th annual meeting on Association for Computational Linguistics, p.146-152, June 18-21, 1991, Berkeley, California 28 Harman, D.
K., editor.
1993. The First Text REtrieval Conference (TREC-1).
U.S. Department of Commerce, Washington, DC.
NIST Special Publication 500--207.
29 Hearst, Marti A.
1991. Noun homograph disambiguation using local context in large text corpora.
In Proceedings of the Seventh Annual Conference of the UW Centre for the New OED and Text Research: Using Corpora, pages 1--22, Oxford.
30 Marti A.
Hearst, Christian Plaunt, Subtopic structuring for full-length document access, Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval, p.59-68, June 27-July 01, 1993, Pittsburgh, Pennsylvania, United States 31 Graeme Hirst, Semantic interpretation and the resolution of ambiguity, Cambridge University Press, New York, NY, 1987 32 Anil K.
Jain, Richard C.
Dubes, Algorithms for clustering data, Prentice-Hall, Inc., Upper Saddle River, NJ, 1988 33 Karov, Yael and Shimon Edelman.
1996. Learning similarity-based word sense disambiguation from sparse data.
In Proceedings of the Fourth Workshop on Very Large Corpora.
34 Kelly, Edward and Phillip Stone.
1975. Computer Recognition of English Word Senses.
North-Holland, Amsterdam.
35 Kilgarriff, Adam.
1993. Dictionary word sense distinctions: An enquiry into their nature.
Computers and the Humanities, 26:365--387.
36 Robert Krovetz, Homonymy and polysemy in information retrieval, Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics, p.72-79, July 07-12, 1997, Madrid, Spain 37 R.
Krovetz, W.
B. Croft, Word sense disambiguation using machine-readable dictionaries, Proceedings of the 12th annual international ACM SIGIR conference on Research and development in information retrieval, p.127-136, June 25-28, 1989, Cambridge, Massachusetts, United States 38 Robert Krovetz, W.
Bruce Croft, Lexical ambiguity and information retrieval, ACM Transactions on Information Systems (TOIS), v.10 n.2, p.115-141, April 1992 39 Leacock, Claudia, Geoffrey Towell, and Ellen Voorhees.
1993. Towards building contextual representations of word senses using statistical models.
In Branimir Boguraev and James Pustejovsky, editors, Acquisition of Lexical Knowledge From Text: Workshop Proceedings, pages 10--21, Ohio.
40 Claudia Leacock, Geoffrey Towell, Ellen Voorhees, Corpus-based statistical sense resolution, Proceedings of the workshop on Human Language Technology, March 21-24, 1993, Princeton, New Jersey 41 Lesk, M.
E. 1969.
Word-word association in document retrieval systems.
American Documentation, 20(1):27--38.
42 Michael Lesk, Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone, Proceedings of the 5th annual international conference on Systems documentation, p.24-26, June 1986, Toronto, Ontario, Canada 43 Miller, George A.
and Walter G.
Charles. 1991.
Contextual correlates of semantic similarity.
Language and Cognitive Processes, 6(1):1--28.
44 Yoshiki Niwa, Yoshihiko Nitta, Co-occurrence vectors from corpora vs.
distance vectors from dictionaries, Proceedings of the 15th conference on Computational linguistics, August 05-09, 1994, Kyoto, Japan 45 Ott, Lyman.
1992. An Introduction to Statistical Methods and Data Analysis.
Wadsworth, Belmont, CA.
46 Pedersen, Ted and Rebecca Bruce.
1997. Distinguishing word senses in untagged text.
In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, pages 197--207, Providence, RI.
47 Fernando Pereira, Naftali Tishby, Lillian Lee, Distributional clustering of English words, Proceedings of the 31st annual meeting on Association for Computational Linguistics, p.183-190, June 22-26, 1993, Columbus, Ohio 48 Yonggang Qiu, Hans-Peter Frei, Concept based query expansion, Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval, p.160-169, June 27-July 01, 1993, Pittsburgh, Pennsylvania, United States 49 Gerda Ruge, Experiment on linguistically-based term associations, Information Processing and Management: an International Journal, v.28 n.3, p.317-332, 1992 50 Salton, Gerard.
1971. Experiments in automatic thesaurus construction for information retrieval.
In Proceedings IFIP Congress, pages 43--49.
51 Salton, Gerard and Chris Buckley.
1990. Improving retrieval performance by relevance feedback.
Journal of the American Society for Information Science, 41(4):288--297.
52 Gerard Salton, Michael J.
McGill, Introduction to Modern Information Retrieval, McGraw-Hill, Inc., New York, NY, 1986 53 Mark Sanderson, Word sense disambiguation and information retrieval, Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval, p.142-151, July 03-06, 1994, Dublin, Ireland 54 Schtze, Hinrich.
1992a. Context space.
In Robert Goldman, Peter Norvig, Eugene Charniak, and Bill Gale, editors, Working Notes of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language, pages 113--120, AAAI Press, Menlo Park, CA.
55 H.
Schtze, Dimensions of meaning, Proceedings of the 1992 ACM/IEEE conference on Supercomputing, p.787-796, November 16-20, 1992, Minneapolis, Minnesota, United States 56 Schtze, Hinrich.
1997. Ambiguity Resolution in Language Learning.
CSLI Publications, Stanford, CA.
57 Schtze, Hinrich and Jan O.
Pedersen. 1995.
Information retrieal based on word senses.
In Proceedings for the Fourth Annual Symposium on Document Analysis and Information Retrieval, pages 161--175, Las Vegas, NV.
58 Hinrich Schtze, Jan O.
Pedersen, A cooccurrence-based thesaurus and two applications to information retrieval, Information Processing and Management: an International Journal, v.33 n.3, p.307-318, May 1997 59 Sparck-Jones, Karen.
1986. Synonymy and Semantic Classification.
Edinburgh University Press, Edinburgh.
(Publication of Ph.D. thesis, University of Cambridge, 1964).
60 Karen Sparck Jones, Notes and references on early automatic classification work, ACM SIGIR Forum, v.25 n.1, p.10-17, Spring 1991 61 C.
J. Van Rijsbergen, Information Retrieval, Butterworth-Heinemann, Newton, MA, 1979 62 Ellen M.
Voorhees, Using WordNet to disambiguate word senses for text retrieval, Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval, p.171-180, June 27-July 01, 1993, Pittsburgh, Pennsylvania, United States 63 Walker, Donald E.
and Robert A.
Amsler. 1986.
The use of machine-readable dictionaries in sublanguage analysis.
In Ralph Grishman and Richard Kittredge, editors, Analyzing Language in Restricted Domains: Sublanguage Description and Processing.
L. Erlbaum Associates, Hillsdale, NJ, pages 69--84.
64 Wilks, Yorick A., Dan C.
Fass, Cheng Ming Guo, James E.
McDonald, Tony Plate, and Brian M.
Slator. 1990.
Providing machine tractable dictionary tools.
Journal of Computers and Translation, 2.
65 Peter Willett, Recent trends in hierarchic document clustering: a critical review, Information Processing and Management: an International Journal, v.24 n.5, p.577-597, 1988 66 Winer, B.
J. 1971.
Statistical Principles in Experimental Design.
Second edition.
McGraw-Hill, New York, NY.
67 David Yarowsky, Word-sense disambiguation using statistical models of Roget's categories trained on large corpora, Proceedings of the 14th conference on Computational linguistics, August 23-28, 1992, Nantes, France 68 David Yarowsky, Unsupervised word sense disambiguation rivaling supervised methods, Proceedings of the 33rd annual meeting on Association for Computational Linguistics, p.189-196, June 26-30, 1995, Cambridge, Massachusetts
NEW YORK UNIVERSITY DESCRIPTION OF THE PROTEUS SYSTEM AS USED FOR MUC4 Ralph Grishman, Catherine Macleod, and John Sterling, The PROTEUS Project Computer Science Departmen t New York University 715 Broadway, 7th Floor New York, NY 1000 3 { grishman,macleod,sterling)@cs.nyu.edu HISTORY The PROTEUS system which we have used for MUC-4 is largely unchanged from that used for MUC-3 . It has three main components : a syntactic analyzer, a semantic analyzer, and a template generator . The PROTEUS Syntactic Analyzer was developed starting in the fall of 1984 as a common base for all th e applications of the PROTEUS Project.
Many aspects of its design reflect its heritage in the Linguistic Strin g Parser, previously developed and still in use at New York University . The current system, including the Restriction Language compiler, the lexical analyzer, and the parser proper, comprise approximately 4500 lines of Commo n Lisp.
The Semantic Analyzer was initially developed in 1987 for the MUCK-I (RAINFORMs) application, extended for the MUCK-II (OPREPs) application, and has been incrementally revised since . It currently consist s of about 3000 lines of Common Lisp (excluding the domain-specific information) . The Template Generator was written from scratch for the MUC-3 task and then revised for the MUC-4 templates; it is about 1200 lines of Common Lisp..
STAGES OF PROCESSING The text goes through the five major stages of processing : lexical analysis, syntactic analysis, semantic analysis, reference resolution, and template generation (see Figure 1) . In addition, some restructuring of the logical form is performed both after semantic analysis and after reference resolution (only the restructuring after referenc e resolution is shown in Figure 1) . Processing is basically sequential: each sentence goes through lexical, syntactic, and semantic analysis and reference resolution ; the logical form for the entire message is then fed to template generation . However, semantic (selectional) checking is performed during syntactic analysis, employing essentiall y the same code later used for semantic analysis . Each of these stages is described in a section which follows . LEXICAL ANALYSI S Dictionary Format Our dictionaries contain only syntactic information : the parts of speech for each word, information about the complement structure of verbs, distributional information (e .g ., for adjectives and adverbs), etc . We follow closely the set of syntactic features established for the NYU Linguistic String Parser . This information is entered in LISP form using noun, verb, adjective, and adverb macros for the open-class words, and a word macro for other parts of speech : (ADVERB "ABRUPTLY" :ATTRIBUTES (DSA) ) (ADJECTIVE "ABRUPT" ) (NOUN :ROOT "ABSCESS" :ATTRIBUTES (NCOUNT) ) Knowledge Sources Dictionary Text Lexical Analysi s Grammar Syntactic Analysi s Semantic Models Semantic Analysis Concept Hier.
Reference Resolutio n Mapping Rules LF Transformatio n Template Generatio n Template s Figure 1 . Structure of the Proteus System as used for MUC-4 (VERB :ROOT "ABSCOND" :OBJLIST (NULLOBJ PN (PVAL (FROM WITH))) ) The noun and verb macros automatically generate the regular inflectional forms . Dictionary Files The primary source of our dictionary information about open-class words (nouns, verbs, adjectives, an d adverbs) is the machine-readable version of the Oxford Advanced Learner's Dictionary ("OALD") . We have writ ten programs which take the SGML (Standard Generalized Markup Language) version of the dictionary, extrac t information on inflections, parts of speech, and verb subcategorization (including information on adverbial particles and prepositions gleaned from the examples), and generate the LISP-ified form shown above . This is supplemented by a manually-coded dictionary (about 1500 lines, 900 entries) for closed-class words, words no t fiadequately defined in the OALD, and a few very common words . For MUC-4 we used several additional dictionaries . There was a dictionary (about 900 lines) for domain specific English words not defined in the OALD, or too richly defined there . In addition, we extracted from the tex t and templates lists of organizations, locations, and proper names, and prepared small dictionaries for each (abou t 2500 lines total) . Looku p The text reader splits the input text into tokens and then attempts to assign to each token (or sequence o f tokens, in the case of an idiom) a definition (part of speech and syntactic attributes) . The matching process proceeds in four steps: dictionary lookup, lexical pattern matching, spelling correction, and prefix stripping . Dictionary lookup immediately retrieves definitions assigned by any of the dictionaries (including inflected forms), while lexical pattern matching is used to identify a variety of specialized patterns, such as numbers, dates, times, and possessive forms . If neither dictionary lookup nor lexical pattern matching is successful, spelling correction and prefix strippin g are attempted.
For words of any length, we identify an input token as a misspelled form of a dictionary entry if on e of the two has a single instance of a letter while the other has a doubled instance of the letter (e .g ., "mispelled" and "misspelled") . For words of 8 or more letters, we use a more general spelling corrector which allows for any singl e insertion, deletion, or substitution.
[ The prefix stripper attempts to identify the token as a combination of a prefix and a word defined in the dictionary.
We currently use a list of 17 prefixes, including standard English ones like "un" and MUC-3/MUC-4 specials like "narco-" . If all of these procedures fail, the word is tagged as a proper noun (name), since we found that most of ou r remaining undefined words were names . For MUC-4, we have incorporated the stochastic part-of-speech tagger from BBN in order to assign probabilities to each part-of-speech assigned by the lexical analyzer.
The log probabilities are used as scores, and combined with other scores to determine the overall score of each parsing hypothesis . Filtering In order to avoid full processing of sentences which would make no contribution to the templates, we per form a keyword-based filtering at the sentence level : if a sentence contains no key terms, it is skipped . This filtering is done after lexical analysis because the lexical analysis has identified the root form of all inflected words ; these root forms provide links into the semantic hierarchy . The filtering can therefore be specified in terms of a small number of word classes, one of which must be present for the sentence to be worth processing . SYNTACTIC ANALYSI S Syntactic analysis involves two stages of processing : parsing and syntactic regularization . At the core of th e system is an active chart parser.
The grammar is an augmented context-free grammar, consisting of BNF rules plu s procedural restrictions which check grammatical constraints not easily captured in the BNF rules . Most restrictions are stated in PROTEUS Restriction Language (a variant of the language developed for the Linguistic String Parser ) and translated into LISP ; a few are coded directly in LISP [1] . For example, the count noun restriction (that singular countable nouns have a determiner) is stated as WCOUNT = IN LNR AFTER NVAR : IF BOTH CORE Xcore IS NCOUNT AND Xcore IS SINGULA R THEN IN LN, TPOS IS NOT EMPTY . Associated with each BNF rule is a regularization rule, which computes the regularized form of each node i n the parse tree from the regularized forms of its immediate constituents . These regularization rules are based on lambda-reduction, as in GPSG . The primary function of syntactic regularization is to reduce all clauses to a standard form consisting of aspect and tense markers, the operator (verb or adjective), and syntactically marked cases . ' The minimum word length requirement is needed to avoid false hits where proper names are incorrectly identified as misspellings of words defined in the dictionary.
For example, the definition of assertion, the basic S structure in our grammar, i s <assertion> . ._ <sa> <subject> <sa> <verb> <sa> <object> <sa > :(s !(<object> <subject> <verb> <sa*>)) . Here the portion after the single colon defines the regularized structure.
Coordinate conjunction is introduced by a metarule (as in GPSG), which is applied to the context-free components of the grammar prior to parsing . The regularization procedure expands any conjunction into a conjuntio n of clauses or of noun phrases . The output of the parser for the first sentence of TST2-0048, "SALVADORAN PRESIDENT-ELEC T ALFREDO CRISTIANI CONDEMNED THE TERRORIST KILLING OF ATTORNEY GENERAL ROBERTO GARCIA ALVARADO AND ACCUSED THE FARABUNDO MARTI NATIONAL LIBERATION FRON T (FMLN) OF THE CRIME.", i s (SENTENC E (CENTERS (CENTE R (ASSERTIO N (ASSERTIO N (SUBJEC T (NSTG (LNR (LN (NPOS (NPOSVAR (LCDN (ADJ "SALVADORAN")) (N "PRESIDENT" "-" "ELECT"))) ) (NVAR (NAMESTG (LNAMER (N "ALFREDO") (MORENAME (N "CRISTIANI"))))))) ) (VERB (LTVR (TV "CONDEMNED")) ) (OBJECT (NSTGO (NSTG (LNR (LN (TPOS (LTR (T "THE"))) (NPOS (NPOSVAR (N "TERRORIST"))) ) (NVAR (N "KILLING") ) (RN (RN-VA L (PN (P "OF" ) (NSTGO (NST G (LNR (LN (NPOS (NPOSVAR (N "ATTORNEY" "GENERAL"))) ) (NVA R (NAMEST G (LNAMER (N "ROBERTO" ) (MORENAME (N "GARCIA") (MORENAME (N "ALVARADO")))))))))))))))) ) (CONJ-WORD ("AND" "AND") ) (ASSERTION (SUBJEC T (NSTG (LN R (LN (NPOS (NPOSVAR (LCDN (ADJ "SALVADORAN")) (N "PRESIDENT" "-" "ELECT"))) ) (NVAR (NAMESTG (LNAMER (N "ALFRED O " ) (MORENAME (N "CRISTIANI"))))))) ) (VERB (LTVR (TV "ACCUSED")) ) (OBJEC T (NPN (NSTG O (NST G (LNR (LN (TPOS (LTR (T "THE"))) ) (NVAR (NAMESTG (LNAMER (N "FARABUNDO" "MARTI" "NATIONAL" "LIBERATION" "FRONT") ) fi(NAME-APPOS ("(" "(" ) (NSTG (LNR (NVAR (NAMESTG (LNAMER (N "FMLN")))))) (' ')".
")")))))) ) (PN (P "OF" ) (NSTGO (NSTG (LNR (LN (TPOS (LTR (T "THE")))) (NVAR (N "CRIME"))))))))))) ) (ENDMARK (" ".
" ")).
) and the corresponding regularized structure is (AND (S CONDEMN (VTENSE PAST ) (SUBJEC T (NP A-NAME SINGULAR (NAMES (ALFREDO CRISTIANI)) (SN NP1499 ) (N-POS (NP PRESIDENT-ELECT SINGULAR (SN NP1489) (A-POS SALVADORAN)))) ) (OBJEC T (NP KILLING SINGULAR (SN NP1532) (T-POS THE ) (N-POS (NP TERRORIST SINGULAR (SN NP1504)) ) (OF (NP A-NAME SINGULAR (NAMES (ROBERTO GARCIA ALVARADO)) (SN NP1531 ) (N-POS (NP 'ATTORNEY GENERAL' SINGULAR (SN NP1506))))))) ) (S ACCUSE (VTENSE PAST ) (SUBJECT (NP A-NAME SINGULAR (NAMES (ALFREDO CRISTIANI)) (SN NP1499 ) (N-POS (NP PRESIDENT-ELECT SINGULAR (SN NP1489) (A-POS SALVADORAN)))) ) (OBJEC T (NP FMLN SINGULAR (RN-APPOS (NP FMLN SINGULAR (SN NP1539))) (SN NP1544 ) (T-POS THE)) ) (OF (NP CRIME SINGULAR (SN NP1543) (T-POS THE)))) ) The system uses a chart parser operating top-down, left-to-right . As edges are completed (i.e., as nodes of the parse tree are built), restrictions associated with those productions are invoked to assign and test features of th e parse tree nodes . If a restriction fails, that edge is not added to the chart . When certain levels of the tree are complete (those producing noun phrase and clause structures), the regularization rules are invoked to compute a regularized structure for the partial parse, and selection is invoked to verify the semantic well-formedness of the structure (as noted earlier, selection uses the same "semantic analysis" code subsequently employed to translate the tre e into logical form).
One unusual feature of the parser is its weighting capability . Restrictions may assign scores to nodes ; th e parser will perform a best-first search for the parse tree with the highest score . This scoring is used to implement various preference mechanisms:  closest attachment of modifiers (we penalize each modifier by the number of words separating it from it s head)  preferred narrow conjoining for clauses (we penalize a conjoined clause structure by the number of words i t subsumes)  preference semantics (selection does not reject a structure, but imposes a heavy penalty if the structure doe s not match any lexico-semantic model, and a lesser penalty if the structure matches a model but with som e operands or modifiers left over) [2,3 ]  relaxation of certain syntactic constraints, such as the count noun constraint, adverb position constraints, and comma constraint s  disfavoring (penalizing) headless noun phrases and headless relatives (this is important for parsing efficiency) The grammar is based on Harris's Linguistic String Theory and adapted from the larger Linguistic Strin g Project (LSP) grammar developed by Naomi Sager at NYU [4] . The grammar is gradually being enlarged to cove r more of the LSP grammar.
The current grammar is 1600 lines of BNF and Restriction Language plus 300 lines o f Lisp; it includes 186 non-terminals, 464 productions, and 132 restrictions . Over the course of the MUCs we have added several mechanisms for recovering from sentences the gram mar cannot fully parse : allowing the grammar to skip a single word, or a series of words enclosed in parentheses or dashes, with a large score penalty if no parse is obtained for the entire sentence, taking the analysis which, starting at the first word, subsumes the most word s optionally, taking the remainder of the sentence and "covering" it with noun phrases and clauses, preferrin g the longest noun phrases or clauses which can be identifie d SEMANTIC ANALYSIS AND REFERENCE RESOLUTIO N The output of syntactic analysis goes through semantic analysis and reference resolution and is then added t o the accumulating logical form for the message . Following both semantic analysis and reference resolution certai n transformations are performed to simplify the logical form . All of this processing makes use of a concept hierarch y which captures the class/subclass/instance relations in the domain.
Semantic analysis uses a set of lexico-semantic models to map the regularized syntactic analysis into a semantic representation.
Each model specifies a class of verbs, adjectives, or nouns and a set of operands ; for eac h operand it indicates the possible syntactic case markers, the semantic class of the operand, whether or not th e operand is required, and the semantic case to be assigned to the operand in the output representation . For example, the model for "<explosive-object> damages <target>" i s (add-clause-model :id 'clause-damage3 :parent 'clause-an y :constraint 'damag e :operands (list (make-specifie r :marker 'subjec t :class 'explosive-objec t :case :instrument ) (make-specifie r :marker 'objec t :class 'target-entit y :case :patien t :essential-required 'required)) ) The models are arranged in a shallow hierarchy with inheritance, so that arguments and modifiers which are shared by a class of verbs need only be stated once.
The model above inherits only from the most general clause model, clause--any, which includes general clausal modifiers such as negation, time, tense, modality, etc . The evaluated MUC-4 system had 124 clause models, 21 nominalization models, and 39 other noun phrase models, a total of about 2500 lines . The class explosive--object in the clause model refers to the concept in the concept hierarchy, whose entries have the form : (defconcept explosive-object :typeof instrument-type ) (defconcept explosive :typeof explosive-objec t :muctype explosive ) (defconcept grenade :typeof explosive ) (defconcept explosive-charge :typeof explosiv e :alias (dynamite-charge) ) (defconcept bomb :typeof explosive-objec t :muctype bomb ) (defconcept (VEHICLE BOMB( :typeof explosive-objec t :muctype (VEHICLE BOMB( ) (defconcept car-bomb :typeof VEHICLE BOMB( ) (defconcept bus-bomb :typeof IVEHICLE BOMB( ) (defconcept dynamite :typeof explosive-objec t :alias tnt :muctype DYNAMITE ) There are currently a total of 2474 concepts in the hierarchy, of which 1734 are place names . The output of semantic analysis is a nested set of entity and event structures, with arguments labeled by key words primarily designating semantic roles . For the first sentence of TST2-0048, the output i s Reference Resolution Reference resolution is applied to the output of semantic analysis in order to replace anaphoric noun phrase s (representing either events or entities) by appropriate antecedents . Each potential anaphor is compared to prior entities or events, looking for a suitable antecedent such that the class of the anaphor (in the concept hierarchy) i s equal to or more general than that of the antecedent, the anaphor and antecedent match in number, the restrictiv e modifiers in the anaphor have corresponding arguments in the antecedent, and the non-restrictive modifiers (e .g ., apposition) of the anaphor are not inconsistent with those of the antecedent . Special tests are provided for names (people may be referred to a subset of the ir names) and for referring to groups by typical members ("terrorist force" Logical Form Transformation s The transformations which are applied after semantic analysis and after reference resolution simplify an d regularize the logical form in various ways . For example, if a verb governs an argument of a nominalization, th e argument is inserted into the event created from the nominalization : "x conducts the attack", "x claims responsibility for the attack", "x was accused of the attack" etc . are all mapped to "x attacks" (with appropriate settings of th e confidence slot) . For example, the rule to take "X was accused of Y" and make X the agent of Y i s (((event :predicate accusation-even t :agent ?agent1 :event (event :identifier ?id-1 . ?R2 ) . ?R1 ) (event :identifier ?id-1 . ?R4) ) -> ((modify 2 '( :agent ?agent-1 :confidence 'SUSPECTED OR ACCUSEDI) ) (delete 1)) ) Transformations are also used to expand conjoined structures . For example, there is a rule to expand "the towns o f x and y " into "the town of x and the town of y", and there is a rule to expand "event at location-1 and location-2 " into "event at location-1 and event at location-2" . There are currently 32 such rules.
These transformations are written as productions and applied using a simple data-driven production system interpreter which is part of the PROTEUS system . TEMPLATE GENERATO R Once all the sentences in an article have been processed through syntactic and semantic analysis, the resulting logical forms are sent to the template generator . The template generator operates in four stages . First, a frame structure resembling a simplified template (with incident-type, perpetrator, physical-target, human-target, date, location, instrument, physical-effect, and human-effect slots) is generated for each event . Date and locatio n expressions are reduced to a normalized form at this point . In particular, date expressions such as "tonight", "last month", " last April", "a year ago", etc.
are replaced by explicit dates or date ranges, based on the dateline of th e article.
Second, a series of heuristics attempt to merge these frames, mergin g  frames referring to a common target  frames arising from the same sentenc e  an effect frame following an attack frame (e.g ., "The FMLN attacked the town . Seven civilians died ".
) This merging is blocked if the dates or locations are different, the incident types are incompatible, or the perpetrators are incompatible . Third, a series of filters removes frames involving only military targets and those involvin g events more than two months old . Finally, MUC templates are generated from these frames . fiSPONSORSHIP The development of the entire PROTEUS system has been sponsored primarily by the Defense Advanced Research Projects Agency as part of the Strategic Computing Program, under Contract N00014-85-K-0163 an d Grant N00014-90-J-1851 from the Office of Naval Research . Additional support has been received from th e National Science Foundation under grant DCR-85-01843 for work on enhancing system robustness . REFERENCES [1] Grishman, R . PROTEUS Parser Reference Manual . PROTEUS Project Memorandum #4-C, Computer Science Department, New York University, May 1990.
[2] Grishman, R., and Sterling, J . Preference Semantics for Message Understanding.
Proc . DARPA Speech and Natural Language Workshop, Morgan Kaufman, 1990 (proceedings of the conference at Harwich Port, MA, Oct . 15-18, 1989) . [3] Grishman, R., and Sterling, J . Information Extraction and Semantic Constraints . Proc . 13th Int' I Conf Computational Linguistics (COLING 90), Helsinki, August 20-25, 1990 . [4] Sager, N . Natural Language Information Processing, Addison-Wesley, 1981 .
Shallow Semantic Parsing of Chinese Honglin Sun1 Center for Spoken Language Research University of Colorado at Boulder Daniel Jurafsky2 Center for Spoken Language Research University of Colorado at Boulder Abstract In this paper we address the question of assigning semantic roles to sentences in Chinese.
We show that good semantic parsing results for Chinese can be achieved with a small 1100-sentence training set.
In order to extract features from Chinese, we describe porting the Collins parser to Chinese, resulting in the best performance currently reported on Chinese syntactic parsing; we include our headrules in the appendix.
Finally, we compare English and Chinese semantic-parsing performance.
While slight differences in argument labeling make a perfect comparison impossible, our results nonetheless suggest significantly better performance for Chinese.
We show that much of this difference is due to grammatical differences between English and Chinese, such as the prevalence of passive in English, and the strict word order constraints on adjuncts in Chinese.
based on the SVM-based algorithm proposed for English by Pradhan et al (2003).
We first describe our creation of a small 1100-sentence Chinese corpus labeled according to principles from the English and (in-progress) Chinese PropBanks.
We then introduce the features used by our SVM classifier, and show their performance on semantic parsing for both seen and unseen verbs, given hand-corrected (Chinese TreeBank) syntactic parses.
We then describe our port of the Collins (1999) parser to Chinese.
Finally, we apply our SVM semantic parser to a matching English corpus, and discuss the differences between English and Chinese that lead to significantly better performance on Chinese.
2 Semantic
Annotation and the Corpus Work on semantic parsing in English has generally related on the PropBank, a portion of the Penn TreeBank in which the arguments of each verb are annotated with semantic roles.
Although a project to produce a Chinese PropBank is underway (Xue and Palmer 2003), this data is not expected to be available for another year.
For these experiments, we therefore hand-labeled a small corpus following the Penn Chinese Propbank labeling guidelines (Xue, 2002).
In this section, we first describe the semantic roles we used in the annotation and then introduce the data for our experiments.
2.1 Semantic
roles Semantic roles in the English (Kingsbury et al 2002) and Chinese (Xue 2002) PropBanks are grouped into two major types: (1) arguments, which represent central participants in an event.
A verb may require one, two or more arguments and they are represented with a contiguous sequence of numbers prefixed by arg, as arg0, arg1.
(2) adjuncts, which are optional for an event but supply more information about an event, such as time, location, 1 Introduction Thematic roles (AGENT, THEME, LOCATION, etc) provide a natural level of shallow semantic representation for a sentence.
A number of algorithms have been proposed for automatically assigning such shallow semantic structure to English sentences.
But little is understood about how these algorithms may perform in other languages, and in general the role of language-specific idiosyncracies in the extraction of semantic content and how to train these algorithms when large hand-labeled training sets are not available.
In this paper we address the question of assigning semantic roles to sentences in Chinese.
Our work is Currently at Department of Computer Science, Queens College, City University of New York.
Email: sunh@qc.edu.
2 Currently
at Department of Linguistics, Stanford University.
Email: jurafsky@stanford.edu.
fireason, condition, etc.
An adjunct role is represented with argM plus a tag.
For example, argM-TMP stands for temporal, argM-LOC for location.
In our corpus three argument roles and 15 adjunct roles appear.
The whole set of roles is given at Table 1.
Role arg0 arg1 arg2 argM-ADV argM-BFY argM-CMP argM-CND argM-CPN argM-DGR argM-FRQ argM-LOC argM-MNR argM-PRP argM-RNG argM-RST argM-SRC argM-TMP argM-TPC Table1 The list of semantic roles Freq Freq Note 556 872 23 train of senses, argument numbers and frequencies are given in Table 2.
List of verbs for experiments # of Arg Freq senses number /set up 1 2 106 /emerge 1 1 80 /publish 1 2 113 /give 2 3/2 41 /build into 2 2/3 113 /enter 1 2 123 /take place 1 2 230 /pass 3 2 75 /hope 1 2 90 /increase 1 2 167 Table 2 Verb Test adverbial beneficiary(e.g.
give support [to the plan]) object to be compared condition companion (e.g.
talk [with you]) degree frequency location manner purpose or reason range(e.g.
help you [in this aspect]) result(e.g.
increase [to $100]) source(e.g.
increase [from $50] to $100) temporal topic 3 Semantic Parsing 3.1 Architecture and Classifier Following the architecture of earlier semantic parsers like Gildea and Jurafsky (2002), we treat the semantic parsing task as a 1-of-N classification problem.
For each (non-aux/non-copula) verb in each sentence, our classifier examines each node in the syntactic parse tree for the sentence and assigns it a semantic role label.
Most constituents are not arguments of the verb, and so the most common label is NULL.
Our architecture is based on a Support Vector Machine classifier, following Pradhan et al.(2003). Since SVMs are binary classifiers, we represent this 1-of-19 classification problem (18 roles plus NULL) by training 19 binary one-versus-all classifiers.
Following Pradhan et al.(2003), we used tinySVM along with YamCha (Kudo and Matsumoto 2000, 2001) as the SVM training and test software.
The system uses a polynominal kernel with degree 2; the cost per unit violation of the margin, C=1; tolerance of the termination criterion e=0.001.
3.2 Features
The literature on semantic parsing in English relies on a number of features extracted from the input sentence and its parse.
These include the constituent's syntactic phrase type, head word, and governing category, the syntactic path in the parse tree connecting it to the verb, whether the constitutent is before or after the verb, the subcategorization bias of the verb, and the voice (active/passive) of the verb.
We investigated each of these features in Chinese; some acted quite similarly to English, while others showed interesting differences.
Features that acted similarly to English include the target verb, the phrase type, the syntactic category of the constituent.
(NP, PP, etc), and the subcategorization of the target verb.
The sub-categorization feature represents the phrase structure rule for the verb phrase 2.2 The training and test sets We created our training and test corpora by choosing 10 Chinese verbs, and then selecting all sentences containing these 10 verbs from the 250K-word Penn Chinese Treebank 2.0.
We chose the 10 verbs by considering frequency, syntactic diversity, and word sense.
We chose words that were frequent enough to provide sufficient training data.
The frequencies of the 10 verbs range from 41 to 230, with an average of 114.
We chose verbs that were representative of the variety of verbal syntactic behavior in Chinese, including verbs with one, two, and three arguments, and verbs with various patterns of argument linking.
Finally, we chose verbs that varied in their number of word senses.
In total, we selected 1138 sentences.
The first author then labeled each verbal argument/adjunct in each sentence with a role label.
We created our training and test sets by splitting the data for each verb into two parts: 90% for training and 10% for test.
Thus there are 1025 sentences in the training set and 113 sentences in the test set, and each test set verb has been seen in the training set.
The list of verbs chosen and their number ficontaining the target verb (e.g., VP -> VB NP, etc).
Five features (path, position, governing category, headword, and voice) showed interesting patterns that are discussed below.
3.2.1 Path
in the syntactic parse tree.
The path feature represents the path from a constituent to the target verb in the syntactic parse tree, using "^" for ascending a parse tree, and ""! for descending.
This feature manifests the syntactic relationship between the constituent and the target verb.
For example the path "NP^IP!VP!VP!VV" indicates that the constituent is an "NP" which is the subject of the predicate verb.
In general, we found the path feature to be sparse.
In our test set, 60% of path types and 39% of path tokens are unseen in the training.
The distributions of paths are very uneven.
In the whole corpus, paths for roles have an average frequency of 14.5 while paths for non-roles have an average of 2.7.
Within the role paths, a small number of paths account for majority of the total occurrences; among the 188 role path types, the top 20 paths account for 86% of the tokens.
Thus, although the path feature is sparse, its sparsity may not be a major problem in role recognition.
Of the 291 role tokens in our test set, only 9 have unseen paths, i.e., most of the unseen paths are due to non-roles.
Table 3 The positional distribution of roles Role arg0 arg1 arg2 argM-ADV argM-BFY argM-CMP argM-CND argM-CPN argM-DGR argM-FRQ argM-LOC argM-MNR argM-PRP argM-RNG argM-RST argM-SRC argM-TMP argM-TPC Total Before verb 547 319 223 28 38 15 10 233 11 11 9 12 408 14 1878 After verb 72 644 28 Total 619 963 28 223 28 38 15 10 57 3 238 11 11 9 16 12 421 14 2716 example, 88% of arg0s are before the verb, 67% of arg1s are after the verb and all the arg2s are after the verb.
Adjuncts have even a stronger bias.
Ten of the adjunct types can only occur before the verb, while three are always after the verb.
The two most common adjunct roles, argM-LOC and argM-TMP are almost always before the verb, a sharp difference from English.
The details are shown seen in Table 3.
3.2.3 Governing
Category.
The governing category feature is only applicable for NPs.
In the original formulation for English in Gildea and Jurafsky (2002), it answers the question: Is the NP governed by IP or VP?
An NP governed by an IP is likely to be a subject, while an NP governed by a VP is more likely to be an object.
For Chinese, we added a third option in which the governing category of an NP is neither IP nor VP, but an NP.
This is caused by the "DE" construction, in which a clause is used as a modifier of an NP.
For instance, in the example indicated in Figure 1, for the last NP, " "("international Olympic conference") the parent node is NP, from where it goes down to the target verb " "("taking place").
NP CP VP      in Paris take place  DEC DE             NP intl Olympic conf.
"the international Olympic Conference held in Paris" Figure 1 Example of DE construction Since the governing category information is encoded in the path feature, it may be redundant; indeed this redundancy might explain why the governing category feature was used in Gildea & Jurafsky(2002) but not in Gildea and Palmer(2002).
Since the "DE" construction caused us to modify the feature for Chinese, we conducted several experiments to test whether the governing category feature is useful or whether it is redundant with the path and position features.
Using the paradigm to be described in section 3.4, we found a small improvement using governing category, and so we include it in our model.
3.2.4 Head
word and its part of speech.
The head word is a useful but sparse feature.
In our corpus, of the 2716 roles, 1016 head words (type) are used, in which 646 are used only once.
The top 20 words are given in Table 4.
3.2.2 Position
before or after the verb.
The position feature indicates that a constituent is before or after the target verb.
In our corpus, 69% of the roles are before the verb while 31% are after the verb.
As in English, the position is a useful cue for role identity.
For 3.3 Experimental Results for Seen Verbs We now test the performance of our classifier, trained on the 1025-sentence training set and tested on the 113sentence test set introduced in Section 2.2.
Recall that in this `stratified' test set, each verb has been seen in the training data.
The last row in Table 5 shows the current best performance of our system on this test set.
The preceding rows show various subsets of the feature set, beginning with the path feature.
Table 5 Semantic parsing results on seen verbs feature set P R F (%) (%) (%) path 71.8 59.4 65.0 path + pt 72.9 62.9 67.5 path + position 72.5 60.8 66.2 path + head POS 77.6 63.3 69.7 path + sub-cat 80.8 63.6 71.2 path + head word 85.0 66.0 74.3 path + target verb 85.8 68.4 76.1 path + pt + gov + position + subcat + target + head word + head POS 91.7 76.0 83.1 As Table 5 shows, the most important feature is path, followed by target verb and head word.
In general, the lexicalized features are more important than the other features.
The combined feature set outperforms any other feature sets with less features and it has an Fscore of 83.1.
The performance is better for the arguments (i.e., only ARG0-2), 86.7 for arg0 and 89.4 for arg1.
3.4 Experimental
Results for Unseen Verbs To test the performance of the semantic parser on unseen verbs, we used cross-validation, selecting one verb as test and the other 9 as training, and iterating with each verb as test.
All the results are given in Table 6.
The results for some verbs are almost equal to the performance on seen verbs.
For example for "    " and "    ", the F-scores are over 80.
However, for some verbs, the results are much worse.
The worst case is the verb " ", which has an F-score of 11.
This is due to the special syntactic characteristics of this verb.
This verb can only have one argument and this argument most often follows the verb, in object position.
In the surface structure, there is often an NP before the verb working as its subject, but semantically this subject cannot be analyzed as arg0.
For example: (1) /China  /not  /will  /emerge  /food  /crisis.
(A food crisis won't emerge in China).
(2) /Finland /economy  /emerge  /AUX  /post-war  /most     /serious  /AUX     /depression.
(The most severe post-war depression emerged in the Finland economy.) In the top 20 words, 4 are prepositions (" /in    /at /than /for") and 3 are temporal nouns("  /today      /present      /recently") and 2 are adverbs("  /already,  /will").
These closed class words are highly correlated with specific semantic roles.
For example," /for" occurs 195 times as the head of a constituent, of which 172 are non-roles, 19 are argM-BFYs, 3 are arg1s and 1 is an argM-TPC." /in" occurs 644 times as a head, of which 430 are nonroles, 174 are argM-LOCs, 24 are argM-TMPs, 9 are argM-RNGs, and 7 are argM-CND.
"  /already" occurs 135 times as a head, of which 97 are non-roles and 38 are argM-ADVs.
" /today" occurs 69 times as a head, of which 41 are argM-TMPs and 28 are nonroles.
Within the open class words, some are closely correlated to the target verb.
For example, "     /meeting; conference" occurs 43 times as a head for roles, of which 24 are for the target " /take place" and 19 for "    /pass".
"    /ceremony" occurs 28 times and all are arguments of " "(take place)."  /statement" occurs 19 times, 18 for "    /release; publish" and one for " /hope".
These statistics emphasize the key role of the lexicalized head word feature in capturing the collocation between verbs and their arguments.
Due to the sparsity of the head word feature, we also use the part-of-speech of the head word, following Surdeanu et al (2003).
For example, "7  26  /July 26" may not be seen in the training, but its POS, NT(temporal noun), is a good indicator that it is a temporal.
3.2.5 Voice.
The passive construction in English gives information about surface location of arguments.
In Chinese the marked passive voice is indicated by the use of the preposition " /by" (POS tag LB in Penn Chinese Treebank).
This passive, however, is seldom used in Chinese text.
In our entire 1138-sentence corpus, only 13 occurrences of "LB" occur, and only one (in the training set) is related to the target verb.
Thus we do not use the voice feature in our system.
The subjects, " /China" in (1) and " /Finland /economy", are locatives, i.e. argM-LOC, and the objects, " /food  /crisis" in (1) and " /postwar  /most  /serious  /AUX  /depression" in (2), are analyzed as arg0.
But the parser classified the subjects as arg0 and the objects as arg1.
These are correct for most common verbs but wrong for this particular verb.
It is difficult to know how common this problem would be in a larger, test set.
The fact that we considered diversity of syntactic behavior when selecting verbs certainly helps make this test set reflect the difficult cases.
If most verbs prove not to be as idiosyncratic as " /emerge", the real performance of the parser on unseen verbs may be better than the average given here.
Table 6 Experimental Results for Unseen Verbs target P(%) R(%) F(%)  /publish 90.7 72.9 80.8  /increase 49.6 34.3 40.5   /take place 90.1 63.3 74.4  /build into 65.2 55.5 60.0  /give 65.7 37.9 48.1  /pass 85.9 77.0 81.2  /emerge 12.6 10.2 11.3  /enter 81.9 58.8 68.4  /set up 79.0 61.1 68.9  /hope 77.7 35.9 49.1 Average 69.8 50.7 58.3 Another important difficulty in processing unseen verbs is the fact that roles in PropBank are defined in a verb-dependent way.
This may be easiest to see with an English example.
The roles arg2, arg3, arg4 have different meaning for different verbs; underlined in the following are some examples of arg2: (a) The state gave CenTrust 30 days to sell the Rubens.
(b) Revenue increased 11 to 2.73 billion from 2.46 billion.
(c) One of Ronald Reagan 's attributes as President was that he rarely gave his blessing to the claptrap that passes for consensus in various international institutions.
In (a), arg2 represents the goal of "give", in (b), it represents the amount of increase, and in (c) it represents yet another role.
These complete different semantic relations are given the same semantic label.
For unseen verbs, this makes it difficult for the semantic parser to know what would count as an arg2.
parser, the Collins (1999) parser, ported to Chinese.
We first describe how we ported the Collins parser to Chinese and then present the results of the semantic parser with features drawn from the automatic parses.
4.1 The
Collins parser for Chinese The Collins parser is a state-of-the-art statistical parser that has high performance on English (Collins, 1999) and Czech(Collins et al.1999). There have been attempts in applying other algorithms in Chinese parsing (Bikel and Chiang, 2000; Chiang and Bikel 2002; Levy and Manning 2003), but there has been no report on applying the Collins parser on Chinese.
The Collins parser is a lexicalized statistical parser based on a head-driven extended PCFG model; thus the choice of head node is crucial to the success of the parser.
We analyzed the Penn Chinese Treebank data and worked out head rules for the Chinese Treebank grammar (we were unable to find any published head rules for Chinese in the literature).
There are two major differences in the head rules between English and Chinese.
First, NP heads in Chinese are rigidly rightmost, that is to say, no modifiers of an NP can follow the head.
In contrast, in English a modifier may follow the head.
Second, just as with NPs in Chinese, the head of ADJP is rigidly rightmost.
In English, by contrast, the head of an ADJP is mainly the leftmost constituent.
Our head rules for the Chinese Treebank grammar are given in the Appendix.
In addition to the head rules, we modified the POS tags for all punctuation.
This is because all cases of punctuation in the Penn Chinese Treebank are assigned the same POS tag "PU".
The Collins parser, on the other hand, expects the punctuation tags in the English TreeBank format, where the tag for a punctuation mark is the punctuation mark itself.
We therefore replaced the POS tags for all punctuation marks in the Chinese data to conform to the conventions in English.
Finally, we made one further augmentation also related to punctuation.
Chinese has one punctuation mark that does not exist in English.
This commonly used mark, `semi-stop', is used in Chinese to link coordinates within a sentence (for example between elements of a list).
This function is represented in English by a comma.
But the comma in English is ambiguous; in addition to its use in coordination and lists, it can also represent the end of a clause.
In Chinese, by contrast the semi-stop has only the conjunction/list function.
Chinese thus uses the regular comma only for representing clause boundaries.
We investigated two ways to model the use of the Chinese semi-stop: (1) just converting the semi-stop to the comma, thus conflating the two functions as in English; and (2) by giving the semi-stop the POS tag "CC", a conjunction.
We compared parsing results with these two methods; the latter (conjunction) method gained 0.5% net 4 Using Automatic Parses The results in the last section are based on the use of perfect (hand-corrected) parses drawn from the Penn Chinese Treebank.
In practical use, of course, automatic parses will not be as accurate.
In this section we describe experiments on semantic parsing when given automatic parses produced by an automatic fiimprovement in F-score over the former one.
We therefore include it in our Collins parser port.
We trained the Collins parser on the Penn Chinese Treebank(CTB) Release 2 with 250K words, first removing from the training set any sentences that occur in the test set for the semantic parsing experiments.
We then tested on the test set used in the semantic parsing which includes 113 sentences(TEST1).
The results of the syntactic parsing on the test set are shown in Table 7.
Table 7 Results for syntactic parsing, trained on CTB Release 2, tested on test set in semantic parsing LP(%) LR(%) F1(%) overall 81.6 82.1 81.0 len<=40 86.1 85.5 86.7 To compare the performance of the Collins parser on Chinese with those of other parsers, we conducted an experiment in which we used the same training and test data (Penn Chinese Treebank Release 1, with 100K words) as used in those reports.
In this experiment, we used articles 1-270 for training and 271-300 as test(TEST2).
Table 8 shows the results and the comparison with other parsers.
Table 8 only shows the performance on sentences 40 words.
Our performance on all the sentences TEST2 is P/R/F=82.2/83.3/82.7.
It may seem surprising that the overall F-score on TEST2 (82.7) is higher than the overall F-score on TEST1 (81.0) despite the fact that our TEST1 system had more than twice as much training as our TEST2 system.
The reason lies in the makeup of the two test sets; TEST1 consists of randomly selected long sentences; TEST2 consists of sequential text, including many short sentences.
The average sentence length in TEST1 is 35.2 words, vs.
22.1 in TEST2.
TEST1 has 32% long sentences (>40 words) while TEST2 has only 13%.
Comparison with other parsers: TEST2 40 words LP(%) LR(%) F1(%) Bikel & Chiang 2000 77.2 76.2 76.7 Chiang & Bikel 2002 81.1 78.8 79.9 Levy & Manning 2003 78.4 79.2 78.8 Collins parser 86.4 85.5 85.9 4.2 Semantic parsing using Collins parses In the test set of 113 sentences, there are 3 sentences in which target verbs are given the wrong POS tags, so they can not be used for semantic parsing.
For the remaining 100 sentences, we used the feature set containing eight features (path, pt, gov, position, subcat, target, head word and head POS), the same as Table 8 that used in the experiment on perfect parses.
The results are shown in Table 9.
Table 9 Result for semantic parsing using automatic syntactic parses P(%) R(%) F(%) 110 sentences 86.0 70.8 77.6 113 sentences 86.0 69.2 76.7 Compared to the F-score using hand-corrected syntactic parses from the TreeBank, using automatic parses decreases the F-score by 6.4. 5 Comparison with English English build emerge enter found give Freq 46 30 108 248 124 Chinese      English hold hope increase pass publish Freq 120 63 231 143 77 Chinese      Table 12 Role argM-ADV argM-LOC argM-MNR argM-TMP Before verb 22 25 22 119 After verb 43 82 75 164 The comparison between adjuncts in English and Chinese English Chinese Freq in PRF Before After Freq in test (%) verb verb test 5 0 00 223 0 37 11 80 36.4 50 233 5 31 14 0 00 11 0 1 37 66.7 27 38.5 408 13 44 F 70 88.5 0 78.4 After the verbs were chosen, we extracted every sentence containing these verbs from section 02 to section 21 of the Wall Street Journal data from the Penn English Propbank.
The number of sentences for each verb is given in Table 10.
5.2 Experimental
Results As in our Chinese experiments, we used our SVMbased classifier, using N one-versus-all classifiers.
Table 11 shows the performance on our English test set (with Chinese for comparison), beginning with the path feature, and incrementally adding features until in the last row we combine all 8 features together.
Experimental results of English Chinese English feature set R/F/P P/R/F path 71.8/59.4/65.0 78.2/48.3/59.7 path + pt 72.9/62.9/67.5 77.4/51.2/61.6 path + position 72.5/60.8/66.2 75.7/50.9/60.8 path + hd POS 77.6/63.3/69.7 79.1/49.7/61.0 path + sub-cat 80.8/63.6/71.2 79.9/45.3/57.8 path + hd word 85.0/66.0/74.3 84.0/47.7/60.8 path + target 85.8/68.4/76.1 85.7/49.1/62.5 COMBINED 91.7/76.0/83.1 84.1/62.2/71.5 It is immediately clear from Table 11 that using similar verbs, the same amount of data, the same classifier, the same number of roles, and the same features, the results from English are much worse than those for Chinese.
While some part of the difference is probably due to idiosyncracies of particular sentences in the English and Chinese data, other aspects of the difference might be accounted for systematically, as we discuss in the next section.
5.3 Discussion: English/Chinese differences We first investigated whether the differences between English and Chinese could be attributed to particular semantic roles.
We found that this was indeed the case.
The great bulk of the error rate difference between English and Chinese was caused by the 4 adjunct classes argM-ADV, argM-LOC, argM-MNR, and argM-TMP, which together account for 19.6% of the role tokens in our English corpus.
The average F-score in English for the four roles is 36.7, while in Chinese Table 11 the F-score for the four roles is 78.6.
Why should these roles be so much more difficult to identify in English than Chinese?
We believe the answer lies in the analysis of the position feature in section 3.2.2.
This is repeated, with error rate information in Table 12.
We see there that adjuncts in English have no strong preference for occurring before or after the verb.
Chinese adjuncts, by contrast, are well-known to have an extremely strong preference to be preverbal, as Table 12 shows.
The relatively fixed word order of adjuncts makes it much easier in Chinese to map these roles from surface syntactic constituents than in English.
If the average F-score of the four adjuncts in English is raised to the level of that in Chinese, the overall Fscore on English would be raised from 71.5 to 79.7, accounting for 8.2 of the 11.6 difference in F-scores between the two languages.
We next investigated the one feature from our original English-specific feature set that we had dropped in our Chinese system: passive.
Recall that we dropped this feature because marked passives are extremely rare in Chinese.
When we added this feature back into our English system, the performance rose from P/R/F=84.1/62.2/71.5 to 86.4/65.1/74.3.
As might be expected, this effect of voice is mainly reflected in an improvement on arg0 and arg1, as Table 13 shows below: Table 13.
Improvement in English semantic parsing with the addition of the voice feature -voice +voice P RF P R F arg0 88.9 75.3 81.5 94.4 80 86.6 arg1 86.5 82.8 84.6 88.5 86.2 87.3 A third source of English-Chinese differences is the distribution of roles; the Chinese data has proportionally more adjuncts (ARGMs), while the English data has proportionally more oblique arguments (ARG2, ARG3, ARG4).
Oblique arguments are more difficult to process than other arguments, as was discussed in section 3.4.
This difference is most likely to be caused by labeling factors rather than by true structural differences between English in Chinese.
In summary, the higher performance in our Chinese system is due to 3 factors: the importance of passive in English; the strict word-order constraints of Chinese adverbials, and minor labeling differences.
Conclusions We can draw a number of conclusions from our investigation of semantic parsing in Chinese.
First, reasonably good performance can be achieved with a very small (1100 sentences) training set.
Second, the features that we extracted for English semantic parsing worked well when applied to Chinese.
Many of these features required creating an automatic parse; in doing so we showed that the Collins (1999) parser when ported to Chinese achieved the best reported performance on Chinese syntactic parsing.
Finally, we showed that semantic parsing is significantly easier in Chinese than in English.
We show that this counterintuitive result seems to be due to the strict constraints on adjunct ordering in Chinese, making adjuncts easier to find and label.
Acknowledgements This work was partially supported by the National Science Foundation via a KDD Supplement to NSF CISE/IRI/Interactive Systems Award IIS-9978025.
Many thanks to Ying Chen for her help on the Collins parser port, and to Nianwen Xue and Sameer Pradhan for providing the data.
Thanks to Kadri Hacioglu, Wayne Ward, James Martin, Martha Palmer, and three anonymous reviewers for helpful advice.
Appendix: Head rules for Chinese Parent ADJP ADVP CLP CP DNP DP DVP IP LCP LST NP PP PRN QP UCP VCD VP VPT VRD VSB Direction Right Right Right Right Right Left Right Right Right Right Right Left Left Right Left Left Left Left Left Right Priority List ADJP JJ AD ADVP AD CS JJ NP PP P VA VV CLP M NN NP CP IP VP DEG DNP DEC QP M(r) DP DT OD DEV AD VP VP IP NP LCP LC CD NP QP NP NN IP NR NT P PP PU QP CLP CD IP NP VP VV VA VE VE VC VV VNV VPT VRD VSB VCD VP VA VV VVl VA VV VE References Baker, Collin F., Charles J.
Fillmore, and John B.
Lowe. 1998.
The Berkekey FrameNet Project.
In Proceeding of COLING/ACL.
Bikel, Daniel and David Chiang.
2000. Two Statistical Parsing models Applied to the Chinese Treebank.
In Proceedings of the Second Chinese Language Processing Workshop, pp.
1-6. Chiang, David and Daniel Bikel.
2002. Recovering Latent Information in Treebanks.
In Proceedings of COLING-2002, pp.183-189.
Collins, Michael.
1999. Head-driven Statistical Models for Natural Language Parsing.
Ph.D. dissertation, University of Pennsylvannia.
Collins, Michael, Jan Hajic, Lance Ramshaw and Christoph Tillmann.
1999. A th Statistical Parser for Czech.
In Proceedings of the 37 Meeting of the ACL, pp.
505-512. Gildea, Daniel and Daniel Jurafsky.
2002. Automatic Labeling of Semantic Roles.
Computational Linguistics, 28(3):245-288.
Gildea, Daniel and Martha Palmer.
2002. The Necessity of Parsing for Predicate Argument Recognition, In Proceedings of the 40th Meeting of the ACL, pp.
239-246. Kingsbury, Paul, Martha Palmer, and Mitch Marcus.
2002. Adding semantic annotation to the Penn Treebank.
In Proceedings of HLT-02.
Kudo, Taku and Yuji Matsumoto.
2000. Use of support vector learning for chunk Identification.
In Proceedings of the 4th Conference on CoNLL, pp.
142-144. Kudo, Taku and Yuji Matsumoto.
2001 Chunking with Support Vector Machines.
In Proceeding of the 2nd Meeting of the NAACL.
pp.192-199. Levy, Roger and Christopher Manning.
2003. Is it harder to parse Chinese, or the Chinese Treebank?
ACL 2003, pp.
439-446. Pradhan, Sameer, Kadri Hacioglu,.
Wayne Ward, James Martin, and Daniel Jurafsky.
2003. "Semantic Role Parsing: Adding Semantic Structure to Unstructured Text".
In the Proceedings of the International Conference on Data Mining (ICDM2003), Melbourne, FL, 2003 Surdeanu, Mihai, Sanda Harabagiu, John Williams and Paul Aarseth.
2003. Using Predicate-Argument Structures for Information Extraction, In Proceedings of ACL.
Xue, Nianwen.
2002.stGuidelines for the Penn Chinese Proposition Bank (1 Draft), UPenn.
Xue, Nianwen, Fu-Dong Chiou and Martha Palmer.
2002. Building a large-scale annotated Chinese corpus.
In Proceedings of COLING-2002.
Xue, Nianwen, Martha Palmer.
2003. Annotating the propositions in the Penn Chinese Treebank.
In Proceedings of the 2nd SIGHAN Workshop on Chinese Language Processing.
Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp.
64-71. Generalized Encoding of Description Spaces and its Application to Typed Feature Structures Gerald Penn Department of Computer Science University of Toronto 10 King's College Rd.
Toronto M5S 3G4, Canada Abstract This paper presents a new formalization of a unificationor join-preserving encoding of partially ordered sets that more essentially captures what it means for an encoding to preserve joins, generalizing the standard definition in AI research.
It then shows that every statically typable ontology in the logic of typed feature structures can be encoded in a data structure of fixed size without the need for resizing or additional union-find operations.
This is important for any grammar implementation or development system based on typed feature structures, as it significantly reduces the overhead of memory management and reference-pointer-chasing during unification.
adj noun nom acc plus minus subst case bool head Figure 1: A sample type system with appropriateness conditions.
the types of values (value restrictions) those feature values must have.
In Figure 1,1 for example, all head-typed TFSs must have bool-typed values for the features MOD and PRD, and no values for any other feature.
Relative to data structures like arrays or logical terms, typed feature structures (TFSs) can be regarded as an expressive refinement in two different ways.
First, they are typed, and the type system allows for subtyping chains of unbounded depth.
Figure 1 has a chain of length from to noun.
Pointers to arrays and logical terms can only monotonically "refine" their (syntactic) type from unbound (for logical terms, variables) to bound.
Second, although all the TFSs of a given type have the same features because of appropriateness, a TFS may acquire more features when it promotes to a subtype.
If a head-typed TFS promotes to noun in the type system above, for example, it acquires one extra casevalued feature, CASE.
When a subtype has two or 1 In this paper, Carpenter's (1992) convention of using as the most general type, and depicting subtypes above their supertypes is used.
1 Motivation
The logic of typed feature structures (Carpenter, 1992) has been widely used as a means of formalizing and developing natural language grammars that support computationally efficient parsing, generation and SLD resolution, notably grammars within the Head-driven Phrase Structure Grammar (HPSG) framework, as evidenced by the recent successful development of the LinGO reference grammar for English (LinGO, 1999).
These grammars are formulated over a finite vocabulary of features and partially ordered types, in respect of constraints called appropriateness conditions.
Appropriateness specifies, for each type, all and only the features that take values in feature structures of that type, along with fiAn order-embedding preserves the behavior of the order relation (for TFS type systems, subtyping; more incomparable supertypes, a TFS can also multiply inherit features from other supertypes when it promotes.
The overwhelmingly most prevalent operation when working with TFS-based grammars is unification, which corresponds mathematically to finding a least upper bound or join.
The most common instance of unification is the special case in which a TFS is unified with the most general TFS that satisfies a description stated in the grammar.
This special case can be decomposed at compile-time into more atomic operations that (1) promote a type to a subtype, (2) bind a variable, or (3) traverse a feature path, according to the structure of the description.
TFSs actually possess most of the properties of fixed-arity terms when it comes to unification, due to appropriateness.
Nevertheless, unbounded subtyping chains and acquiring new features conspire to force most internal representations of TFSs to perform extra work when promoting a type to a subtype to earn the expressive power they confer.
Upon being repeatedly promoted to new subtypes, they must be repeatedly resized or repeatedly referenced with a pointer to newly allocated representations, both of which compromise locality of reference in memory and/or involve pointer-chasing.
These costs are significant.
Because appropriateness involves value restrictions, simply padding a representation with some extra space for future features at the outset must guarantee a proper means of filling that extra space with the right value when it is used.
Internal representations that lazily fill in structure must also be wary of the common practice in description languages of binding a variable to a feature value with a scope larger than a single TFS -for example, in sharing structure between a daughter category and a mother category in a phrase structure rule.
In this case, the representation of a feature's value must also be interpretable independent of its context, because two separate TFSs may refer to that variable.
These problems are artifacts of not using a representation which possesses what in knowledge representation is known as a join-preserving encoding of a grammar's TFSs -in other words, a representation with an operation that naturally behaves like TFS-unification.
The next section presents the standard definition of join-preserving encodings and provides a generalization that more essentially captures what it means for an encoding to preserve joins.
Section 3 formalizes some of the defining characteristics of TFSs as they are used in computational linguistics.
Section 4 shows that these characteristics quite fortuitously agree with what is required to guarantee the existence of a joinpreserving encoding of TFSs that needs no resizing or extra referencing during type promotion.
Section 5 then shows that a generalized encoding exists in which variable-binding scope can be larger than a single TFS -a property no classical encoding has.
Earlier work on graph unification has focussed on labelled graphs with no appropriateness, so the central concern was simply to minimize structure copying.
While this is clearly germane to TFSs, appropriateness creates a tradeoff among copying, the potential for more compact representations, and other memory management issues such as locality of reference that can only be optimized empirically and relative to a given grammar and corpus (a recent example of which can be found in Callmeier (2001)).
While the present work is a more theoretical consideration of how unification in one domain can simulate unification in another, the data structure described here is very much motivated by the encoding of TFSs as Prolog terms allocated on a contiguous WAM-style heap.
In that context, the emphasis on fixed arity is really an attempt to avoid copying, and lazily filling in structure is an attempt to make encodings compact, but only to the extent that join preservation is not disturbed.
While this compromise solution must eventually be tested on larger and more diverse grammars, it has been shown to reduce the total parsing time of a large corpus on the ALE HPSG benchmark grammar of English (Penn, 1993) by a factor of about 4 (Penn, 1999).
2 Join-Preserving Encodings We may begin with a familiar definition from discrete mathematics: and Definition 1 Given two partial orders, a function is an orderembedding iff, for every, iff. fif Figure 3: A non-classical join-preserving encoding between BCPOs for which no classical joinpreserving encoding exists.
Bounded completeness ensures that unification or joins are well-defined among consistent types.
Definition 3 Given two BCPOs, and, is a classical join-preserving encoding of into iff: injectivity is an injection, Join-preserving encodings are automatically orderembeddings because iff . There is actually a more general definition: Definition 4 Given two BCPOs, and, is a (generalized) join-preserving encoding of into iff: disjointness to mean We use the notation to mean is defined.
totality for all iff is undefined, and join homomorphism, where they exist.
zero preservation, and iff Definition 2 A partial order is bounded complete (BCPO) iff every set of elements with a common upper bound has a least upper bound.
When maps elements of to singleton sets in, then reduces to a classical join-preserving encoding.
It is not necessary, however, to require that only one element of represent an element of, provided that it does not matter which representative we choose at any given time.
Figure 3 shows a generalized join-preserving encoding between two partial orders for which no classical encoding exists.
There is no classical encoding of into because no three elements can be found in that pairwise unify to a common join.
A generalized encoding exists because we can choose three potential representatives for : one ( ) for unifying the representatives of and, one ( ) for unifying the representatives of and, and one ( ) for unifying the representatives of and . Notice that the set of representatives for must be closed under unification.
Although space does not permit here, this generalization has been used to prove that well-typing, an alternative interpretation of appropriateness, is equivalent in its expressive power to the interpretation used here (called total well-typing; Carpenter, 1992); that multi-dimensional inheritance (Erbach, 1994) adds no expressive power to any TFS type system; that TFS type systems can encode systemic networks in polynomial space using extensional types (Carpenter, 1992); and that certain uses of parametjoin homomorphism for all and,, where they exist.
zero preservation for all, iff, and for TFSs themselves, subsumption) in the encoding codomain.
As shown in Figure 2, however, order embeddings do not always preserve operations such as least upper bounds.
The reason is that the image of may not be closed under those operations in the codomain.
In fact, the codomain could provide joins where none were supposed to exist, or, as in Figure 2, no joins where one was supposed to exist.
Mellish (1991; 1992) was the first to formulate join-preserving encodings correctly, by explicitly refor the quiring this preservation.
Let us write join of and in partial order . Figure 2: An example order-embedding that cannot translate least upper bounds.
and fihead (Upward Closure / Right Monotonicity) if F and, then F and F F . The function Approp maps a feature and type to the value restriction on that feature when it is appropriate to that type.
If it is not appropriate, then Approp is undefined at that pair.
Feature introduction ensures that every feature has a least type to which it is appropriate.
This makes description compilation more efficient.
Upward closure ensures that subtypes inherit their supertypes' features, and with consistent value restrictions.
The combination of these two properties allows us to annotate a BCPO of types with features and value restrictions only where the feature is introduced or the value restriction is refined, as in Figure 1.
A very useful property for type systems to have is static typability.
This means that if two TFSs that are well-formed according to appropriateness are unifiable, then their unification is automatically well-formed as well -no additional work is necessary.
Theorem 1 (Carpenter, 1992) An appropriateness specification is statically typable iff, for all types such that, and all F : unrestricted if only if only otherwise if and if (Feature type Introduction) MOD PRD plus plus Figure 5: A TFS of type head from the type system in Figure 1.
Not all type systems are statically typable, but a type system can be transformed into an equivalent statically typable type system plus a set of universal constraints, the proof of which is omitted here.
In linguistic applications, we normally have a set of universal constraints anyway for encoding principles of grammar, so it is easy and computationally inexpensive to conduct this transformation.
4 Static
Encodability As mentioned in Section 1, what we want is an encoding of TFSs with a notion of unification that naturally corresponds to TFS-unification.
As discussed in Section 3, static typability is something we can reasonably guarantee in our type systems, and is therefore something we expect to be reflected in our encodings -no extra work should be done apart from combining the types and recursing on feature values.
If we can ensure this, then we have avoided the extra work that comes with resizing or unnecessary referencing and pointer-chasing.
As mentioned above, what would be best from the standpoint of memory management is simply a fixed array of memory cells, padded with extra space to accommodate features that might later be added.
We will call these frames.
Figure 4 depicts a frame for the head-typed TFS in Figure 5.
In a frame, the representation of the type can either be (1) a bit vector encoding the type,3 or (2) a reference pointer Instead of a bit vector, we could also use an index into a table if least upper bounds are computed by table look-up.
PQ Definition 5 A TFS type system consists of a finite BCPO of types,, a finite set of features Feat, and a partial function, such that, for every F : Figure 4: A fixed array representation of the TFS in Figure 5.
There are only a few common-sense restrictions we need to place on our type systems: 3 TFS Type Systems ric typing with TFSs also add no expressive power to the type system (Penn, 2000).
(head representation) (MOD representation) (PRD representation) fiF: G: H: Figure 6: A type system with three features and a three-colorable feature graph.
module of its type.
Even this number can normally be reduced: Definition 7 The feature graph,, of module is an undirected graph, whose vertices correspond to the features introduced in, and in which there is an edge,, iff and are appropriate to a common maximally specific type in . Proposition 1 The least number of feature slots required for a frame of any type in is the least for which is -colorable.
There are type systems, of course, for which modularization and graph-coloring will not help.
Figure 6, for example, has one module, three features, and a three-clique for a feature graph.
There are statistical refinements that one could additionally make, such as determining the empirical probability that a particular feature will be acquired and electing to pay the cost of resizing or referencing for improbable features in exchange for smaller frames.
4.2 Correctness
of Frames Restricting the Size of Frames At first blush, the prospect of adding as many extra slots to a frame as there could be extra features in a TFS sounds hopelessly unscalable to large grammars.
While recent experience with LinGO (1999) suggests a trend towards modest increases in numbers of features compared to massive increases in numbers of types as grammars grow large, this is nevertheless an important issue to address.
There are two discrete methods that can be used in combination to reduce the required number of extra slots:, the set of Definition 6 Given a finite BCPO, modules of is the finest partition of,, such that (1) each is upward-closed (with respect to subtyping), and (2) if two types have a least upper bound, then they belong to the same module.
Trivially, if a feature is introduced at a type in one module, then it is not appropriate to any type in any other module.
As a result, a frame for a TFS only needs to allow for the features appropriate to the Prolog terms require one additional unbound variable per TFS (sub)term in order to preserve the intensionality of the logic -unlike Prolog terms, structurally identical TFS substructures are not identical unless explicitly structure-shared.
With the exception of extra slots for unused feature values, frames are clearly isomorphic in their structure to the TFSs they represent.
The implementation of unification that we prefer to avoid resizing and referencing is to (1) find the least upper bound of the types of the frames being unified, (2) update one frame's type to the least upper bound, and point the other's type representation to it, and (3) recurse on respective pairs of feature values.
The frame does not need to be resized, only the types need to be referenced, and in the special case of promoting the type of a single TFS to a subtype, the type only needs to be trailed.
If cyclic TFSs are not supported, then acyclicity must also be enforced with an occurscheck.
The correctness of frames as a join-preserving encoding of TFSs thus depends on being able to make sense of the values in these unused positions.
The to another frame.
If backtracking is supported in search, changes to the type representation must be trailed.
For each appropriate feature, there is also a pointer to a frame for that feature's value.
There are also additional pointers for future features (for head, CASE) that are grounded to some distinguished value indicating that they are unused -usually a circular reference to the referring array position.
Cyclic TFSs, if they are supported, would be represented with cyclic (but not 1-cyclic) chains of pointers.
Frames can be implemented either directly as arrays, or as Prolog terms.
In Prolog, the type representation could either be a term-encoding of the type, which is guaranteed to exist for any finite BCPO (Mellish, 1991; Mellish, 1992), or in extended Prologs, another trailable representation such as a mutable term (Aggoun and Beldiceanu, 1990) or an attributed value (Holzbaur, 1992).
Padding the representation with extra space means using a Prolog term with extra arity.
A distinguished value for unused arguments must then be a unique unbound variable.4 Figure 7: A type system that introduces a feature at a join-reducible type.
head Figure 9: The frame for Figure 8.
MOD PRD PRD (head representation) Figure 11: The frame for Figure 10.
5 The
sole exception is a TFS of type, which by definition belongs to no module and has no features.
Its representation is a distinguished circular reference, unless two or more feature values share a single -typed TFS value, in which case one is a circular reference and the rest point to it.
The circular one can be chosen canonically to ensure that the encoding is still classical.
(MOD/PRD representation) problem is that features may be introduced at joinreducible types, as in Figure 7.
There is only one module, so the frames for a and b must have a slot available for the feature F.
When an a-typed TFS unifies with a b-typed TFS, the result will be of type c, so leaving the slot marked unused after recursion would be incorrect -we would need to look in a table to see what value to assign it.
An alternative would be to place that value in the frames for a and b from the beginning.
But since the value itself must be of type a in the case of Figure 7, this strategy would not yield a finite representation.
The answer to this conundrum is to use a distinguished circular reference in a slot iff the slot is either unused or the value it contains is (1) the most general satisfier of the value restriction of the feature it represents and (2) not structure-shared with any other feature in the TFS.5 During unification, if one TFS is a circular reference, and the other is not, the circular reference is referenced to the other.
If both values are circular references, then one is referenced to the other, which remains circular.
The feature structure in Figure 8, for example, has the frame representation shown in Figure 9.
The PRD value is a TFS of type bool, and this value is not shared with any other structure in the TFS.
If the values of MOD and PRD are both bool-typed, then if Figure 10: A TFS of type head in which both feature values are most general satisfiers of the value restrictions, but they are shared.
lar references (Figure 11), and if they are not shared (Figure 12), both of them use a different circular reference (Figure 13).
With this convention for circular references, frames are a classical join-preserving encoding of the TFSs of any statically typable type system.
Although space does not permit a complete proof here, the intuition is that (1) most general satisfiers of value restrictions necessarily subsume every other value that a totally well-typed TFS could take at that feature, and (2) when features are introduced, their initial values are not structure-shared with any other substructure.
Static typability ensures that value restrictions unify to yield value restrictions, except in the final case of Theorem 1.
The following lemma deals with this case: Lemma 1 If Approp is statically typable, and for some F, F F, then either F, and or MOD Figure 8: A TFS of type head in which one feature value is a most general satisfier of its feature's value restriction.
head bool PQ plus bool they are shared (Figure 10), they do not use circu(head representation) (MOD representation) fihead MOD PRD (head representation) Figure 15: A statically typable "type system" that multiply introduces F at join-reducible elements with different value restrictions.
duction, but in fact, the result holds if we allow for multiple introducing types, provided that all of them agree on what the value restriction for the feature should be.
Would-be type systems that multiply introduce a feature at join-reducible elements (thus requiring some kind of distinguished-value encoding), disagree on the value restriction, and still remain statically typable are rather difficult to come by, but they do exist, and for them, a frame encoding will not work.
Figure 15 shows one such example.
In this signature, the unification: s Figure 13: The frame for Figure 12.
Proof: does not exist, but the unification of their frame encodings must succeed because the -typed TFS's F value must be encoded as a circular reference.
To the best of the author's knowledge, there is no fixedsize encoding for Figure 15.
5 Generalized
Term Encoding In practice, this classical encoding is not good for much.
Description languages typically need to bind variables to various substructures of a TFS,, and then pass those variables outside the substructures of where they can be used to instantiate the value of another feature structure's feature, or as arguments to some function call or procedural goal.
If a value in a single frame is a circular reference, we can properly understand what that reference encodes with the above convention by looking at its context, i.e., the type.
Outside the scope of that frame, we have no way of knowing which feature's value restriction it is supposed to encode.
Figure 14: The second case in the proof of Lemma 1.
. F, so F and F So there are three cases to consider: Intro2 F : then the result trivially holds.
Intro F but Intro F4 (or by symmetry, the opposite): then we have the situation in Figure 14.
It must be that F4, so by static typability, the lemma holds.
Intro F and Intro F : and F, so and F4 are consistent.
By bounded completeness, F and F . By upward closure, F F4 and by static typability, F F4 F F.
Furthermore, F ; thus by static typability the lemma holds.
This lemma is very significant in its own right -it says that we know more than Carpenter's Theorem 1.
An introduced feature's value restriction can always be predicted in a statically typable type system.
The lemma implicitly relies on feature introF F Suppose Figure 12: A TFS of type head in which both feature values are most general satisfiers of the value restrictions, and they are not shared.
F: F: bool bool H' F: PQ FG!
FG! FG!
E Introduced feature has variable encoding Figure 16: A pictorial overview of the generalized encoding.
A generalized term encoding provides an elegant solution to this problem.
When a variable is bound to a substructure that is a circular reference, it can be filled in with a frame for the most general satisfier that it represents and then passed out of context.
Having more than one representative for the original TFS is consistent, because the set of representatives is closed under this filling operation.
A schematic overview of the generalized encoding is in Figure 16.
Every set of frames that encode a particular TFS has a least element, in which circular references are always opted for as introduced feature values.
This is the same element as the classical encoding.
It also has a greatest element, in which every unused slot still has a circular reference, but all unshared most general satisfiers are filled in with frames.
Whenever we bind a variable to a substructure of a TFS, filling pushes the TFS's encoding up within the same set to some other encoding.
As a result, at any given point in time during a computation, we do not exactly know which encoding we are using to represent a given TFS.
Furthermore, when two TFSs are unified successfully, we do not know exactly what the result will be, but we do know that it falls inside the correct set of representatives because there is at least one frame with circular references for the values of every newly introduced feature.
variable binding 6 Conclusion Simple frames with extra slots and a convention for filling in feature values provide a join-preserving encoding of any statically typable type system, with no resizing and no referencing beyond that of type representations.
A frame thus remains stationary in memory once it is allocated.
A generalized encoding, moreover, is robust to side-effects such as extra-logical variable-sharing.
Frames have many potential implementations, including Prolog terms, WAM-style heap frames, or fixed-sized records.
References A.
Aggoun and N.
Beldiceanu. 1990.
Time stamp techniques for the trailed data in constraint logic programming systems.
In S.
Bourgault and M.
Dincbas, editors, Programmation en Logique, Actes du 8eme Seminaire, pages 487509.
U. Callmeier.
2001. Efficient parsing with large-scale unification grammars.
Master's thesis, Universitaet des Saarlandes.
B. Carpenter.
1992. The Logic of Typed Feature Structures.
Cambridge. G.
Erbach. 1994.
Multi-dimensional inheritance.
In Proceedings of KONVENS 94.
Springer. C.
Holzbaur. 1992.
Metastructures vs.
attributed variables in the context of extensible unification.
In M.
Bruynooghe and M.
Wirsing, editors, Programming Language Implementation and Logic Programming, pages 260268.
Springer Verlag.
LinGO. 1999.
The LinGO grammar and lexicon.
Available on-line at http://lingo.stanford.edu.
C. Mellish.
1991. Graph-encodable description spaces.
Technical report, University of Edinburgh Department of Artificial Intelligence.
DYANA Deliverable R3.2B.
C. Mellish.
1992. Term-encodable description spaces.
In D.R.
Brough, editor, Logic Programming: New Frontiers, pages 189207.
Kluwer. G.
Penn. 1993.
The ALE HPSG benchmark grammar.
Available on-line at http://www.cs.toronto.edu/ gpenn/ale.html.
G. Penn.
1999. An optimized Prolog encoding of typed feature structures.
In Proceedings of the 16th International Conference on Logic Programming (ICLP-99), pages 124138.
G. Penn.
2000. The Algebraic Structure of Attributed Type Signatures.
Ph.D. thesis, Carnegie Mellon University .
A High-Performance Semi-Supervised Learning Method for Text Chunking Rie Kubota Ando Tong Zhang IBM T.J.
Watson Research Center Yorktown Heights, NY 10598, U.S.A.
rie1@us.ibm.com tongz@us.ibm.com Abstract In machine learning, whether one can build a more accurate classifier by using unlabeled data (semi-supervised learning) is an important issue.
Although a number of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear.
This paper presents a novel semi-supervised method that employs a learning paradigm which we call structural learning.
The idea is to find "what good classifiers are like" by learning from thousands of automatically generated auxiliary classification problems on unlabeled data.
By doing so, the common predictive structure shared by the multiple classification problems can be discovered, which can then be used to improve performance on the target problem.
The method produces performance higher than the previous best results on CoNLL'00 syntactic chunking and CoNLL'03 named entity chunking (English and German).
(Blum and Mitchell, 1998) automatically bootstraps labels, and such labels are not necessarily reliable (Pierce and Cardie, 2001).
A related idea is to use Expectation Maximization (EM) to impute labels.
Although useful under some circumstances, when a relatively large amount of labeled data is available, the procedure often degrades performance (e.g.
Merialdo (1994)).
A number of bootstrapping methods have been proposed for NLP tasks (e.g.
Yarowsky (1995), Collins and Singer (1999), Riloff and Jones (1999)).
But these typically assume a very small amount of labeled data and have not been shown to improve state-of-the-art performance when a large amount of labeled data is available.
Our goal has been to develop a general learning framework for reliably using unlabeled data to improve performance irrespective of the amount of labeled data available.
It is exactly this important and difficult problem that we tackle here.
This paper presents a novel semi-supervised method that employs a learning framework called structural learning (Ando and Zhang, 2004), which seeks to discover shared predictive structures (i.e.
what good classifiers for the task are like) through jointly learning multiple classification problems on unlabeled data.
That is, we systematically create thousands of problems (called auxiliary problems) relevant to the target task using unlabeled data, and train classifiers from the automatically generated `training data'.
We learn the commonality (or structure) of such many classifiers relevant to the task, and use it to improve performance on the target task.
One example of such auxiliary problems for chunking tasks is to `mask' a word and predict whether it is "people" or not from the context, like language modeling.
Another example is to predict the pre1 Introduction In supervised learning applications, one can often find a large amount of unlabeled data without difficulty, while labeled data are costly to obtain.
Therefore, a natural question is whether we can use unlabeled data to build a more accurate classifier, given the same amount of labeled data.
This problem is often referred to as semi-supervised learning.
Although a number of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear.
For example, co-training Proceedings of the 43rd Annual Meeting of the ACL, pages 19, Ann Arbor, June 2005.
c 2005 Association for Computational Linguistics fidiction of some classifier trained for the target task.
These auxiliary classifiers can be adequately learned since we have very large amounts of `training data' for them, which we automatically generate from a very large amount of unlabeled data.
The contributions of this paper are two-fold.
First, we present a novel robust semi-supervised method based on a new learning model and its application to chunking tasks.
Second, we report higher performance than the previous best results on syntactic chunking (the CoNLL'00 corpus) and named entity chunking (the CoNLL'03 English and German corpora).
In particular, our results are obtained by using unlabeled data as the only additional resource while many of the top systems rely on hand-crafted resources such as large name gazetteers or even rulebased post-processing.
model complexity.
ERM-based methods for discriminative learning are known to be effective for NLP tasks such as chunking (e.g.
Kudoh and Matsumoto (2001), Zhang and Johnson (2003)).
2.2 Linear
model for structural learning We present a linear prediction model for structural learning, which extends the traditional model to multiple problems.
Specifically, we assume that there exists a low-dimensional predictive structure shared by multiple prediction problems.
We seek to discover this structure through joint empirical risk minimization over the multiple problems.
Consider  problems indexed by   , each with  samples   indexed by   . In our joint linear model, a predictor for problem takes the following form 2 A Model for Learning Structures This work uses a linear formulation of structural learning.
We first briefly review a standard linear prediction model and then extend it for structural learning.
We sketch an optimization algorithm using SVD and compare it to related methods.
2.1 Standard
linear prediction model In the standard formulation of supervised learning, we seek a predictor that maps an input vector  to the corresponding output   . Linear prediction models are based on real-valued predictors of  the form  where is called a weight vector.
For binary problems, the sign of the linear prediction gives the class label.
For -way classification (with ), a typical method is winner takes all, where we train one predictor per class and choose the class with the highest output value.
A frequently used method for finding an accurate predictor is regularized empirical risk minimization (ERM), which minimizes an empirical loss of the predictor (with regularization) on the  training examples  : where we use to denote the identity matrix.
Matrix  (whose rows are orthonormal) is the common structure parameter shared by all the problems; and are weight vectors specific to each prediction problem . The idea of this model is to discover a common low-dimensional predictive structure (shared by the  problems) parameterized by the projection matrix .
In this setting, the goal of structural learning may also be regarded as learning a good feature map  -a low-dimensional feature vector parameterized by .
In joint ERM, we seek  (and weight vectors) that minimizes the empirical risk summed over all the problems: It can be shown that using joint ERM, we can reliably estimate the optimal joint parameter  as long as  is large (even when each  is small).
This is the key reason why structural learning is effective.
A formal PAC-style analysis can be found in (Ando and Zhang, 2004).
2.3 Alternating
structure optimization (ASO)  is a loss function to quantify the difference between the prediction   and the true output, and  is a regularization term to control the The optimization problem (2) has a simple solution using SVD when we choose square regularization: ter is given.
For clarity, let be a weight vector   Then, for problem such that: (2) becomes the minimization of the joint empirical risk written as: Input: training data ( ) Parameters: dimension and regularization param Output: matrix with rows , and arbitrary Initialize: iterate to  do for With fixed and, solve for : This minimization can be approximately solved by the following alternating optimization procedure: minimizes the joint empirical risk ((3)., and find  minimizes the joint empirical risk (3).
that  Let endfor Compute the SVD of   Let the rows of be the left singular vectors of corresponding to the largest singular values.
until converge Figure 1: SVD-based Alternating Structure Optimization (SVD-ASO) Algorithm  Iterate until a convergence criterion is met.
In the first step, we train  predictors independently.
It is the second step that couples all the problems.
Its solution is given by the SVD (singular value decomposition) of the predictor matrix   : the rows of the optimum  are given by the most sigIntuitively, the nificant left singular vectors1 of optimum  captures the maximal commonality of the  predictors (each derived from ).
These  predictors are updated using the new structure matrix  in the next iteration, and the process repeats.
Figure 1 summarizes the algorithm sketched above, which we call the alternating structure optimization (ASO) algorithm.
The formal derivation can be found in (Ando and Zhang, 2004).
Comparison with existing techniques It is important to note that this SVD-based ASO (SVD-ASO) procedure is fundamentally different from the usual principle component analysis (PCA), which can be regarded as dimension reduction in the data space . By contrast, the dimension reduction performed in the SVD-ASO algorithm is on the predictor space (a set of predictors).
This is possible because we observe multiple predictors from multiple learning tasks.
If we regard the observed predictors as sample points of the predictor distribution in 1 is computed so that the best low-rank In other words, in the least square sense is obtained by approximation of see e.g.
Golub and Loan projecting onto the row space of (1996) for SVD.
the predictor space (corrupted with estimation error, or noise), then SVD-ASO can be interpreted as finding the "principle components" (or commonality) of these predictors (i.e., "what good predictors are like").
Consequently the method directly looks for low-dimensional structures with the highest predictive power.
By contrast, the principle components of input data in the data space (which PCA seeks) may not necessarily have the highest predictive power.
The above argument also applies to the feature generation from unlabeled data using LSI (e.g.
Ando (2004)).
Similarly, Miller et al.(2004) used word-cluster memberships induced from an unannotated corpus as features for named entity chunking.
Our work is related but more general, because we can explore additional information from unlabeled data using many different auxiliary problems.
Since Miller et al.(2004)'s experiments used a proprietary corpus, direct performance comparison is not possible.
However, our preliminary implementation of the word clustering approach did not provide any improvement on our tasks.
As we will see, our starting performance is already high.
Therefore the additional information discovered by SVD-ASO appears crucial to achieve appreciable improvements.
3 Semi-supervised Learning Method For semi-supervised learning, the idea is to create many auxiliary prediction problems (relevant to the task) from unlabeled data so that we can learn the fishared structure  (useful for the task) using the ASO algorithm.
In particular, we want to create auxiliary problems with the following properties: we need to automatically generate various "labeled" data for the auxiliary problems from unlabeled data.
auxiliary problems should be related to the target problem.
That is, they should share a certain predictive structure.
The final classifier for the target task is in the form of (1), a linear predictor for structural learning.
We fix  (learned from unlabeled data through auxiliary problems) and optimize weight vectors and on the given labeled data.
We summarize this semisupervised learning procedure below.
Ex 3.1 Predict words.
Create auxiliary problems by regarding the word at each position as an auxiliary label, which we want to predict from the context.
For instance, predict whether a word is "Smith" or not from its context.
This problem is relevant to, for instance, named entity chunking since knowing a word is "Smith" helps to predict whether it is part of a name.
One binary classification problem can be created for each possible word value (e.g., "IBM", "he", "get",    ).
Hence, many auxiliary problems can be obtained using this idea.
More generally, given a feature representation of the input data, we may mask some features as unobserved, and learn classifiers to predict these `masked' features based on other features that are not masked.
The automatic-labeling requirement is satisfied since the auxiliary labels are observable to us.
To create relevant problems, we should choose to (mask and) predict features that have good correlation to the target classes, such as words on text tagging/chunking tasks.
3.1.2 Partially-supervised strategy Auxiliary problem creation The idea is to discover useful features (which do not necessarily appear in the labeled data) from the unlabeled data through learning auxiliary problems.
Clearly, auxiliary problems more closely related to the target problem will be more beneficial.
However, even if some problems are less relevant, they will not degrade performance severely since they merely result in some irrelevant features (originated from irrelevant -components), which ERM learners can cope with.
On the other hand, potential gains from relevant auxiliary problems can be significant.
In this sense, our method is robust.
We present two general strategies for generating useful auxiliary problems: one in a completely unsupervised fashion, and the other in a partiallysupervised fashion.
3.1.1 Unsupervised
strategy In the first strategy, we regard some observable as auxiliary class substructures of the input data labels, and try to predict these labels using other parts of the input data.
The second strategy is motivated by co-training.
We use two (or more) distinct feature maps:  and  . First, we train a classifier  for the target task, using the feature map  and the labeled data.
The auxiliary tasks are to predict the behavior of this classifier  (such as predicted labels) on the unlabeled data, by using the other feature map  . Note that unlike co-training, we only use the classifier as a means of creating auxiliary problems that meet the relevancy requirement, instead of using it to bootstrap labels.
Ex 3.2 Predict the topchoices of the classifier.
Predict the combination of (a few) classes to which  assigns the highest output (confidence) values.
For instance, predict whether  assigns the highest confidence values to CLASS1 and CLASS2 in this order.
By setting , the auxiliary task is simply to predict the label prediction of classifier  . By set, fine-grained distinctions (related to inting trinsic sub-classes of target classes) can be learned.
From a -way classification problem,   binary prediction problems can be created.
fi4 Algorithms Used in Experiments Using auxiliary problems introduced above, we study the performance of our semi-supervised learning method on named entity chunking and syntactic chunking.
This section describes the algorithmic aspects of the experimental framework.
The taskspecific setup is described in Sections 5 and 6.
4.1 Extension
of the basic SVD-ASO algorithm In our experiments, we use an extension of SVDASO.
In NLP applications, features have natural grouping according to their types/origins such as `current words', `parts-of-speech on the right', and so forth.
It is desirable to perform a localized optimization for each of such natural feature groups.
Hence, we associate each feature group with a submatrix of structure matrix .
The optimization algorithm for this extension is essentially the same as SVD-ASO in Figure 1, but with the SVD step performed separately for each group.
See (Ando and Zhang, 2004) for the precise formulation.
In addition, we regularize only those components of which correspond to the non-negative part of . The motivation is that positive weights are usually directly related to the target concept, while negative ones often yield much less specific information representing `the others'.
The resulting extension, in effect, only uses the positive components of in the SVD computation.
(Zhang, 2004).
As we will show in Section 7.3, our formulation (rowis relatively insensitive to the change in dimension of the structure matrix).
We fix (for each feature group) to 50, and use it in all settings.
The most time-consuming process is the training of  auxiliary predictors on the unlabeled data in Figure 1).
Fixing the number of (computing iterations to a constant, it runs in linear to  and the number of unlabeled instances and takes hours in our settings that use more than 20 million unlabeled instances.
Baseline algorithms Supervised classifier For comparison, we train a classifier using the same features and algorithm, but in effect).
without unlabeled data ( Chunking algorithm, loss function, training algorithm, and parameter settings Co-training We test co-training since our idea of partially-supervised auxiliary problems is motivated by co-training.
Our implementation follows the original work (Blum and Mitchell, 1998).
The two (or more) classifiers (with distinct feature maps) are trained with labeled data.
We maintain a pool of  unlabeled instances by random selection.
The classifier proposes labels for the instances in this pool.
We choose  instances for each classifier with high confidence while preserving the class distribution observed in the initial labeled data, and add them to the labeled data.
The process is then repeated.
We explore =50K, 100K, =50,100,500,1K, and commonly-used feature splits: `current vs.
context' and `current+left-context vs.
current+right-context'. Self-training Single-view bootstrapping is sometimes called self-training.
We test the basic selftraining2, which replaces multiple classifiers in the co-training procedure with a single classifier that employs all the features.
co/self-training oracle performance To avoid the issue of parameter selection for the coand selftraining, we report their best possible oracle performance, which is the best F-measure number among all the coand self-training parameter settings including the choice of the number of iterations.
2 We
also tested "self-training with bagging", which Ng and Cardie (2003) used for co-reference resolution.
We omit results since it did not produce better performance than the supervised baseline.
As is commonly done, we encode chunk information into word tags to cast the chunking problem to that of sequential word tagging.
We perform Viterbistyle decoding to choose the word tag sequence that maximizes the sum of tagging confidence values.
In all settings (including baseline methods), the loss function is a modification of the Huber's ro   bust loss for regression:    if  ; and  otherwise; with square regularization (  ).
One may select other loss functions such as SVM or logistic regression.
The specific choice is not important for the purpose of this paper.
The training algorithm is stochastic gradient descent, which is argued to perform well for regularized convex ERM learning formulations words, parts-of-speech (POS), character types, 4 characters at the beginning/ending in a 5-word window.
words in a 3-syntactic chunk window.
labels assigned to two words on the left.
bi-grams of the current word and the label on the left.
labels assigned to previous occurrences of the current word.
Figure 2: Feature types for named entity chunking.
POS and syntactic chunk information is provided by the organizer.
# of aux.
problems 1000 1000 1000 72 72 72 72 Auxiliary labels previous words current words next words  's top-2 choices  's top-2 choices  's top-2 choices 's top-2 choices Features used for learning aux problems all but previous words all but current words all but next words  (all but left context)  (left context) (all but right context)  (right context) Figure 3: Auxiliary problems used for named entity chunking.
3000 problems `mask' words and predict them from the other features on unlabeled data.
288 problems predict classifier 's predictions on unlabeled data, where is trained with labeled data using feature map . There are 72 possible top-2 choices from 9 classes (beginning/inside of four types of name chunks and `outside').
5 Named
Entity Chunking Experiments We report named entity chunking performance on the CoNLL'03 shared-task3 corpora (English and German).
We choose this task because the original intention of this shared task was to test the effectiveness of semi-supervised learning methods.
However, it turned out that none of the top performing systems used unlabeled data.
The likely reason is that the number of labeled data is relatively large ( 200K), making it hard to benefit from unlabeled data.
We show that our ASO-based semi-supervised learning method (hereafter, ASO-semi) can produce results appreciably better than all of the top systems, by using unlabeled data as the only additional resource.
In particular, we do not use any gazetteer information, which was used in all other systems.
The CoNLL corpora are annotated with four types of named entities: persons, organizations, locations, and miscellaneous names (e.g., "World Cup").
We use the official training/development/test splits.
Our unlabeled data sets consist of 27 million words (English) and 35 million words (German), respectively.
They were chosen from the same sources  Reuters and ECI Multilingual Text Corpus  as the provided corpora but disjoint from them.
5.1 Features
of the classifier" using feature splits `left context vs.
the others' and `right context vs.
the others'.
For word-prediction problems, we only consider the instances whose current words are either nouns or adjectives since named entities mostly consist of these types.
Also, we leave out all but at most 1000 binary prediction problems of each type that have the largest numbers of positive examples to ensure that auxiliary predictors can be adequately learned with a sufficiently large number of examples.
The results we report are obtained by using all the problems in Figure 3 unless otherwise specified.
5.3 Named
entity chunking results methods Our feature representation is a slight modification of a simpler configuration (without any gazetteer) in (Zhang and Johnson, 2003), as shown in Figure 2.
We use POS and syntactic chunk information provided by the organizer.
5.2 Auxiliary
problems test diff.
from supervised data F prec.
recall F English, small (10K examples) training set ASO-semi dev.
81.25 +10.02 +7.00 +8.51 co/self oracle 73.10 +0.32 +0.39 +0.36 ASO-semi test 78.42 +9.39 +10.73 +10.10 co/self oracle 69.63 +0.60 +1.95 +1.31 English, all (204K) training examples ASO-semi dev.
93.15 +2.25 +3.00 +2.62 co/self oracle 90.64 +0.04 +0.20 +0.11 ASO-semi test 89.31 +3.20 +4.51 +3.86 co/self oracle 85.40 German, all (207K) training examples ASO-semi dev.
74.06 +7.04 +10.19 +9.22 co/self oracle 66.47 +4.39 +1.63 ASO-semi test 75.27 +4.64 +6.59 +5.88 co/self oracle 70.45 +2.59 +1.06 Figure 4: Named entity chunking results.
No gazetteer.
Fmeasure and performance improvements over the supervised baseline in precision, recall, and F.
For coand self-training (baseline), the oracle performance is shown.
As shown in Figure 3, we experiment with auxiliary problems from Ex 3.1 and 3.2: "Predict current (or previous or next) words"; and "Predict top-2 choices Figure 4 shows results in comparison with the supervised baseline in six configurations, each trained fiwith one of three sets of labeled training examples: a small English set (10K examples randomly chosen), the entire English training set (204K), and the entire German set (207K), tested on either the development set or test set.
ASO-semi significantly improves both precision and recall in all the six configurations, resulting in improved F-measures over the supervised baseline by +2.62% to +10.10%.
Coand self-training, at their oracle performance, improve recall but often degrade precision; consequently, their F-measure improvements are relatively low: 0.05% to +1.63%.
Comparison with top systems As shown in Figure 5, ASO-semi achieves higher performance than the top systems on both English and German data.
Most of the top systems boost performance by external hand-crafted resources such as: large gazetteers4 ; a large amount (2 million words) of labeled data manually annotated with finer-grained named entities (FIJZ03); and rule-based post processing (KSNM03).
Hence, we feel that our results, obtained by using unlabeled data as the only additional resource, are encouraging.
System ASO-semi FIJZ03 CN03 KSNM03 Eng.
89.31 88.76 88.31 86.31 Ger.
75.27 72.41 65.67 71.90 Additional resources unlabeled data gazetteers; 2M-word labeled data (English) gazetteers (English); (also very elaborated features) rule-based post processing uniand bi-grams of words and POS in a 5-token window.
word-POS bi-grams in a 3-token window.
POS tri-grams on the left and right.
labels of the two words on the left and their bi-grams.
bi-grams of the current word and two labels on the left.
Figure 6: Feature types for syntactic chunking.
POS information is provided by the organizer.
prec. 93.83 94.57 93.76 recall 93.37 94.20 93.56 supervised ASO-semi co/self oracle Figure 7: Syntactic chunking results.
use the WSJ articles in 1991 (15 million words) from the TREC corpus as the unlabeled data.
6.1 Features
and auxiliary problems Our feature representation is a slight modification of a simpler configuration (without linguistic features) in (Zhang et al., 2002), as shown in Figure 6.
We use the POS information provided by the organizer.
The types of auxiliary problems are the same as in the named entity experiments.
For word predictions, we exclude instances of punctuation symbols.
6.2 Syntactic
chunking results Figure 5: Named entity chunking.
F-measure on the test sets.
Previous best results: FIJZ03 (Florian et al., 2003), CN03 (Chieu and Ng, 2003), KSNM03 (Klein et al., 2003).
As shown in Figure 7, ASO-semi improves both precision and recall over the supervised baseline.
It   in F-measure, which outperforms achieves the supervised baseline by  .
Coand selftraining again slightly improve recall but slightly degrade precision at their oracle performance, which demonstrates that it is not easy to benefit from unlabeled data on this task.
Comparison with the previous best systems As shown in Figure 8, ASO-semi achieves performance higher than the previous best systems.
Though the space constraint precludes providing the detail, we note that ASO-semi outperforms all of the previous top systems in both precision and recall.
Unlike named entity chunking, the use of external resources on this task is rare.
An exception is the use of output from a grammar-based full parser as features in ZDJ02+, which our system does not use.
KM01 and CM03 boost performance by classifier combinations.
SP03 trains conditional random fields for NP 6 Syntactic Chunking Experiments Next, we report syntactic chunking performance on the CoNLL'00 shared-task5 corpus.
The training and test data sets consist of the Wall Street Journal corpus (WSJ) sections 1518 (212K words) and section 20, respectively.
They are annotated with eleven types of syntactic chunks such as noun phrases.
We 4 Whether or not gazetteers are useful depends on their coverage.
A number of top-performing systems used their own gazetteers in addition to the organizer's gazetteers and reported significant performance improvements (e.g., FIJZ03, CN03, and ZJ03).
5 http://cnts.uia.ac.be/conll2000/chunking firow# ASO-semi KM01 CM03 SP03 ZDJ02 ZDJ02+ all 94.39 93.91 93.74  93.57 94.17 NP 94.70 94.39 94.41 94.38 93.89 94.38 description +unlabeled data SVM combination perceptron in two layers conditional random fields generalized Winnow +full parser output 4 7 9 11 15 26 Figure 8: Syntactic chunking F-measure.
Comparison with previous best results: KM01 (Kudoh and Matsumoto, 2001), CM03 (Carreras and Marquez, 2003), SP03 (Sha and Pereira, 2003), ZDJ02 (Zhang et al., 2002).
Features corresponding to significant entries Ltd, Inc, Plc, International, Ltd., Association, Group, Inc.
Co, Corp, Co., Company, Authority, Corp., Services PCT, N/A, Nil, Dec, BLN, Avg, Year-on-year, UNCH New, France, European, San, North, Japan, Asian, India Peter, Sir, Charles, Jose, Paul, Lee, Alan, Dan, John, James June, May, July, Jan, March, August, September, April Interpretation organizations organizations no names locations persons months (noun phrases) only.
ASO-semi produces higher NP chunking performance than the others.
Figure 10: Interpretation of computed from wordprediction (unsupervised) problems for named entity chunking.
words beginning with upper-case letters (i.e., likely to be names in English).
Our method captures the spirit of predictive word-clustering but is more general and effective on our tasks.
It is possible to develop a general theory to show that the auxiliary problems we use are helpful under reasonable conditions.
The intuition is as follows.
Suppose we split the features into two parts  and  and predict  based on .
Suppose features in  are correlated to the class labels (but not necessarily correlated among themselves).
Then, the auxiliary prediction problems are related to the target task, and thus can reveal useful structures of  . Under some conditions, it can be shown that features in  with similar predictive performance tend to map to similar low-dimensional vectors through .
This effect can be empirically observed in Figure 10 and will be formally shown elsewhere.
7 Empirical
Analysis 7.1 Effectiveness of auxiliary problems English named entity German named entity 90 76 89 74 88 72 87 70 86 68 85 1 supervised set dev w/ "Predict (previous, current, or next) words" w/ "Predict top-2 choices" w/ "Predict words" + "Predict top-2 choices" Figure 9: Named entity F-measure produced by using individual types of auxiliary problems.
Trained with the entire training sets and tested on the test sets.
Figure 9 shows F-measure obtained by computing  from individual types of auxiliary problems on named entity chunking.
Both types  "Predict words" and "Predict top-2 choices of the classifier"  are useful, producing significant performance improvements over the supervised baseline.
The best performance is achieved when  is produced from all of the auxiliary problems.
7.2 Interpretation
of  Effect of the  dimension 20 40 ASO-semi supervised 60 80 100 dimension To gain insights into the information obtained from unlabeled data, we examine the  entries associated with the feature `current words', computed for the English named entity task.
Figure 10 shows the features associated with the entries of  with the largest values, computed from the 2000 unsupervised auxiliary problems: "Predict previous words" and "Predict next words".
For clarity, the figure only shows Figure 11: F-measure in relation to the row-dimension of English named entity chunking, test set.
Recall that throughout the experiments, we fix the row-dimension of  (for each feature group) to 50.
Figure 11 plots F-measure in relation to the rowdimension of , which shows that the method is relatively insensitive to the change of this parameter, at least in the range which we consider.
We presented a novel semi-supervised learning method that learns the most predictive lowdimensional feature projection from unlabeled data using the structural learning algorithm SVD-ASO.
On CoNLL'00 syntactic chunking and CoNLL'03 named entity chunking (English and German), the method exceeds the previous best systems (including those which rely on hand-crafted resources) by using unlabeled data as the only additional resource.
The key idea is to create auxiliary problems automatically from unlabeled data so that predictive structures can be learned from that data.
In practice, it is desirable to create as many auxiliary problems as possible, as long as there is some reason to believe in their relevancy to the task.
This is because the risk is relatively minor while the potential gain from relevant problems is large.
Moreover, the auxiliary problems used in our experiments are merely possible examples.
One advantage of our approach is that one may design a variety of auxiliary problems to learn various aspects of the target problem from unlabeled data.
Structural learning provides a framework for carrying out possible new ideas.
Acknowledgments Part of the work was supported by ARDA under the NIMD program PNWD-SW-6059.
References Rie Kubota Ando and Tong Zhang.
2004. A framework for learning predictive structures from multiple tasks and unlabeled data.
Technical report, IBM.
RC23462. Rie Kubota Ando.
2004. Semantic lexicon construction: Learning from unlabeled data via spectral analysis.
In Proceedings of CoNLL-2004.
Avrim Blum and Tom Mitchell.
1998. Combining labeled and unlabeled data with co-training.
In proceedings of COLT-98.
Xavier Carreras and Lluis Marquez.
2003. Phrase recognition by filtering and ranking with perceptrons.
In Proceedings of RANLP-2003.
Hai Leong Chieu and Hwee Tou Ng.
2003. Named entity recognition with a maximum entropy approach.
In Proceedings CoNLL-2003, pages 160163.
Michael Collins and Yoram Singer.
1999. Unsupervised models for named entity classification.
In Proceedings of EMNLP/VLC'99.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and Tong Zhang.
2003. Named entity recognition through classifier combination.
In Proceedings CoNLL-2003, pages 168171.
Gene H.
Golub and Charles F.
Van Loan.
1996. Matrix computations third edition.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christopher D.
Manning. 2003.
Named entity recognition with character-level models.
In Proceedings CoNLL2003, pages 188191.
Taku Kudoh and Yuji Matsumoto.
2001. Chunking with support vector machines.
In Proceedings of NAACL 2001.
Bernard Merialdo.
1994. Tagging English text with a probabilistic model.
Computational Linguistics, 20(2):155171.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discriminative training.
In Proceedings of HLT-NAACL-2004.
Vincent Ng and Claire Cardie.
2003. Weakly supervised natural language learning without redundant views.
In Proceedings of HLT-NAACL-2003.
David Pierce and Claire Cardie.
2001. Limitations of co-training for natural language learning from large datasets.
In Proceedings of EMNLP-2001.
Ellen Riloff and Rosie Jones.
1999. Learning dictionaries for information extraction by multi-level bootstrapping.
In Proceedings of AAAI-99.
Fei Sha and Fernando Pereira.
2003. Shallow parsing with conditional random fields.
In Proceedings of HLT-NAACL'03.
David Yarowsky.
1995. Unsupervised word sense disambiguation rivaling supervised methods.
In Proceedings of ACL-95.
Tong Zhang and David E.
Johnson. 2003.
A robust risk minimization based named entity recognition system.
In Proceedings CoNLL-2003, pages 204207.
Tong Zhang, Fred Damerau, and David E.
Johnson. 2002.
Text chunking based on a generalization of Winnow.
Journal of Machine Learning Research, 2:615 637.
Tong Zhang.
2004. Solving large scale linear prediction problems using stochastic gradient descent algorithms.
In ICML 04, pages 919926 .
Scaling Conditional Random Fields Using Error-Correcting Codes Trevor Cohn Andrew Smith Department of Computer Science Division of Informatics and Software Engineering University of Edinburgh University of Melbourne, Australia United Kingdom tacohn@csse.unimelb.edu.au a.p.smith-2@sms.ed.ac.uk Miles Osborne Division of Informatics University of Edinburgh United Kingdom miles@inf.ed.ac.uk Abstract Conditional Random Fields (CRFs) have been applied with considerable success to a number of natural language processing tasks.
However, these tasks have mostly involved very small label sets.
When deployed on tasks with larger label sets, the requirements for computational resources mean that training becomes intractable.
This paper describes a method for training CRFs on such tasks, using error correcting output codes (ECOC).
A number of CRFs are independently trained on the separate binary labelling tasks of distinguishing between a subset of the labels and its complement.
During decoding, these models are combined to produce a predicted label sequence which is resilient to errors by individual models.
Error-correcting CRF training is much less resource intensive and has a much faster training time than a standardly formulated CRF, while decoding performance remains quite comparable.
This allows us to scale CRFs to previously impossible tasks, as demonstrated by our experiments with large label sets.
models that define a conditional distribution over label sequences given an observation sequence.
They allow the use of arbitrary, overlapping, non-independent features as a result of their global conditioning.
This allows us to avoid making unwarranted independence assumptions over the observation sequence, such as those required by typical generative models.
Efficient inference and training methods exist when the graphical structure of the model forms a chain, where each position in a sequence is connected to its adjacent positions.
CRFs have been applied with impressive empirical results to the tasks of named entity recognition (McCallum and Li, 2003), simplified part-of-speech (POS) tagging (Lafferty et al., 2001), noun phrase chunking (Sha and Pereira, 2003) and extraction of tabular data (Pinto et al., 2003), among other tasks.
CRFs are usually estimated using gradient-based methods such as limited memory variable metric (LMVM).
However, even with these efficient methods, training can be slow.
Consequently, most of the tasks to which CRFs have been applied are relatively small scale, having only a small number of training examples and small label sets.
For much larger tasks, with hundreds of labels and millions of examples, current training methods prove intractable.
Although training can potentially be parallelised and thus run more quickly on large clusters of computers, this in itself is not a solution to the problem: tasks can reasonably be expected to increase in size and complexity much faster than any increase in computing power.
In order to provide scalability, the factors which most affect the resource usage and runtime of the training method Introduction Conditional random fields (CRFs) (Lafferty et al., 2001) are probabilistic models for labelling sequential data.
CRFs are undirected graphical Proceedings of the 43rd Annual Meeting of the ACL, pages 1017, Ann Arbor, June 2005.
c 2005 Association for Computational Linguistics fimust be addressed directly  ideally the dependence on the number of labels should be reduced.
This paper presents an approach which enables CRFs to be used on larger tasks, with a significant reduction in the time and resources needed for training.
This reduction does not come at the cost of performance  the results obtained on benchmark natural language problems compare favourably, and sometimes exceed, the results produced from regular CRF training.
Error correcting output codes (ECOC) (Dietterich and Bakiri, 1995) are used to train a community of CRFs on binary tasks, with each discriminating between a subset of the labels and its complement.
Inference is performed by applying these `weak' models to an unknown example, with each component model removing some ambiguity when predicting the label sequence.
Given a sufficient number of binary models predicting suitably diverse label subsets, the label sequence can be inferred while being robust to a number of individual errors from the weak models.
As each of these weak models are binary, individually they can be efficiently trained, even on large problems.
The number of weak learners required to achieve good performance is shown to be relatively small on practical tasks, such that the overall complexity of error-correcting CRF training is found to be much less than that of regular CRF training methods.
We have evaluated the error-correcting CRF on the CoNLL 2003 named entity recognition (NER) task (Sang and Meulder, 2003), where we show that the method yields similar generalisation performance to standardly formulated CRFs, while requiring only a fraction of the resources, and no increase in training time.
We have also shown how the errorcorrecting CRF scales when applied to the larger task of POS tagging the Penn Treebank and also the even larger task of simultaneously noun phrase chunking (NPC) and POS tagging using the CoNLL 2000 data-set (Sang and Buchholz, 2000).
model are connected by edges to form a linear chain.
The joint distribution of the label sequence, y, given the input observation sequence, x, is given by p(y|x) = k fk (t, yt-1, yt, x) where T is the length of both sequences and k are the parameters of the model.
The functions fk are feature functions which map properties of the observation and the labelling into a scalar value.
Z(x) is the partition function which ensures that p is a probability distribution.
A number of algorithms can be used to find the optimal parameter values by maximising the loglikelihood of the training data.
Assuming that the training sequences are drawn IID from the population, the conditional log likelihood L is given by L= T (i) +1 (i) (i) Conditional random fields CRFs are undirected graphical models used to specify the conditional probability of an assignment of output labels given a set of input observations.
We consider only the case where the output labels of the where x(i) and y(i) are the ith observation and label sequence.
Note that a prior is often included in the L formulation; it has been excluded here for clarity of exposition.
CRF estimation methods include generalised iterative scaling (GIS), improved iterative scaling (IIS) and a variety of gradient based methods.
In recent empirical studies on maximum entropy models and CRFs, limited memory variable metric (LMVM) has proven to be the most efficient method (Malouf, 2002; Wallach, 2002); accordingly, we have used LMVM for CRF estimation.
Every iteration of LMVM training requires the computation of the log-likelihood and its derivative with respect to each parameter.
The partition function Z(x) can be calculated efficiently using dynamic programming with the forward algorithm.
Z(x) is given by y T (y) where are the forward values, defined recursively as t+1 (y) = t (y ) exp The derivative of the log-likelihood is given by L k = Error Correcting Output Codes T (i) +1 (i) (i) The first term is the empirical count of feature k, and the second is the expected count of the feature under the model.
When the derivative equals zero  at convergence  these two terms are equal.
Evaluating the first term of the derivative is quite simple.
However, the sum over all possible labellings in the second term poses more difficulties.
This term can be factorised, yielding p(Yt-1 = y, Yt = y|x(i) )fk (t, y, y, x(i) ) This term uses the marginal distribution over pairs of labels, which can be efficiently computed from the forward and backward values as t-1 (y ) exp The backward probabilities are defined by the recursive relation t (y) = t+1 (y ) exp Typically CRF training using LMVM requires many hundreds or thousands of iterations, each of which involves calculating of the log-likelihood and its derivative.
The time complexity of a single iteration is O(L2 N T F ) where L is the number of labels, N is the number of sequences, T is the average length of the sequences, and F is the average number of activated features of each labelled clique.
It is not currently possible to state precise bounds on the number of iterations required for certain problems; however, problems with a large number of sequences often require many more iterations to converge than problems with fewer sequences.
Note that efficient CRF implementations cache the feature values for every possible clique labelling of the training data, which leads to a memory requirement with the same complexity of O(L2 N T F )  quite demanding even for current computer hardware.
Since the time and space complexity of CRF estimation is dominated by the square of the number of labels, it follows that reducing the number of labels will significantly reduce the complexity.
Error-correcting coding is an approach which recasts multiple label problems into a set of binary label problems, each of which is of lesser complexity than the full multiclass problem.
Interestingly, training a set of binary CRF classifiers is overall much more efficient than training a full multi-label model.
This is because error-correcting CRF training reduces the L2 complexity term to a constant.
Decoding proceeds by predicting these binary labels and then recovering the encoded actual label.
Error-correcting output codes have been used for text classification, as in Berger (1999), on which the following is based.
Begin by assigning to each of the m labels a unique n-bit string Ci, which we will call the code for this label.
Now train n binary classifiers, one for each column of the coding matrix (constructed by taking the labels' codes as rows).
The j th classifier, j, takes as positive instances those with label i where Cij = 1.
In this way, each classifier learns a different concept, discriminating between different subsets of the labels.
We denote the set of binary classifiers as = { 1, 2,. . ., n }, which can be used for prediction as follows.
Classify a novel instance x with each of the binary classifiers, yielding a n-bit vector (x) = { 1 (x), 2 (x), . . ., n (x)}.
Now compare this vector to the codes for each label.
The vector may not exactly match any of the labels due to errors in the individual classifiers, and thus we chose the actual label which minimises the distance argmini ((x), Ci ).
Typically the Hamming distance is used, which simply measures the number of differing bit positions.
In this manner, prediction is resilient to a number of prediction errors by the binary classifiers, provided the codes for the labels are sufficiently diverse.
3.1 Error-correcting CRF training Error-correcting codes can also be applied to sequence labellers, such as CRFs, which are capable of multiclass labelling.
ECOCs can be used with CRFs in a similar manner to that given above for ficlassifiers.
A series of CRFs are trained, each on a relabelled variant of the training data.
The relabelling for each binary CRF maps the labels into binary space using the relevant column of the coding matrix, such that label i is taken as a positive for the j th model example if Cij = 1.
Training with a binary label set reduces the time and space complexity for each training iteration to O(N T F ); the L2 term is now a constant.
Provided the code is relatively short (i.e.
there are few binary models, or weak learners), this translates into considerable time and space savings.
Coding theory doesn't offer any insights into the optimal code length (i.e.
the number of weak learners).
When using a very short code, the error-correcting CRF will not adequately model the decision boundaries between all classes.
However, using a long code will lead to a higher degree of dependency between pairs of classifiers, where both model similar concepts.
The generalisation performance should improve quickly as the number of weak learners (code length) increases, but these gains will diminish as the inter-classifier dependence increases.
3.2 Error-correcting CRF decoding While training of error-correcting CRFs is simply a logical extension of the ECOC classifier method to sequence labellers, decoding is a different matter.
We have applied three decoding different strategies.
The Standalone method requires each binary CRF to find the Viterbi path for a given sequence, yielding a string of 0s and 1s for each model.
For each position t in the sequence, the tth bit from each model is taken, and the resultant bit string compared to each of the label codes.
The label with the minimum Hamming distance is then chosen as the predicted label for that site.
This method allows for error correction to occur at each site, however it discards information about the uncertainty of each weak learner, instead only considering the most probable paths.
The Marginals method of decoding uses the marginal probability distribution at each position in the sequence instead of the Viterbi paths.
This distribution is easily computed using the forward backward algorithm.
The decoding proceeds as before, however instead of a bit string we have a vector of probabilities.
This vector is compared to each of the label codes using the L1 distance, and the closest label is chosen.
While this method incorporates the uncertainty of the binary models, it does so at the expense of the path information in the sequence.
Neither of these decoding methods allow the models to interact, although each individual weak learner may benefit from the predictions of the other weak learners.
The Product decoding method addresses this problem.
It treats each weak model as an independent predictor of the label sequence, such that the probability of the label sequence given the observations can be re-expressed as the product of the probabilities assigned by each weak model.
A given labelling y is projected into a bit string for each weak learner, such that the ith entry in the string is Ckj for the j th weak learner, where k is the index of label yi . The weak learners can then estimate the probability of the bit string; these are then combined into a global product to give the probability of the label sequence p(y|x) = 1 Z (x) pj (bj (y)|x) where pj (q|x) is the predicted probability of q given x by the j th weak learner, bj (y) is the bit string representing y for the j th weak learner and Z (x) is the partition function.
The log probability is {Fj (bj (y), x)  j log Zj (x)} log Z (x) where Fj (y, x) = T +1 fj (t, yt-1, yt, x).
This log t=1 probability can then be maximised using the Viterbi algorithm as before, noting that the two log terms are constant with respect to y and thus need not be evaluated.
Note that this decoding is an equivalent formulation to a uniformly weighted logarithmic opinion pool, as described in Smith et al.(2005). Of the three decoding methods, Standalone has the lowest complexity, requiring only a binary Viterbi decoding for each weak learner.
Marginals is slightly more complex, requiring the forward and backward values.
Product, however, requires Viterbi decoding with the full label set, and many features  the union of the features of each weak learner  which can be quite computationally demanding.
fi3.3 Choice of code The accuracy of ECOC methods are highly dependent on the quality of the code.
The ideal code has diverse rows, yielding a high error-correcting capability, and diverse columns such that the weak learners model highly independent concepts.
When the number of labels, k, is small, an exhaustive code with every unique column is reasonable, given there are 2k-1 1 unique columns.
With larger label sets, columns must be selected with care to maximise the inter-row and inter-column separation.
This can be done by randomly sampling the column space, in which case the probability of poor separation diminishes quickly as the number of columns increases (Berger, 1999).
Algebraic codes, such as BCH codes, are an alternative coding scheme which can provide near-optimal error-correcting capability (MacWilliams and Sloane, 1977), however these codes provide no guarantee of good column separation.
Model Decoding Multiclass Coded standalone marginals product MLE 88.04 88.23 88.23 88.69 Regularised 89.78 88.67 89.19 89.69 Table 1: F1 scores on NER task.
trained without regularisation and with a Gaussian prior.
An exhaustive code was created with all 127 unique columns.
All of the weak learners were trained with the same feature set, each having around 315,000 features.
The performance of the standard and error-correcting models are shown in Table 1.
We tested for statistical significance using the matched pairs test (Gillick and Cox, 1989) at p < 0.001.
Those results which are significantly better than the corresponding multiclass MLE or regularised model are flagged with a, and those which are significantly worse with a . These results show that error-correcting CRF training achieves quite similar performance to the multiclass CRF on the task (which incidentally exceeds McCallum (2003)'s result of 89.0 using feature induction).
Product decoding was the better of the three methods, giving the best performance both with and without regularisation, although this difference was only statistically significant between the regularised standalone and the regularised product decoding.
The unregularised error-correcting CRF significantly outperformed the multiclass CRF with all decoding strategies, suggesting that the method already provides some regularisation, or corrects some inherent bias in the model.
Using such a large number of weak learners is costly, in this case taking roughly ten times longer to train than the multiclass CRF.
However, much shorter codes can also achieve similar results.
The simplest code, where each weak learner predicts only a single label (a.k.a.
one-vs-all), achieved an F score of 89.56, while only requiring 8 weak learners and less than half the training time as the multiclass CRF.
This code has no error correcting capability, suggesting that the code's column separation (and thus interdependence between weak learners) is more important than its row separation.
Experiments Our experiments show that error-correcting CRFs are highly accurate on benchmark problems with small label sets, as well as on larger problems with many more labels, which would be otherwise prove intractable for traditional CRFs.
Moreover, with a good code, the time and resources required for training and decoding can be much less than that of the standardly formulated CRF.
4.1 Named
entity recognition CRFs have been used with strong results on the CoNLL 2003 NER task (McCallum, 2003) and thus this task is included here as a benchmark.
This data set consists of a 14,987 training sentences (204,567 tokens) drawn from news articles, tagged for person, location, organisation and miscellaneous entities.
There are 8 IOB-2 style labels.
A multiclass (standardly formulated) CRF was trained on these data using features covering word identity, word prefix and suffix, orthographic tests for digits, case and internal punctuation, word length, POS tag and POS tag bigrams before and after the current word.
Only features seen at least once in the training data were included in the model, resulting in 450,345 binary features.
The model was fiAn exhaustive code was used in this experiment simply for illustrative purposes: many columns in this code were unnecessary, yielding only a slight gain in performance over much simpler codes while incurring a very large increase in training time.
Therefore, by selecting a good subset of the exhaustive code, it should be possible to reduce the training time while preserving the strong generalisation performance.
One approach is to incorporate skew in the label distribution in our choice of code  the code should minimise the confusability of commonly occurring labels more so than that of rare labels.
Assuming that errors made by the weak learners are independent, the probability of a single error, q, as a function of the code length n can be bounded by 90 89 88 F1 score 87 86 85 84 83 random random with replacement minimum loss bound oracle MLE multiclass CRF Regularised multiclass CRF 10 15 20 25 30 code length 35 40 45 50 Figure 1: NER F1 scores for standalone decoding with random codes, a minimum loss code and a greedy oracle.
Coding Multiclass Coded 200 One-vs-all Decoding standalone marginals product MLE 95.69 95.63 95.68 94.90 Regularised 95.78 96.03 96.03 96.57 where p(l) is the marginal probability of the label l, hl is the minimum Hamming distance between l and any other label, and p is the maximum probability ^ of an error by a weak learner.
The performance achieved by selecting the code with the minimum loss bound from a large random sample of codes is shown in Figure 1, using standalone decoding, where p was estimated on the development set.
For ^ comparison, randomly sampled codes and a greedy oracle are shown.
The two random sampled codes show those samples where no column is repeated, and where duplicate columns are permitted (random with replacement).
The oracle repeatedly adds to the code the column which most improves its F1 score.
The minimum loss bound method allows the performance plateau to be reached more quickly than random sampling; i.e. shorter codes can be used, thus allowing more efficient training and decoding.
Note also that multiclass CRF training required 830Mb of memory, while error-correcting training required only 380Mb.
Decoding of the test set (51,362 tokens) with the error-correcting model (exhaustive, MLE) took between 150 seconds for standalone decoding and 173 seconds for integrated decoding.
The multiclass CRF was much faster, taking only 31 seconds, however this time difference could be reduced with suitable optimisations.
Table 2: POS tagging accuracy.
4.2 Part-of-speech Tagging CRFs have been applied to POS tagging, however only with a very simple feature set and small training sample (Lafferty et al., 2001).
We used the Penn Treebank Wall Street Journal articles, training on sections 221 and testing on section 24.
In this task there are 45,110 training sentences, a total of 1,023,863 tokens and 45 labels.
The features used included word identity, prefix and suffix, whether the word contains a number, uppercase letter or a hyphen, and the words one and two positions before and after the current word.
A random code of 200 columns was used for this task.
These results are shown in Table 2, along with those of a multiclass CRF and an alternative one-vsall coding.
As for the NER experiment, the decoding performance levelled off after 100 bits, beyond which the improvements from longer codes were only very slight.
This is a very encouraging characteristic, as only a small number of weak learners are required for good performance.
The random code of 200 bits required 1,300Mb of RAM, taking a total of 293 hours to train and 3 hours to decode (54,397 tokens) on similar machines to those used before.
We do not have figures regarding the resources used by Lafferty et al.'s CRF for the POS tagging task and our attempts to train a multiclass CRF for full-scale POS tagging were thwarted due to lack of sufficient available computing resources.
Instead we trained on a 10,000 sentence subset of the training data, which required approximately 17Gb of RAM and 208 hours to train.
Our best result on the task was achieved using a one-vs-all code, which reduced the training time to 25 hours, as it only required training 45 binary models.
This result exceeds Lafferty et al.'s accuracy of 95.73% using a CRF but falls short of Toutanova et al.(2003)'s state-of-the-art 97.24%.
This is most probably due to our only using a first-order Markov model and a fairly simple feature set, where Tuotanova et al.include a richer set of features in a third order model.
4.3 Part-of-speech Tagging and Noun Phrase Segmentation The joint task of simultaneously POS tagging and noun phrase chunking (NPC) was included in order to demonstrate the scalability of error-correcting CRFs.
The data was taken from the CoNLL 2000 NPC shared task, with the model predicting both the chunk tags and the POS tags.
The training corpus consisted of 8,936 sentences, with 47,377 tokens and 118 labels.
A 200-bit random code was used, with the following features: word identity within a window, prefix and suffix of the current word and the presence of a digit, hyphen or upper case letter in the current word.
This resulted in about 420,000 features for each weak learner.
A joint tagging accuracy of 90.78% was achieved using MLE training and standalone decoding.
Despite the large increase in the number of labels in comparison to the earlier tasks, the performance also began to plateau at around 100 bits.
This task required 220Mb of RAM and took a total of 30 minutes to train each of the 200 binary CRFs, this time on Pentium 4 machines with 1Gb RAM.
Decoding of the 47,377 test tokens took 9,748 seconds and 9,870 seconds for the standalone and marginals methods respectively.
Sutton et al.(2004) applied a variant of the CRF, the dynamic CRF (DCRF), to the same task, modelling the data with two interconnected chains where one chain predicted NPC tags and the other POS tags.
They achieved better performance and training times than our model; however, this is not a fair comparison, as the two approaches are orthogonal.
Indeed, applying the error-correcting CRF algorithms to DCRF models could feasibly decrease the complexity of the DCRF, allowing the method to be applied to larger tasks with richer graphical structures and larger label sets.
In all three experiments, error-correcting CRFs have achieved consistently good generalisation performance.
The number of weak learners required to achieve these results was shown to be relatively small, even for tasks with large label sets.
The time and space requirements were lower than those of a traditional CRF for the larger tasks and, most importantly, did not increase substantially when the number of labels was increased.
Related work Most recent work on improving CRF performance has focused on feature selection.
McCallum (2003) describes a technique for greedily adding those feature conjuncts to a CRF which significantly improve the model's log-likelihood.
His experimental results show that feature induction yields a large increase in performance, however our results show that standardly formulated CRFs can perform well above their reported 73.3%, casting doubt on the magnitude of the possible improvement.
Roark et al.(2004) have also employed feature selection to the huge task of language modelling with a CRF, by partially training a voted perceptron then removing all features that the are ignored by the perceptron.
The act of automatic feature selection can be quite time consuming in itself, while the performance and runtime gains are often modest.
Even with a reduced number of features, tasks with a very large label space are likely to remain intractable.
Conclusion Standard training methods for CRFs suffer greatly from their dependency on the number of labels, making tasks with large label sets either difficult or impossible.
As CRFs are deployed more widely to tasks with larger label sets this problem will become more evident.
The current `solutions' to these scaling problems  namely feature selection, and the use of large clusters  don't address the heart of the problem: the dependence on the square of number of labels.
Error-correcting CRF training allows CRFs to be applied to larger problems and those with larger label sets than were previously possible, without requiring computationally demanding methods such as feature selection.
On standard tasks we have shown that error-correcting CRFs provide comparable or better performance than the standardly formulated CRF, while requiring less time and space to train.
Only a small number of weak learners were required to obtain good performance on the tasks with large label sets, demonstrating that the method provides efficient scalability to the CRF framework.
Error-correction codes could be applied to other sequence labelling methods, such as the voted perceptron (Roark et al., 2004).
This may yield an increase in performance and efficiency of the method, as its runtime is also heavily dependent on the number of labels.
We plan to apply error-correcting coding to dynamic CRFs, which should result in better modelling of naturally layered tasks, while increasing the efficiency and scalability of the method.
We also plan to develop higher order CRFs, using error-correcting codes to curb the increase in complexity.
Acknowledgements This work was supported in part by a PORES travelling scholarship from the University of Melbourne, allowing Trevor Cohn to travel to Edinburgh.
References Adam Berger.
1999. Error-correcting output coding for text classification.
In Proceedings of IJCAI: Workshop on machine learning for information filtering.
Thomas G.
Dietterich and Ghulum Bakiri.
1995. Solving multiclass learning problems via error-correcting output codes.
Journal of Artificial Intelligence Reseach, 2:263286.
L. Gillick and Stephen Cox.
1989. Some statistical issues in the comparison of speech recognition algorithms.
In Proceedings of the IEEE Conference on Acoustics, Speech and Signal Processing, pages 532535, Glasgow, Scotland.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models for segmenting and labelling sequence data.
In Proceedings of ICML 2001, pages 282289.
Florence MacWilliams and Neil Sloane.
1977. The theory of error-correcting codes.
North Holland, Amsterdam.
Robert Malouf.
2002. A comparison of algorithms for maximum entropy parameter estimation.
In Proceedings of CoNLL 2002, pages 4955.
Andrew McCallum and Wei Li.
2003. Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons.
In Proceedings of CoNLL 2003, pages 188191.
Andrew McCallum.
2003. Efficiently inducing features of conditional random fields.
In Proceedings of UAI 2003, pages 403410.
David Pinto, Andrew McCallum, Xing Wei, and Bruce Croft.
2003. Table extraction using conditional random fields.
In Proceedings of the Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 235242.
Brian Roark, Murat Saraclar, Michael Collins, and Mark Johnson.
2004. Discriminative language modeling with conditional random fields and the perceptron algorithm.
In Proceedings of ACL 2004, pages 4855.
Erik F.
Tjong Kim Sang and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 shared task: Chunking.
In Proceedings of CoNLL 2000 and LLL 2000, pages 127132.
Erik F.
Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition.
In Proceedings of CoNLL 2003, pages 142147, Edmonton, Canada.
Fei Sha and Fernando Pereira.
2003. Shallow parsing with conditional random fields.
In Proceedings of HLT-NAACL 2003, pages 213220.
Andrew Smith, Trevor Cohn, and Miles Osborne.
2005. Logarithmic opinion pools for conditional random fields.
In Proceedings of ACL 2005.
Charles Sutton, Khashayar Rohanimanesh, and Andrew McCallum.
2004. Dynamic conditional random fields: Factorized probabilistic models for labelling and segmenting sequence data.
In Proceedings of the ICML 2004.
Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer.
2003. Feature rich part-of-speech tagging with a cyclic dependency network.
In Proceedings of HLTNAACL 2003, pages 252259.
Hanna Wallach.
2002. Efficient training of conditional random fields.
Master's thesis, University of Edinburgh .
Logarithmic Opinion Pools for Conditional Random Fields Andrew Smith Trevor Cohn Miles Osborne Division of Informatics Department of Computer Science Division of Informatics University of Edinburgh and Software Engineering University of Edinburgh United Kingdom University of Melbourne, Australia United Kingdom a.p.smith-2@sms.ed.ac.uk tacohn@csse.unimelb.edu.au miles@inf.ed.ac.uk Abstract Recent work on Conditional Random Fields (CRFs) has demonstrated the need for regularisation to counter the tendency of these models to overfit.
The standard approach to regularising CRFs involves a prior distribution over the model parameters, typically requiring search over a hyperparameter space.
In this paper we address the overfitting problem from a different perspective, by factoring the CRF distribution into a weighted product of individual "expert" CRF distributions.
We call this model a logarithmic opinion pool (LOP) of CRFs (LOP-CRFs).
We apply the LOP-CRF to two sequencing tasks.
Our results show that unregularised expert CRFs with an unregularised CRF under a LOP can outperform the unregularised CRF, and attain a performance level close to the regularised CRF.
LOP-CRFs therefore provide a viable alternative to CRF regularisation without the need for hyperparameter search.
1 Introduction
In recent years, conditional random fields (CRFs) (Lafferty et al., 2001) have shown success on a number of natural language processing (NLP) tasks, including shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003) and information extraction from research papers (Peng and McCallum, 2004).
In general, this work has demonstrated the susceptibility of CRFs to overfit the training data during parameter estimation.
As a consequence, it is now standard to use some form of overfitting reduction in CRF training.
Recently, there have been a number of sophisticated approaches to reducing overfitting in CRFs, including automatic feature induction (McCallum, 2003) and a full Bayesian approach to training and inference (Qi et al., 2005).
These advanced methods tend to be difficult to implement and are often computationally expensive.
Consequently, due to its ease of implementation, the current standard approach to reducing overfitting in CRFs is the use of a prior distribution over the model parameters, typically a Gaussian.
The disadvantage with this method, however, is that it requires adjusting the value of one or more of the distribution's hyperparameters.
This usually involves manual or automatic tuning on a development set, and can be an expensive process as the CRF must be retrained many times for different hyperparameter values.
In this paper we address the overfitting problem in CRFs from a different perspective.
We factor the CRF distribution into a weighted product of individual expert CRF distributions, each focusing on a particular subset of the distribution.
We call this model a logarithmic opinion pool (LOP) of CRFs (LOP-CRFs), and provide a procedure for learning the weight of each expert in the product.
The LOPCRF framework is "parameter-free" in the sense that it does not involve the requirement to adjust hyperparameter values.
LOP-CRFs are theoretically advantageous in that their Kullback-Leibler divergence with a given distribution can be explicitly represented as a function of the KL-divergence with each of their expert distributions.
This provides a well-founded framework for designing new overfitting reduction schemes: Proceedings of the 43rd Annual Meeting of the ACL, pages 1825, Ann Arbor, June 2005.
c 2005 Association for Computational Linguistics filook to factorise a CRF distribution as a set of diverse experts.
We apply LOP-CRFs to two sequencing tasks in NLP: named entity recognition and part-of-speech tagging.
Our results show that combination of unregularised expert CRFs with an unregularised standard CRF under a LOP can outperform the unregularised standard CRF, and attain a performance level that rivals that of the regularised standard CRF.
LOP-CRFs therefore provide a viable alternative to CRF regularisation without the need for hyperparameter search.
E p(o,s) [ fk ] E p(s|o) [ fk ] = 0, k ~ In general this cannot be solved for the k in closed form so numerical routines must be used.
Malouf (2002) and Sha and Pereira (2003) show that gradient-based algorithms, particularly limited memory variable metric (LMVM), require much less time to reach convergence, for some NLP tasks, than the iterative scaling methods (Della Pietra et al., 1997) previously used for log-linear optimisation problems.
In all our experiments we use the LMVM method to train the CRFs.
For CRFs with general graphical structure, calculation of E p(s|o) [ fk ] is intractable, but for the linear chain case Lafferty et al.(2001) describe an efficient dynamic programming procedure for inference, similar in nature to the forward-backward algorithm in hidden Markov models.
Conditional Random Fields A linear chain CRF defines the conditional probability of a state or label sequence s given an observed sequence o via1 : 1 exp Z(o) Logarithmic Opinion Pools where T is the length of both sequences, k are parameters of the model and Z(o) is the partition function that ensures (1) represents a probability distribution.
The functions f k are feature functions representing the occurrence of different events in the sequences s and o.
The parameters k can be estimated by maximising the conditional log-likelihood of a set of labelled training sequences.
The log-likelihood is given by: L ( ) = = ~ p(o, s) log p(s | o; ) In this paper an expert model refers a probabilistic model that focuses on modelling a specific subset of some probability distribution.
The concept of combining the distributions of a set of expert models via a weighted product has previously been used in a range of different application areas, including economics and management science (Bordley, 1982), and NLP (Osborne and Baldridge, 2004).
In this paper we restrict ourselves to sequence models.
Given a set of sequence model experts, indexed by, with conditional distributions p (s | o) and a set of non-negative normalised weights w, a logarithmic opinion pool 2 is defined as the distribution: pLOP (s | o) = 1 [p (s | o)]w ZLOP (o) (2) where p(o, s) and p(o) are empirical distributions ~ ~ defined by the training set.
At the maximum likelihood solution the model satisfies a set of feature constraints, whereby the expected count of each feature under the model is equal to its empirical count on the training data: this paper we assume there is a one-to-one mapping between states and labels, though this need not be the case.
with w 0 and w = 1, and where ZLOP (o) is the normalisation constant: ZLOP (o) = [p (s | o)]w (1999) introduced a variant of the LOP idea called Product of Experts, in which expert distributions are multiplied under a uniform weight distribution.
2 Hinton
The weight w encodes our confidence in the opinion of expert. Suppose that there is a "true" conditional distribution q(s | o) which each p (s | o) is attempting to model.
Heskes (1998) shows that the KL divergence between q(s | o) and the LOP, can be decomposed into two terms: K(q, pLOP ) = E A = log Z(o) = log ZLOP (o) + w log Z (o) LOP This relationship will be useful below, when we describe how to train the weights w of a LOP-CRF.
In this paper we will use the term LOP-CRF weights to refer to the weights w in the weighted product of the LOP-CRF distribution and the term parameters to refer to the parameters k of each expert CRF . 3.2 Training LOP-CRFs In our LOP-CRF training procedure we first train the expert CRFs unregularised on the training data.
Then, treating the experts as static pre-trained models, we train the LOP-CRF weights w to maximise the log-likelihood of the training data.
This training process is "parameter-free" in that neither stage involves the use of a prior distribution over expert CRF parameters or LOP-CRF weights, and so avoids the requirement to adjust hyperparameter values.
The likelihood of a data set under a LOP-CRF, as a function of the LOP-CRF weights, is given by: L(w) = = This tells us that the closeness of the LOP model to q(s | o) is governed by a trade-off between two terms: an E term, which represents the closeness of the individual experts to q(s | o), and an A term, which represents the closeness of the individual experts to the LOP, and therefore indirectly to each other.
Hence for the LOP to model q well, we desire models p which are individually good models of q (having low E) and are also diverse (having large A).
3.1 LOPs
for CRFs Because CRFs are log-linear models, we can see from equation (2) that CRF experts are particularly well suited to combination under a LOP.
Indeed, the resulting LOP is itself a CRF, the LOP-CRF, with potential functions given by a log-linear combination of the potential functions of the experts, with weights w . As a consequence of this, the normalisation constant for the LOP-CRF can be calculated efficiently via the usual forward-backward algorithm for CRFs.
Note that there is a distinction between normalisation constant for the LOP-CRF, ZLOP as given in equation (3), and the partition function of the LOP-CRF, Z.
The two are related as follows: 1 [p (s | o)]w ZLOP (o) 1 U (s | o) Z (o) ZLOP (o) [U (s | o)] ZLOP (o) [Z (o)]w LOP 1 p (s | o)w ZLOP (o; w) After taking logs and rearranging, the loglikelihood can be expressed as: ~ p(o, s) w log p (s | o) ~ p(o) log Z LOP (o; w) T +1 where U = exp t=1 k k fk (st-1, st, o,t) and so For the first two terms, the quantities that are multiplied by w inside the (outer) sums are independent of the weights, and can be evaluated once at the fibeginning of training.
The third term involves the partition function for the LOP-CRF and so is a function of the weights.
It can be evaluated efficiently as usual for a standard CRF.
Taking derivatives with respect to w and rearranging, we obtain: use a prior distribution over the LOP-CRF weights.
As the weights are non-negative and normalised we use a Dirichlet distribution, whose density function is given by: p(w) = ( ) w -1 ( ) L (w) w where the are hyperparameters.
Under this distribution, ignoring terms that are independent of the weights, the regularised loglikelihood involves an additional term: logUt (o, s) where Ut (o, s) is the value of the potential function for expert on clique t under the labelling s for observation o.
In a way similar to the representation of the expected feature count in a standard CRF, the third term may be re-written as: pLOP (st-1 = s, st = s, o) logUt (s, s, o) o t s,s We assume a single value across all weights.
The derivative of the regularised log-likelihood with respect to weight w then involves an additional 1 term w ( 1).
In our experiments we use the development set to optimise the value of . We will refer to a LOP-CRF with weights trained using this procedure as a regularised LOP-CRF.
Hence the derivative is tractable because we can use dynamic programming to efficiently calculate the pairwise marginal distribution for the LOP-CRF.
Using these expressions we can efficiently train the LOP-CRF weights to maximise the loglikelihood of the data set.3 We make use of the LMVM method mentioned earlier to do this.
We will refer to a LOP-CRF with weights trained using this procedure as an unregularised LOP-CRF.
3.2.1 Regularisation
The "parameter-free" aspect of the training procedure we introduced in the previous section relies on the fact that we do not use regularisation when training the LOP-CRF weights w . However, there is a possibility that this may lead to overfitting of the training data.
In order to investigate this, we develop a regularised version of the training procedure and compare the results obtained with each.
We 3 We must ensure that the weights are non-negative and normalised.
We achieve this by parameterising the weights as functions of a set of unconstrained variables via a softmax transformation.
The values of the log-likelihood and its derivatives with respect to the unconstrained variables can be derived from the corresponding values for the weights w . The Tasks In this paper we apply LOP-CRFs to two sequence labelling tasks in NLP: named entity recognition (NER) and part-of-speech tagging (POS tagging).
4.1 Named
Entity Recognition NER involves the identification of the location and type of pre-defined entities within a sentence and is often used as a sub-process in information extraction systems.
With NER the CRF is presented with a set of sentences and must label each word so as to indicate whether the word appears outside an entity (O), at the beginning of an entity of type X (B-X) or within the continuation of an entity of type X (I-X).
All our results for NER are reported on the CoNLL-2003 shared task dataset (Tjong Kim Sang and De Meulder, 2003).
For this dataset the entity types are: persons (PER), locations (LOC), organisations (ORG) and miscellaneous (MISC).
The training set consists of 14, 987 sentences and 204, 567 tokens, the development set consists of 3, 466 sentences and 51, 578 tokens and the test set consists of 3, 684 sentences and 46, 666 tokens.
fi4.2 Part-of-Speech Tagging POS tagging involves labelling each word in a sentence with its part-of-speech, for example noun, verb, adjective, etc.
For our experiments we use the CoNLL-2000 shared task dataset (Tjong Kim Sang and Buchholz, 2000).
This has 48 different POS tags.
In order to make training time manageable4, we collapse the number of POS tags from 48 to 5 following the procedure used in (McCallum et al., 2003).
In summary:  All types of noun collapse to category N.
 All types of verb collapse to category V.
 All types of adjective collapse to category J.
 All types of adverb collapse to category R.
 All other POS tags collapse to category O.
The training set consists of 7, 300 sentences and 173, 542 tokens, the development set consists of 1, 636 sentences and 38, 185 tokens and the test set consists of 2, 012 sentences and 47, 377 tokens.
4.3 Expert
sets For each task we compare the performance of the LOP-CRF to that of the standard CRF by defining a single, complex CRF, which we call a monolithic CRF, and a range of expert sets.
The monolithic CRF for NER comprises a number of word and POS tag features in a window of five words around the current word, along with a set of orthographic features defined on the current word.
These are based on those found in (Curran and Clark, 2003).
Examples include whether the current word is capitalised, is an initial, contains a digit, contains punctuation, etc.
The monolithic CRF for NER has 450, 345 features.
The monolithic CRF for POS tagging comprises word and POS features similar to those in the NER monolithic model, but over a smaller number of orthographic features.
The monolithic model for POS tagging has 188, 448 features.
Each of our expert sets consists of a number of CRF experts.
Usually these experts are designed to 4 See (Cohn et al., 2005) for a scaling method allowing the full POS tagging task with CRFs.
focus on modelling a particular aspect or subset of the distribution.
As we saw earlier, the aim here is to define experts that model parts of the distribution well while retaining mutual diversity.
The experts from a particular expert set are combined under a LOP-CRF and the weights are trained as described previously.
We define our range of expert sets as follows:  Simple consists of the monolithic CRF and a single expert comprising a reduced subset of the features in the monolithic CRF.
This reduced CRF models the entire distribution rather than focusing on a particular aspect or subset, but is much less expressive than the monolithic model.
The reduced model comprises 24, 818 features for NER and 47, 420 features for POS tagging.
 Positional consists of the monolithic CRF and a partition of the features in the monolithic CRF into three experts, each consisting only of features that involve events either behind, at or ahead of the current sequence position.
 Label consists of the monolithic CRF and a partition of the features in the monolithic CRF into five experts, one for each label.
For NER an expert corresponding to label X consists only of features that involve labels B-X or IX at the current or previous positions, while for POS tagging an expert corresponding to label X consists only of features that involve label X at the current or previous positions.
These experts therefore focus on trying to model the distribution of a particular label.
 Random consists of the monolithic CRF and a random partition of the features in the monolithic CRF into four experts.
This acts as a baseline to ascertain the performance that can be expected from an expert set that is not defined via any linguistic intuition.
Experiments To compare the performance of LOP-CRFs trained using the procedure we described previously to that of a standard CRF regularised with a Gaussian prior, we do the following for both NER and POS tagging: fi Train a monolithic CRF with regularisation using a Gaussian prior.
We use the development set to optimise the value of the variance hyperparameter.
 Train every expert CRF in each expert set without regularisation (each expert set includes the monolithic CRF, which clearly need only be trained once).
 For each expert set, create a LOP-CRF from the expert CRFs and train the weights of the LOP-CRF without regularisation.
We compare its performance to that of the unregularised and regularised monolithic CRFs.
 To investigate whether training the LOP-CRF weights contributes significantly to the LOPCRF's performance, for each expert set we create a LOP-CRF with uniform weights and compare its performance to that of the LOP-CRF with trained weights.
 To investigate whether unregularised training of the LOP-CRF weights leads to overfitting, for each expert set we train the weights of the LOP-CRF with regularisation using a Dirichlet prior.
We optimise the hyperparameter in the Dirichlet distribution on the development set.
We then compare the performance of the LOP-CRF with regularised weights to that of the LOP-CRF with unregularised weights.
Expert Monolithic unreg.
Monolithic reg.
Reduced Positional 1 Positional 2 Positional 3 Label LOC Label MISC Label ORG Label PER Label O Random 1 Random 2 Random 3 Random 4 Table 1: Development set F scores for NER experts 6.2 LOP-CRFs with unregularised weights In this section we present results for LOP-CRFs with unregularised weights.
Table 2 gives F scores for NER LOP-CRFs while Table 3 gives accuracies for the POS tagging LOP-CRFs.
The monolithic CRF scores are included for comparison.
Both tables illustrate the following points:  In every case the LOP-CRFs outperform the unregularised monolithic CRF  In most cases the performance of LOP-CRFs rivals that of the regularised monolithic CRF, and in some cases exceeds it.
We use McNemar's matched-pairs test (Gillick and Cox, 1989) on point-wise labelling errors to examine the statistical significance of these results.
We test significance at the 5% level.
At this threshold, all the LOP-CRFs significantly outperform the corresponding unregularised monolithic CRF.
In addition, those marked with show a significant improvement over the regularised monolithic CRF.
Only the value marked with in Table 3 significantly under performs the regularised monolithic.
All other values a do not differ significantly from those of the regularised monolithic CRF at the 5% level.
These results show that LOP-CRFs with unregularised weights can lead to performance improvements that equal or exceed those achieved from a conventional regularisation approach using a Gaussian prior.
The important difference, however, is that the LOP-CRF approach is "parameter-free" in the Results 6.1 Experts Before presenting results for the LOP-CRFs, we briefly give performance figures for the monolithic CRFs and expert CRFs in isolation.
For illustration, we do this for NER models only.
Table 1 shows F scores on the development set for the NER CRFs.
We see that, as expected, the expert CRFs in isolation model the data relatively poorly compared to the monolithic CRFs.
Some of the label experts, for example, attain relatively low F scores as they focus only on modelling one particular label.
Similar behaviour was observed for the POS tagging models.
Expert set Monolithic unreg.
Monolithic reg.
Simple Positional Label Random Development set 88.33 89.84 90.26 90.35 89.30 88.84 Test set 81.87 83.98 84.22 84.71 83.27 83.06 Expert set Simple Positional Label Random Development set 98.30 97.97 97.85 97.82 Test set 98.12 97.79 97.73 97.74 Table 2: F scores for NER unregularised LOP-CRFs Expert set Monolithic unreg.
Monolithic reg.
Simple Positional Label Random Development set 97.92 98.02 98.31 98.03 97.99 97.99 Test set 97.65 97.84 98.12 97.81 97.77 97.76 Table 4: Accuracies for POS tagging uniform LOPCRFs general LOP-CRFs with uniform weights, although still performing significantly better than the unregularised monolithic CRF, generally under perform LOP-CRFs with trained weights.
This suggests that the choice of weights can be important, and justifies the weight training stage.
6.4 LOP-CRFs with regularised weights To investigate whether unregularised training of the LOP-CRF weights leads to overfitting, we train the LOP-CRF with regularisation using a Dirichlet prior.
The results we obtain show that in most cases a LOP-CRF with regularised weights achieves an almost identical performance to that with unregularised weights, and suggests there is little to be gained by weight regularisation.
This is probably due to the fact that in our LOP-CRFs the number of experts, and therefore weights, is generally small and so there is little capacity for overfitting.
We conjecture that although other choices of expert set may comprise many more experts than in our examples, the numbers are likely to be relatively small in comparison to, for example, the number of parameters in the individual experts.
We therefore suggest that any overfitting effect is likely to be limited.
6.5 Choice
of Expert Sets We can see from Tables 2 and 3 that the performance of a LOP-CRF varies with the choice of expert set.
For example, in our tasks the simple and positional expert sets perform better than those for the label and random sets.
For an explanation here, we refer back to our discussion of equation (5).
We conjecture that the simple and positional expert sets achieve good performance in the LOP-CRF because they consist of experts that are diverse while simultaneously being reasonable models of the data.
The label expert set exhibits greater diversity between the experts, because each expert focuses on modelling a particular label only, but each expert is a relatively Table 3: Accuracies for POS tagging unregularised LOP-CRFs sense that each expert CRF in the LOP-CRF is unregularised and the LOP weight training is also unregularised.
We are therefore not required to search a hyperparameter space.
As an illustration, to obtain our best results for the POS tagging regularised monolithic model, we re-trained using 15 different values of the Gaussian prior variance.
With the LOP-CRF we trained each expert CRF and the LOP weights only once.
As an illustration of a typical weight distribution resulting from the training procedure, the positional LOP-CRF for POS tagging attaches weight 0.45 to the monolithic model and roughly equal weights to the other three experts.
6.3 LOP-CRFs with uniform weights By training LOP-CRF weights using the procedure we introduce in this paper, we allow the weights to take on non-uniform values.
This corresponds to letting the opinion of some experts take precedence over others in the LOP-CRF's decision making.
An alternative, simpler, approach would be to combine the experts under a LOP with uniform weights, thereby avoiding the weight training stage.
We would like to ascertain whether this approach will significantly reduce the LOP-CRF's performance.
As an illustration, Table 4 gives accuracies for LOPCRFs with uniform weights for POS tagging.
A similar pattern is observed for NER.
Comparing these values to those in Tables 2 and 3, we can see that in fipoor model of the entire distribution and the corresponding LOP-CRF performs worse.
Similarly, the random experts are in general better models of the entire distribution but tend to be less diverse because they do not focus on any one aspect or subset of it.
Intuitively, then, we want to devise experts that provide diverse but accurate views on the data.
The expert sets we present in this paper were motivated by linguistic intuition, but clearly many choices exist.
It remains an important open question as to how to automatically construct expert sets for good performance on a given task, and we intend to pursue this avenue in future research.
Conclusion and future work In this paper we have introduced the logarithmic opinion pool of CRFs as a way to address overfitting in CRF models.
Our results show that a LOPCRF can provide a competitive alternative to conventional regularisation with a prior while avoiding the requirement to search a hyperparameter space.
We have seen that, for a variety of types of expert, combination of expert CRFs with an unregularised standard CRF under a LOP with optimised weights can outperform the unregularised standard CRF and rival the performance of a regularised standard CRF.
We have shown how these advantages a LOPCRF provides have a firm theoretical foundation in terms of the decomposition of the KL-divergence between a LOP-CRF and a target distribution, and how this provides a framework for designing new overfitting reduction schemes in terms of constructing diverse experts.
In this work we have considered training the weights of a LOP-CRF using pre-trained, static experts.
In future we intend to investigate cooperative training of LOP-CRF weights and the parameters of each expert in an expert set.
Acknowledgements We wish to thank Stephen Clark, our colleagues in Edinburgh and the anonymous reviewers for many useful comments.
References R.
F. Bordley.
1982. A multiplicative formula for aggregating probability assessments.
Management Science, (28):1137 1148.
T. Cohn, A.
Smith, and M.
Osborne. 2005.
Scaling conditional random fields using error-correcting codes.
In Proc.
ACL 2005.
J. Curran and S.
Clark. 2003.
Language independent NER using a maximum entropy tagger.
In Proc.
CoNLL-2003. S.
Della Pietra, Della Pietra V., and J.
Lafferty. 1997.
Inducing features of random fields.
In IEEE PAMI, volume 19(4), pages 380393.
L. Gillick and S.
Cox. 1989.
Some statistical issues in the comparison of speech recognition algorithms.
In International Conference on Acoustics, Speech and Signal Processing, volume 1, pages 532535.
T. Heskes.
1998. Selecting weighting factors in logarithmic opinion pools.
In Advances in Neural Information Processing Systems 10.
G. E.
Hinton. 1999.
Product of experts.
In ICANN, volume 1, pages 16.
J. Lafferty, A.
McCallum, and F.
Pereira. 2001.
Conditional random fields: Probabilistic models for segmenting and labeling sequence data.
In Proc.
ICML 2001.
R. Malouf.
2002. A comparison of algorithms for maximum entropy parameter estimation.
In Proc.
CoNLL-2002. A.
McCallum and W.
Li. 2003.
Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons.
In Proc.
CoNLL-2003. A.
McCallum, K.
Rohanimanesh, and C.
Sutton. 2003.
Dynamic conditional random fields for jointly labeling multiple sequences.
In NIPS-2003 Workshop on Syntax, Semantics and Statistics.
A. McCallum.
2003. Efficiently inducing features of conditional random fields.
In Proc.
UAI 2003.
M. Osborne and J.
Baldridge. 2004.
Ensemble-based active learning for parse selection.
In Proc.
NAACL 2004.
F. Peng and A.
McCallum. 2004.
Accurate information extraction from research papers using conditional random fields.
In Proc.
HLT-NAACL 2004.
Y. Qi, M.
Szummer, and T.
P. Minka.
2005. Bayesian conditional random fields.
In Proc.
AISTATS 2005.
F. Sha and F.
Pereira. 2003.
Shallow parsing with conditional random fields.
In Proc.
HLT-NAACL 2003.
E. F.
Tjong Kim Sang and S.
Buchholz. 2000.
Introduction to the CoNLL-2000 shared task: Chunking.
In Proc.
CoNLL2000. E.
F. Tjong Kim Sang and F.
De Meulder.
2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition.
In Proc. CoNLL-2003 .
Supersense Tagging of Unknown Nouns using Semantic Similarity James R.
Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au Abstract The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words.
Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET.
Ciaramita and Johnson (2003) present a tagger which uses synonym set glosses as annotated training examples.
We describe an unsupervised approach, based on vector-space similarity, which does not require annotated examples but significantly outperforms their tagger.
We also demonstrate the use of an extremely large shallow-parsed corpus for calculating vector-space semantic similarity.
WORDNET with the UMLS medical resource and found only a very small degree of overlap.
Also, lexicalsemantic resources suffer from: bias towards concepts and senses from particular topics.
Some specialist topics are better covered in WORDNET than others, e.g. dog has finer-grained distinctions than cat and worm although this does not reflect finer distinctions in reality; limited coverage of infrequent words and senses.
Ciaramita and Johnson (2003) found that common nouns missing from WORDNET 1.6 occurred every 8 sentences in the BLLIP corpus.
By WORDNET 2.0, coverage has improved but the problem of keeping up with language evolution remains difficult.
consistency when classifying similar words into categories.
For instance, the WORDNET lexicographer file for ionosphere (location) is different to exosphere and stratosphere (object), two other layers of the earth's atmosphere.
These problems demonstrate the need for automatic or semi-automatic methods for the creation and maintenance of lexical-semantic resources.
Broad semantic classification is currently used by lexicographers to organise the manual insertion of words into WORDNET, and is an experimental precursor to automatically inserting words directly into the WORDNET hierarchy.
Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNET's hierarchical structure to create many annotated training instances from the synset glosses.
This paper describes an unsupervised approach to supersense tagging that does not require annotated sentences.
Instead, we use vector-space similarity to retrieve a number of synonyms for each unknown common noun.
The supersenses of these synonyms are then combined to determine the supersense.
This approach significantly outperforms the multi-class perceptron on the same dataset based on WORDNET 1.6 and 1.7.1. 1 Introduction Lexical-semantic resources have been applied successful to a wide range of Natural Language Processing (NLP) problems ranging from collocation extraction (Pearce, 2001) and class-based smoothing (Clark and Weir, 2002), to text classification (Baker and McCallum, 1998) and question answering (Pasca and Harabagiu, 2001).
In particular, WORDNET (Fellbaum, 1998) has significantly influenced research in NLP.
Unfortunately, these resource are extremely timeconsuming and labour-intensive to manually develop and maintain, requiring considerable linguistic and domain expertise.
Lexicographers cannot possibly keep pace with language evolution: sense distinctions are continually made and merged, words are coined or become obsolete, and technical terms migrate into the vernacular.
Technical domains, such as medicine, require separate treatment since common words often take on special meanings, and a significant proportion of their vocabulary does not overlap with everyday vocabulary.
Burgun and Bodenreider (2001) compared an alignment of Proceedings of the 43rd Annual Meeting of the ACL, pages 2633, Ann Arbor, June 2005.
c 2005 Association for Computational Linguistics fiLEX-FILE DESCRIPTION act animal artifact attribute body cognition communication event feeling food group location motive object person phenomenon plant possession process quantity relation shape state substance time acts or actions animals man-made objects attributes of people and objects body parts cognitive processes and contents communicative processes and contents natural events feelings and emotions foods and drinks groupings of people or objects spatial position goals natural objects (not man-made) people natural phenomena plants possession and transfer of possession natural processes quantities and units of measure relations between people/things/ideas two and three dimensional shapes stable states of affairs substances time and temporal relations ing directly underneath.
Other alternative sets of supersenses can be created by an arbitrary cut through the WORDNET hierarchy near the top, or by using topics from a thesaurus such as Roget's (Yarowsky, 1992).
These topic distinctions are coarser-grained than WORDNET senses, which have been criticised for being too difficult to distinguish even for experts.
Ciaramita and Johnson (2003) believe that the key sense distinctions are still maintained by supersenses.
They suggest that supersense tagging is similar to named entity recognition, which also has a very small set of categories with similar granularity (e.g.
location and person) for labelling predominantly unseen terms.
Supersense tagging can provide automated or semiautomated assistance to lexicographers adding words to the WORDNET hierarchy.
Once this task is solved successfully, it may be possible to insert words directly into the fine-grained distinctions of the hierarchy itself.
Clearly, this is the ultimate goal, to be able to insert new terms into lexical resources, extending the structure where necessary.
Supersense tagging is also interesting for many applications that use shallow semantics, e.g. information extraction and question answering.
Table 1: 25 noun lexicographer files in WORDNET Previous Work Supersenses There are 26 broad semantic classes employed by lexicographers in the initial phase of inserting words into the WORDNET hierarchy, called lexicographer files (lexfiles).
For the noun hierarchy, there are 25 lex-files and a file containing the top level nodes in the hierarchy called Tops.
Other syntactic classes are also organised using lex-files: 15 for verbs, 3 for adjectives and 1 for adverbs.
Lex-files form a set of coarse-grained sense distinctions within WORDNET.
For example, company appears in the following lex-files in WORDNET 2.0: group, which covers company in the social, commercial and troupe fine-grained senses; and state, which covers companionship.
The names and descriptions of the noun lex-files are shown in Table 1.
Some lex-files map directly to the top level nodes in the hierarchy, called unique beginners, while others are grouped together as hyponyms of a unique beginner (Fellbaum, 1998, page 30).
For example, abstraction subsumes the lex-files attribute, quantity, relation, communication and time.
Ciaramita and Johnson (2003) call the noun lex-file classes supersenses.
There are 11 unique beginners in the WORDNET noun hierarchy which could also be used as supersenses.
Ciaramita (2002) has produced a miniWORDNET by manually reducing the WORDNET hierarchy to 106 broad categories.
Ciaramita et al.(2003) describe how the lex-files can be used as root nodes in a two level hierarchy with the WORDNET synsets appearA considerable amount of research addresses structurally and statistically manipulating the hierarchy of WORDNET and the construction of new wordnets using the concept structure from English.
For lexical FreeNet, Beeferman (1998) adds over 350 000 collocation pairs (trigger pairs) extracted from a 160 million word corpus of broadcast news using mutual information.
The co-occurrence window was 500 words which was designed to approximate average document length.
Caraballo and Charniak (1999) have explored determining noun specificity from raw text.
They find that simple frequency counts are the most effective way of determining the parent-child ordering, achieving 83% accuracy over types of vehicle, food and occupation.
The other measure they found to be successful was the entropy of the conditional distribution of surrounding words given the noun.
Specificity ordering is a necessary step for building a noun hierarchy.
However, this approach clearly cannot build a hierarchy alone.
For instance, entity is less frequent than many concepts it subsumes.
This suggests it will only be possible to add words to an existing abstract structure rather than create categories right up to the unique beginners.
Hearst and Sch tze (1993) flatten WORDNET into 726 u categories using an algorithm which attempts to minimise the variance in category size.
These categories are used to label paragraphs with topics, effectively repeating Yarowsky's (1992) experiments using the their categories rather than Roget's thesaurus.
Sch tze's (1992) u WordSpace system was used to add topical links, such as between ball, racquet and game (the tennis problem).
Further, they also use the same vector-space techniques to label previously unseen words using the most common class assigned to the top 20 synonyms for that word.
Widdows (2003) uses a similar technique to insert words into the WORDNET hierarchy.
He first extracts synonyms for the unknown word using vector-space similarity measures based on Latent Semantic Analysis and then searches for a location in the hierarchy nearest to these synonyms.
This same technique as is used in our approach to supersense tagging.
Ciaramita and Johnson (2003) implement a supersense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems.
Their insight was to use the WORDNET glosses as annotated training data and massively increase the number of training instances using the noun hierarchy.
They developed an efficient algorithm for estimating the model over hierarchical training data.
WORDNET 1.6 NOUN SUPERSENSE WORDNET 1.7.1 NOUN SUPERSENSE stock index fast food bottler subcompact advancer cash flow downside discounter trade-off billionaire communication food group artifact person possession cognition artifact act person week buyout insurer partner health income contender cartel lender planner time act group person state possession person group person artifact Table 2: Example nouns and their supersenses largest NLP processed corpus described in published research.
The corpus consists of the British National Corpus (BNC), the Reuters Corpus Volume 1 (RCV1), and most of the Linguistic Data Consortium's news text collected since 1987: Continuous Speech Recognition III (CSR-III); North American News Text Corpus (NANTC); the NANTC Supplement (NANTS); and the ACQUAINT Corpus.
The components and their sizes including punctuation are given in Table 3.
The LDC has recently released the English Gigaword corpus which includes most of the corpora listed above.
CORPUS BNC RCV1 CSR-III NANTC NANTS ACQUAINT DOCS.
SENTS. WORDS Evaluation Ciaramita and Johnson (2003) propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET.
They use the common nouns that have been added to WORDNET 1.7.1 since WORDNET 1.6 and compare this evaluation with a standard cross-validation approach that uses a small percentage of the words from their WORDNET 1.6 training set for evaluation.
Their results suggest that the WORDNET 1.7.1 test set is significantly harder because of the large number of abstract category nouns, e.g. communication and cognition, that appear in the 1.7.1 data, which are difficult to classify.
Our evaluation will use exactly the same test sets as Ciaramita and Johnson (2003).
The WORDNET 1.7.1 test set consists of 744 previously unseen nouns, the majority of which (over 90%) have only one sense.
The WORDNET 1.6 test set consists of several cross-validation sets of 755 nouns randomly selected from the BLLIP training set used by Ciaramita and Johnson (2003).
They have kindly supplied us with the WORDNET 1.7.1 test set and one cross-validation run of the WORDNET 1.6 test set.
Our development experiments are performed on the WORDNET 1.6 test set with one final run on the WORDNET 1.7.1 test set.
Some examples from the test sets are given in Table 2 with their supersenses.
Table 3: 2 billion word corpus statistics We have tokenized the text using the Grok-OpenNLP tokenizer (Morton, 2002) and split the sentences using MXTerminator (Reynar and Ratnaparkhi, 1997).
Any sentences less than 3 words or more than 100 words long were rejected, along with sentences containing more than 5 numbers or more than 4 brackets, to reduce noise.
The rest of the pipeline is described in the next section.
Semantic Similarity Corpus We have developed a 2 billion word corpus, shallowparsed with a statistical NLP pipeline, which is by far the Vector-space models of similarity are based on the distributional hypothesis that similar words appear in similar contexts.
This hypothesis suggests that semantic similarity can be measured by comparing the contexts each word appears in.
In vector-space models each headword is represented by a vector of frequency counts recording the contexts that it appears in.
The key parameters are the context extraction method and the similarity measure used to compare context vectors.
Our approach to fivector-space similarity is based on the SEXTANT system described in Grefenstette (1994).
Curran and Moens (2002b) compared several context extraction methods and found that the shallow pipeline and grammatical relation extraction used in SEXTANT was both extremely fast and produced high-quality results.
SEXTANT extracts relation tuples (w, r, w ) for each noun, where w is the headword, r is the relation type and w is the other word.
The efficiency of the SEXTANT approach makes the extraction of contextual information from over 2 billion words of raw text feasible.
We describe the shallow pipeline in detail below.
Curran and Moens (2002a) compared several different similarity measures and found that Grefenstette's weighted JACCARD measure performed the best: min(wgt(w1, r, w ), wgt(w2, r, w )) max(wgt(w1, r, w ), wgt(w2, r, w )) (1) RELATION DESCRIPTION adj dobj iobj nn nnprep subj Table 4: Grammatical relations from SEXTANT against the CELEX lexical database (Minnen et al., 2001)  and is very efficient, analysing over 80 000 words per second.
morpha often maintains sense distinctions between singular and plural nouns; for instance: spectacles is not reduced to spectacle, but fails to do so in other cases: glasses is converted to glass.
This inconsistency is problematic when using morphological analysis to smooth vector-space models.
However, morphological smoothing still produces better results in practice.
6.3 Grammatical
Relation Extraction where wgt(w, r, w ) is the weight function for relation (w, r, w ).
Curran and Moens (2002a) introduced the TTEST weight function, which is used in collocation extraction.
Here, the t-test compares the joint and product probability distributions of the headword and context: p(w, r, w ) p(, r, w )p(w,, ) p(, r, w )p(w,, ) (2) where indicates a global sum over that element of the relation tuple.
JACCARD and TTEST produced better quality synonyms than existing measures in the literature, so we use Curran and Moen's configuration for our supersense tagging experiments.
6.1 Part
of Speech Tagging and Chunking Our implementation of SEXTANT uses a maximum entropy POS tagger designed to be very efficient, tagging at around 100 000 words per second (Curran and Clark, 2003), trained on the entire Penn Treebank (Marcus et al., 1994).
The only similar performing tool is the Trigrams `n' Tags tagger (Brants, 2000) which uses a much simpler statistical model.
Our implementation uses a maximum entropy chunker which has similar feature types to Koeling (2000) and is also trained on chunks extracted from the entire Penn Treebank using the CoNLL 2000 script.
Since the Penn Treebank separates PPs and conjunctions from NPs, they are concatenated to match Grefenstette's table-based results, i.e. the SEXTANT always prefers noun attachment.
6.2 Morphological
Analysis Our implementation uses morpha, the Sussex morphological analyser (Minnen et al., 2001), which is implemented using lex grammars for both affix splitting and generation.
morpha has wide coverage  nearly 100% After the raw text has been POS tagged and chunked, the grammatical relation extraction algorithm is run over the chunks.
This consists of five passes over each sentence that first identify noun and verb phrase heads and then collect grammatical relations between each common noun and its modifiers and verbs.
A global list of grammatical relations generated by each pass is maintained across the passes.
The global list is used to determine if a word is already attached.
Once all five passes have been completed this association list contains all of the nounmodifier/verb pairs which have been extracted from the sentence.
The types of grammatical relation extracted by SEXTANT are shown in Table 4.
For relations between nouns (nn and nnprep), we also create inverse relations (w, r, w) representing the fact that w can modify w.
The 5 passes are described below.
Pass 1: Noun Pre-modifiers This pass scans NPs, left to right, creating adjectival (adj) and nominal (nn) pre-modifier grammatical relations (GRs) with every noun to the pre-modifier's right, up to a preposition or the phrase end.
This corresponds to assuming right-branching noun compounds.
Within each NP only the NP and PP heads remain unattached.
Pass 2: Noun Post-modifiers This pass scans NPs, right to left, creating post-modifier GRs between the unattached heads of NPs and PPs.
If a preposition is encountered between the noun heads, a prepositional noun (nnprep) GR is created, otherwise an appositional noun (nn) GR is created.
This corresponds to assuming right-branching PP attachment.
After this phrase only the NP head remains unattached.
Tense Determination The rightmost verb in each VP is considered the head.
A fiis initially categorised as active.
If the head verb is a form of be then the VP becomes attributive.
Otherwise, the algorithm scans the VP from right to left: if an auxiliary verb form of be is encountered the VP becomes passive; if a progressive verb (except being) is encountered the VP becomes active.
Only the noun heads on either side of VPs remain unattached.
The remaining three passes attach these to the verb heads as either subjects or objects depending on the voice of the VP.
Pass 3: Verb Pre-Attachment This pass scans sentences, right to left, associating the first NP head to the left of the VP with its head.
If the VP is active, a subject (subj) relation is created; otherwise, a direct object (dobj) relation is created.
For example, antigen is the subject of represent.
Pass 4: Verb Post-Attachment This pass scans sentences, left to right, associating the first NP or PP head to the right of the VP with its head.
If the VP was classed as active and the phrase is an NP then a direct object (dobj) relation is created.
If the VP was classed as passive and the phrase is an NP then a subject (subj) relation is created.
If the following phrase is a PP then an indirect object (iobj) relation is created.
The interaction between the head verb and the preposition determine whether the noun is an indirect object of a ditransitive verb or alternatively the head of a PP that is modifying the verb.
However, SEXTANT always attaches the PP to the previous phrase.
Pass 5: Verb Progressive Participles The final step of the process is to attach progressive verbs to subjects and objects (without concern for whether they are already attached).
Progressive verbs can function as nouns, verbs and adjectives and once again a nave api proximation to the correct attachment is made.
Any progressive verb which appears after a determiner or quantifier is considered a noun.
Otherwise, it is a verb and passes 3 and 4 are repeated to attach subjects and objects.
Finally, SEXTANT collapses the nn, nnprep and adj relations together into a single broad noun-modifier grammatical relation.
Grefenstette (1994) claims this extractor has a grammatical relation accuracy of 75% after manually checking 60 sentences.
VP SUFFIX EXAMPLE SUPERSENSE remoteness annulment statesman bowling viscosity electronics arsine mariner entomology attribute act person act attribute cognition substance person cognition Table 5: Hand-coded rules for supersense guessing fall-back method is a simple hand-coded classifier which examines the unknown noun and makes a guess based on simple morphological analysis of the suffix.
These rules were created by inspecting the suffixes of rare nouns in WORDNET 1.6.
The supersense guessing rules are given in Table 5.
If none of the rules match, then the default supersense artifact is assigned.
The problem now becomes how to convert the ranked list of extracted synonyms for each unknown noun into a single supersense selection.
Each extracted synonym votes for its one or more supersenses that appear in WORDNET 1.6.
There are many parameters to consider:     how many extracted synonyms to use; how to weight each synonym's vote; whether unreliable synonyms should be filtered out; how to deal with polysemous synonyms.
Approach Our approach uses voting across the known supersenses of automatically extracted synonyms, to select a supersense for the unknown nouns.
This technique is similar to Hearst and Sch tze (1993) and Widdows (2003).
u However, sometimes the unknown noun does not appear in our 2 billion word corpus, or at least does not appear frequently enough to provide sufficient contextual information to extract reliable synonyms.
In these cases, our The experiments described below consider a range of options for these parameters.
In fact, these experiments are so quick to run we have been able to exhaustively test many combinations of these parameters.
We have experimented with up to 200 voting extracted synonyms.
There are several ways to weight each synonym's contribution.
The simplest approach would be to give each synonym the same weight.
Another approach is to use the scores returned by the similarity system.
Alternatively, the weights can use the ranking of the extracted synonyms.
Again these options have been considered below.
A related question is whether to use all of the extracted synonyms, or perhaps filter out synonyms for which a small amount of contextual information has been extracted, and so might be unreliable.
The final issue is how to deal with polysemy.
Does every supersense of each extracted synonym get the whole weight of that synonym or is it distributed evenly between the supersenses like Resnik (1995)?
Another alternative is to only consider unambiguous synonyms with a single supersense in WORDNET.
A disadvantage of this similarity approach is that it requires full synonym extraction, which compares the unknown word against a large number of words when, in fiSYSTEM WN Ciaramita and Johnson baseline Ciaramita and Johnson perceptron Similarity based results WN WORDNET 1.6 SUPERSENSE N P R F WORDNET 1.7.1 N P R F Table 6: Summary of supersense tagging accuracies fact, we want to calculate the similarity to a small number of supersenses.
This inefficiency could be reduced significantly if we consider only very high frequency words, but even this is still expensive.
Results We have used the WORDNET 1.6 test set to experiment with different parameter settings and have kept the WORDNET 1.7.1 test set as a final comparison of best results with Ciaramita and Johnson (2003).
The experiments were performed by considering all possible configurations of the parameters described above.
The following voting options were considered for each supersense of each extracted synonym: the initial voting weight for a supersense could either be a constant (IDENTITY) or the similarity score (SCORE) of the synonym.
The initial weight could then be divided by the number of supersenses to share out the weight (SHARED).
The weight could also be divided by the rank (RANK) to penalise supersenses further down the list.
The best performance on the 1.6 test set was achieved with the SCORE voting, without sharing or ranking penalties.
The extracted synonyms are filtered before contributing to the vote with their supersense(s).
This filtering involves checking that the synonym's frequency and number of contexts are large enough to ensure it is reliable.
We have experimented with a wide range of cutoffs and the best performance on the 1.6 test set was achieved using a minimum cutoff of 5 for the synonym's frequency and the number of contexts it appears in.
The next question is how many synonyms are considered.
We considered using just the nearest unambiguous synonym, and the top 5, 10, 20, 50, 100 and 200 synonyms.
All of the top performing configurations used 50 synonyms.
We have also experimented with filtering out highly polysemous nouns by eliminating words with two, three or more synonyms.
However, such a filter turned out to make little difference.
Finally, we need to decide when to use the similarity measure and when to fall-back to the guessing rules.
This is determined by looking at the frequency and number of attributes for the unknown word.
Not surprisingly, the similarity system works better than the guessing rules if it has any information at all.
The results are summarised in Table 6.
The accuracy of the best-performing configurations was 68% on the Table 7: Breakdown of results by supersense WORDNET 1.6 test set with several other parameter combinations described above performing nearly as well.
On the previously unused WORDNET 1.7.1 test set, our accuracy is 63% using the best system on the WORDNET 1.6 test set.
By optimising the parameters on the 1.7.1 test set we can increase that to 64%, indicating that we have not excessively over-tuned on the 1.6 test set.
Our results significantly outperform Ciaramita and Johnson (2003) on both test sets even though our system is unsupervised.
The large difference between our 1.6 and 1.7.1 test set accuracy demonstrates that the 1.7.1 set is much harder.
Table 7 shows the breakdown in performance for each supersense.
The columns show the number of instances of each supersense with the precision, recall and f-score measures as percentages.
The most frequent supersenses in both test sets were person, attribute and act.
Of the frequent categories, person is the easiest supersense to get correct in both the 1.6 and 1.7.1 test sets, followed by food, artifact and substance.
This is not surprising since these concrete words tend to have very fewer other senses, well constrained contexts and a relatively high frequency.
These factors are conducive for extracting reliable synonyms.
These results also support Ciaramita and Johnson's view that abstract concepts like communication, cognition and state are much harder.
We would expect the location fisupersense to perform well since it is quite concrete, but unfortunately our synonym extraction system does not incorporate proper nouns, so many of these words were classified using the hand-built classifier.
Also, in the data from Ciaramita and Johnson all of the words are in lower case, so no sensible guessing rules could help.
Other Alternatives and Future Work An alternative approach worth exploring is to create context vectors for the supersense categories themselves and compare these against the words.
This has the advantage of producing a much smaller number of vectors to compare against.
In the current system, we must compare a word against the entire vocabulary (over 500 000 headwords), which is much less efficient than a comparison against only 26 supersense context vectors.
The question now becomes how to construct vectors of supersenses.
The most obvious solution is to sum the context vectors across the words which have each supersense.
However, our early experiments suggest that this produces extremely large vectors which do not match well against the much smaller vectors of each unseen word.
Also, the same questions arise in the construction of these vectors.
How are words with multiple supersenses handled?
Our preliminary experiments suggest that only combining the vectors for unambiguous words produces the best results.
One solution would be to take the intersection between vectors across words for each supersense (i.e.
to find the common contexts that these words appear in).
However, given the sparseness of the data this may not leave very large context vectors.
A final solution would be to consider a large set of the canonical attributes (Curran and Moens, 2002a) to represent each supersense.
Canonical attributes summarise the key contexts for each headword and are used to improve the efficiency of the similarity comparisons.
There are a number of problems our system does not currently handle.
Firstly, we do not include proper names in our similarity system which means that location entities can be very difficult to identify correctly (as the results demonstrate).
Further, our similarity system does not currently incorporate multi-word terms.
We overcome this by using the synonyms of the last word in the multi-word term.
However, there are 174 multi-word terms (23%) in the WORDNET 1.7.1 test set which we could probably tag more accurately with synonyms for the whole multi-word term.
Finally, we plan to implement a supervised machine learner to replace the fallback method, which currently has an accuracy of 37% on the WORDNET 1.7.1 test set.
We intend to extend our experiments beyond the Ciaramita and Johnson (2003) set to include previous and more recent versions of WORDNET to compare their difficulty, and also perform experiments over a range of corpus sizes to determine the impact of corpus size on the quality of results.
We would like to move onto the more difficult task of insertion into the hierarchy itself and compare against the initial work by Widdows (2003) using latent semantic analysis.
Here the issue of how to combine vectors is even more interesting since there is the additional structure of the WORDNET inheritance hierarchy and the small synonym sets that can be used for more fine-grained combination of vectors.
Conclusion Our application of semantic similarity to supersense tagging follows earlier work by Hearst and Sch tze (1993) u and Widdows (2003).
To classify a previously unseen common noun our approach extracts synonyms which vote using their supersenses in WORDNET 1.6.
We have experimented with several parameters finding that the best configuration uses 50 extracted synonyms, filtered by frequency and number of contexts to increase their reliability.
Each synonym votes for each of its supersenses from WORDNET 1.6 using the similarity score from our synonym extractor.
Using this approach we have significantly outperformed the supervised multi-class perceptron Ciaramita and Johnson (2003).
This paper also demonstrates the use of a very efficient shallow NLP pipeline to process a massive corpus.
Such a corpus is needed to acquire reliable contextual information for the often very rare nouns we are attempting to supersense tag.
This application of semantic similarity demonstrates that an unsupervised methods can outperform supervised methods for some NLP tasks if enough data is available.
Acknowledgements We would like to thank Massi Ciaramita for supplying his original data for these experiments and answering our queries, and to Stephen Clark and the anonymous reviewers for their helpful feedback and corrections.
This work has been supported by a Commonwealth scholarship, Sydney University Travelling Scholarship and Australian Research Council Discovery Project DP0453131.
References L.
Douglas Baker and Andrew McCallum.
1998. Distributional clustering of words for text classification.
In Proceedings of the 21st annual international ACM SIGIR conference on Research and Development in Information Retrieval, pages 96103, Melbourne, Australia.
Doug Beeferman.
1998. Lexical discovery with an enriched semantic network.
In Proceedings of the Workshop on Usage of WordNet in Natural Language Processing Systems, pages 358364, Montreal, Quebec, Canada.
Thorsten Brants.
2000. TnT a statistical part-of-speech tagger.
In Proceedings of the 6th Applied Natural Language Processing Conference, pages 224231, Seattle, WA USA.
Anita Burgun and Olivier Bodenreider.
2001. Comparing terms, concepts and semantic classes in WordNet and the Unified Medical Language System.
In Proceedings of the Workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations, pages 7782, Pittsburgh, PA USA.
Sharon A.
Caraballo and Eugene Charniak.
1999. Determining the specificity of nouns from text.
In Proceedings of the Joint ACL SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 6370, College Park, MD USA.
Massimiliano Ciaramita and Mark Johnson.
2003. Supersense tagging of unknown nouns in WordNet.
In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 168175, Sapporo, Japan.
Massimiliano Ciaramita, Thomas Hofmann, and Mark Johnson.
2003. Hierarchical semantic classification: Word sense disambiguation with world knowledge.
In Proceedings of the 18th International Joint Conference on Artificial Intelligence, Acapulco, Mexico.
Massimiliano Ciaramita.
2002. Boosting automatic lexical acquisition with morphological information.
In Proceedings of the Workshop on Unsupervised Lexical Acquisition, pages 1725, Philadelphia, PA, USA.
Stephen Clark and David Weir.
2002. Class-based probability estimation using a semantic hierarchy.
Computational Linguistics, 28(2):187206, June.
Koby Crammer and Yoram Singer.
2001. Ultraconservative online algorithms for multiclass problems.
In Proceedings of the 14th annual Conference on Computational Learning Theory and 5th European Conference on Computational Learning Theory, pages 99115, Amsterdam, The Netherlands.
James R.
Curran and Stephen Clark.
2003. Investigating GIS and smoothing for maximum entropy taggers.
In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics, pages 9198, Budapest, Hungary.
James R.
Curran and Marc Moens.
2002a. Improvements in automatic thesaurus extraction.
In Proceedings of the Workshop on Unsupervised Lexical Acquisition, pages 5966, Philadelphia, PA, USA.
James R.
Curran and Marc Moens.
2002b. Scaling context space.
In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 231238, Philadelphia, PA, USA.
Christiane Fellbaum, editor.
1998. WordNet: An Electronic Lexical Database.
MIT Press, Cambridge, MA USA.
Gregory Grefenstette.
1994. Explorations in Automatic Thesaurus Discovery.
Kluwer Academic Publishers, Boston, MA USA.
Marti A.
Hearst and Hinrich Schutze.
1993. Customizing a lexicon to better suit a computational task.
In Proceedings of the Workshop on Acquisition of Lexical Knowledge from Text, pages 5569, Columbus, OH USA.
Rob Koeling.
2000. Chunking with maximum entropy models.
In Proceedings of the 4th Conference on Computational Natural Language Learning and of the 2nd Learning Language in Logic Workshop, pages 139141, Lisbon, Portugal.
Mitchell P.
Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz.
1993. Building a large annotated corpus of English: the Penn Treebank.
Computational Linguistics, 19(2):313330.
Guido Minnen, John Carroll, and Darren Pearce.
2001. Applied morphological processing of English.
Natural Language Engineering, 7(3):207223.
Tom Morton.
2002. Grok tokenizer.
Grok OpenNLP toolkit.
Marius Pasca and Sanda M.
Harabagiu. 2001.
The informative role of WordNet in open-domain question answering.
In Proceedings of the Workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations, pages 138143, Pittsburgh, PA USA.
Darren Pearce.
2001. Synonymy in collocation extraction.
In Proceedings of the Workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations, pages 4146, Pittsburgh, PA USA.
Philip Resnik.
1995. Using information content to evaluate semantic similarity.
In Proceedings of the 14th International Joint Conference on Artificial Intelligence, pages 448453, Montreal, Canada.
Jeffrey C.
Reynar and Adwait Ratnaparkhi.
1997. A maximum entropy approach to identifying sentence boundaries.
In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 1619, Washington, D.C.
USA. Hinrich Schutze.
1992. Context space.
In Intelligent Probau bilistic Approaches to Natural Language, number FS-92-04 in Fall Symposium Series, pages 113120, Stanford University, CA USA.
Dominic Widdows.
2003. Unsupervised methods for developing taxonomies by combining syntactic and statistical information.
In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 276283, Edmonton, Alberta Canada.
David Yarowsky.
1992. Word-sense disambiguation using statistical models of Roget's categories trained on large corpora.
In Proceedings of the 14th international conference on Computational Linguistics, pages 454460, Nantes, France.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 121??24, Prague, June 2007.
c2007 Association for Computational Linguistics Predicting Evidence of Understanding by Monitoring User?s Task Manipulation in Multimodal Conversations Yukiko I.
Nakano ??
Yoshiko Arimoto ??
?? Tokyo University of Agriculture and Technology 2-24-16 Nakacho, Koganeishi, Tokyo 184-8588, Japan {nakano, kmurata, menomoto}@cc.tuat.ac.jp Kazuyoshi Murata ??
Yasuhiro Asa ??
?? Tokyo University of Technology 1404-1 Katakura, Hachioji, Tokyo 192-0981, Japan ar@mf.teu.ac.jp Mika Enomoto ??
Hirohiko Sagawa ??
?? Central Research Laboratory, Hitachi, Ltd.
1-280, Higashi-koigakubo Kokubunji-shi, Tokyo 185-8601, Japan {yasuhiro.asa.mk, hirohiko.sagawa.cu}@hitachi.com Abstract The aim of this paper is to develop animated agents that can control multimodal instruction dialogues by monitoring user?s behaviors.
First, this paper reports on our Wizard-of-Oz experiments, and then, using the collected corpus, proposes a probabilistic model of fine-grained timing dependencies among multimodal communication behaviors: speech, gestures, and mouse manipulations.
A preliminary evaluation revealed that our model can predict a instructor?s grounding judgment and a listener?s successful mouse manipulation quite accurately, suggesting that the model is useful in estimating the user?s understanding, and can be applied to determining the agent?s next action.
1 Introduction
In face-to-face conversation, speakers adjust their utterances in progress according to the listener?s feedback expressed in multimodal manners, such as speech, facial expression, and eye-gaze.
In taskmanipulation situations where the listener manipulates objects by following the speaker?s instructions, correct task manipulation by the listener serves as more direct evidence of understanding (Brennan 2000, Clark and Krych 2004), and affects the speaker?s dialogue control strategies.
Figure 1 shows an example of a software instruction dialogue in a video-mediated situation (originally in Japanese).
While the learner says nothing, the instructor gives the instruction in small pieces, simultaneously modifying her gestures and utterances according to the learner?s mouse movements.
To accomplish such interaction between human users and animated help agents, and to assist the user through natural conversational interaction, this paper proposes a probabilistic model that computes timing dependencies among different types of behaviors in different modalities: speech, gestures, and mouse events.
The model predicts (a) whether the instructor?s current utterance will be successfully understood by the learner and grounded (Clark and Schaefer 1989), and (b) whether the learner will successfully manipulate the object in the near future.
These predictions can be used as constraints in determining agent actions.
For example, if the current utterance will not be grounded, then the help agent must add more information.
In the following sections, first, we collect human-agent conversations by employing a Wizardof-Oz method, and annotate verbal and nonverbal behaviors.
The annotated corpus is used to build a Bayesian network model for the multimodal instruction dialogues.
Finally, we will evaluate how ?That??(204ms pause) Pointing gesture <preparation> <stroke> Mouse move Instructor: Learner: ?at the most??(395ms pause) ?left-hand side??
Instructor: Learner: Instructor: Mouse move Figure 1: Example of task manipulation dialogue 121 accurately the model can predict the events in (a) and (b) mentioned above.
2 Related
work In their psychological study, Clark and Krych (2004) showed that speakers alter their utterances midcourse while monitoring not only the listener?s vocal signals, but also the listener?s gestural signals as well as through other mutually visible events.
Such a bilateral process functions as a joint activity to ground the presented information, and task manipulation as a mutually visible event contributes to the grounding process (Brennan 2000, Whittaker 2003).
Dillenbourg, Traum, et al.(1996) also discussed cross-modality in grounding: verbally presented information is grounded by an action in the task environment.
Studies on interface agents have presented computational models of multimodal interaction (Cassell, Bickmore, et al.2000). Paek and Horvitz (1999) focused on uncertainty in speech-based interaction, and employed a Bayesian network to understand the user?s speech input.
For user monitoring, Nakano, Reinstein, et al.(2003) used a head tracker to build a conversational agent which can monitor the user?s eye-gaze and head nods as nonverbal signals in grounding.
These previous studies provide psychological evidence about the speaker?s monitoring behaviors as well as conversation modeling techniques in computational linguistics.
However, little has been studied about how systems (agents) should monitor the user?s task manipulation, which gives direct evidence of understanding to estimate the user?s understanding, and exploits the predicted evidence as constraints in selecting the agent?s next action.
Based on these previous attempts, this study proposes a multimodal interaction model by focusing on task manipulation, and predicts conversation states using probabilistic reasoning.
3 Data
collection A data collection experiment was conducted using a Wizard-of-Oz agent assisting a user in learning a PCTV application, a system for watching and recording TV programs on a PC.
The output of the PC operated by the user was displayed on a 23-inch monitor in front of the user, and also projected on a 120-inch big screen, in front of which a human instructor was standing (Figure 2 (a)).
Therefore, the participants shared visual events output from the PC (Figure 2 (b)) while sitting in different rooms.
In addition, a rabbit-like animated agent was controlled through the instructor?s motion data captured by motion sensors.
The instructor?s voice was changed through a voice transformation system to make it sound like a rabbit agent.
4 Corpus
We collected 20 conversations from 10 pairs, and annotated 11 conversations of 6 pairs using the Anvil video annotating tool (Kipp 2004).
Agent?s verbal behaviors: The agent?s (actually, instructor?s) speech data was split by pauses longer than 200ms.
For each inter pausal unit (IPU), utterance content type defined as follows was assigned.
?? Identification (id): identification of a target object for the next operation ??
Operation (op): request to execute a mouse click or a similar primitive action on the target ??
Identification + operation (idop): referring to identification and operation in one IPU In addition to these main categories, we also used: State (referring to a state before/after an operation), Function (explaining a function of the system), Goal (referring to a task goal to be accomplished), and Acknowledgment.
The intercoder agreement for this coding scheme is very high K=0.89 (Cohen?s Kappa), suggesting that the assigned tags are reliable.
Agent?s nonverbal behaviors: As the most salient instructor?s nonverbal behaviors in the collected data, we annotated agent pointing gestures: ??
Agent movement: agent?s position movement ??
Agent touching target (att): agent?s touching the target object as a stroke of a pointing gesture (a) Instructor (b) PC output Figure 2: Wizard-of-Oz agent controlled by instructor 122 User?s nonverbal behaviors: We annotated three types of mouse manipulation for the user?s task manipulation as follows: ??
Mouse movement: movement of the mouse cursor ??
Mouse-on-target: the mouse cursor is on the target object ??
Click target: click on the target object 4.1 Example of collected data An example of an annotated corpus is shown in Figure 3.
The upper two tracks illustrate the agent?s verbal and nonverbal behaviors, and the other two illustrate the user?s behaviors.
The agent was pointing at the target (att) and giving a sequence of identification descriptions [a1-3].
Since the user?s mouse did not move at all, the agent added another identification IPU [a4] accompanied by another pointing gesture.
Immediately after that, the user?s mouse cursor started moving towards the target object.
After finishing the next IPU, the agent finally requested the user to click the object in [a6].
Note that the collected Wizard-of-Oz conversations are very similar to the human-human instruction dialogues shown in Figure 1.
While carefully monitoring the user?s mouse actions, the Wizard-of-Oz agent provided information in small pieces.
If it was uncertain that the user was following the instruction, the agent added more explanation without continuing.
5 Probabilistic
model of user-agent multimodal interaction 5.1 Building a Bayesian network model To consider multiple factors for verbal and nonverbal behaviors in probabilistic reasoning, we employed a Bayesian network technique, which can infer the likelihood of the occurrence of a target event based on the dependencies among multiple kinds of evidence.
We extracted the conversational data from the beginning of an instructor's identification utterance for a new target object to the point that the user clicks on the object.
Each IPU was split at 500ms intervals, and 1395 intervals were obtained.
As shown in Figure 4, the network consists of 9 properties concerning verbal and nonverbal behaviors for past, current, and future interval(s).
5.2 Predicting
evidence of understanding As a preliminary evaluation, we tested how accurately our Bayesian network model can predict an instructor?s grounding judgment, and the user?s mouse click.
The following five kinds of evidence were given to the network to predict future states.
As evidence for the previous three intervals (1.5 sec), we used (1) the percentage of time the agent touched the target (att), (2) the number of the user?s mouse movements.
Evidence for the current interval is (3) current IPU?s content type, (4) whether the end of the current interval will be the end of the IPU (i.e.
whether a pause will follow after the current interval), and (5) whether the mouse is on the target object.
Well, Yes View the TV right of Yes Beside the DVD There is a button starts with ?V??
Ah, yes Er, yes Press it This User Agent Speech Gesture Mouser actions id id id id id+op Mouse move click att att att Mouse on target [a2] [a3] [a4] [a5] [a6][a1] ack ack ack ack Speech Off On Figure 3: Example dialogue between Wizard-of-Oz agent and user Figure 4: Bayesian network model 123 (a) Predicting grounding judgment: We tested how accurately the model can predict whether the instructor will go on to the next leg of the instruction or will give additional explanations using the same utterance content type (the current message will not be grounded).
The results of 5-fold cross-validation are shown in Table 1.
Since 83% of the data are ?same content??cases, prediction for ?same content??is very accurate (F-measure is 0.90).
However, it is not very easy to find ?content change??case because of its less frequency (F-measure is 0.68).
It would be better to test the model using more balanced data.
(b) Predicting user?s mouse click: As a measure of the smoothness of task manipulation, the network predicted whether the user?s mouse click would be successfully performed within the next 5 intervals (2.5sec).
If a mouse click is predicted, the agent should just wait without annoying the user by unnecessary explanation.
Since randomized data is not appropriate to test mouse click prediction, we used 299 sequences of utterances that were not used for training.
Our model predicted 84% of the user?s mouse clicks: 80% of them were predicted 3-5 intervals before the actual occurrence of the mouse click, and 20% were predicted 1 interval before.
However, the model frequently generates wrong predictions.
Improving precision rate is necessary.
6 Discussion
and Future Work We employed a Bayesian network technique to our goal of developing conversational agents that can generate fine-grained multimodal instruction dialogues, and we proposed a probabilistic model for predicting grounding judgment and user?s successful mouse click.
The results of preliminary evaluation suggest that separate models of each modality for each conversational participant cannot properly describe the complex process of on-going multimodal interaction, but modeling the interaction as dyadic activities with multiple tracks of modalities is a promising approach.
The advantage of employing the Bayesian network technique is that, by considering the cost of misclassification and the benefit of correct classification, the model can be easily adjusted according to the purpose of the system or the user?s skill level.
For example, we can make the model more cautious or incautious.
Thus, our next step is to implement the proposed model into a conversational agent, and evaluate our model not only in its accuracy, but also in its effectiveness by testing the model with various utility values.
References Brennan, S.
2000. Processes that shape conversation and their implications for computational linguistics.
In Proceedings of 38th Annual Meeting of the ACL.
Cassell, J., Bickmore, T., Campbell, L., Vilhjalmsson, H.
and Yan, H.
(2000). Human Conversation as a System Framework: Designing Embodied Conversational Agents.
Embodied Conversational Agents.
J. Cassell, J.
Sullivan, S.
Prevost and E.
Churchill. Cambridge, MA, MIT Press: 29-63.
Clark, H.
H. and Schaefer, E.
F. 1989.
Contributing to discourse.
Cognitive Science 13: 259-294.
Clark, H.
H. and Krych, M.
A. 2004.
Speaking while monitoring addressees for understanding.
Journal of Memory and Language 50(1): 62-81.
Dillenbourg, P., Traum, D.
R. and Schneider, D.
1996. Grounding in Multi-modal Task Oriented Collaboration.
In Proceedings of EuroAI&Education Conference: 415-425.
Kipp, M.
2004. Gesture Generation by Imitation From Human Behavior to Computer Character Animation, Boca Raton, Florida: Dissertation.com.
Nakano, Y.
I., Reinstein, G., Stocky, T.
and Cassell, J.
2003. Towards a Model of Face-to-Face Grounding.
In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics: 553-561.
Paek, T.
and Horvitz, E.
(1999). Uncertainty, Utility, and Misunderstanding: A Decision-Theoretic Perspective on Grounding in Conversational Systems.
Working Papers of the AAAI Fall Symposium on Psychological Models of Communication in Collaborative Systems.
S. E.
Brennan, A.
Giboin and D.
Traum: 85-92.
Whittaker, S.
(2003). Theories and Methods in Mediated Communication.
The Handbook of Discourse Processes.
A. Graesser, MIT Press.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 125??28, Prague, June 2007.
c2007 Association for Computational Linguistics Automatically Assessing the Post Quality in Online Discussions on Software Markus Weimer and Iryna Gurevych and Max Muhlhauser Ubiquitous Knowledge Processing Group, Division of Telecooperation Darmstadt University of Technology, Germany http://www.ukp.informatik.tu-darmstadt.de [mweimer,gurevych,max]@tk.informatik.tu-darmstadt.de Abstract Assessing the quality of user generated content is an important problem for many web forums.
While quality is currently assessed manually, we propose an algorithm to assess the quality of forum posts automatically and test it on data provided by Nabble.com.
We use state-of-the-art classification techniques and experiment with five feature classes: Surface, Lexical, Syntactic, Forum specific and Similarity features.
We achieve an accuracy of 89% on the task of automatically assessing post quality in the software domain using forum specific features.
Without forum specific features, we achieve an accuracy of 82%.
1 Introduction
Web 2.0 leads to the proliferation of user generated content, such as blogs, wikis and forums.
Key properties of user generated content are: low publication threshold and a lack of editorial control.
Therefore, the quality of this content may vary.
The end user has problems to navigate through large repositories of information and find information of high quality quickly.
In order to address this problem, many forum hosting companies like Google Groups1 and Nabble2 introduce rating mechanisms, where users can rate the information manually on a scale from 1 (low quality) to 5 (high quality).
The ratings have been shown to be consistent with the user community by Lampe and Resnick (2004).
However, the 1http://groups.google.com 2http://www.nabble.com percentage of manually rated posts is very low (0.1% in Nabble).
Departing from this, the main idea explored in the present paper is to investigate the feasibility of automatically assessing the perceived quality of user generated content.
We test this idea for online forum discussions in the domain of software.
The perceived quality is not an objective measure.
Rather, it models how the community at large perceives post quality.
We choose a machine learning approach to automatically assess it.
Our main contributions are: (1) An algorithm for automatic quality assessment of forum posts that learns from human ratings.
We evaluate the system on online discussions in the software domain.
(2) An analysis of the usefulness of different classes of features for the prediction of post quality.
2 Related
work To the best of our knowledge, this is the first work which attempts to assess the quality of forum posts automatically.
However, on the one hand work has been done on automatic assessment of other types of user generated content, such as essays and product reviews.
On the other hand, student online discussions have been analyzed.
Automatic text quality assessment has been studied in the area of automatic essay scoring (Valenti et al., 2003; Chodorow and Burstein, 2004; Attali and Burstein, 2006).
While there exist guidelines for writing and assessing essays, this is not the case for forum posts, as different users cast their rating with possibly different quality criteria in mind.
The same argument applies to the automatic assessment of product review usefulness (Kim et al., 2006c): 125 Stars Label on the website Number star Poor Post 1251 starstar Below Average Post 44 starstarstar Average Post 69 starstarstarstar Above Average Post 183 starstarstarstarstar Excellent Post 421 Table 1: Categories and their usage frequency.
Readers of a review are asked ?Was this review helpful to you???with the answer choices Yes/No.
This is very well defined compared to forum posts, which are typically rated on a five star scale that does not advertise a specific semantics.
Forums have been in the focus of another track of research.
Kim et al.(2006b) found that the relation between a student?s posting behavior and the grade obtained by that student can be assessed automatically.
The main features used are the number of posts, the average post length and the average number of replies to posts of the student.
Feng et al.(2006) and Kim et al.(2006a) describe a system to find the most authoritative answer in a forum thread.
The latter add speech act analysis as a feature for this classification.
Another feature is the author?s trustworthiness, which could be computed based on the automatic quality classification scheme proposed in the present paper.
Finding the most authoritative post could also be defined as a special case of the quality assessment.
However, it is definitely different from the task studied in the present paper.
We assess the perceived quality of a given post, based solely on its intrinsic features.
Any discussion thread may contain an indefinite number of good posts, rather than a single authoritative one.
3 Experiments
We seek to develop a system that adapts to the quality standards existing in a certain user community by learning the relation between a set of features and the perceived quality of posts.
We experimented with features from five classes described in table 2: Surface, Lexical, Syntactic, Forum specific and Similarity features.
We use forum discussions from the Software category of Nabble.com.5 The data consists of 1968 rated posts in 1788 threads from 497 forums.
Posts can be rated by multiple users, but that happens 5http://www.nabble.com/Software-f94.html rarely.
1927 posts were rated by one, 40 by two and 1 post by three users.
Table 1 shows the distribution of average ratings on a five star scale.
From this statistics, it becomes evident that users at Nabble prefer extreme ratings.
Therefore, we decided to treat the posts as being binary rated.: Posts with less than three stars are rated ?bad??
Posts with more than three stars are ?good??
We removed 61 posts where all ratings are exactly three stars.
We removed additional 14 posts because they had contradictory ratings on the binary scale.
Those posts were mostly spam, which was voted high for commercial interests and voted down for being spam.
Additionally, we removed 30 posts that did not contain any text but only attachments like pictures.
Finally, we removed 331 non English posts using a simple heuristics: Posts that contained a certain percentage of words above a pre-defined threshold, which are non-English according to a dictionary, were considered to be non-English.
This way, we obtained 1532 binary classified posts: 947 good posts and 585 bad posts.
For each post, we compiled a feature vector, and feature values were normalized to the range [0.0,...,1.0].
We use support vector machines as a state-of-theart-algorithm for binary classification.
For all experiments, we used a C-SVM with a gaussian RBF kernel as implemented by LibSVM in the YALE toolkit (Chang and Lin, 2001; Mierswa et al., 2006).
Parameters were set to C = 10 and  = 0.1.
We performed stratified ten-fold cross validation6 to estimate the performance of our algorithm.
We repeated several experiments according to the leave-one-out evaluation scheme and found comparable results to the ones reported in this paper.
4 Results
and Analysis We compared our algorithm to a majority class classifier as a baseline, which achieves an accuracy of 62%.
As it is evident from table 3, most system configurations outperform the baseline system.
The best performing single feature category are the Forum specific features.
As we seek to build an adaptable system, analyzing the performance without these features is worthwhile: Using all other features, we 6See (Witten and Frank, 2005), chapter 5.3 for an in-depth description.
126 Feature category Feature name Description Surface Features Length The number of tokens in a post.
Question Frequency The percentage of sentences ending with ????
Exclamation Frequency The percentage of sentences ending with ????
Capital Word Frequency The percentage of words in CAPITAL, which is often associated with shouting.
Lexical Features Information about the wording of the posts Spelling Error Frequency The percentage of words that are not spelled correctly.3 Swear Word Frequency The percentage of words that are on a list of swear words we compiled from resources like WordNet and Wikipedia4, which contains more than eighty words like ?asshole?? but also common transcriptions like ?f*ckin??
Syntactic Features The percentage of part-of-speech tags as defined in the PENN Treebank tag set (Marcus et al., 1994).
We used TreeTagger (Schmid, 1995) based on the english parameter files supplied with it.
Forum specific features Properties of a post that are only present in forum postings IsHTML Whether or not a post contains HTML.
In our data, this is encoded explicitly, but it can also be determined by regular expressions matching HTML tags.
IsMail Whether or not a post has been copied from a mailing list.
This is encoded explicitly in our data.
Quote Fraction The fraction of characters that are inside quotes of other posts.
These quotes are marked explicitly in our data.
URL and Path Count The number of URLs and filesystem paths.
Post quality in the software domain may be influenced by the amount of tangible information, which is partly captured by these features.
Similarity features Forums are focussed on a topic.
The relatedness of a post to the topic of the forum may influence post quality.
We capture this relatedness by the cosine between the posts unigram vector and the unigram vector of the forum.
Table 2: Features used for the automatic quality assessment of posts.
achieve an only slightly worse classification accuracy.
Thus, the combination of all other features captures the quality of a post fairly well.
SUF LEX SYN FOR SIM Avg.
accuracy Baseline 61.82%?????????? 89.10%??
????????61.82% ??????????71.82% ??????????82.64% ??????????85.05% ??????????62.01% ??????????89.10%??
????????89.36%???? ??????85.03%??????
????82.90%???????? ??88.97% ??????????88.56%??
????????85.12% ??????????88.74% Table 3: Accuracy with different feature sets.
SUF: Surface, LEX: Lexical, SYN: Syntax, FOR: Forum specific, SIM: similarity.
The baseline results from a majority class classifier.
We performed additional experiments to identify the most important features from the Forum specific ones.
Table 4 shows that IsMail and Quote Fraction are the dominant features.
This is noteworthy, as those features are not based on the domain of discussion.
Thus, we believe that these features will perform well in future experiments on other data.
ISM ISH QFR URL PAC Avg.
accuracy?????????? 85.05%??
????????73.30% ??????????61.82% ??????????73.76% ??????????61.29% ??????????61.82% ??????????74.41%??
????????85.05%???? ??????73.30%??????
????85.05%???????? ??85.05%??
????????84.99%?????? ????85.05% Table 4: Accuracy with different forum specific features.
ISM: IsMail, ISH: IsHTML, QFR: QuoteFraction, URL: URLCount, PAC: PathCount.
Error Analysis Table 5 shows the confusion matrix of the system using all features.
Many posts that were misclassified as good ones show no apparent reason to be classified as bad posts to us.
The understanding of their rating seems to require deep knowledge about the specific subject of discussion.
The few remaining posts are either spam or rated negatively to signalize dissent with the opinion expressed in the post.
Posts that were misclassified as bad ones often contain program code, digital signatures or other non-textual parts in the body.
We plan to address these issues with better preprocessing in 127 true good true bad sum pred.
good 490 72 562 pred.
bad 95 875 970 sum 585 947 1532 Table 5: Confusion matrix for the system using all features.
the future.
However, the relatively high accuracy already achieved shows that these issues are rare.
5 Conclusion
and Future Work Assessing post quality is an important problem for many forums on the web.
Currently, most forums need their users to rate the posts manually, which is error prone, labour intensive and last but not least may lead to the problem of premature negative consent (Lampe and Resnick, 2004).
We proposed an algorithm that has shown to be able to assess the quality of forum posts.
The algorithm applies state-of-the-art classification techniques using features such as Surface, Lexical, Syntactic, Forum specific and Similarity features to do so.
Our best performing system configuration achieves an accuracy of 89.1%, which is significantly higher than the baseline of 61.82%.
Our experiments show that forum specific features perform best.
However, slightly worse but still satisfactory performance can be obtained even without those.
So far, we have not made use of the structural information in forum threads yet.
We plan to perform experiments investigating speech act recognition in forums to improve the automatic quality assessment.
We also plan to apply our system to further domains of forum discussion, such as the discussions among active Wikipedia users.
We believe that the proposed algorithm will support important applications beyond content filtering like automatic summarization systems and forum specific search.
Acknowledgments This work was supported by the German Research Foundation as part of the Research Training Group ?Feedback-Based Quality Management in eLearning??under the grant 1223.
We are thankful to Nabble for providing their data.
References Yigal Attali and Jill Burstein.
2006. Automated essay scoring with e-rater v.2.
The Journal of Technology, Learning, and Assessment, 4(3), February.
Chih-Chung Chang and Chih-Jen Lin, 2001.
LIBSVM: a library for support vector machines.
Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.
Martin Chodorow and Jill Burstein.
2004. Beyond essay length: Evaluating e-raters performance on toefl essays.
Technical report, ETS.
Donghui Feng, Erin Shaw, Jihie Kim, and Eduard Hovy.
2006. Learning to detect conversation focus of threaded discussions.
In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (HLT-NNACL).
Jihie Kim, Grace Chern, Donghui Feng, Erin Shaw, and Eduard Hovya.
2006a. Mining and assessing discussions on the web through speech act analysis.
In Proceedings of the Workshop on Web Content Mining with Human Language Technologies at the 5th International Semantic Web Conference.
Jihie Kim, Erin Shaw, Donghui Feng, Carole Beal, and Eduard Hovy.
2006b. Modeling and assessing student activities in on-line discussions.
In Proceedings of the Workshop on Educational Data Mining at the conference of the American Association of Artificial Intelligence (AAAI-06), Boston, MA.
Soo-Min Kim, Patrick Pantel, Tim Chklovski, and Marco Penneacchiotti.
2006c. Automatically assessing review helpfulness.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 423430, Sydney, Australia, July.
Cliff Lampe and Paul Resnick.
2004. Slash(dot) and burn: Distributed moderation in a large online conversation space.
In Proceedings of ACM CHI 2004 Conference on Human Factors in Computing Systems, Vienna Austria, pages 543 550.
Mitchell P.
Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz.
1994. Building a Large Annotated Corpus of English: The Penn Treebank.
Computational Linguistics, 19(2):313??30.
Ingo Mierswa, Michael Wurst, Ralf Klinkenberg, Martin Scholz, and Timm Euler.
2006. YALE: Rapid prototyping for complex data mining tasks.
In KDD ??6: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 935??40, New York, NY, USA.
ACM Press.
Helmut Schmid.
1995. Probabilistic Part-of-Speech Tagging Using Decision Trees.
In International Conference on New Methods in Language Processing, Manchester, UK.
Salvatore Valenti, Francesca Neri, and Alessandro Cucchiarelli.
2003. An overview of current research on automated essay grading.
Journal of Information Technology Education, 2:319??29.
Ian H.
Witten and Eibe Frank.
2005. Data Mining: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2 edition.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 129??32, Prague, June 2007.
c2007 Association for Computational Linguistics WordNet-based Semantic Relatedness Measures in Automatic Speech Recognition for Meetings Michael Pucher Telecommunications Research Center Vienna Vienna, Austria Speech and Signal Processing Lab, TU Graz Graz, Austria pucher@ftw.at Abstract This paper presents the application of WordNet-based semantic relatedness measures to Automatic Speech Recognition (ASR) in multi-party meetings.
Different word-utterance context relatedness measures and utterance-coherence measures are defined and applied to the rescoring of Nbest lists.
No significant improvements in terms of Word-Error-Rate (WER) are achieved compared to a large word-based ngram baseline model.
We discuss our results and the relation to other work that achieved an improvement with such models for simpler tasks.
1 Introduction
As (Pucher, 2005) has shown different WordNetbased measures and contexts are best for word prediction in conversational speech.
The JCN (Section 2.1) measure performs best for nouns using the noun-context.
The LESK (Section 2.1) measure performs best for verbs and adjectives using a mixed word-context.
Text-based semantic relatedness measures can improve word prediction on simulated speech recognition hypotheses as (Demetriou et al., 2000) have shown.
(Demetriou et al., 2000) generated N-best lists from phoneme confusion data acquired from a speech recognizer, and a pronunciation lexicon.
Then sentence hypotheses of varying Word-ErrorRate (WER) were generated based on sentences from different genres from the British National Corpus (BNC).
It was shown by them that the semantic model can improve recognition, where the amount of improvement varies with context length and sentence length.
Thereby it was shown that these models can make use of long-term information.
In this paper the best performing measures from (Pucher, 2005), which outperform baseline models on word prediction for conversational telephone speech are used for Automatic Speech Recognition (ASR) in multi-party meetings.
Thereby we want to investigate if WordNet-based models can be used for rescoring of ?real??N-best lists in a difficult task.
1.1 Word
prediction by semantic similarity The standard n-gram approach in language modeling for speech recognition cannot cope with long-term dependencies.
Therefore (Bellegarda, 2000) proposed combining n-gram language models, which are effective for predicting local dependencies, with Latent Semantic Analysis (LSA) based models for covering long-term dependencies.
WordNet-based semantic relatedness measures can be used for word prediction using long-term dependencies, as in this example from the CallHome English telephone speech corpus: (1) B: I I well, you should see what the floorleftstudentsfloorright B: after they torture them for six floorleftyearsfloorright in middle floorleftschoolfloorright and high floorleftschoolfloorright they don?t want to do anything in floorleftcollegefloorright particular.
In Example 1 college can be predicted from the noun context using semantic relatedness measures, 129 here between students and college.
A 3-gram model gives a ranking of college in the context of anything in.
An 8-gram predicts college from they don?t want to do anything in, but the strongest predictor is students.
1.2 Test
data The JCN and LESK measure that are defined in the next section are used for N-best list rescoring.
For the WER experiments N-best lists generated from the decoding of conference room meeting test data of the NIST Rich Transcription 2005 Spring (RT05S) meeting evaluation (Fiscus et al., 2005) are used.
The 4-gram that has to be improved by the WordNet-based models is trained on various corpora from conversational telephone speech to web data that together contain approximately 1 billion words.
2 WordNet-based semantic relatedness measures 2.1 Basic measures Two similarity/distance measures from the Perl package WordNet-Similarity written by (Pedersen et al., 2004) are used.
The measures are named after their respective authors.
All measures are implemented as similarity measures.
JCN (Jiang and Conrath, 1997) is based on the information content, and LESK (Banerjee and Pedersen, 2003) allows for comparison across Part-of-Speech (POS) boundaries.
2.2 Word
context relatedness First the relatedness between words is defined based on the relatedness between senses.
S(w) are the senses of word w.
Definition 2 also performs wordsense disambiguation.
rel(w,wprime) = max ci?S(w) cj?S(wprime) rel(ci,cj) (2) The relatedness of a word and a context (relW) is defined as the average of the relatedness of the word and all words in the context.
relW(w,C) = 1| C | summationdisplay wi?C rel(w,wi) (3) 2.3 Word utterance (context) relatedness The performance of the word-context relatedness (Definition 3) shows how well the measures work for algorithms that proceed in a left-to-right manner, since the context is restricted to words that have already been seen.
For the rescoring of N-best lists it is not necessary to proceed in a left-to-right manner.
The word-utterance-context relatedness can be used for the rescoring of N-best lists.
This relatedness does not only use the context of the preceding words, but the whole utterance.
Suppose U = ?w1,...,wn??is an utterance.
Let pre(wi,U) be the set uniontextj<i wj and post(wi,U) be the set uniontextj>i wj.
Then the word-utterance-context relatedness is defined as relU1(wi,U,C) = relW(wi,pre(wi,U) ??post(wi,U) ??C). (4) In this case there are two types of context.
The first context comes from the respective meeting, and the second context comes from the actual utterance.
Another definition is obtained if the context C is eliminated (C = ?? and just the utterance context U is taken into account.
relU2(wi,U) = relW(wi,pre(wi,U) ??post(wi,U)) (5) Both definitions can be modified for usage with rescoring in a left-to-right manner by restricting the contexts only to the preceding words.
relU3(wi,U,C) = relW(wi,pre(wi,U) ??C) (6) relU4(wi,U) = relW(wi,pre(wi,U)) (7) 2.4 Defining utterance coherence Using Definitions 4-7 different concepts of utterance coherence can be defined.
For rescoring the utterance coherence is used, when a score for each element of an N-best list is needed.
U is again an utterance U = ?w1,...,wn?? 130 cohU1(U,C) = 1| U | summationdisplay w?U relU1(w,U,C) (8) The first semantic utterance coherence measure (Definition 8) is based on all words in the utterance as well as in the context.
It takes the mean of the relatedness of all words.
It is based on the wordutterance-context relatedness (Definition 4).
cohU2(U) = 1| U | summationdisplay w?U relU2(w,U) (9) The second coherence measure (Definition 9) is a pure inner-utterance-coherence, which means that no history apart from the utterance is needed.
Such a measure is very useful for rescoring, since the history is often not known or because there are speech recognition errors in the history.
It is based on Definition 5.
cohU3(U,C) = 1| U | summationdisplay w?U relU3(w,U,C) (10) The third (Definition 10) and fourth (Definition 11) definition are based on Definition 6 and 7, that do not take future words into account.
cohU4(U) = 1| U | summationdisplay w?U relU4(w,U) (11) 3 Word-error-rate (WER) experiments For the rescoring experiments the first-best element of the previous N-best list is added to the context.
Before applying the WordNet-based measures, the N-best lists are POS tagged with a decision tree tagger (Schmid, 1994).
The WordNet measures are then applied to verbs, nouns and adjectives.
Then the similarity values are used as scores, which have to be combined with the language model scores of the N-best list elements.
The JCN measure is used for computing a noun score based on the noun context, and the LESK measure is used for computing a verb/adjective score based on the noun/verb/adjective context.
In the end there is a lesk score and a jcn score for each N-best list.
The final WordNet score is the sum of the two scores.
The log-linear interpolation method used for the rescoring is defined as p(S) ??pwordnet(S) pn-gram(S)1??(12) where ??denotes normalization.
Based on all WordNet scores of an N-best list a probability is estimated, which is then interpolated with the n-gram model probability.
If only the elements in an Nbest list are considered, log-linear interpolation can be used since it is not necessary to normalize over all sentences.
Then there is only one parameter  to optimize, which is done with a brute force approach.
For this optimization a small part of the test data is taken and the WER is computed for different values of .
As a baseline the n-gram mixture model trained on all available training data (??1 billion words) is used.
It is log-linearly interpolated with the WordNet probabilities.
Additionally to this sophisticated interpolation, solely the WordNet scores are used without the n-gram scores.
3.1 WER
experiments for inner-utterance coherence In this first group of experiments Definitions 8 and 9 are applied to the rescoring task.
Similarity scores for each element in an N-best list are derived according to the definitions.
The first-best element of the last list is always added to the context.
The context size is constrained to the last 20 words.
Definition 8 includes context apart from the utterance context, Definition 9 only uses the utterance context.
No improvement over the n-gram baseline is achieved for these two measures.
Neither with the log-linearly interpolated models nor with the WordNet scores alone.
The differences between the methods in terms of WER are not significant.
3.2 WER
experiments for utterance coherence In the second group of experiments Definitions 10 and 11 are applied to the rescoring task.
There is again one measure that uses dialog context (10) and one that only uses utterance context (11).
Also for these experiments no improvement over the n-gram baseline is achieved.
Neither with the 131 log-linearly interpolated models nor with the WordNet scores alone.
The differences between the methods in terms of WER are also not significant.
There are also no significant differences in performance between the second group and the first group of experiments.
4 Summary
and discussion We showed how to define more and more complex relatedness measures on top of the basic relatedness measures between word senses.
The LESK and JCN measures were used for the rescoring of N-best lists.
It was shown that speech recognition of multi-party meetings cannot be improved compared to a 4-gram baseline model, when using WordNet models.
One reason for the poor performance of the models could be that the task of rescoring simulated Nbest lists, as presented in (Demetriou et al., 2000), is significantly easier than the rescoring of ?real??Nbest lists.
(Pucher, 2005) has shown that WordNet models can outperform simple random models on the task of word prediction, in spite of the noise that is introduced through word-sense disambiguation and POS tagging.
To improve the wordsense disambiguation one could use the approach proposed by (Basili et al., 2004).
In the above WER experiments a 4-gram baseline model was used, which was trained on nearly 1 billion words.
In (Demetriou et al., 2000) a simpler baseline has been used.
650 sentences were used there to generate sentence hypotheses with different WER using phoneme confusion data and a pronunciation lexicon.
Experiments with simpler baseline models ignore that these simpler models are not used in today?s recognition systems.
We think that these prediction models can still be useful for other tasks where only small amounts of training data are available.
Another possibility of improvement is to use other interpolation techniques like the maximum entropy framework.
WordNetbased models could also be improved by using a trigger-based approach.
This could be done by not using the whole WordNet and its similarities, but defining word-trigger pairs that are used for rescoring.
5 Acknowledgements
This work was supported by the European Union 6th FP IST Integrated Project AMI (Augmented Multiparty Interaction, and by Kapsch Carrier-Com AG and Mobilkom Austria AG together with the Austrian competence centre programme Kplus.
References Satanjeev Banerjee and Ted Pedersen.
2003. Extended gloss overlaps as a measure of semantic relatedness.
In Proceedings of the 18th Int.
Joint Conf.
on Artificial Intelligence, pages 805??10, Acapulco.
Roberto Basili, Marco Cammisa, and Fabio Massimo Zanzotto.
2004. A semantic similarity measure for unsupervised semantic tagging.
In Proc.
of the Fourth International Conference on Language Resources and Evaluation (LREC2004), Lisbon, Portugal.
Jerome Bellegarda.
2000. Large vocabulary speech recognition with multispan statistical language models.
IEEE Transactions on Speech and Audio Processing, 8(1), January.
G. Demetriou, E.
Atwell, and C.
Souter. 2000.
Using lexical semantic knowledge from machine readable dictionaries for domain independent language modelling.
In Proc.
of LREC 2000, 2nd International Conference on Language Resources and Evaluation.
Jonathan G.
Fiscus, Nicolas Radde, John S.
Garofolo, Audrey Le, Jerome Ajot, and Christophe Laprun.
2005. The rich transcription 2005 spring meeting recognition evaluation.
In Rich Transcription 2005 Spring Meeting Recognition Evaluation Workshop, Edinburgh, UK.
Jay J.
Jiang and David W.
Conrath. 1997.
Semantic similarity based on corpus statistics and lexical taxonomy.
In Proceedings of the International Conference on Research in Computational Linguistics, Taiwan.
Ted Pedersen, S.
Patwardhan, and J.
Michelizzi. 2004.
WordNet::Similarity Measuring the relatedness of concepts.
In Proc.
of Fifth Annual Meeting of the North American Chapter of the ACL (NAACL-04), Boston, MA.
Michael Pucher.
2005. Performance evaluation of WordNet-based semantic relatedness measures for word prediction in conversational speech.
In IWCS 6, Sixth International Workshop on Computational Semantics, Tilburg, Netherlands.
H Schmid.
1994. Probabilistic part-of-speech tagging using decision trees.
In Proceedings of International Conference on New Methods in Language Processing, Manchester, UK, September .
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 133??36, Prague, June 2007.
c2007 Association for Computational Linguistics Building Emotion Lexicon from Weblog Corpora Changhua Yang Kevin Hsin-Yih Lin Hsin-Hsi Chen Department of Computer Science and Information Engineering National Taiwan University #1 Roosevelt Rd.
Sec. 4, Taipei, Taiwan 106 {d91013, f93141, hhchen}@csie.ntu.edu.tw Abstract An emotion lexicon is an indispensable resource for emotion analysis.
This paper aims to mine the relationships between words and emotions using weblog corpora.
A collocation model is proposed to learn emotion lexicons from weblog articles.
Emotion classification at sentence level is experimented by using the mined lexicons to demonstrate their usefulness.
1 Introduction
Weblog (blog) is one of the most widely used cybermedia in our internet lives that captures and shares moments of our day-to-day experiences, anytime and anywhere.
Blogs are web sites that timestamp posts from an individual or a group of people, called bloggers.
Bloggers may not follow formal writing styles to express emotional states.
In some cases, they must post in pure text, so they add printable characters, such as ??-)??(happy) and ??-(??(sad), to express their feelings.
In other cases, they type sentences with an internet messengerstyle interface, where they can attach a special set of graphic icons, or emoticons.
Different kinds of emoticons are introduced into text expressions to convey bloggers??emotions.
Since thousands of blog articles are created everyday, emotional expressions can be collected to form a large-scale corpus which guides us to build vocabularies that are more emotionally expressive.
Our approach can create an emotion lexicon free of laborious efforts of the experts who must be familiar with both linguistic and psychological knowledge.
2 Related
Works Some previous works considered emoticons from weblogs as categories for text classification.
Mishne (2005), and Yang and Chen (2006) used emoticons as tags to train SVM (Cortes and Vapnik, 1995) classifiers at document or sentence level.
In their studies, emoticons were taken as moods or emotion tags, and textual keywords were taken as features.
Wu et al.(2006) proposed a sentencelevel emotion recognition method using dialogs as their corpus.
?Happy, ?Unhappy?? or ?Neutral?? was assigned to each sentence as its emotion category.
Yang et al.(2006) adopted Thayer?s model (1989) to classify music emotions.
Each music segment can be classified into four classes of moods.
In sentiment analysis research, Read (2005) used emoticons in newsgroup articles to extract instances relevant for training polarity classifiers.
3 Training
and Testing Blog Corpora We select Yahoo!
Kimo Blog1 posts as our source of emotional expressions.
Yahoo! Kimo Blog service has 40 emoticons which are shown in Table 1.
When an editing article, a blogger can insert an emoticon by either choosing it or typing in the corresponding codes.
However, not all articles contain emoticons.
That is, users can decide whether to insert emoticons into articles/sentences or not.
In this paper, we treat these icons as emotion categories and taggings on the corresponding text expressions.
The dataset we adopt consists of 5,422,420 blog articles published at Yahoo!
Kimo Blog from January to July, 2006, spanning a period of 212 days.
In total, 336,161 bloggers??articles were collected.
Each blogger posts 16 articles on average.
We used the articles from January to June as the training set and the articles in July as the testing set.
Table 2 shows the statistics of each set.
On average, 14.10% of the articles contain emotion-tagged expressions.
The average length of articles with tagged emotions, i.e., 272.58 characters, is shorter 1 http://tw.blog.yahoo.com/ 133 than that of articles without tagging, i.e., 465.37 characters.
It seems that people tend to use emoticons to replace certain amount of text expressions to make their articles more succinct.
Figure 1 shows the three phases for the construction and evaluation of emotion lexicons.
In phase 1, 1,185,131 sentences containing only one emoticon are extracted to form a training set to build emotion lexicons.
In phase 2, sentence-level emotion classifiers are constructed using the mined lexicons.
In phase 3, a testing set consisting of 307,751 sentences is used to evaluate the classifiers.
4 Emotion
Lexicon Construction The blog corpus contains a collection of bloggers?? emotional expressions which can be analyzed to construct an emotion lexicon consisting of words that collocate with emoticons.
We adopt a variation of pointwise mutual information (Manning and Schtze, 1999) to measure the collocation strength co(e,w) between an emotion e and a word w: )()( ),(log),(),o( wPeP wePwecwec = (1) where P(e,w)=c(e,w)/N, P(e)=c(e)/N, P(w)=c(w)/N, c(e) and c(w) are the total occurrences of emoticon e and word w in a tagged corpus, respectively, c(e,w) is total co-occurrences of e and w, and N denotes the total word occurrences.
A word entry of a lexicon may contain several emotion senses.
They are ordered by the collocation strength co.
Figure 2 shows two Chinese example words, ???(ha1ha1) and ???
(ke3wu4). The former collocates with ?laughing?? and ?big grin??emoticons with collocation strength 25154.50 and 2667.11, respectively.
Similarly, the latter collocates with ?angry??and ?phbbbbt??
When all collocations (i.e., word-emotion pairs) are listed in a descending order of co, we can choose top n collocations to build an emotion lexicon.
In this paper, two lexicons (Lexicons A and B) are extracted by setting n to 25k and 50k.
Lexicon A contains 4,776 entries with 25,000 sense pairs and Lexicon B contains 11,243 entries and 50,000 sense pairs.
5 Emotion
Classification Suppose a sentence S to be classified consists of n emotion words.
The emotion of S is derived by a mapping from a set of n emotion words to m emotion categories as follows: },...,{?},...,{ 11 m tionclassifican eeeewewS ???
Table 1.
Yahoo! Kimo Blog Emoticon Set.
ID Emoticon Code Description ID Emoticon Code Description ID Emoticon Code Description ID Emoticon Code Description 1 :) happy 11 :O surprise 21 0:) angel 31 (:| yawn 2 :( sad 12 X-( angry 22 :-B nerd 32 =P~ drooling 3 ;) winking 13 :> smug 23 =; talk to the hand 33 :-? thinking 4 :D big grin 14 B-) cool 24 I-) asleep 34 ;)) hee hee 5 ;;) batting eyelashes 15 :-S worried 25 8-) rolling eyes 35 =D> applause 6 :-/ confused 16 >:) devil 26 :-& sick 36 [-o< praying 7 :x love struck 17 :(( crying 27 :-$ don't tell anyone 37 :-< sigh 8 :?? blushing 18 :)) laughing 28 [-( not talking 38 >:P phbbbbt 9 :p tongue 19 :| straight face 29 :o) clown 39 @};rose 10 :* kiss 20 /:) raised eyebrow 30 @-) hypnotized 40 :@) pig Table 2.
Statistics of the Weblog Dataset.
Dataset Article # Tagged # Percentage Tagged Len.
Untagged L.
Training 4,187,737 575,009 13.86% 269.77 chrs.
468.14 chrs.
Testing 1,234,683 182,999 14.92% 281.42 chrs.
455.82 chrs.
Total 5,422,420 764,788 14.10% 272.58 chrs.
465.37 chrs.
Testing Set Figure 1.
Emotion Lexicon Construction and Evaluation.
Extraction Blog Articles Features Classifiers Evaluation Lexicon Construction Training Set Phase 2 Phase 3 Emotion Lexicon Phase 1 134 For each emotion word ewi, we may find several emotion senses with the corresponding collocation strength co by looking up the lexicon.
Three alternatives are proposed as follows to label a sentence S with an emotion: (a) Method 1 (1) Consider all senses of ewi as votes.
Label S with the emotion that receives the most votes.
(2) If more than two emotions get the same number of votes, then label S with the emotion that has the maximum co.
(b) Method 2 Collect emotion senses from all ewi.
Label S with the emotion that has the maximum co.
(c) Method 3 The same as Method 1 except that each ewi votes only one sense that has the maximum co.
In past research, the approach used by Yang et al.(2006) was based on the Thayer?s model (1989), which divided emotions into 4 categories.
In sentiment analysis research, such as Read?s study (2006), a polarity classifier separated instances into positive and negative classes.
In our experiments, we not only adopt fine-grain classification, but also coarse-grain classification.
We first select 40 emoticons as a category set, and also adopt the Thayer?s model to divide the emoticons into 4 quadrants of the emotion space.
As shown in Figure 3, the top-right side collects the emotions that are more positive and energetic and the bottom-left side is more negative and silent.
A polarity classifier uses the right side as positive and the left side as negative.
6 Evaluation
Table 3 shows the performance under various combinations of lexicons, emotion categories and classification methods.
?Hit #??stands for the number of correctly-answered instances.
The baseline represents the precision of predicting the majority category, such as ?happy??or ?positive?? as the answer.
The baseline method?s precision increases as the number of emotion classes decreases.
The upper bound recall indicates the upper limit on the fraction of the 307,751 instances solvable by the corresponding method and thus reflects the limitation of the method.
The closer a method?s actual recall is to the upper bound recall, the better the method.
For example, at most 40,855 instances (14.90%) can be answered using Method 1 in combination with Lexicon A.
But the actual recall is 4.55% only, meaning that Method 1?s recall is more than 10% behind its upper bound.
Methods which have a larger set of candidate answers have higher upper bound recalls, because the probability that the correct answer is in their set of candidate answers is greater.
Experiment results show that all methods utilizing Lexicon A have performance figures lower than the baseline, so Lexicon A is not useful.
In contrast, Lexicon B, which provides a larger collection of vocabularies and emotion senses, outperforms Lexicon A and the baseline.
Although Method 3 has the smallest candidate answer set and thus has the smallest upper bound recall, it outperforms the other two methods in most cases.
Method 2 achieves better precisions when using ???? (ha1ha1) ?hah hah??
Sense 1.
(laughing) ??co: 25154.50 e.g., ?...
??? ?hah hah??
I am getting lucky~??
Sense 2.
(big grin) ??co: 2667.11 e.g., ??~? ?I only memorized vowels today~ haha ??
???????? (ke3wu4) ?darn??
Sense 1.
(angry) ??co: 2797.82 e.g., ????..?? ?What's the hacker doing... darn it ??
Sense 2.
(phbbbbt) ??co: 619.24 e.g., ??????
?Damn those aliens ??
Figure 2.
Some Example Words in a Lexicon.
Arousal (energetic) Valence (negative) (positive) (silent) unassigned: Figure 3.
Emoticons on Thayer?s model.
135 Thayer?s emotion categories.
Method 1 treats the vote to every sense equally.
Hence, it loses some differentiation abilities.
Method 1 performs the best in the first case (Lexicon A, 40 classes).
We can also apply machine learning to the dataset to train a high-precision classification model.
To experiment with this idea, we adopt LIBSVM (Fan et al., 2005) as the SVM kernel to deal with the binary polarity classification problem.
The SVM classifier chooses top k (k = 25, 50, 75, and 100) emotion words as features.
Since the SVM classifier uses a small feature set, there are testing instances which do not contain any features seen previously by the SVM classifier.
To deal with this problem, we use the class prediction from Method 3 for any testing instances without any features that the SVM classifier can recognize.
In Table 4, the SVM classifier employing 25 features has the highest precision.
On the other hand, the SVM classifier employing 50 features has the highest F measure when used in conjunction with Method 3.
7 Conclusion
and Future Work Our methods for building an emotional lexicon utilize emoticons from blog articles collaboratively contributed by bloggers.
Since thousands of blog articles are created everyday, we expect the set of emotional expressions to keep expanding.
In the experiments, the method of employing each emotion word to vote only one emotion category achieves the best performance in both fine-grain and coarse-grain classification.
Acknowledgment Research of this paper was partially supported by Excellent Research Projects of National Taiwan University, under the contract of 95R0062-AE0002.
We thank Yahoo!
Taiwan Inc.
for providing the dataset for researches.
References Corinna Cortes and V.
Vapnik. 1995.
Support-Vector Network.
Machine Learning, 20:273??97.
Rong-En Fan, Pai-Hsuen Chen and Chih-Jen Lin.
2005. Working Set Selection Using Second Order Information for Training Support Vector Machines.
Journal of Machine Learning Research, 6:1889??918.
Gilad Mishne.
2005. Experiments with Mood Classification in Blog Posts.
Proceedings of 1st Workshop on Stylistic Analysis of Text for Information Access.
Jonathon Read.
2005. Using Emotions to Reduce Dependency in Machine Learning Techniques for Sentiment Classification.
Proceedings of the ACL Student Research Workshop, 43-48.
Robert E.
Thayer. 1989.
The Biopsychology of Mood and Arousal, Oxford University Press.
Changhua Yang and Hsin-Hsi Chen.
2006. A Study of Emotion Classification Using Blog Articles.
Proceedings of Conference on Computational Linguistics and Speech Processing, 253-269.
Yi-Hsuan Yang, Chia-Chu Liu, and Homer H.
Chen. 2006.
Music Emotion Classification: A Fuzzy Approach.
Proceedings of ACM Multimedia, 81-84.
Chung-Hsien Wu, Ze-Jing Chuang, and Yu-Chung Lin.
2006. Emotion Recognition from Text Using Semantic Labels and Separable Mixture Models.
ACM Transactions on Asian Language Information Processing, 5(2):165-182.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 137??40, Prague, June 2007.
c2007 Association for Computational Linguistics Construction of Domain Dictionary for Fundamental Vocabulary Chikara Hashimoto Faculty of Engineering, Yamagata University 4-3-16 Jonan, Yonezawa-shi, Yamagata, 992-8510 Japan Sadao Kurohashi Graduate School of Informatics, Kyoto University 36-1 Yoshida-Honmachi, Sakyo-ku, Kyoto, 606-8501 Japan Abstract For natural language understanding, it is essential to reveal semantic relations between words.
To date, only the IS-A relation has been publicly available.
Toward deeper natural language understanding, we semiautomatically constructed the domain dictionary that represents the domain relation between Japanese fundamental words.
This is the first Japanese domain resource that is fully available.
Besides, our method does not require a document collection, which is indispensable for keyword extraction techniques but is hard to obtain.
As a task-based evaluation, we performed blog categorization.
Also, we developed a technique for estimating the domain of unknown words.
1 Introduction
We constructed a lexical resource that represents the domain relation among Japanese fundamental words (JFWs), and we call it the domain dictionary.1 It associates JFWs with domains in which they are typically used.
For example, a0a2a1a4a3a6a5a6a7 home run is associated with the domain SPORTS2.
That is, we aim to make explicit the horizontal relation between words, the domain relation, while thesauri indicate the vertical relation called IS-A.3 1In fact, there have been a few domain resources in Japanese like Yoshimoto et al.(1997). But they are not publicly available.
2Domains are CAPITALIZED in this paper.
3The lack of the horizontal relationship is also known as the ?tennis problem??(Fellbaum, 1998, p.10). 2 Two Issues You have to address two issues.
One is what domains to assume, and the other is how to associate words with domains without document collections.
The former is paraphrased as how people categorize the real world, which is really a hard problem.
In this study, we avoid being too involved in the problem and adopt a simple domain system that most people can agree on, which is as follows: CULTURE RECREATION SPORTS HEALTH LIVING DIET TRANSPORTATION EDUCATION SCIENCE BUSINESS MEDIA GOVERNMENT It has been created based on web directories such as Open Directory Project with some adjustments.
In addition, NODOMAIN was prepared for those words that do not belong to any particular domain.
As for the latter issue, you might use keyword extraction techniques; identifying words that represent a domain from the document collection using statistical measures like TF*IDF and matching between extracted words and JFWs.
However, you will find that document collections of common domains such as those assumed here are hard to obtain.4 Hence, we had to develop a method that does not require document collections.
The next section details it.
4Initially, we tried collecting web pages in Yahoo!
JAPAN. However, we found that most of them were index pages with a few text contents, from which you cannot extract reliable keywords.
Though we further tried following links in those index pages to acquire enough texts, extracted words turned out to be site-specific rather than domain-specific since many pages were collected from a particular web site.
137 Table 1: Examples of Keywords for each Domain Domain Examples of Keywords CULTURE a0a2a1 movie, a3a5a4 music RECREATION a6a5a7 tourism, a8a10a9 firework SPORTS a11a13a12 player, a14a5a15 baseball HEALTH a12a13a16 surgery, a17a19a18 diagnosis LIVING a20a13a21 childcare, a22a10a23 furniture DIET a24 chopsticks, a25a27a26 lunch TRANSPORTATION a28 station, a29a13a30 road EDUCATION a31a10a32 teacher, a33a5a34 arithmetic SCIENCE a35a10a36 research, a37a13a38 theory BUSINESS a39a13a40 import, a41a10a42 market MEDIA a43a13a44 broadcast, a45a27a46 reporter GOVERNMENT a47a13a48 judicatory, a49 tax 3 Domain Dictionary Construction To identify which domain a JFW is associated with, we use manually-prepared keywords for each domain rather than document collections.
The construction process is as follows: 1 Preparing keywords for each domain (3.1).
2 Associating
JFWs with domains (3.2).
3 Reassociating
JFWs with NODOMAIN (3.3).
4 Manual
correction (3.5).
3.1 Preparing
Keywords for each Domain About 20 keywords for each domain were collected manually from words that appear most frequently in the Web.
Table 1 shows examples of the keywords.
3.2 Associating
JFWs with Domains A JFW is associated with a domain of the highest Ad score.
An Ad score of domain is calculated by summing up the top five Ak scores of the domain.
Then, an Ak score, which is defined between a JFW and a keyword of a domain, is a measure that shows how strongly the JFW and the keyword are related (Figure 1).
Assuming that two words are related if they cooccur more often than chance in a corpus, we adopt the ?2 statistics to calculate an Ak score and use web pages as a corpus.
The number of co-occurrences is approximated by the number of search engine hits when the two words are used as queries.
Among various alternatives, the combination of the ?2 statistics and web pages is adopted following Sasaki et al.(2006). Based on Sasaki et al.(2006), Ak score between JFWs JFW1 JFW2 JFW3  DOMAIN1 kw1a kw1b  DOMAIN2 kw2a kw2b   Adscore JFWm kwna kwnb  DOMAINn Ak scores Figure 1: Associating JFWs with Domains a JFW (jw) and a keyword (kw) is given as below.
Ak(jw,kw) = n(ad ??bc) 2 (a + b)(c + d)(a + c)(b + d) where n is the total number of Japanese web pages, a = hits(jw & kw), b = hits(jw) ??a, c = hits(kw) ??a, d = n ??(a + b + c).
Note that hits(q) represents the number of search engine hits when q is used as a query.
3.3 Reassociating
JFWs with NODOMAIN JFWs that do not belong to any particular domain, i.e. whose highest Ad score is low should be reassociated with NODOMAIN.
Thus, a threshold for determining if a JFW?s highest Ad score is low is required.
The threshold for a JFW (jw) needs to be changed according to hits(jw); the greater hits(jw) is, the higher the threshold should be.
To establish a function that takes jw and returns the appropriate threshold for it, the following semiautomatic process is required after all JFWs are associated with domains: (i) Sort all tuples of the form < jw, hits(jw), the highest Ad of the jw > by hits(jw).5 (ii) Segment the tuples.
(iii) For each segment, extract manually tuples whose jw should be associated with one of the 12 domains and those whose jw should be deemed as NODOMAIN.
Note that the former tuples usually have higher Ad scores than the latter tuples.
(iv) For each segment, identify a threshold that distinguishes between the former tuples and the latter tuples by their Ad scores.
At this point, pairs of the number of hits (represented by each segment) and the appropriate threshold for it are obtained.
(v) Approximate the relation between 5Note that we acquire the number of search engine hits and the Ad score for each jw in the process 2. 138 the number of hits and its threshold by a linear function using least-square method.
Finally, this function indicates the appropriate threshold for each jw.
3.4 Performance
of the Proposed Method We applied the method to JFWs installed on JUMAN (Kurohashi et al., 1994), which are 26,658 words consisting of commonly used nouns and verbs.
As an evaluation, we sampled 380 pairs of a JFW and its domain, and measured accuracy.6 As a result, the proposed method attained the accuracy of 81.3% (309/380).
3.5 Manual
Correction Our policy is that simpler is better.
Thus, as one of our guidelines for manual correction, we avoid associating a JFW with multiple domains as far as possible.
JFWs to associate with multiple domains are restricted to those that are EQUALLY relevant to more than one domain.
4 Blog
Categorization As a task-based evaluation, we categorized blog articles into the domains assumed here.
4.1 Categorization
Method (i) Extract JFWs from the article.
(ii) Classify the extracted JFWs into the domains using the domain dictionary.
(iii) Sort the domains by the number of JFWs classified in descending order.
(iv) Categorize the article as the top domain.
If the top domain is NODOMAIN, the article is categorized as the second domain under the condition below.
|W(2ND DOMAIN)|  |W(NODOMAIN)| > 0.03 where |W(D)| is the number of JFWs classified into the domain D.
4.2 Data
We prepared two blog collections; Bcontrolled and Brandom.
As Bcontrolled, 39 blog articles were collected (3 articles for each domain including NODOMAIN) by the following procedure: (i) Query the Web using a keyword of the domain.7 (ii) From 6In the evaluation, one of the authors judged the correctness of each pair.
7To collect articles that are categorized as NODOMAIN, we used a0a2a1 diary as a query.
Table 2: Breakdown of Brandom Domain # CULTURE 4 RECREATION 1 SPORTS 3 HEALTH 1 Domain # DIET 4 BUSINESS 12 NODOMAIN 5 the top of the search result, collect 3 articles that meet the following conditions; there are enough text contents in it, and people can confidently make a judgment about which domain it is categorized as.
As Brandom, 30 articles were randomly sampled from the Web.
Table 2 shows its breakdown.
Note that we manually removed peripheral contents like author profiles or banner advertisements from the articles in both Bcontrolled and Brandom.
4.3 Result
We measured the accuracy of blog categorization.
As a result, the accuracy of 89.7% (35/39) was attained in categorizing Bcontrolled, while Brandom was categorized with 76.6% (23/30) accuracy.
5 Domain
Estimation for Unknown Words We developed an automatic way of estimating the domain of unknown word (uw) using the dictionary.
5.1 Estimation
Method (i) Search the Web by using uw as a query.
(ii) Retrieve the top 30 documents of the search result.
(iii) Categorize the documents as one of the domains by the method described in 4.1.
(iv) Sort the domains by the number of documents in descending order.
(v) Associate uw with the top domain.
5.2 Experimental
Condition (i) Select 10 words from the domain dictionary for each domain.
(ii) For each word, estimate its domain by the method in 5.1 after removing the word from the dictionary so that the word is unknown.
5.3 Result
Table 3 shows the number of correctly domainestimated words (out of 10) for each domain.
Accordingly, the total accuracy is 67.5% (81/120).
139 Table 3: # of Correctly Domain-estimated Words Domain # CULTURE 7 RECREATION 4 SPORTS 9 HEALTH 9 LIVING 3 DIET 7 Domain # TRANSPORTATION 7 EDUCATION 9 SCIENCE 6 BUSINESS 9 MEDIA 2 GOVERNMENT 9 As for the poor accuracy for RECREATION, LIVING, and MEDIA, we found that it was due to either the ambiguous nature of the words of domain or a characteristic of the estimation method.
The former brought about the poor accuracy for MEDIA.
That is, some words of MEDIA are often used in other contexts.
For example, a0a2a1 live coverage is often used in the SPORTS context.
On the other hand, the method worked poorly for RECREATION and LIVING for the latter reason; the method exploits the Web.
Namely, some words of the domains, such as a3a5a4 tourism and a6a5a7 a7a9a8 a1 shampoo, are often used in the web sites of companies (BUSINESS) that provide services or goods related to RECREATION or LIVING.
As a result, the method tends to wrongly associate those words with BUSINESS.
6 Related
Work HowNet (Dong and Dong, 2006) and WordNet provide domain information for Chinese and English, but there has been no domain resource for Japanese that are publicly available.8 Domain dictionary construction methods that have been developed so far are all based on highly structured lexical resources like LDOCE or WordNet (Guthrie et al., 1991; Agirre et al., 2001) and hence not applicable to languages for which such highly structured lexical resources are not available.
Accordingly, contributions of this study are twofold: (i) We constructed the first Japanese domain dictionary that is fully available.
(ii) We developed the domain dictionary construction method that requires neither document collections nor highly structured lexical resources.
8Some human-oriented dictionaries provide domain information.
However, domains they cover are all technical ones rather than common domains such as those assumed here.
7 Conclusion
Toward deeper natural language understanding, we constructed the first Japanese domain dictionary that contains 26,658 JFWs.
Our method requires neither document collections nor structured lexical resources.
The domain dictionary can satisfactorily classify blog articles into the 12 domains assumed in this study.
Also, the dictionary can reliably estimate the domain of unknown words except for words that are ambiguous in terms of domains and those that appear frequently in web sites of companies.
Among our future work is to deal with domain information of multiword expressions.
For example, a10a5a11 fount and a12a14a13 collection constitute a10a14a11 a12a14a13 tax deduction at source.
Note that while a10a5a11 itself belongs to NODOMAIN, a10a15a11 a12a14a13 should be associated with GOVERNMENT.
Also, we will install the domain dictionary on JUMAN (Kurohashi et al., 1994) to make the domain information fully and easily available.
References Eneko Agirre, Olatz Ansa, David Martinez, and Ed Hovy.
2001. Enriching wordnet concepts with topic signatures.
In Proceedings of the SIGLEX Workshop on ?WordNet and Other Lexical Resources: Applications, Extensions, and Customizations??in conjunction with NAACL.
Zhendong Dong and Qiang Dong.
2006. HowNet And the Computation of Meaning.
World Scientific Pub Co Inc.
Christiane Fellbaum.
1998. WordNet: An Electronic Lexical Database.
MIT Press.
Joe A.
Guthrie, Louise Guthrie, Yorick Wilks, and Homa Aidinejad.
1991. Subject-Dependent Co-Occurence and Word Sense Disambiguation.
In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, pages 146??52.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto, and Makoto Nagao.
1994. Improvements of Japanese Mophological Analyzer JUMAN.
In Proceedings of the International Workshop on Sharable Natural Language Resources, pages 22??8.
Yasuhiro Sasaki, Satoshi Sato, and Takehito Utsuro.
2006. Related Term Collection.
Journal of Natural Language Processing, 13(3):151??76.
(in Japanese).
Yumiko Yoshimoto, Satoshi Kinoshita, and Miwako Shimazu.
1997. Processing of proper nouns and use of estimated subject area for web page translation.
In tmi97, pages 10??8, Santa Fe .
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 141??44, Prague, June 2007.
c2007 Association for Computational Linguistics Extracting Word Sets with Non-Taxonomical Relation Eiko Yamamoto Hitoshi Isahara Computational Linguistics Group National Institute of Information and Communications Technology 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289, Japan {eiko, isahara}@nict.go.jp Abstract At least two kinds of relations exist among related words: taxonomical relations and thematic relations.
Both relations identify related words useful to language understanding and generation, information retrieval, and so on.
However, although words with taxonomical relations are easy to identify from linguistic resources such as dictionaries and thesauri, words with thematic relations are difficult to identify because they are rarely maintained in linguistic resources.
In this paper, we sought to extract thematically (non-taxonomically) related word sets among words in documents by employing case-marking particles derived from syntactic analysis.
We then verified the usefulness of word sets with non-taxonomical relation that seems to be a thematic relation for information retrieval.
1. Introduction Related word sets are useful linguistic resources for language understanding and generation, information retrieval, and so on.
In previous research on natural language processing, many methodologies for extracting various relations from corpora have been developed, such as the ?is-a??relation (Hearst 1992), ?part-of??relation (Berland and Charniak 1999), causal relation (Girju 2003), and entailment relation (Geffet and Dagan 2005).
Related words can be used to support retrieval in order to lead users to high-quality information.
One simple method is to provide additional words related to the key words users have input, such as an input support function within the Google search engine.
What kind of relation between the key words that have been input and the additional word is effective for information retrieval?
As for the relations among words, at least two kinds of relations exist: the taxonomical relation and the thematic relation.
The former is a relation representing the physical resemblance among objects, which is typically a semantic relation such as a hierarchal, synonymic, or antonymic relation; the latter is a relation between objects through a thematic scene, such as ?milk??and ?cow??as recollected in the scene ?milking a cow,??and ?milk?? and ?baby,??as recollected in the scene ?giving baby milk,??which include causal relation and entailment relation.
Wisniewski and Bassok (1999) showed that both relations are important in recognizing those objects.
However, while taxonomical relations are comparatively easy to identify from linguistic resources such as dictionaries and thesauri, thematic relations are difficult to identify because they are rarely maintained in linguistic resources.
In this paper, we sought to extract word sets with a thematic relation from documents by employing case-marking particles derived from syntactic analysis.
We then verified the usefulness of word sets with non-taxonomical relation that seems to be a thematic relation for information retrieval.
2. Method In order to derive word sets that direct users to obtain information, we applied a method based on the Complementary Similarity Measure (CSM), which can determine a relation between two words in a corpus by estimating inclusive relations between two vectors representing each appearance pattern for each words (Yamamoto et al.2005). 141 We first extracted word pairs having an inclusive relation between the words by calculating the CSM values.
Extracted word pairs are expressed by a tuple <w i, w j >, where CSM(V i, V j ) is greater than CSM(V j, V i ) when words w i and w j have each appearance pattern represented by each binary vector V i and V j. Then, we connected word pairs with CSM values greater than a certain threshold and constructed word sets.
A feature of the CSM-based method is that it can extract not only pairs of related words but also sets of related words because it connects tuples consistently.
Suppose we have <A, B>, <B, C>, <Z, B>, <C, D>, <C, E>, and <C, F> in the order of their CSM values, which are greater than the threshold.
For example, let <B, C> be an initial word set {B, C}.
First, we find the tuple with the greatest CSM value among the tuples in which the word C at the tail of the current word set is the left word, and connect the right word behind C.
In this example, word ?D??is connected to {B, C} because <C, D> has the greatest CSM value among the three tuples <C, D>, <C, E>, and <C, F>, making the current word set {B, C, D}.
This process is repeated until no tuples exist.
Next, we find the tuple with the greatest CSM value among the tuples in which the word B at the head of the current word set is the right word, and connect the left word before B.
This process is repeated until no tuples exist.
In this example, we obtain the word set {A, B, C, D}.
Finally, we removed ones with a taxonomical relation by using thesaurus.
The rest of the word sets have a non-taxonomical relation ??including a thematic relation ??among the words.
We then extracted those word sets that do not agree with the thesaurus as word sets with a thematic relation.
3. Experiment In our experiment, we used domain-specific Japanese documents within the medical domain (225,402 sentences, 10,144 pages, 37MB) gathered from the Web pages of a medical school and the 2005 Medical Subject Headings (MeSH) thesaurus 1 . Recently, there has been a study on query expansion with this thesaurus as domain information (Friberg 2007).
1 The
U.S.
National Library of Medicine created, maintains, and provides the MeSH  thesaurus.
We extracted word sets by utilizing inclusive relations of the appearance pattern between words based on a modified/modifier relationship in documents.
The Japanese language has casemarking particles that indicate the semantic relation between two elements in a dependency relation.
Then, we collected from documents dependency relations matching the following five patterns; ?A <no (of)> B,???P <wo (object)> V,???Q <ga (subject)> V,???R <ni (dative)> V,??and ?S <ha (topic)> V,??where A, B, P, Q, R, and S are nouns, V is a verb, and <X> is a case-marking particle.
From such collected dependency relations, we compiled the following types of experimental data; NN-data based on co-occurrence between nouns for each sentence, NV-data based on a dependency relation between noun and verb for each case-marking particle <wo>, <ga>, <ni>, and <ha>, and SO-data based on a collocation between subject and object that depends on the same verb V as the subject.
These data are represented with a binary vector which corresponds to the appearance pattern of a noun and these vectors are used as arguments of CSM.
We translated descriptors in the MeSH thesaurus into Japanese and used them as Japanese medical terms.
The number of terms appearing in this experiment is 2,557 among them.
We constructed word sets consisting of these medical terms.
Then, we chose 977 word sets consisting of three or more terms from them, and removed word sets with a taxonomical relation from them with the MeSH thesaurus in order to obtain the rest 847 word sets as word sets with a thematic relation.
4. Verification In verifying the capability of our word sets to retrieve Web pages, we examined whether they could help limit the search results to more informative Web pages with Google as a search engine.
We assume that addition of suitable key words to the query reduces the number of pages retrieved and the remaining pages are informative pages.
Based on this assumption, we examined the decrease of the retrieved pages by additional key words and the contents of the retrieved pages in order to verify the availability of our word sets.
Among 847 word sets, we used 294 word sets in which one of the terms is classified into one category and the rest are classified into another.
142 ovary spleen palpation (NN) variation cross reactions outbreaks secretion (Wo) bleeding pyrexia hematuria consciousness disorder vertigo high blood pressure (Ga) space flight insemination immunity (Ni) cough fetus bronchiolitis obliterans organizing pneumonia (Ha) latency period erythrocyte hepatic cell (SO) Figure 1.
Examples of word sets used to verify.
Figure 1 shows examples of the word sets, where terms in a different category are underlined.
In retrieving Web pages for verification, we input the terms composed of these word sets into the search engine.
We created three types of search terms from the word set we extracted.
Suppose the extracted word set is {X 1, ..., X n, Y}, where X i is classified into one category and Y is classified into another.
The first type uses all terms except the one classified into a category different from the others: {X 1, ..., X n } removing Y.
The second type uses all terms except the one in the same category as the rest: {X 1, ..., X k-1, X k+1, ..., X n } removing X k from Type 1.
In our experiment, we removed the term X k with the highest or lowest frequency among X i . The third type uses terms in Type 2 and Y: {X 1, ..., X k-1, X k+1, ..., X n, Y}.
In other words, when we consider the terms in Type 2 as base key words, the terms in Type 1 are key words with the addition of one term having the highest or lowest frequency among the terms in the same category; i.e., the additional term has a feature related to frequency in the documents and is taxonomically related to other terms.
The terms in Type 3 are key words with the addition of one term in a category different from those of the other component terms; i.e., the additional term seems to be thematically related ??at least nontaxonomically related ??to other terms.
First, we quantitatively compared the retrieval results.
We used the estimated number of pages retrieved by Google?s search engine.
Suppose that we first input Type 2 as key words into Google, did not satisfy the result extracted, and added one word to the previous key words.
We then sought to determine whether to use Type 1 or Type 3 to obtain more suitable results.
The results are shown in Figures 2 and 3, which include the results for the highest frequency and the lowest frequency, respectively.
In these figures, the horizontal axis is the number of pages retrieved with Type 2 and the vertical axis is the number of pages retrieved when 1 10 100 1000 10000 100000 1000000 10000000 100000000 1 10 100 1000 10000 100000 1000000 10000000 100000000 1000000000 Number of Web pages retrieved with Type2 (base key words) Number of Web pages retrieved when a term is added to Type2 Type3: With additional term in a different category Type1: With additional term in same category Figure 2.
Fluctuation of number of pages retrieved (with the high frequency term).
NV Type of Data NN Wo Ga Ni Ha Word sets for verification 175 43 23 13 26 Cases in which Type 3 defeated Type 1 in retrieval 108 37 15 12 18 Table 1.
Number of cases in which Type 3 defeated Type 1 with the high frequency term.
a certain term is added to Type 2.
The circles (?? show the retrieval results with additional key word related taxonomically (Type 1).
The crosses () show the results with additional key word related non-taxonomically (Type 3).
The diagonal line shows that adding one term to the base key words does not affect the number of Web pages retrieved.
In Figure 2, most crosses fall further below the line.
This graph indicates that when searching by Google, adding a search term related nontaxonomically tends to make a bigger difference than adding a term related taxonomically and with high frequency.
This means that adding a term related non-taxonomically to the other terms is crucial to retrieving informative pages; that is, such terms are informative terms themselves.
Table 1 shows the number of cases in which term in different category decreases the number of hit pages more than high frequency term.
By this table, we found that most of the additional terms with high frequency contributed less than additional terms related non-taxonomically to decreasing the number of Web pages retrieved.
This means that, in comparison to the high frequency terms, which might not be so informative in themselves, the terms in the other category ??related nontaxonomically ??are effective for retrieving useful Web pages.
In Figure 3, most circles fall further below the line, in contrast to Figure 2.
This indicates that 143 Figure 3.
Fluctuation of number of pages retrieved (with the low frequency term).
NV Type of Data NN Wo Ga Ni Ha Word sets for verification 175 43 23 13 26 Cases in which Type 3 defeated Type 1 in retrieval 61 18 7 6 13 Table 2.
Number of cases in which Type 3 defeated Type 1 with the low frequency term.
adding a term related taxonomically and with low frequency tends to make a bigger difference than adding a term with high frequency.
Certainly, additional terms with low frequency would be informative terms, even though they are related taxonomically, because they may be rare terms on the Web and therefore the number of pages containing the term would be small.
Table 2 shows the number of cases in which term in different category decreases the number of hit pages more than low frequency term.
In comparing these numbers, we found that the additional term with low frequency helped to reduce the number of Web pages retrieved, making no effort to determine the kind of relation the term had with the other terms.
Thus, the terms with low frequencies are quantitatively effective when used for retrieval.
However, if we compare the results retrieved with Type 1 search terms and Type 3 search terms, it is clear that big differences exist between them.
For example, consider ?latency period erythrocyte hepatic cell??obtained from SO-data in Figure 1.
?Latency period??is classified into a category different from the other terms and ?hepatic cell?? has the lowest frequency in this word set.
When we used all the three terms, we obtained pages related to ?malaria??at the top of the results and the title of the top page was ?What is malaria???in Japanese.
With ?latency period??and ?erythrocyte,??we again obtained the same page at the top, although it was not at the top when we used ?erythrocyte??and ?hepatic cell??which have a taxonomical relation.
Type3: With additional term in a different category Type1: With additional term in same category 1 10 100 1000 10000 100000 1000000 10000000 As we showed above, the terms with thematic relations with other search terms are effective at directing users to informative pages.
Quantitatively, terms with a high frequency are not effective at reducing the number of pages retrieved; qualitatively, low frequency terms may not effective to direct users to informative pages.
We will continue our research in order to extract terms in thematic relation more accurately and verify the usefulness of them more quantitatively and qualitatively.
5. Conclusion We sought to extract word sets with a thematic relation from documents by employing casemarking particles derived from syntactic analysis.
We compared the results retrieved with terms related only taxonomically and the results retrieved with terms that included a term related nontaxonomically to the other terms.
As a result, we found adding term which is thematically related to terms that have already been input as key words is effective at retrieving informative pages.
References Berland, M.
and Charniak, E.
1999. Finding parts in very large corpora, In Proceedings of ACL 99, 57??4.
Friberg, K.
2007. Query expansion using domain information in compounds, In Proceedings of NAACL-HLT 2007 Doctoral Consortium, 1??.
Geffet, M.
and Dagan, I.
2005. The distribution inclusion hypotheses and lexical entailment.
In Proceedings of ACL 2005, 107??14.
Girju, R.
2003. Automatic detection of causal relations for question answering.
In Proceedings of ACL Workshop on Multilingual summarization and question answering, 76??14.
Hearst, M.
A. 1992, Automatic acquisition of hyponyms from large text corpora, In Proceedings of Coling 92, 539??45.
Wisniewski, E.
J. and Bassok.
M. 1999.
What makes a man similar to a tie?
Cognitive Psychology, 39: 208??238.
Yamamoto, E., Kanzaki, K., and Isahara, H.
2005. Extraction of hierarchies based on inclusion of co-occurring words with frequency information.
In Proceedings of IJCAI 2005, 1166??172 .
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 145??48, Prague, June 2007.
c2007 Association for Computational Linguistics A Linguistic Service Ontology for Language Infrastructures Yoshihiko Hayashi Graduate School of Language and Culture, Osaka University 1-8 Machikaneyama-cho, Toyonaka, 560-0043 Japan hayashi@lang.osaka-u.ac.jp Abstract This paper introduces conceptual framework of an ontology for describing linguistic services on network-based language infrastructures.
The ontology defines a taxonomy of processing resources and the associated static language resources.
It also develops a sub-ontology for abstract linguistic objects such as expression, meaning, and description; these help define functionalities of a linguistic service.
The proposed ontology is expected to serve as a solid basis for the interoperability of technical elements in language infrastructures.
1 Introduction
Several types of linguistic services are currently available on the Web, including text translation and dictionary access.
A variety of NLP tools is also available and public.
In addition to these, a number of community-based language resources targeting particular domains of application have been developed, and some of them are ready for dissemination.
A composite linguistic service tailored to a particular user's requirements would be composable, if there were a language infrastructure on which elemental linguistic services, such as NLP tools, and associated language resources could be efficiently combined.
Such an infrastructure should provide an efficient mechanism for creating workflows of composite services by means of authoring tools for the moment, and through an automated planning in the future.
To this end, technical components in an infrastructure must be properly described, and the semantics of the descriptions should be defined based on a shared ontology.
2 Architecture
of a Language Infrastructure The linguistic service ontology described in this paper has not been intended for a particular language infrastructure.
However we expect that the ontology should be first introduced in an infrastructure like the Language Grid 1, because it, unlike other research-oriented infrastructures, tries to incorporate a wide range of NLP tools and community-based language resources (Ishida, 2006) in order to be useful for a range of intercultural collaboration activities.
The fundamental technical components in the Language Grid could be: (a) external web-based services, (b) on-site NLP core functions, (c) static language resources, and (d) wrapper programs.
Figure 1 depicts the general architecture of the infrastructure.
The technical components listed above are deployed as shown in the figure.
Computational nodes in the language grid are classified into the following two types as described in (Murakami et al., 2006).
ces. Tthe most important desideratum for the ontology, therefore, is that it be able to specify the input/output constraints of a linguistic service properly.
Such input/output specifications enable us to derive a taxonomy of linguistic service and the associated language resources.
3 The
Upper Ontology 3.1 The top level We have developed the upper part of the service ontology so far, and have been working on detailing some of its core parts.
Figure 2 shows the top level of the proposed linguistic service ontology.
Figure 2.
The Top Level of the Ontology.
The topmost class is NL_Resource, which is partitioned into ProcessingResource, and LanguageResource.
Here, as in GATE (Cunningham, 2002), processing resource refers to programmatic or algorithmic resources, while language resource refers to data-only static resources such as lexicons or corpora.
The innate relation between these two classes is: a processing resource can use language resources.
This relationship is specifically introduced to properly define linguistic services that are intended to provide access functions to language resources.
As shown in the figure, LinguisticService is provided by a processing resource, stressing that any linguistic service is realized by a processing resource, even if its prominent functionality is accessing language resources in response to a user?s query.
It also has the meta-information for advertising its non-functional descriptions.
The fundamental classes for abstract linguistic objects, Expression, Meaning, and Description and the innate relations among them are illustrated in Figure 3.
These play roles in defining functionalities of some types of processing resources and associated language resources.
As shown in Fig.
3, an expression may denote a meaning, and the meaning can be further described by a description, especially for human uses.
Figure 3.
Classes for Abstract Linguistic Objects.
In addition to these, NLProcessedStatus and LinguisticAnnotation are important in the sense that NLP status represents the so-called IOPE (Input-Output-Precondition-Effect) parameters of a linguistic processor, which is a subclass of the processing resource, and the data schema for the results of a linguistic analysis is defined by using the linguistic annotation class.
3.2 Taxonomy
of language resources The language resource class currently is partitioned into subclasses for Corpus and Dictionary.
The immediate subclasses of the dictionary class are: (1) MonolingualDictionary, (2) BihasNLProcessedStatus* NLP Tool Linguistic Service External Linguistic Service Language Resource Access Mechanism Language Resource maintains -profiles registry -workflows Core Node Service Node Application Program wrapper 146 lingualDictionary, (3) MultilingualTerminology, and (4) ConceptLexicon.
The major instances of (1) and (2) are so-called machine-readable dictionaries (MRDs).
Many of the community-based special language resources should fall into (3), including multilingual terminology lists specialized for some application domains.
For subclass (4), we consider the computational concept lexicons, which can be modeled by a WordNet-like encoding framework (Hayashi and Ishida, 2006).
3.3 Taxonomy
of processing resources The top level of the processing resource class consists of the following four subclasses, which take into account the input/output constraints of processing resources, as well as the language resources they utilize.
urce it accesses.
The input to a language resource accessor is a query (LR_AccessQuery, sub-class of Expression), and the output is a kind of ?dictionary meaning??(DictionaryMeaning), which is a sub-class of meaning class.
The dictionary meaning class is further divided into sub-classes by referring to the taxonomy of dictionary.
notations by itself or by incorporating some external standard, such as LAF (Ide and Romary, 2004).
3.5 NLP
status and the associated issues Figure 5 illustrates our working taxonomy of NLP processed status.
Note that, in this figure, only the portion related to linguistic analyzer is detailed.
Benefits from the NLP status class will be twofold: (1) as a part of the description of a linguistic analyzer, we assign corresponding instances of this class as its precondition/effect parameters, (2) any instance of the expression class can be concisely 147 ?tagged??by instances of the NLP status class, according to how ?deeply??the expression has been linguistically analyzed so far.
Essentially, such information can be retrieved from the attached linguistic annotations.
In this sense, the NLP status class might be redundant.
Tagging an instance of expression in that way, however, can be reasonable: we can define the input/output constraints of a linguistic analyzer concisely with this device.
Figure 5.
Taxonomy of NLP Status.
Each subclass in the taxonomy represents the type or level of a linguistic analysis, and the hierarchy depicts the processing constraints among them.
For example, if an expression has been parsed, it would already have been morphologically analyzed, because parsing usually requires the input to be morphologically analyzed beforehand.
The subsumption relations encoded in the taxonomy allow simple reasoning in possible composite service composition processes.
However note that the taxonomy is only preliminary.
The arrangement of the subclasses within the hierarchy may end up being far different, depending on the languages considered, and the actual NLP tools, these are essentially idiosyncratic, that are at hand.
For example, the notion of ?chunk??may be different from language to language.
Despite of these, if we go too far in this direction, constructing a taxonomy would be meaningless, and we would forfeit reasonable generalities.
4 Related
Works Klein and Potter (2004) have once proposed an ontology for NLP services with OWL-S definitions.
Their proposal however has not included detailed taxonomies either for language resources, or for abstract linguistic objects, as shown in this paper.
Graa, et al.(2006) introduced a framework for integrating NLP tools with a client-server architecture having a multi-layered repository.
They also proposed a data model for encoding various types of linguistic information.
However the model itself is not ontologized as proposed in this paper.
5 Concluding
Remarks Although the proposed ontology successfully defined a number of first class objects and the innate relations among them, it must be further refined by looking at specific NLP tools/systems and the associated language resources.
Furthermore, its effectiveness in composition of composite linguistic services or wrapper generation should be demonstrated on a specific language infrastructure such as the Language Grid.
Acknowledgments The presented work has been partly supported by NICT international joint research grant.
The author would like to thank to Thierry Declerck and Paul Buitelaar (DFKI GmbH, Germany) for their helpful discussions.
References H.
Cunningham, et al.2002. GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications.
Proc. of ACL 2002, pp.168-175.
J. Graa, et al.2006. NLP Tools Integration Using a Multi-Layered Repository.
Proc. of LREC 2006 Workshop on Merging and Layering Linguistic Information.
Y. Hayashi and T.
Ishida. 2006.
A Dictionary Model for Unifying Machine Readable Dictionaries and Computational Concept Lexicons.
Proc. of LREC 2006, pp.1-6.
N. Ide and L.
Romary. 2004.
International Standard for a Linguistic Annotation Framework.
Journal of Natural Language Engineering, Vol.10:3-4, pp.211-225.
T. Ishida.
2006. Language Grid: An Infrastructure for Intercultural Collaboration.
Proc. of SAINT-06, pp.
96-100, keynote address.
E. Klein and S.
Potter. 2004.
An Ontology for NLP Services.
Proc. of LREC 2004 Workshop on Registry of Linguistic Data Categories.
Y. Murakami, et al.2006. Infrastructure for Language Service Composition.
Proc. of
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 149??52, Prague, June 2007.
c2007 Association for Computational Linguistics Empirical Measurements of Lexical Similarity in Noun Phrase Conjuncts Deirdre Hogan??
Department of Computer Science Trinity College Dublin Dublin 2, Ireland dhogan@computing.dcu.ie Abstract The ability to detect similarity in conjunct heads is potentially a useful tool in helping to disambiguate coordination structures a difficult task for parsers.
We propose a distributional measure of similarity designed for such a task.
We then compare several different measures of word similarity by testing whether they can empirically detect similarity in the head nouns of noun phrase conjuncts in the Wall Street Journal (WSJ) treebank.
We demonstrate that several measures of word similarity can successfully detect conjunct head similarity and suggest that the measure proposed in this paper is the most appropriate for this task.
1 Introduction
Some noun pairs are more likely to be conjoined than others.
Take the follow two alternate bracketings: 1.
busloads of ((executives) and (their spouses)) and 2.
((busloads of executives) and (their spouses)).
The two head nouns coordinated in 1 are executives and spouses, and (incorrectly) in 2: busloads and spouses.
Clearly, the former pair of head nouns is more likely and, for the purpose of discrimination, a parsing model would benefit if it could learn that executives and spouses is a more likely combination than busloads and spouses.
If nouns co-occurring in coordination patterns are often semantically similar, and if a simi??Now at the National Centre for Language Technology, Dublin City University, Ireland.
larity measure could be defined so that, for example: sim(executives, spouses) > sim(busloads, spouses) then it is potentially useful for coordination disambiguation.
The idea that nouns co-occurring in conjunctions tend to be semantically related has been noted in (Riloff and Shepherd, 1997) and used effectively to automatically cluster semantically similar words (Roark and Charniak, 1998; Caraballo, 1999; Widdows and Dorow, 2002).
The tendency for conjoined nouns to be semantically similar has also been exploited for coordinate noun phrase disambiguation by Resnik (1999) who employed a measure of similarity based on WordNet to measure which were the head nouns being conjoined in certain types of coordinate noun phrase.
In this paper we look at different measures of word similarity in order to discover whether they can detect empirically a tendency for conjoined nouns to be more similar than nouns which co-occur but are not conjoined.
In Section 2 we introduce a measure of word similarity based on word vectors and in Section 3 we briefly describe some WordNet similarity measures which, in addition to our word vector measure, will be tested in the experiments of Section 4.
2 Similarity
based on Coordination Co-occurrences The potential usefulness of a similarity measure depends on the particular application.
An obvious place to start, when looking at similarity functions for measuring the type of semantic similarity common for coordinate nouns, is a similarity function based on distributional similarity with context de149 fined in terms of coordination patterns.
Our measure of similarity is based on noun co-occurrence information, extracted from conjunctions and lists.
We collected co-occurrence data on 82, 579 distinct word types from the BNC and the WSJ treebank.
We extracted all noun pairs from the BNC which occurred in a pattern of the form: noun cc noun1, as well as lists of any number of nouns separated by commas and ending in cc noun.
Each noun in the list is linked with every other noun in the list.
Thus for a list: n1, n2, and n3, there will be co-occurrences between words n1 and n2, between n1 and n3 and between n2 and n3.
To the BNC data we added all head noun pairs from the WSJ (sections 02 to 21) that occurred together in a coordinate noun phrase.2 From the co-occurrence data we constructed word vectors.
Every dimension of a word vector represents another word type and the values of the components of the vector, the term weights, are derived from the coordinate word co-occurrence counts.
We used dampened co-occurrence counts, of the form: 1 + log(count), as the term weights for the word vectors.
To measure the similarity of two words, w1 and w2, we calculate the cosine of the angle between the two word vectors, vectorw1 and vectorw2.
3 WordNet-Based Similarity Measures We also examine the following measures of semantic similarity which are WordNet-based.3 Wu and Palmer (1994) propose a measure of similarity of two concepts c1 and c2 based on the depth of concepts in the WordNet hierarchy.
Similarity is measured from the depth of the most specific node dominating both c1 and c2, (their lowest common subsumer), and normalised by the depths of c1 and c2.
In (Resnik, 1995) concepts in WordNet are augmented by corpus statistics and an informationtheoretic measure of semantic similarity is calculated.
Similarity of two concepts is measured 1It would be preferable to ensure that the pairs extracted are unambiguously conjoined heads.
We leave this to future work.
2We did not include coordinate head nouns from base noun phrases (NPB) (i.e.
noun phrases that do not dominate other noun phrases) because the underspecified annotation of NPBs in the WSJ means that the conjoined head nouns can not always be easily identified.
3All of the WordNet-based similarity measure experiments, as well as a random similarity measure, were carried out with the WordNet::Similarity package, http://search.cpan.org/dist/WordNet-Similarity.
by the information content of their lowest common subsumer in the is-a hierarchy of WordNet.
Both Jiang and Conrath (1997) and Lin (1998) propose extentions of Resnik?s measure.
Leacock and Chodorow (1998)?s measure takes into account the path length between two concepts, which is scaled by the depth of the hierarchy in which they reside.
In (Hirst and St-Onge, 1998) similarity is based on path length as well as the number of changes in the direction in the path.
In (Banerjee and Pedersen, 2003) semantic relatedness between two concepts is based on the number of shared words in their WordNet definitions (glosses).
The gloss of a particular concept is extended to include the glosses of other concepts to which it is related in the WordNet hierarchy.
Finally, Patwardhan and Pederson (2006) build on previous work on second-order co-occurrence vectors (Schutze, 1998) by constructing second-order co-occurrence vectors from WordNet glosses, where, as in (Banerjee and Pedersen, 2003), the gloss of a concept is extended so that it includes the gloss of concepts to which it is directly related in WordNet.
4 Experiments
We selected two sets of data from sections 00, 01, 22 and 24 of the WSJ treebank.
The first consists of all nouns pairs which make up the head words of two conjuncts in coordinate noun phrases (again not including coordinate NPBs).
We found 601 such coordinate noun pairs.
The second data set consists of 601 word pairs which were selected at random from all head-modifier pairs where both head and modifier words are nouns and are not coordinated.
We tested the 9 different measures of word similarity just described on each data set in order to see if a significant difference could be detected between the similarity scores for the coordinate words sample and non-coordinate words sample.
Initially both the coordinate and non-coordinate pair samples each contained 601 word pairs.
However, before running the experiments we removed all pairs where the words in the pair were identical.
This is because identical words occur more often in coordinate head words than in other lexical dependencies (there were 43 pairs with identical words in the coordination set, compared to 3 such pairs in the 150 SimTest ncoord xcoord SDcoord nnonCoord xnonCoord SDnonCoord 95% CI p-value coordDistrib 503 0.11 0.13 485 0.06 0.09 [0.04 0.07] 0.000 (Resnik, 1995) 444 3.19 2.33 396 2.43 2.10 [0.46 1.06] 0.000 (Lin, 1998) 444 0.27 0.26 396 0.19 0.22 [0.04 0.11] 0.000 (Jiang and Conrath, 1997) 444 0.13 0.65 395 0.07 0.08 [-0.01 0.11] 0.083 (Wu and Palmer, 1994) 444 0.63 0.19 396 0.55 0.19 [0.06 0.11] 0.000 (Leacock and Chodorow, 1998) 444 1.72 0.51 396 1.52 0.47 [0.13 0.27] 0.000 (Hirst and St-Onge, 1998) 459 1.599 2.03 447 1.09 1.87 [0.25 0.76] 0.000 (Banerjee and Pedersen, 2003) 451 114.12 317.18 436 82.20 168.21 [-1.08 64.92] 0.058 (Patwardhan and Pedersen, 2006) 459 0.67 0.18 447 0.66 0.2 [-0.02 0.03] 0.545 random 483 0.89 0.17 447 0.88 0.18 [-0.02 0.02] 0.859 Table 1: Summary statistics for 9 different word similarity measures (plus one random measure):ncoord and nnonCoord are the sample sizes for the coordinate and non-coordinate noun pairs samples, respectively; xcoord, SDcoord and xnonCoord, SDnonCoord are the sample means and standard deviations for the two sets.
The 95% CI column shows the 95% confidence interval for the difference between the two sample means.
The p-value is for a Welch two sample two-sided t-test.
coordDistrib is the measure introduced in Section 2.
non-coordination set).
If we had not removed them, a statistically significant difference between the similarity scores of the pairs in the two sets could be found simply by using a measure which, say, gave one score for identical words and another (lower) score for all non-identical word pairs.
Results for all similarity measure tests on the data sets described above are displayed in Table 1.
In one final experiment we used a random measure of similarity.
For each experiment we produced two samples, one consisting of the similarity scores given by the similarity measure for the coordinate noun pairs, and another set of similarity scores generated for the non-coordinate pairs.
The sample sizes, means, and standard deviations for each experiment are shown in the table.
Note that the variation in the sample size is due to coverage: the different measures did not produce a score for all word pairs.
Also displayed in Table 1 are the results of statistical significance tests based on the Welsh two sample t-test.
A 95% confidence interval for the difference of the sample means is shown along with the p-value.
5 Discussion
For all but three of the experiments (excluding the random measure), the difference between the mean similarity measures is statistically significant.
Interestingly, the three tests where no significant difference was measured between the scores on the coordination set and the non-coordination set (Jiang and Conrath, 1997; Banerjee and Pedersen, 2003; Patwardhan and Pedersen, 2006) were the three top scoring measures in (Patwardhan and Pedersen, 2006), where a subset of six of the above WordNetbased experiments were compared and the measures evaluated against human relatedness judgements and in a word sense disambiguation task.
In another comparative study (Budanitsky and Hirst, 2002) of five of the above WordNet-based measures, evaluated as part of a real-word spelling correction system, Jiang and Conrath (1997)?s similarity score performed best.
Although performing relatively well under other evaluation criteria, these three measures seem less suited to measuring the kind of similarity occurring in coordinate noun pairs.
One possible explanation for the unsuitability of the measures of (Patwardhan and Pedersen, 2006) for the coordinate similarity task could be based on how context is defined when building context vectors.
Context for an instance of the the word w is taken to be the words that surround w in the corpus within a given number of positions, where the corpus is taken as all the glosses in WordNet.
Words that form part of collocations such as disk drives or task force would then tend to have very similar contexts, and thus such word pairs, from non-coordinate modifier-head relations, could be given too high a similarity score.
Although the difference between the mean similarity scores seems rather slight in all experiments, it is worth noting that not all coordinate head words are semantically related.
To take a couple of examples from the coordinate word pair set: work/harmony extracted from hard work and harmony, and power/clause extracted from executive power and the appropriations clause.
We would not expect these word pairs to get a high similarity score.
On the other hand, it is also possible that 151 some of the examples of non-coordinate dependencies involve semantically similar words.
For example, nouns in lists are often semantically similar, and we did not exclude nouns extracted from lists from the non-coordinate test set.
Although not all coordinate noun pairs are semantically similar, it seems clear, on inspection of the two sets of data, that they are more likely to be semantically similar than modifier-head word pairs, and the tests carried out for most of the measures of semantic similarity detect a significant difference between the similarity scores assigned to coordinate pairs and those assigned to non-coordinate pairs.
It is not possible to judge, based on the significance tests alone, which might be the most useful measure for the purpose of disambiguation.
However, in terms of coverage, the distributional measure introduced in Section 2 clearly performs best4.
This measure of distributional similarity is perhaps more suited to the task of coordination disambiguation because it directly measures the type of similarity that occurs between coordinate nouns.
That is, the distributional similarity measure presented in Section 2 defines two words as similar if they occur in coordination patterns with a similar set of words and with similar distributions.
Whether the words are semantically similar becomes irrelevant.
A measure of semantic similarity, on the other hand, might find words similar which are quite unlikely to appear in coordination patterns.
For example, Cederberg and Widdows (2003) note that words appearing in coordination patterns tend to be on the same ontological level: ?fruit and vegetables??is quite likely to occur, whereas ?fruit and apples??is an unlikely cooccurrence.
A WordNet-based measure of semantic similarity, however, might give a high score to both of the noun pairs.
In the future we intend to use the similarity measure outlined in Section 2 in a lexicalised parser to help resolve coordinate noun phrase ambiguities.
Acknowledgements Thanks to the TCD Broad Curriculum Fellowship and to the SFI Research Grant 04/BR/CS370 for funding this research.
Thanks also to Padraig Cunningham, Saturnino Luz and Jennifer Foster for helpful discussions.
4Somewhat unsurprisingly given it is part trained on data from the same domain.
References Satanjeev Banerjee and Ted Pedersen.
2003 Extended Gloss Overlaps as a Measure of Semantic Relatedness.
In Proceeding of the 18th IJCAI.
Alexander Budanitsky and Graeme Hirst.
2002 Semantic Distance in WordNet: An experimental, application-oriented Evaluation of Five Measures In Proceedings of the 3rd CICLING.
Sharon Caraballo.
1999 Automatic construction of a hypernym-labeled noun hierarchy from text In Proceedings of the 37th ACL.
Scott Cederberg and Dominic Widdows.
2003. Using LSA and Noun Coordination Information to Improve the Precision and Recall of Automatic Hyponymy Extraction.
In Proceedings of the 7th CoNLL.
G. Hirst and D.
St-Onge 1998.
Lexical Chains as representations of context for the detection and correction of malapropisms.
WordNet: An electronic lexical database.
MIT Press.
J. Jiang and D.
Conrath. 1997.
Semantic similarity based on corpus statistics and lexical taxonomy.
In Proceedings of the ROCLING.
C. Leacock and M.
Chodorow. 1998.
Combining local context and WordNet similarity for word sense identification.
Word-Net: An electronic lexical database.
MIT Press.
D. Lin.
1998. An information-theoretic definition of similarity.
In Proceedings of the 15th ICML.
Siddharth Patwardhan and Ted Pedersen.
2006. Using WordNet-based Context Vectors to Estimate the Semantic Relatedness of Concepts.
In Proceedings of Making Sense of Sense Bringing Computational Linguistics and Psycholinguistics Together, EACL.
Philip Resnik.
1995. Using Information Content to Evaluate Semantic Similarity.
In Proceedings of IJCAI.
Philip Resnik.
1999. Semantic Similarity in a Taxonomy: An Information-Based Measure and its Application to Problems of Ambiguity in Natural Language.
In Journal of Artificial Intelligence Research, 11:95-130.
Ellen Riloff and Jessica Shepherd 1997.
A Corpus-based Approach for Building Semantic Lexicon.
In Proceedings of the 2nd EMNLP.
Brian Roark and Eugene Charniak 1998.
Noun-phrase Co-occurrence Statistics for Semi-automatic semantic lexicon construction.
In Proceedings of the COLING-ACL.
Hinrich Schutze.
1998. Automatic Word Sense Discrimination.
Computational Linguistics, 24(1):97-123.
Dominic Widdows and Beate Dorow.
2002. A Graph Model for Unsupervised Lexical Acquisition.
In Proceedings of the 19th COLING.
Zhibiao Wu and Martha Palmer.
1994. Verb Semantics and Lexical Selection.
In Proceedings of the ACL.
kProceedings of the ACL 2007 Demo and Poster Sessions, pages 153??56, Prague, June 2007.
c2007 Association for Computational Linguistics Automatic Discovery of Named Entity Variants ??Grammar-driven Approaches to Non-alphabetical Transliterations Chu-Ren Huang Institute of Linguistics Academia Sinica, Taiwan churenhuang@gmail.com Petr ?Simon Institute of Linguistics Academia Sinica, Taiwan sim@klubko.net Shu-Kai Hsieh DoFLAL NIU, Taiwan shukai@gmail.com Abstract Identification of transliterated names is a particularly difficult task of Named Entity Recognition (NER), especially in the Chinese context.
Of all possible variations of transliterated named entities, the difference between PRC and Taiwan is the most prevalent and most challenging.
In this paper, we introduce a novel approach to the automatic extraction of diverging transliterations of foreign named entities by bootstrapping cooccurrence statistics from tagged and segmented Chinese corpus.
Preliminary experiment yields promising results and shows its potential in NLP applications.
1 Introduction
Named Entity Recognition (NER) is one of the most difficult problems in NLP and Document Understanding.
In the field of Chinese NER, several approaches have been proposed to recognize personal names, date/time expressions, monetary and percentage expressions.
However, the discovery of transliteration variations has not been well-studied in Chinese NER.
This is perhaps due to the fact that the transliteration forms in a non-alphabetic language such as Chinese are opaque and not easy to compare.
On the hand, there is often more than one way to transliterate a foreign name.
On the other hand, dialectal difference as well as different transliteration strategies often lead to the same named entity to be transliterated differently in different Chinese speaking communities.
Corpus Example (Clinton) Frequency XIN ????24382 CNA ????150 XIN ??0 CNA ??120842 Table 1: Distribution of two transliteration variants for ?Clinton??in two sub-corpora Of all possible variations, the cross-strait difference between PRC and Taiwan is the most prevalent and most challenging.1The main reason may lie in the lack of suitable corpus.
Even given some subcorpora of PRC and Taiwan variants of Chinese, a simple contrastive approach is still not possible.
It is because: (1) some variants might overlap and (2) there are more variants used in each corpus due to citations or borrowing crossstrait.
Table 1 illustrates this phenomenon, where CNA stands for Central News Agency in Taiwan, XIN stands for Xinhua News Agency in PRC, respectively.
With the availability of Chinese Gigaword Corpus (CGC) and Word Sketch Engine (WSE) Tools (Kilgarriff, 2004).
We propose a novel approach towards discovery of transliteration variants by utilizing a full range of grammatical information augmented with phonological analysis.
Existing literatures on processing of transliteration concentrate on the identification of either the transliterated term or the original term, given knowledge of the other (e.g.
(Virga and Khudanpur, 1For instance, we found at least 14 transliteration variants for Lewinsky,such as ?fl ?fl fl?fl?
???? fl?fl fl flfl?fl  fl and so on.
153 2003)).
These studies are typically either rule-based or statistics-based, and specific to a language pair with a fixed direction (e.g.
(Wan and Verspoor, 1998; Jiang et al., 2007)).
To the best of our knowledge, ours is the first attempt to discover transliterated NE?s without assuming prior knowledge of the entities.
In particular, we propose that transliteration variants can be discovered by extracting and comparing terms from similar linguistic context based on CGC and WSE tools.
This proposal has great potential of increasing robustness of future NER work by enabling discovery of new and unknown transliterated NE?s.
Our study shows that resolution of transliterated NE variations can be fully automated.
This will have strong and positive implications for cross-lingual and multi-lingual informational retrieval.
2 Bootstrapping
transliteration pairs The current study is based on Chinese Gigaword Corpus (CGC) (Graff el al., 2005), a large corpus contains with 1.1 billion Chinese characters containing data from Central News Agency of Taiwan (ca.
700 million characters), Xinhua News Agency of PRC (ca.
400 million characters).
These two subcorpora represent news dispatches from roughly the same period of time, i.e. 1990-2002.
Hence the two sub-corpora can be expected to have reasonably parallel contents for comparative studies.2 The premises of our proposal are that transliterated NE?s are likely to collocate with other transliterated NE?s, and that collocates of a pair of transliteration variants may form contrasting pairs and are potential variants.
In particular, since the transliteration variations that we are interested in are those between PRC and Taiwan Mandarin, we will start with known contrasting pairs of these two language variants and mine potential variant pairs from their collocates.
These potential variant pairs are then checked for their phonological similarity to determine whether they are true variants or not.
In order to effectively select collocates from specific grammatical constructions, the Chinese Word Sketch3 is adopted.
In particular, we use the Word Sketch dif2To facilitate processing, the complete CGC was segmented and POS tagged using the Academia Sinica segmentation and tagging system (Ma and Huang, 2006).
3http://wordsketch.ling.sinica.edu.tw ference (WSDiff) function to pick the grammatical contexts as well as contrasting pairs.
It is important to bear in mind that Chinese texts are composed of Chinese characters, hence it is impossible to compare a transliterated NE with the alphabetical form in its original language.
The following characteristics of a transliterated NE?s in CGC are exploited to allow discovery of transliteration variations without referring to original NE.
??frequent co-occurrence of named entities within certain syntagmatic relations ??named entities frequently co-occur in relations such as AND or OR and this fact can be used to collect and score mutual predictability.
??foreign named entities are typically transliterated phonetically ??transliterations of the same name entity using different characters can be matched by using simple heuristics to map their phonological value.
??presence and co-occurrence of named entities in a text is dependent on a text type ??journalistic style cumulates many foreign named entities in close relations.
??many entities will occur in different domains ??famous person can be mentioned together with someone from politician, musician, artist or athlete.
Thus allows us to make leaps from one domain to another.
There are, however, several problems with the phonological representation of foreign named entities in Chinese.
Due to the nature of Chinese script, NE transliterations can be realized very differently.
The following is a summary of several problems that have to be taken into account: ??word ending: ?? vs.??
???Arafat??or ? vs.?
???Mubarak?? The final consonant is not always transliterated.
XIN transliterations tend to try to represent all phonemes and often add vowels to a final consonant to form a new syllable, whereas CNA transliteration tends to be shorter and may simply leave out a final consonant.
??gender dependent choice of characters: ? ?Leslie??vs.?fl???Chris??or ?? vs.
??fl 154 ??
Some occidental names are gender neutral.
However, the choice of characters in a personal name in Chinese is often gender sensitive.
So these names are likely to be transliterated differently depending on the gender of its referent.
??divergent representations caused by scope of transliteration, e.g. both given and surname vs.
only surname:  ?/ ? ??Venus Williams??
??difference in phonological interpretation: ?
?vs. ?Rafter??or??flvs.
?fl?Connors?? ??native vs.
non-native pronunciation: ?fl?? vs.
fl???Escudero??or ? vs.
?? ?Federer?? 2.1 Data collection All data were collected from Chinese Gigaword Corpus using Chinese Sketch Engine with WSDiff function, which provides side-by-side syntagmatic comparison of Word Sketches for two different words.
WSDiff query for wi and wj returns patterns that are common for both words and also patterns that are particular for each of them.
Three data sets are thus provided.
We neglect the common patterns set and concentrate only on the wordlists specific for each word.
2.2 Pairs
extraction Transliteration pairs are extracted from the two sets, A and B, collected with WSDiff using default set of seed pairs : for each seed pair in seeds retrieve WSDiff for and/or relation, thus have pairs of word lists, < Ai,Bi > for each word wii ??Ai find best matching counterpart(s) wij ??Bi.
Comparison is done using simple phonological rules, viz.
2.3 use newly extracted pairs as new seeds (original seeds are stored as good pairs and not queried any more) loop until there are no new pairs Notice that even though substantial proportion of borrowing among different communities, there is no mixing in the local context of collocation, which means, local collocation could be the most reliable way to detect language variants with known variants.
2.3 Phonological
comparison All word forms are converted from Chinese script into a phonological representation4 during the pairs extraction phase and then these representations are compared and similarity scores are given to all pair candidates.
A lot of Chinese characters have multiple pronunciations and thus multiple representations are derived.
In case of multiple pronunciations for certain syllable, this syllable is commpared to its counterpart from the other set.
E.g. (??has three pronunciations: y`e, xie, sh`e.
When comparing syllables such as [pei,fei] and [fei], will be represented as [fei].
In case of pairs such as ??[ye er qin] and ? [ye er qin], which have syllables with multiple pronunciations and this multiple representations.
However, since these two potential variants share the first two characters (out of three), they are considered as variants without superfluous phonological checking.
Phonological representations of whole words are then compared by Levenstein algorithm, which is widely used to measure the similarity between two strings.
First, each syllable is split into initial and final components: gao:g+ao.
In case of syllables without initials like er, an ??is inserted before the syllable, thus er:??er.
Before we ran the Levenstein measure, we also apply phonological corrections on each pair of candidate representations.
Rules used for these corrections are derived from phonological features of Mandarin Chinese and extended with few rules from observation of the data: (1) For Initials, (a): voiced/voiceless stop contrasts are considered as similar for initials: g:k, e.g.
[gao] ( ?? vs.
[ke] ( ??,d:t, b:p, (b): r:l ??[rui] ( ??
??[lie] ( ?) is added to distinctive feature set based on observation.
(2). For Finals, (a): pair ei:ui is evaluated as equivalent.5 (b): oppositions of nasalised final is evaluated as dissimilar.
4http://unicode.org/charts/unihan.html 5Pinyin representation of phonology of Mandarin Chinese does not follow the phonological reality exactly: [ui] = [uei] etc.
155 2.4 Extraction algorithm Our algorithm will potentially exhaust the whole corpus, i.e. find most of the named entities that occur with at least few other names entities, but only if seeds are chosen wisely and cover different domains6.
However, some domains might not overlap at all, that is, members of those domains never appear in the corpus in relation and/or.
And concurrence of members within some domains might be sparser than in other, e.g. politicians tend to be mentioned together more often than novelists.
Nature of the corpus also plays important role.
It is likely to retrieve more and/or related names from journalistic style.
This is one of the reasons why we chose Chinese Gigaword Corpus for this task.
3 Experiment
and evaluation We have tested our method on the Chinese Gigaword Second Edition corpus with 11 manually selected seeds Apart from the selection of the starter seeds, the whole process is fully automatic.
For this task we have collected data from syntagmatic relation and/or, which contains words co-occurring frequently with our seed words.
When we make a query for peoples names, it is expected that most of the retrieved items will also be names, perhaps also names of locations, organizations etc.
The whole experiment took 505 iterations in which 494 pairs were extracted.
Our complete experiment with 11 pre-selected transliteration pairs as seed took 505 iterations to end.
The iterations identified 494 effective transliteration variant pairs (i.e.
those which were not among the seeds or pairs identified by earlier iteration).
All the 494 candidate pairs were manually evaluated 445 of them are found to be actual contrast pairs, a precision of 90.01%.
In addition, the number of new transliteration pairs yielded is 4,045%, a very productive yield for NE discovery.
Preliminary results show that this approach is competitive against other approaches reported in previous studies.
Performances of our algorithms is calculated in terms of precision rate with 90.01%. 6The term domain refers to politics,music,sport, film etc.
4 Conclusion
and Future work In this paper, we have shown that it is possible to identify NE?s without having prior knowledge of them.
We also showed that, applying WSE to restrict grammatical context and saliency of collocation, we are able to effectively extract transliteration variants in a language where transliteration is not explicitly represented.
We also show that a small set of seeds is all it needs for the proposed method to identify hundreds of transliteration variants.
This proposed method has important applications in information retrieval and data mining in Chinese data.
In the future, we will be experimenting with a different set of seeds in a different domain to test the robustness of this approach, as well as to discover transliteration variants in our fields.
We will also be focusing on more refined phonological analysis.
In addition, we would like to explore the possibility of extending this proposal to other language pairs.
References Jiang, L.
and M.Zhou and L.f.
Chien. 2007.
Named Entity Discovery based on Transliteration and WWW [In Chinese].
Journal of the Chinese Information Processing Society.
2007 no.1. pp.23-29.
Graff, David et al.2005. Chinese Gigaword Second Edition.
Linguistic Data Consortium, Philadelphia.
Ma, Wei-Yun and Huang, Chu-Ren.
2006. Uniform and Effective Tagging of a Heterogeneous Giga-word Corpus.
Presented at the 5th International Conference on Language Resources and Evaluation (LREC2006), 24-28 May.
Genoa, Italy.
Kilgarriff, Adam et al.2004. The Sketch Engine.
Proceedings of EURALEX 2004.
Lorient, France.
Paola Virga and Sanjeev Khudanpur.
2003. Transliteration of proper names in cross-lingual information retrieval.
In Proc.
of the ACL Workshop on Multi-lingual Named Entity Recognition, pp.57-64.
Wan, Stephen and Cornelia Verspoor.
1998. Automatic English-Chinese Name Transliteration for Development of Multiple Resources.
In Proc.
of COLING/ACL, pp.1352-1356.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 157??60, Prague, June 2007.
c2007 Association for Computational Linguistics Detecting Semantic Relations between Named Entities in Text Using Contextual Features Toru Hirano, Yoshihiro Matsuo, Genichiro Kikui NTT Cyber Space Laboratories, NTT Corporation 1-1 Hikarinooka, Yokosuka-Shi, Kanagawa, 239-0847, Japan {hirano.tohru, matsuo.yoshihiro, kikui.genichiro}@lab.ntt.co.jp Abstract This paper proposes a supervised learning method for detecting a semantic relation between a given pair of named entities, which may be located in different sentences.
The method employs newly introduced contextual features based on centering theory as well as conventional syntactic and word-based features.
These features are organized as a tree structure and are fed into a boosting-based classification algorithm.
Experimental results show the proposed method outperformed prior methods, and increased precision and recall by 4.4% and 6.7%. 1 Introduction Statistical and machine learning NLP techniques are now so advanced that named entity (NE) taggers are in practical use.
Researchers are now focusing on extracting semantic relations between NEs, such as ?George Bush (person)??is ?president (relation)??of ?the United States (location)?? because they provide important information used in information retrieval, question answering, and summarization.
We represent a semantic relation between two NEs with a tuple [NE1, NE2, Relation Label].
Our final goal is to extract tuples from a text.
For example, the tuple [George Bush (person), the U.S.
(location), president (Relation Label)] would be extracted from the sentence ?George Bush is the president of the U.S.??
There are two tasks in extracting tuples from text.
One is detecting whether or not a given pair of NEs are semantically related (relation detection), and the other is determining the relation label (relation characterization).
In this paper, we address the task of relation detection.
So far, various supervised learning approaches have been explored in this field (Culotta and Sorensen, 2004; Zelenko et al., 2003).
They use two kinds of features: syntactic ones and wordbased ones, for example, the path of the given pair of NEs in the parse tree and the word n-gram between NEs (Kambhatla, 2004).
These methods have two problems which we consider in this paper.
One is that they target only intrasentential relation detection in which NE pairs are located in the same sentence, in spite of the fact that about 35% of NE pairs with semantic relations are inter-sentential (See Section 3.1).
The other is that the methods can not detect semantic relations correctly when NE pairs located in a parallel sentence arise from a predication ellipsis.
In the following Japanese example1, the syntactic feature, which is the path of two NEs in the dependency structure, of the pair with a semantic relation (?Ken11??and ?Tokyo12?? is the same as the feature of the pair with no semantic relation (?Ken11??and ?New York14??.
(S-1) Ken11-wa Tokyo12-de, Tom13-wa New York14-de umareta15.
(Ken11 was born15 in Tokyo12, Tom13 in New York14.) To solve the above problems, we propose a supervised learning method using contextual features.
The rest of this paper is organized as follows.
Section 2 describes the proposed method.
We report the results of our experiments in Section 3 and conclude the paper in Section 4.
2 Relation
Detection The proposed method employs contextual features based on centering theory (Grosz et al., 1983) as well as conventional syntactic and word-based features.
These features are organized as a tree structure and are fed into a boosting-based classification algorithm.
The method consists of three parts: preprocessing (POS tagging, NE tagging, and parsing), 1The numbers show correspondences of words between Japanese and English.
157 feature extraction (contextual, syntactic, and wordbased features), and classification.
In this section, we describe the underlying idea of contextual features and how contextual features are used for detecting semantic relations.
2.1 Contextual
Features When a pair of NEs with a semantic relation appears in different sentences, the antecedent NE must be contextually easily referred to in the sentence with the following NE.
In the following Japanese example, the pair ?Ken22??and ?amerika32 (the U.S.)?? have a semantic relation ?wataru33 (go)?? because ?Ken22??is contextually referred to in the sentence with ?amerika32??(In fact, the zero pronoun ?i refers to ?Ken22??.
Meanwhile, the pair ?Naomi25?? and ?amerika32??has no semantic relation, because the sentence with ?amerika32??does not refer to ?Naomi25??
(S-2) asu21, Ken22-wa Osaka23-o otozure24 Naomi25-to au26.
(Ken22 is going to visit24 Osaka23 to see26 Naomi25, tomorrow21.) (S-3) sonogo31, (?i-ga) amerika32-ni watari33 Tom34-to ryoko35 suru.
(Then31, (hei) will go33 to the U.S.32 to travel35 with Tom34.) Furthermore, when a pair of NEs with a semantic relation appears in a parallel sentence arise from predication ellipsis, the antecedent NE is contextually easily referred to in the phrase with the following NE.
In the example of ??S-1)?? the pair ?Ken11?? and ?Tokyo12??have a semantic relation ?umareta15 (was born)??
Meanwhile, the pair ?Ken11??and ?New York14??has no semantic relation.
Therefore, using whether the antecedent NE is referred to in the context with the following NE as features of a given pair of NEs would improve relation detection performance.
In this paper, we use centering theory (Kameyama, 1986) to determine how easily a noun phrase can be referred to in the following context.
2.2 Centering
Theory Centering theory is an empirical sorting rule used to identify the antecedents of (zero) pronouns.
When there is a (zero) pronoun in the text, noun phrases that are in the previous context of the pronoun are sorted in order of likelihood of being the antecedent.
The sorting algorithm has two steps.
First, from the beginning of the text until the pronoun appears, noun Osaka23o asu 21, Naomi25others niga Ken22wa Priority Figure 1: Information Stacked According to Centering Theory phrases are stacked depending on case markers such as particles.
In the above example, noun phrases, ?asu21??
?Ken22?? ?Osaka23??and ?Naomi25?? which are in the previous context of the zero pronoun ?i, are stacked and then the information shown in Figure 1 is acquired.
Second, the stacked information is sorted by the following rules.
1. The priority of case markers is as follows: ?wa > ga > ni > o > others?? 2.
The priority of stack structure is as follows: last-in first-out, in the same case marker For example, Figure 1 is sorted by the above rules and then the order, 1: ?Ken22?? 2: ?Osaka23?? 3: ?Naomi25?? 4: ?asu21?? is assigned.
In this way, using centering theory would show that the antecedent of the zero pronoun ?i is ?Ken22?? 2.3 Applying Centering Theory When detecting a semantic relation between a given pair of NEs, we use centering theory to determine how easily the antecedent NE can be referred to in the context with the following NE.
Note that we do not explicitly execute anaphora resolutions here.
Applied centering theory to relation detection is as follows.
First, from the beginning of the text until the following NE appears, noun phrases are stacked depending on case markers, and the stacked information is sorted by the above rules (Section 2.2).
Then, if the top noun phrase in the sorted order is identical to the antecedent NE, the antecedent NE is ?positive??when being referred to in the context with the following NE.
When the pair of NEs, ?Ken22??and ?amerika32?? is given in the above example, the noun phrases, ?asu21??
?Ken22?? ?Osaka23??and ?Naomi25?? which are in the previous context of the following NE ?amerika32?? are stacked (Figure 1).
Then they are sorted by the above sorting rules and the order, 1: ?Ken22?? 2: ?Osaka23?? 3: ?Naomi25?? 4: ?asu21?? is acquired.
Here, because the top noun phrase in the sorted order is identical to the antecedent NE, the antecedent NE ?Ken22??is ?positive??when be158 amerika32 wa: Ken22 o: Osaka23 others: Naomi25 others: asu21 Figure 2: Centering Structure ing referred to in the context with the following NE ?amerika32??
Whether or not the antecedent NE is referred to in the context with the following NE is used as a feature.
We call this feature Centering Top (CT).
2.4 Using
Stack Structure The sorting algorithm using centering theory tends to rank highly thoes words that easily become subjects.
However, for relation detection, it is necessary to consider both NEs that easily become subjects, such as person and organization, and NEs that do not easily become subjects, such as location and time.
We use the stack described in Section 2.3 as a structural feature for relation detection.
We call this feature Centering Structure (CS).
For example, the stacked information shown in Figure 1 is assumed to be structure information, as shown in Figure 2.
The method of converting from a stack (Figure 1) into a structure (Figure 2) is described as follows.
First, the following NE, ?amerika32?? becomes the root node because Figure 1 is stacked information until the following NE appears.
Then, the stacked information is converted to Figure 2 depending on the case markers.
We use the path of the given pair of NEs in the structure as a feature.
For example, ?amerika32 ??wa:Ken22?? is used as the feature of the given pair ?Ken22??and ?amerika32?? 2.5 Classification Algorithm There are several structure-based learning algorithms proposed so far (Collins and Duffy, 2001; Suzuki et al., 2003; Kudo and Matsumoto, 2004).
The experiments tested Kudo and Matsumoto?s boosting-based algorithm using sub trees as features, which is implemented as the BACT system.
In relation detection, given a set of training examples each of which represents contextual, syntactic, and word-based features of a pair of NEs as a tree labeled as either having semantic relations or not, the BACT system learns that a set of rules are effective in classifying.
Then, given a test instance, which represents contextual, syntactic, and word2?A?B??means A has a dependency relation to B.
Type % of pairs with semantic relations (A) Intra-sentential 31.4% (3333 / 10626) (B) Inter-sentential 0.8% (1777 / 225516) (A)+(B) Total 2.2% (5110 / 236142) Table 1: Percent of pairs with semantic relations in annotated text based features of a pair of NEs as a tree, the BACT system classifies using a set of learned rules.
3 Experiments
We experimented with texts from Japanese newspapers and weblogs to test the proposed method.
The following four models were compared: 1.
WD : Pairs of NEs within n words are detected as pairs with semantic relation.
2. STR : Supervised learning method using syntactic3 and word-based features, the path of the pairs of NEs in the parse tree and the word ngram between pairs of NEs (Kambhatla, 2004) 3.
STR-CT : STR with the centering top feature explained in Section 2.3. 4.
STR-CS : STR with the centering structure feature explained in Section 2.4. 3.1 Setting We used 1451 texts from Japanese newspapers and weblogs, whose semantic relations between person and location had been annotated by humans for the experiments4.
There were 5110 pairs with semantic relations out of 236,142 pairs in the annotated text.
We conducted ten-fold cross-validation over 236,142 pairs of NEs so that sets of pairs from a single text were not divided into the training and test sets.
We also divided pairs of NEs into two types: (A) intra-sentential and (B) inter-sentential.
The reason for dividing them is so that syntactic structure features would be effective in type (A) and contextual features would be effective in type (B).
Another reason is that the percentage of pairs with semantic relations out of the total pairs in the annotated text differ significantly between types, as shown in Table 1.
In the experiments, all features were automatically acquired using a Japanese morphological and dependency structure analyzer.
3There is no syntactic feature in inter-sentential.
4We are planning to evaluate the other pairs of NEs.
159 (A)+(B) Total (A) Intra-sentential (B) Inter-sentential Precision Recall Precision Recall Precsion Recall WD10 43.0(2501/5819) 48.9(2501/5110) 48.1(2441/5075) 73.2(2441/3333) 8.0(60/744) 3.4(60/1777) STR 69.3(2562/3696) 50.1(2562/5110) 75.6(2374/3141) 71.2(2374/3333) 33.9(188/555) 10.6(188/1777) STR-CT 71.4(2764/3870) 54.1(2764/5110) 78.4(2519/3212) 75.6(2519/3333) 37.2(245/658) 13.8(245/1777) STR-CS 73.7(2902/3935) 56.8(2902/5110) 80.1(2554/3187) 76.6(2554/3333) 46.5(348/748) 27.6(348/1777) WD10: NE pairs that appear within 10 words are detected.
Table 2: Results for Relation Detection 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Recall Prec ision WDSTR STR-CTSTR-CS STR-CS STR WD STR-CT Figure 3: Recall-precision Curves: (A)+(B) total 3.2 Results To improve relation detection performance, we investigated the effect of the proposed method using contextual features.
Table 2 shows results for Type (A), Type (B), and (A)+(B).
We also plotted recallprecision curves5, altering threshold parameters, as shown in Figure 3.
The comparison between STR and STR-CT and between STR and STR-CS in Figure 3 indicates that the proposed method effectively contributed to relation detection.
In addition, the results for Type (A): intra-sentential, and (B): inter-sentential, in Table 2 indicate that the proposed method contributed to both Type (A), improving precision by about 4.5% and recall by about 5.4% and Type (B), improving precision by about 12.6% and recall by about 17.0%. 3.3 Error Analysis Over 70% of the errors are covered by two major problems left in relation detection.
Parallel sentence: The proposed method solves problems, which result from when a parallel sentence arises from predication ellipsis.
However, there are several types of parallel sentence that differ from the one we explained.
(For example, Ken and Tom was born in Osaka and New York, respectively.) 5Precision = # of correctly detected pairs / # of detected pairs Recall = # of correctly detected pairs / # of pairs with semantic relations Definite anaphora: Definite noun phrase, such as ?Shusho (the Prime Minister)??and ?Shacho (the President)?? can be anaphors.
We should consider them in centering theory, but it is difficult to find them in Japanese. 4 Conclusion In this paper, we propose a supervised learning method using words, syntactic structures, and contextual features based on centering theory, to improve both inter-sentential and inter-sentential relation detection.
The experiments demonstrated that the proposed method increased precision by 4.4%, up to 73.7%, and increased recall by 6.7%, up to 56.8%, and thus contributed to relation detection.
In future work, we plan to solve the problems relating to parallel sentence and definite anaphora, and address the task of relation characterization.
References M.
Collins and N.
Duffy. 2001.
Convolution Kernels for Natural Language.
Proceedings of the Neural Information Processing Systems, pages 625??32.
A. Culotta and J.
Sorensen. 2004.
Dependency Tree Kernels for Relation Extraction.
Annual Meeting of Association of Computational Linguistics, pages 423??29.
B. J.
Grosz, A.
K. Joshi, and S.
Weistein. 1983.
Providing a unified account of definite nounphrases in discourse.
Annual Meeting of Association of Computational Linguistics, pages 44??0.
N. Kambhatla.
2004. Combining Lexical, Syntactic, and Semantic Features with Maximum Entropy Models for Information Extraction.
Annual Meeting of Association of Computational Linguistics, pages 178??81.
M. Kameyama.
1986. A property-sharing constraint in centering.
Annual Meeting of Association of Computational Linguistics, pages 200??06.
T. Kudo and Y.
Matsumoto. 2004.
A boosting algorithm for classification of semi-structured text.
In Proceedings of the 2004 EMNLP, pages 301??08.
J. Suzuki, T.
Hirao, Y.
Sasaki, and E.
Maeda. 2003.
Hierarchical directed acyclic graph kernel : Methods for structured natural language data.
Annual Meeting of Association of Computational Linguistics, pages 32??9.
D. Zelenko, C.
Aone, and A.
Richardella. 2003.
Kernel Methods for Relation Extraction.
Journal of Machine Learning Research, pages 3:1083??106 .
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 161??64, Prague, June 2007.
c2007 Association for Computational Linguistics Mapping Concrete Entities from PAROLE-SIMPLE-CLIPS to ItalWordNet: Methodology and Results Adriana Roventini, Nilda Ruimy, Rita Marinelli, Marisa Ulivieri, Michele Mammini Istituto di Linguistica Computazionale ??CNR Via Moruzzi,1 ??56124 ??Pisa, Italy {adriana.roventini,nilda.ruimy,rita.marinelli, marisa.ulivieri,michele.mammini}@ilc.cnr.it Abstract This paper describes a work in progress aiming at linking the two largest Italian lexical-semantic databases ItalWordNet and PAROLE-SIMPLE-CLIPS.
The adopted linking methodology, the software tool devised and implemented for this purpose and the results of the first mapping phase regarding 1 st OrderEntities are illustrated here.
1 Introduction
The mapping and the integration of lexical resources is today a main concern in the world of computational linguistics.
In fact, during the past years, many linguistic resources were built whose bulk of linguistic information is often neither easily accessible nor entirely available, whereas their visibility and interoperability would be crucial for HLT applications.
The resources here considered constitute the largest and extensively encoded Italian lexical semantic databases.
Both were built at the CNR Institute of Computational Linguistics, in Pisa.
The ItalWordNet lexical database (henceforth IWN) was first developed in the framework of EuroWordNet project and then enlarged and improved in the national project SI-TAL 1. The theoretical model underlying this lexicon is based on the EuroWordNet lexical model (Vossen, 1998) which is, in its turn, inspired to the Princeton WordNet (Fellbaum, 1998).
PAROLE-SIMPLE-CLIPS (PSC) is a four-level lexicon developed over three different projects: the 1 Integrated System for the Automatic Language Treatment.
LE-PAROLE project for the morphological and syntactic layers, the LE-SIMPLE project for the semantic model and lexicon and the Italian project CLIPS 2 for the phonological level and the extension of the lexical coverage.
The theoretical model underlying this lexicon is based on the EAGLES recommendations, on the results of the EWN and ACQUILEX projects and on a revised version of Pustejovsky?s Generative Lexicon theory (Pustejovsky 1995).
In spite of the different underlying principles and peculiarities characterizing the two lexical models, IWN and PSC lexicons also present many compatible aspects and the reciprocal enhancements that the linking of the resources would entail were illustrated in Roventini et al., (2002); Ruimy & Roventini (2005).
This has prompted us to envisage the semi-automatic link of the two lexical databases, eventually merging the whole information into a common representation framework.
The first step has been the mapping of the 1 st OrderEntities which is described in the following.
This paper is organized as follows: in section 2 the respective ontologies and their mapping are briefly illustrated, in section 3 the methodology followed to link these resources is described; in section 4 the software tool and its workings are explained; section 5 reports on the results of the complete mapping of the 1 st OrderEntities.
Future work is outlined in the conclusion.
2 Mapping
Ontology-based Lexical Resources In both lexicons, the backbone for lexical representation is provided by an ontology of semantic types.
2 Corpora
e Lessici dell'Italiano Parlato e Scritto.
161 The IWN Top Ontology (TO) (Roventini et al., 2003), which slightly differs from the EWN TO 3, consists in a hierarchical structure of 65 languageindependent Top Concepts (henceforth TCs) clustered in three categories distinguishing 1 st OrderEntities, 2 nd OrderEntities and 3 rd Order Entities.
Their subclasses, hierarchically ordered by means of a subsumption relation, are also structured in terms of (disjunctive and nondisjunctive) opposition relations.
The IWN database is organized around the notion of synset, i.e. a set of synonyms.
Each synset is ontologically classified on the basis of its hyperonym and connected to other synsets by means of a rich set of lexical-semantic relations.
Synsets are in most cases cross-classified in terms of multiple, non disjoint TCs, e.g.: informatica (computer science): [Agentive, Purpose, Social, Unboundedevent].
The semantics of a word sense or synset variant is fully defined by its membership in a synset.
The SIMPLE Ontology (SO) 4, which consists of 157 language-independent semantic types, is a multidimensional type system based on hierarchical and non-hierarchical conceptual relations.
In the type system, multidimensionality is captured by qualia roles that define the distinctive properties of semantic types and differentiate their internal semantic constituency.
The SO distinguishes therefore between simple (onedimensional) and unified (multi-dimensional) semantic types, the latter implementing the principle of orthogonal inheritance.
In the PSC lexicon, the basic unit is the word sense, represented by a ?semantic unit??(henceforth, SemU).
Each SemU is assigned one single semantic type (e.g.: informatica: [Domain]), which endows it with a structured set of semantic information.
A primary phase in the process of mapping two ontology-based lexical resources clearly consisted in establishing correspondences between the conceptual classes of both ontologies, with a view to further matching their respective instances.
The mapping will only be briefly outlined here for the 1 st OrderEntity.
More information can be found in (Ruimy & Roventini 2005; Ruimy, 2006).
The IWN 1 st OrderEntity class structures concrete entities (referred to by concrete nouns).
Its main cross-classifying subclasses: Form, Origin, 3 A few changes were in fact necessary to allow the encoding of new syntactic categories.
4 http://www.ilc.cnr.it/clips/Ontology.htm Composition and Function correspond to the four Qualia roles the SIMPLE model avails of to express orthogonal aspects of word meaning.
Their respective subdivisions consist of (mainly) disjoint classes, e.g.
Natural vs.
Artifact. To each class corresponds, in most of the cases, a SIMPLE semantic type or a type hierarchy subsumed by the Concrete_entity top type.
Some other IWN TCs, such as Comestible, Liquid, are instead mappable to SIMPLE distinctive features: e.g.
Plus_Edible, Plus_Liquid, etc.
3 Linking
Methodology Mapping is performed on a semantic type-driven basis.
A semantic type of the SIMPLE ontology is taken as starting point.
Considering the type?s SemUs along with their PoS and ?isa??relation, the IWN resource is explored in search of linking candidates with same PoS and whose ontological classification matches the correspondences established between the classes of both ontologies.
A characteristic of this linking is that it involves lexical elements having a different status, i.e. semantic units and synsets.
During the linking process, two different types of data are returned from each mapping run: 1) A set of matched pairs of word senses, i.e.
SemUs and synset variants with identical string, PoS and whose respective ontological classification perfectly matches.
After human validation, these matched word senses are linked.
2) A set of unmatched word senses, in spite of their identical string and PoS value.
Matching failure is due to a mismatch of the ontological classification of word senses existing in both resources.
Such mismatch may be originated by: a) an incomplete ontological information.
As already explained, IWN synsets are cross-classified in terms of a combination of TCs; however, cases of synsets lacking some meaning component are not rare.
The problem of incomplete ontological classification may often be overcome by relaxing the mapping constraints; yet, this solution can only be applied if the existing ontological label is informative enough.
Far more problematic to deal with are those cases of incomplete or little informative ontological labels, e.g. 1 st OrderEntities as different as medicinale, anello, vetrata (medicine, ring, picture window) and only classified as ?Function?? 162 b) a different ontological information.
Besides mere encoding errors, ontological classification discrepancy may be imputable to: i) a different but equally defensible meaning interpretation (e.g.: ala (aircraft wing) : [Part] vs.
[Artifact Instrument Object]).
Word senses falling into this category are clustered into numerically significant sets according to their semantic typing and then studied with a view to establishing further equivalences between ontological classes or to identify, in their classification schemes, descriptive elements lending themselves to be mapped.
ii) a different level of specificity in the ontological classification, due either to the lexicographer?s subjectivity or to an objective difference of granularity of the ontologies.
The problems in ii) may be bypassed by climbing up the ontological hierarchy, identifying the parent nodes and allowing them to be taken into account in the mapping process.
Hyperonyms of matching candidates are taken into account during the linking process and play a particularly determinant role in the resolution of cases whereby matching fails due to a conflict of ontological classification.
It is the case for sets of word senses displaying a different ontological classification but sharing the same hyperonym, e.g. collana, braccialetto (necklace, bracelet) typed as [Clothing] in PSC and as [Artifact Function] in IWN but sharing the hyperonym gioiello (jewel).
Hyperonyms are also crucial for polysemous senses belonging to different semantic types in PSC but sharing the same ontological classification in IWN, e.g.: SemU1595viola (violet) [Plant] and SemU1596viola (violet) [Flower] vs.
IWN: viola1 (has_hyperonym pianta1 (plant)) and viola3 (has_hyperonym fiore1 (flower)), both typed as [Group Plant].
4 The
Linking Tool The LINKPSC_IWN software tool implemented to map the lexical units of both lexicons works in a semiautomatic way using the ontological classifications, the ?isa??relations and some semantic features of the two resources.
Since the 157 semantic types of the SO provide a more finegrained structure of the lexicon than the 65 top concepts of the IWN ontology, which reflect only fundamental distinctions, mapping is PSC mapping the ?Animal??class).
Some of these word senses proceed from an extension of meaning, e.g.
People-Human: pigmeo, troglodita (pygmy, troglodyte) or Animal-Human verme, leone (worm, lion) and are used with different levels of intentionality: either as a semantic surplus or as dead metaphors (Marinelli, 2006).
More interestingly, the list of unmatched words also contains the IWN word senses whose synset?s ontological classification is incomplete or different w.r.t. the constraints imposed to the mapping run.
Analyzing these data is therefore crucial to identify further mapping constraints.
A list of PSC lexical units missing in IWN is also generated, which is important to appropriately assess the lexical intersection between the two resources.
5 Results
From a quantitative point of view three main issues are worth noting (cf.
Table 1): first, the considerable percentage of linked senses with respect to the linkable ones (i.e.
words with identical string and PoS value); second, the many 163 cases of multiple mappings; third, the extent of overlapping coverage.
SemUs selected 27768 Linkable senses 15193 54,71% Linked senses 10988 72,32% Multiple mappings 1125 10,23% Unmatched senses 4205 27,67% Table 1 summarizing data Multiple mappings depend on the more fine grained sense distinctions performed in IWN.
The eventual merging of the two resources would make up for such discrepancy.
During the linking process, many other possibilities of reciprocal improvement and enrichment were noticed by analyzing the lists of unmatched word-senses.
All the inconsistencies are in fact recorded together with their differences in ontological classification, or in the polysemy treatment that the mapping evidenced.
Some mapping failures have been observed due to a different approach to the treatment of polysemy in the two resources: for example, a single entry in PSC corresponding to two different IWN entries encoding very fined-grained nuances of sense, e.g. galeotto1 (galley rower) and galeotto2 (galley slave).
Other mapping failures are due to cases of encoding inconsistency.
For example, when a word sense from a multi-variant synset is linked to a SemU, all the other variants from the same synset should map to PSC entries sharing the same semantic type, yet in some cases it has been observed that SemUs corresponding to variants of the same synset do not share a common semantic type.
All these encoding differences or inconsistencies were usefully put in the foreground by the linking process and are worthy of further in-depth analysis with a view to the merging, harmonization and interoperability of the two lexical resources.
6 Conclusion
and Future Work In this paper the PSC-IWN linking of concrete entities, the methodology adopted, the tool implemented to this aim and the results obtained are described.
On the basis of the encouraging results illustrated here, the linking process will be carried on by dealing with 3 rd Order Entities.
Our attention will then be devoted to 2 nd OrderEntities which, so far, have only been object of preliminary investigations on Speech act (Roventini 2006) and Feeling verbs.
Because of their intrinsic complexity, the linking of 2 nd OrderEntities is expected to be a far more challenging task.
References James Pustejovsky 1995.
The generative lexicon.
MIT Press.
Christiane Fellbaum (ed).
1998. Wordnet: An Electronic Lexical Database.
MIT Press.
Piek Vossen (ed).
1998. EuroWordNet: A multilingual database with lexical semantic networks.
Kluwer Academic Publishers.
Adriana Roventini et al.2003. ItalWordNet: Building a Large Semantic Database for the Automatic Treatment of Italian.
Computational Linguistics in Pisa, Special Issue, XVIII-XIX, Pisa-Roma, IEPI.
Tomo II, 745--791.
Nilda Ruimy et al.2003. A computational semantic lexicon of Italian: SIMPLE.
In A.
Zampolli, N.
Calzolari, L.
Cignoni, (eds.), Computational Linguistics in Pisa, Special Issue, XVIII-XIX, (2003).
Pisa-Roma, IEPI.
Tomo II, 821-864.
Adriana Roventini, Marisa Ulivieri and Nicoletta Calzolari.
2002 Integrating two semantic lexicons, SIMPLE and ItalWordNet: what can we gain?
LREC Proceedings, Vol.
V, pp.
1473-1477. Nilda Ruimy and Adriana Roventini.
2005 Towards the linking of two electronic lexical databases of Italian, In Zygmunt Veutulani (ed.), L&T'05 Nilda Ruimy.
2006. Merging two Ontology-based Lexical Resources.
LREC Proceedings, CD-ROM, 1716-1721.
Adriana Roventini.
2006. Linking Verbal Entries of Different Lexical Resources.
LREC Proceedings, CD-ROM, 1710-1715.
Rita Marinelli.
2006. Computational Resources and Electronic Corpora in Metaphors Evaluation.
Second International Conference of the German Cognitive Linguistics Association, Munich, 5-7 October. 164
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 169??72, Prague, June 2007.
c2007 Association for Computational Linguistics An OWL Ontology for HPSG Graham Wilcock University of Helsinki PO Box 9 00014 Helsinki, Finland graham.wilcock@helsinki.fi Abstract The paper presents an OWL ontology for HPSG.
The HPSG ontology is integrated with an existing OWL ontology, GOLD, as a community of practice extension.
The basic ideas are illustrated by visualizations of type hierarchies for parts of speech.
1 Introduction
The paper presents an OWL ontology for HPSG (Head-driven Phrase Structure Grammar) (Sag et al., 2003).
OWL is the W3C Web Ontology Language (http://www.w3.org/2004/OWL).
An existing ontology is used as a starting point: GOLD (Section 2) is a general ontology for linguistic description.
As HPSG is a more specific linguistic theory, the HPSG ontology (Section 3) is integrated inside GOLD as a sub-ontology known as a community of practice extension (Section 4).
2 GOLD: A General Ontology for Linguistic Description GOLD, a General Ontology for Linguistic Description (http://www.linguistics-ontology.org/) (Farrar and Langendoen, 2003) is an OWL ontology that aims to capture ?the general knowledge of the field that is usually possessed by a well trained linguist.
This includes knowledge that potentially forms the basis of any theoretical framework.
In particular, GOLD captures the fundamentals of descriptive linguistics.
Examples of such knowledge are ?a verb is a part of speech??
?gender can be semantically grounded?? or ?linguistic expressions realize morphemes????(Farrar and Lewis, 2005).
As far as possible GOLD uses language-neutral and theory-neutral terminology.
For instance, parts of speech are subclasses of gold:GrammaticalUnit asshowninFigure1.
AsGOLDislanguage-neutral, a wide range of parts of speech are included.
For example, both Preposition and Postposition are included as subclasses of Adposition.
The classes in the OWLViz graphical visualization (on the right in Figure 1) have been selected from the complete list in the Asserted Hierarchy (on the left).
Originally GOLD was intended to be neutral where linguistic theories had divergent views, but a recent development is the idea of supporting different sub-communities as communities of practice (Farrar and Lewis, 2005) within the GOLD framework.
A community of practice may focus on developing a consensus in a specific area, for example in phonology or in Bantu languages.
On the other hand, communities of practice may focus on competing theories, where each sub-community has its own distinctive terminology and divergent conceptualization.
In this case, the aim is to capture explicitly the relationship between the sub-community view and the overall framework, in the form of a Community Of Practice Extension (COPE) (Farrar and Lewis, 2005).
A COPE is a sub-ontology that inherits from, and extends, the overall GOLD ontology.
Sub-ontology classes are distinguished from each other by different namespace prefixes, for example gold:Noun and hpsg:noun.
3 An
OWL Ontology for HPSG HPSG OWL is an OWL ontology for HPSG that is currentlyunderdevelopment.
Astheaimsofthefirst version of the ontology are clarity and acceptability, 169 Figure 1: Parts of speech in GOLD it carefully follows the standard textbook version of HPSG by Sag et al.(2003). This also means that the first version is English-specific, as the core grammars presented in the textbook are English-specific.
In HPSG OWL, parts of speech are subclasses of hpsg:pos, as shown in Figure 2.
As this version is English-specific, it has prepositions (hpsg:prep) but not postpositions.
Parts of speech that have agreement features (in English) form a distinct subclass hpsg:agr-pos including hpsg:det (determiner) and hpsg:verb.
Within hpsg:agr-pos, hpsg:comp (complementizer) and hpsg:noun form a further subclass hpsg:nominal.
This particular conceptualization of the type hierarchy is specific to (Sag et al., 2003).
The Protege-OWL (http://protege.stanford.edu) ontology editor supports both visual construction and visual editing of the hierarchy.
For example, if hpsg:adj had agreement features, it could be moved under hpsg:agr-pos by a simple drag-and-drop (in the Asserted Hierarchy pane on the left).
Both the visualization (in the OWLViz pane on the right) and the underlying OWL statements (not shown) are automatically generated.
The grammar writer does not edit OWL statements directly.
This is a significant advantage of the new technology over current grammar development tools.
For example, LKB (Copestake, 2002) can produce a visualizationofthetypehierarchyfromtheunderlying Type Definition Language (TDL) statements, but the hierarchy can only be modified by textually editing the TDL statements.
4 A
Community of Practice Extension HPSG COPE is a community of practice extension that integrates the HPSG ontology within GOLD.
The COPE is an OWL ontology that imports both the GOLD and the HPSG ontologies.
Apart from the import statements, the COPE consists entirely of 170 Figure 2: Parts of speech in HPSG rdfs:subClassOf and rdfs:subPropertyOf statements.
HPSG COPE defines HPSG classes as subclasses of GOLD classes and HPSG properties as subproperties of GOLD properties.
In the COPE, parts of speech in HPSG are subsumed by appropriate parts of speech in GOLD, as shown in Figure 3.
In some cases this is straightforward, for example hpsg:adj is mapped to gold:Adjective.
In other cases, the HPSG theoryspecific terminology differs significantly from the theory-neutral terminology in GOLD.
Some of the mappings are based on definitions of the HPSG terms given in a glossary in (Sag et al., 2003), for example the mapping of hpsg:conj (conjunction) to gold:CoordinatingConnective and the mapping of hpsg:comp (complementizer) to gold:SubordinatingConnective.
Properties in HPSG OWL are defined by HPSG COPE as subproperties of GOLD properties.
For example, the HPSG OWL class hpsg:sign (Sag et al., 2003) (p.
475) properties: PHON type: list (a sequence of word forms) SYN type: gram-cat (a grammatical category) SEM type: sem-struc (a semantic structure) are mapped to the GOLD class gold:LinguisticSign properties: hasForm Range: PhonologicalUnit hasGrammar Range: GrammaticalUnit hasMeaning Range: SemanticUnit by the HPSG COPE rdfs:subPropertyOf definitions: hpsg:PHON subproperty of gold:hasForm hpsg:SYN subproperty of gold:hasGrammar hpsg:SEM subproperty of gold:hasMeaning 5 Conclusion The paper has described an initial version of an OWLontologyforHPSG,togetherwithanapproach to integrating it with GOLD as a community of prac171 Figure 3: Parts of speech in the Community of Practice Extension tice extension.
Perhaps a rigorous foundation of typed feature structures and a clear type hierarchy makes HPSG more amenable to expression as an ontology than other linguistic theories.
Protege-OWL supports visual development and visual editing of the ontology.
This is a significant practical advantage over existing grammar development tools.
OWLViz provides graphical visualizations of any part of the ontology.
OWL DL (Description Logic) reasoners can be run inside Protege to check consistency and to do cross-classification.
One current research topic is how to exploit reasoners to perform automatically the kind of cross-classification that is widely used in HPSG linguistic analyses.
Another current topic is how to implement HPSG lexical rules and grammar rules in the ontology.
An interesting possibility is to use the W3C Semantic Web Rule Language, SWRL (Wilcock, 2006).
References Ann Copestake.
2002. Implementing Typed Feature Structure Grammars.
CSLI Publications, Stanford, CA.
Scott Farrar and D.
Terence Langendoen.
2003. A linguistic ontology for the semantic web.
GLOT International, 7.3:97??00.
Scott Farrar and William D.
Lewis. 2005.
The GOLD Community of Practice: An infrastructure for linguistic data on the web.
http://www.u.arizona.edu/?farrar/. Ivan A.
Sag, Thomas Wasow, and Emily Bender.
2003. Syntactic Theory: A Formal Introduction.
CSLI Publications, Stanford, CA.
Graham Wilcock.
2006. Natural language parsing with GOLD and SWRL.
In RuleML-2006, Rules and Rule Markup Languages for the Semantic Web (Online Proceedings), Athens, GA.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 173??76, Prague, June 2007.
c2007 Association for Computational Linguistics Classifying Temporal Relations Between Events Nathanael Chambers and Shan Wang and Dan Jurafsky Department of Computer Science Stanford University Stanford, CA 94305 {natec,shanwang,jurafsky}@stanford.edu Abstract This paper describes a fully automatic twostage machine learning architecture that learns temporal relations between pairs of events.
The first stage learns the temporal attributes of single event descriptions, such as tense, grammatical aspect, and aspectual class.
These imperfect guesses, combined with other linguistic features, are then used in a second stage to classify the temporal relationship between two events.
We present both an analysis of our new features and results on the TimeBank Corpus that is 3% higher than previous work that used perfect human tagged features.
1 Introduction
Temporal information encoded in textual descriptions of events has been of interest since the early days of natural language processing.
Lately, it has seen renewed interest as Question Answering, Information Extraction and Summarization domains find it critical in order to proceed beyond surface understanding.
With the recent creation of the Timebank Corpus (Pustejovsky et al., 2003), the utility of machine learning techniques can now be tested.
Recent work with the Timebank Corpus has revealed that the six-class classification of temporal relationsisverydifficult,evenforhumanannotators.
The highest score reported on Timebank achieved 62.5% accuracy when using gold-standard features as marked by humans (Mani et al., 2006).
This paper describes an approach using features extracted automatically from raw text that not only duplicates this performance, but surpasses its accuracy by 3%.
We do so through advanced linguistic features and a surprising finding that using automatic ratherthanhand-labeledtenseandaspectknowledge causes only a slight performance degradation.
We briefly describe current work on temporal orderinginsection2.
Section4describesthefirststage of basic temporal extraction, followed by a full description of the second stage in 5.
The evaluation and results on Timebank then follow in section 6.
2 Previous
Work Mani et.
al (2006) built a MaxEnt classifier that assigns each pair of events one of 6 relations from an augmented Timebank corpus.
Their classifier relies on perfect features that were hand-tagged in the corpus, including tense, aspect, modality, polarity and event class.
Pairwise agreement on tense and aspect are also included.
In a second study, they applied rules of temporal transitivity to greatly expand the corpus, providing different results on this enlarged dataset.
We could not duplicate their reported performance on this enlarged data, and instead focus on performing well on the Timebank data itself.
Lapata and Lascarides (2006) trained an event classifierforinter-sententialevents.
Theybuiltacorpus by saving sentences that contained two events, one of which is triggered by a key time word (e.g.
after and before).
Their learner was based on syntax and clausal ordering features.
Boguraev and Ando (2005) evaluated machine learning on related tasks, but not relevant to event-event classification.
Our work is most similar to Mani?s in that we are 173 learning relations given event pairs, but our work extends their results both with new features and by using fully automatic linguistic features from raw text that are not hand selected from a corpus.
3 Data
We used the Timebank Corpus (v1.1) for evaluation, 186 newswire documents with 3345 event pairs.
Solely for comparison with Mani, we add the 73 documentOpinionCorpus(Manietal., 2006)tocreate a larger dataset called the OTC.
We present both Timebank and OTC results so future work can compare against either.
All results below are from 10fold cross validation.
4 Stage
One: Learning Event Attributes The task in Stage One is to learn the five temporal attributes associated with events as tagged in the Timebank Corpus.
(1) Tense and (2) grammatical aspect are necessary in any approach to temporal ordering as they define both temporal location and structure of the event.
(3) Modality and (4) polarity indicate hypothetical or non-occuring situations, and finally, (5) event class is the type of event (e.g.
process, state, etc.).
The event class has 7 values in Timebank, but we believe this paper?s approach is compatible with other class divisions as well.
The range of values for each event attribute is as follows, also found in (Pustejovsky et al., 2003): tense none, present, past, future aspect none, prog, perfect, prog perfect class report, aspectual, state, I state I action, perception, occurrence modality none, to, should, would, could can, might polarity positive, negative 4.1 Machine Learning Classification We used a machine learning approach to learn each of the five event attributes.
We implemented both Naive Bayes and Maximum Entropy classifiers, but found Naive Bayes to perform as well or better than Maximum Entropy.
The results in this paper are from Naive Bayes with Laplace smoothing.
The features we used on this stage include part of speech tags (two before the event), lemmas of the event words, WordNet synsets, and the appearance tense POS-2-event, POS-1-event POS-of-event, have word, be word aspect POS-of-event, modal word, be word class synset modality none polarity none Figure 1: Features selected for learning each temporal attribute.
POS-2 is two tokens before the event.
Timebank Corpus tense aspect class Baseline 52.21 84.34 54.21 Accuracy 88.28 94.24 75.2 Baseline (OTC) 48.52 86.68 59.39 Accuracy (OTC) 87.46 88.15 76.1 Figure 2: Stage One results on classification.
of auxiliaries and modals before the event.
This latter set included all derivations of be and have auxiliaries, modal words (e.g.
may, might, etc.), and the presence/absence of not.
We performed feature selection on this list of features, learning a different set of features for each of the five attributes.
The list of selected features for each is shown in figure 1.
Modality and polarity did not select any features because their majority class baselines were so high (98%) that learning these attributes does not provide much utility.
A deeper analysis of event interaction would require a modal analysis, but it seems that a newswire domain does not provide great variation in modalities.
Consequently, modality and polarity are not used in Stage Two.
Tense, aspect and class are shown in figure 2 with majority class baselines.
Tense classification achieves36%absolute improvement, aspect 10% and class 21%.
Performance on the OTC set is similar, although aspect is not as good.
These guesses are then passed to Stage Two.
5 Stage
Two: Event-Event Features The task in this stage is to choose the temporal relation between two events, given the pair of events.
We assume that the events have been extracted and that there exists some relation between them; the task is to choose the relation.
The Timebank Corpus uses relations that are based on Allen?s set of thir174 teen (Allen, 1984).
Six of the relations are inverses of the other six, and so we condense the set to before, ibefore, includes, begins, ends and simultaneous.
We map the thirteenth identity into simultaneous.
One oddity is that Timebank includes both during and included by relations, but during does not appear in Timebank documentation.
While we don?t know how previous work handles this, we condense during into included by (invert to includes).
5.1 Features
Event Specific: The five temporal attributes from Stage One are used for each event in the pair, as well as the event strings, lemmas and WordNet synsets.
Mani added two other features from these, indicators if the events agree on tense and aspect.
We add a third, event class agreement.
Further, to capture the dependency between events in a discourse, we create new bigram features of tense, aspect and class (e.g.
?present past??if the first event is in the present, and the second past).
PartofSpeech: Foreachevent, weincludethePenn Treebank POS tag of the event, the tags for the two tokens preceding, and one token following.
We use the Stanford Parser1 to extract them.
We also extend previousworkandcreatebigramPOSfeaturesofthe event and the token before it, as well as the bigram POS of the first event and the second event.
Event-Event Syntactic Properties: A phrase P is said to dominate another phrase Q if Q is a daughter node of P in the syntactic parse tree.
We leverage the syntactic output of the parser to create the dominance feature for intra-sentential events.
It is either on or off, depending on the two events??syntactic dominance.
Lapata used a similar feature for subordinate phrases and an indicator before for textualeventordering.
Weadoptthesefeaturesandalso add a same-sentence indicator if the events appear in the same sentence.
Prepositional Phrase: Since preposition heads are often indicators of temporal class, we created a new feature indicating when an event is part of a prepositional phrase.
The feature?s values range over 34 English prepositions.
Combined with event dominance (above), these two features capture direct 1http://nlp.stanford.edu/software/lex-parser.shtml intra-sentential relationships.
To our knowledge, we are the first to use this feature in temporal ordering.
Temporal Discourse: Seeing tense as a type of anaphora, it is a natural conclusion that the relationship between two events becomes stronger as the textual distance draws closer.
Because of this, we adopted the view that intra-sentential events are generated from a different distribution than intersentential events.
We therefore train two models during learning, one for events in the same sentence, and the other for events crossing sentence boundaries.
It essentially splits the data on the same sentence feature.
As we will see, this turned out to be a very useful feature.
It is called the split approach in the next section.
Example (require, compromise): ?Their solution required a compromise...??
Features (lemma1: require) (lemma2: compromise) (dominates: yes) (tense-bigram: past-none) (aspect-bigram: none-none) (tensematch: no) (aspect-match: yes) (before: yes) (same-sent: yes) 6 Evaluation and Results All results are from a 10-fold cross validation using SVM (Chang and Lin, 2001).
We also evaluated Naive Bayes and Maximum Entropy.
Naive Bayes (NB) returned similar results to SVM and we presentfeatureselectionresultsfromNBtocompare the added value of our new features.
The input to Stage Two is a list of pairs of events; the task is to classify each according to one of six temporal relations.
Four sets of results are shown in figure 3.
Mani, Mani+Lapata and All+New correspond to performance on features as listed in the figure.
The three table columns indicate how a goldstandard Stage One (Gold) compares against imperfect guesses (Auto) and the guesses with split distributions (Auto-Split).
A clear improvement is seen in each row, indicating that our new features provide significant improvement over previous work.
A decrease in performance is seen between columns gold and auto, as expected, because imperfect data is introduced, however, the drop is manageable.
The auto-split distributions make significant gains for the Mani and Lapata features, but less when all new features are 175 Timebank Corpus Gold Auto Auto-Split Baseline 37.22 37.22 46.58 Mani 50.97 50.19 53.42 Mani+Lapata 52.29 51.57 55.10 All+New 60.45 59.13 59.43 Mani stage one attributes, tense/aspect-match, event strings Lapata dominance, before, lemma, synset New prep-phrases, same-sent, class-match, POS uni/bigrams, tense/aspect/class-bigrams Figure 3: Incremental accuracy by adding features.
Same Sentence Diff Sentence POS-1 Ev1 2.5% Tense Pair 1.6% POS Bigram Ev1 3.5% Aspect Ev1 0.5% Preposition Ev1 2.0% POS Bigram 0.2% Tense Ev2 0.7% POS-1 Ev2 0.3% Preposition Ev2 0.6% Word EV2 0.2% Figure4: Top5featuresasaddedinfeatureselection w/ Naive Bayes, with their percentage improvement.
involved. The highest fully-automatic accuracy on Timebank is 59.43%, a 4.3% gain from our new features.
Wealsoreport67.57%gold and65.48%autosplit on the OTC dataset to compare against Mani?s reported hand-tagged features of 62.5%, a gain of 3% with our automatic features.
7 Discussion
Previous work on OTC achieved classification accuracy of 62.5%, but this result was based on ?perfect data??from human annotators.
A low number from gooddataisatfirstdisappointing, however, weshow that performance can be improved through more linguistic features and by isolating the distinct tasks of ordering inter-sentential and intra-sentential events.
Our new features show a clear improvement over previous work.
The features that capture dependencies between the events, rather than isolated features provide the greatest utility.
Also, the impact of imperfect temporal data is surprisingly minimal.
Using Stage One?s results instead of gold values hurts performance by less than 1.4%.
This suggests that much of the value of the hand-coded information can be achieved via automatic approaches.
Stage One?s event class shows room for improvement, yet the negative impact on Event-Event relationships is manageable.
It is conceivable that more advanced featureswouldbetterclassifytheeventclass, butimprovement on the event-event task would be slight.
Finally, it is important to note the difference in classifying events in the same sentence vs.
crossboundary.
Splitting the 3345 pairs of corpus events into two separate training sets makes our data more sparse, but we still see a performance improvement when using Mani/Lapata features.
Figure 4 gives a hint to the difference in distributions as the best features of each task are very different.
Intra-sentence events rely on syntax cues (e.g.
preposition phrases and POS), while inter-sentence events use tense and aspect.
However, the differences are minimized as more advanced features are added.
The final row in figure 3 shows minimal split improvement.
8 Conclusion
We have described a two-stage machine learning approach to event-event temporal relation classification.
We have shown that imperfect event attributescanbeusedeffectively, thatarangeofeventevent dependency features provide added utility to a classifier, and that events within the same sentence have distinct characteristics from those across sentence boundaries.
This fully automatic raw text approach achieves a 3% improvement over previous work based on perfect human tagged features.
Acknowledgement: This work was supported in part by the DARPA GALE Program and the DTO AQUAINT Program.
References James Allen.
1984. Towards a general theory of action and time.
Artificial Intelligence, 23:123??54.
Branimir Boguraev and Rie Kubota Ando.
2005. Timeml-compliant text analysis for temporal reasoning.
In IJCA-05.
Chih-Chung Chang and Chih-Jen Lin, 2001.
LIBSVM: a library for support vector machines.
Software available at http://www.csie.ntu.edu.tw/ cjlin/libsvm.
Mirella Lapata and Alex Lascarides.
2006. Learning sentence-internal temporal relations.
In Journal of AI Research, volume 27, pages 85??17.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min Lee, and James Pustejovsky.
2006. Machine learning of temporal relations.
In ACL-06, July.
James Pustejovsky, Patrick Hanks, Roser Sauri, Andrew See, David Day, Lisa Ferro, Robert Gaizauskas, Marcia Lazo, Andrea Setzer, and Beth Sundheim.
2003. The timebank corpus.
Corpus Linguistics, pages 647??56.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177??80, Prague, June 2007.
c2007 Association for Computational Linguistics Moses: Open Source Toolkit for Statistical Machine Translation Philipp Koehn Hieu Hoang Alexandra Birch Chris Callison-Burch University of Edinburgh 1 Marcello Federico Nicola Bertoldi ITC-irst 2 Brooke Cowan Wade Shen Christine Moran MIT 3 Richard Zens RWTH Aachen 4 Chris Dyer University of Maryland 5 Ond ej Bojar Charles University 6 Alexandra Constantin Williams College 7 Evan Herbst Cornell 8 1 pkoehn@inf.ed.ac.uk, {h.hoang, A.C.Birch-Mayne}@sms.ed.ac.uk, callison-burch@ed.ac.uk.
2 {federico, bertoldi}@itc.it.
3 brooke@csail.mit.edu, swade@ll.mit.edu, weezer@mit.edu.
4 zens@i6.informatik.rwth-aachen.de.
5 redpony@umd.edu.
6 bojar@ufal.ms.mff.cuni.cz.
7 07aec_2@williams.edu.
8 evh4@cornell.edu Abstract We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models.
In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks.
1 Motivation
Phrase-based statistical machine translation (Koehn et al.2003) has emerged as the dominant paradigm in machine translation research.
However, until now, most work in this field has been carried out on proprietary and in-house research systems.
This lack of openness has created a high barrier to entry for researchers as many of the components required have had to be duplicated.
This has also hindered effective comparisons of the different elements of the systems.
By providing a free and complete toolkit, we hope that this will stimulate the development of the field.
For this system to be adopted by the community, it must demonstrate performance that is comparable to the best available systems.
Moses has shown that it achieves results comparable to the most competitive and widely used statistical machine translation systems in translation quality and run-time (Shen et al.2006). It features all the capabilities of the closed sourced Pharaoh decoder (Koehn 2004).
Apart from providing an open-source toolkit for SMT, a further motivation for Moses is to extend phrase-based translation with factors and confusion network decoding.
The current phrase-based approach to statistical machine translation is limited to the mapping of small text chunks without any explicit use of linguistic information, be it morphological, syntactic, or semantic.
These additional sources of information have been shown to be valuable when integrated into pre-processing or post-processing steps.
Moses also integrates confusion network decoding, which allows the translation of ambiguous input.
This enables, for instance, the tighter integration of speech recognition and machine translation.
Instead of passing along the one-best output of the recognizer, a network of different word choices may be examined by the machine translation system.
Efficient data structures in Moses for the memory-intensive translation model and language model allow the exploitation of much larger data resources with limited hardware.
177 2 Toolkit The toolkit is a complete out-of-the-box translation system for academic research.
It consists of all the components needed to preprocess data, train the language models and the translation models.
It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al.2002). Moses uses standard external tools for some of the tasks to avoid duplication, such as GIZA++ (Och and Ney 2003) for word alignments and SRILM for language modeling.
Also, since these tasks are often CPU intensive, the toolkit has been designed to work with Sun Grid Engine parallel environment to increase throughput.
In order to unify the experimental stages, a utility has been developed to run repeatable experiments.
This uses the tools contained in Moses and requires minimal changes to set up and customize.
The toolkit has been hosted and developed under sourceforge.net since inception.
Moses has an active research community and has reached over 1000 downloads as of 1 st March 2007.
The main online presence is at http://www.statmt.org/moses/ where many sources of information about the project can be found.
Moses was the subject of this year?s Johns Hopkins University Workshop on Machine Translation (Koehn et al.2006). The decoder is the core component of Moses.
To minimize the learning curve for many researchers, the decoder was developed as a drop-in replacement for Pharaoh, the popular phrase-based decoder.
In order for the toolkit to be adopted by the community, and to make it easy for others to contribute to the project, we kept to the following principles when developing the decoder: ??Accessibility ??Easy to Maintain ??Flexibility ??Easy for distributed team development ??Portability It was developed in C++ for efficiency and followed modular, object-oriented design.
3 Factored
Translation Model Non-factored SMT typically deals only with the surface form of words and has one phrase table, as shown in Figure 1.
i am buying you a green cat using phrase dictionary: i am buying you a green cat je achte vous un vert chat a une je vous achte un chat vert Translate: In factored translation models, the surface forms may be augmented with different factors, such as POS tags or lemma.
This creates a factored representation of each word, Figure 2.
111/ sing/ je vous achet un chat PRO PRO VB ART NN je vous acheter un chat st st st present masc masc ?????? ??
?????? ??
?????? ??
?????? ??
?????? ??
?????? ?? 1 1 / 1 sing sing i buy you a cat PRO VB PRO ART NN i tobuy you a cat st st present st ????????
???????? ????????
???????? ????????
???????? Mapping of source phrases to target phrases may be decomposed into several steps.
Decomposition of the decoding process into various steps means that different factors can be modeled separately.
Modeling factors in isolation allows for flexibility in their application.
It can also increase accuracy and reduce sparsity by minimizing the number dependencies for each step.
For example, we can decompose translating from surface forms to surface forms and lemma, as shown in Figure 3.
Figure 2.
Factored translation Figure 1.
Non-factored translation 178 Figure 3.
Example of graph of decoding steps By allowing the graph to be user definable, we can experiment to find the optimum configuration for a given language pair and available data.
The factors on the source sentence are considered fixed, therefore, there is no decoding step which create source factors from other source factors.
However, Moses can have ambiguous input in the form of confusion networks.
This input type has been used successfully for speech to text translation (Shen et al.2006). Every factor on the target language can have its own language model.
Since many factors, like lemmas and POS tags, are less sparse than surface forms, it is possible to create a higher order language models for these factors.
This may encourage more syntactically correct output.
In Figure 3 we apply two language models, indicated by the shaded arrows, one over the words and another over the lemmas.
Moses is also able to integrate factored language models, such as those described in (Bilmes and Kirchhoff 2003) and (Axelrod 2006).
4 Confusion
Network Decoding Machine translation input currently takes the form of simple sequences of words.
However, there are increasing demands to integrate machine translation technology into larger information processing systems with upstream NLP/speech processing tools (such as named entity recognizers, speech recognizers, morphological analyzers, etc.).
These upstream processes tend to generate multiple, erroneous hypotheses with varying confidence.
Current MT systems are designed to process only one input hypothesis, making them vulnerable to errors in the input.
In experiments with confusion networks, we have focused so far on the speech translation case, where the input is generated by a speech recognizer.
Namely, our goal is to improve performance of spoken language translation by better integrating speech recognition and machine translation models.
Translation from speech input is considered more difficult than translation from text for several reasons.
Spoken language has many styles and genres, such as, formal read speech, unplanned speeches, interviews, spontaneous conversations; it produces less controlled language, presenting more relaxed syntax and spontaneous speech phenomena.
Finally, translation of spoken language is prone to speech recognition errors, which can possibly corrupt the syntax and the meaning of the input.
There is also empirical evidence that better translations can be obtained from transcriptions of the speech recognizer which resulted in lower scores.
This suggests that improvements can be achieved by applying machine translation on a large set of transcription hypotheses generated by the speech recognizers and by combining scores of acoustic models, language models, and translation models.
Recently, approaches have been proposed for improving translation quality through the processing of multiple input hypotheses.
We have implemented in Moses confusion network decoding as discussed in (Bertoldi and Federico 2005), and developed a simpler translation model and a more efficient implementation of the search algorithm.
Remarkably, the confusion network decoder resulted in an extension of the standard text decoder.
5 Efficient
Data Structures for Translation Model and Language Models With the availability of ever-increasing amounts of training data, it has become a challenge for machine translation systems to cope with the resulting strain on computational resources.
Instead of simply buying larger machines with, say, 12 GB of main memory, the implementation of more efficient data structures in Moses makes it possible to exploit larger data resources with limited hardware infrastructure.
A phrase translation table easily takes up gigabytes of disk space, but for the translation of a single sentence only a tiny fraction of this table is needed.
Moses implements an efficient representation of the phrase translation table.
Its key properties are a prefix tree structure for source words and on demand loading, i.e. only the fraction of the phrase table that is needed to translate a sentence is loaded into the working memory of the decoder.
179 For the Chinese-English NIST task, the memory requirement of the phrase table is reduced from 1.7 gigabytes to less than 20 mega bytes, with no loss in translation quality and speed (Zens and Ney 2007).
The other large data resource for statistical machine translation is the language model.
Almost unlimited text resources can be collected from the Internet and used as training data for language modeling.
This results in language models that are too large to easily fit into memory.
The Moses system implements a data structure for language models that is more efficient than the canonical SRILM (Stolcke 2002) implementation used in most systems.
The language model on disk is also converted into this binary format, resulting in a minimal loading time during start-up of the decoder.
An even more compact representation of the language model is the result of the quantization of the word prediction and back-off probabilities of the language model.
Instead of representing these probabilities with 4 byte or 8 byte floats, they are sorted into bins, resulting in (typically) 256 bins which can be referenced with a single 1 byte index.
This quantized language model, albeit being less accurate, has only minimal impact on translation performance (Federico and Bertoldi 2006).
6 Conclusion
and Future Work This paper has presented a suite of open-source tools which we believe will be of value to the MT research community.
We have also described a new SMT decoder which can incorporate some linguistic features in a consistent and flexible framework.
This new direction in research opens up many possibilities and issues that require further research and experimentation.
Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al.2006) and (Koehn and Hoang 2007).
References Axelrod, Amittai.
"Factored Language Model for Statistical Machine Translation".
MRes Thesis.
Edinburgh University, 2006.
Bertoldi, Nicola, and Marcello Federico.
"A New Decoder for Spoken Language Translation Based on Confusion Networks".
Automatic Speech Recognition and Understanding Workshop (ASRU), 2005.
Bilmes, Jeff A, and Katrin Kirchhoff.
"Factored Language Models and Generalized Parallel Back-off".
HLT/NACCL, 2003.
Koehn, Philipp.
"Pharaoh: A Beam Search Decoder for Phrase-Based Statistical Machine Translation Models".
AMTA, 2004.
Koehn, Philipp, Marcello Federico, Wade Shen, Nicola Bertoldi, Ondrej Bojar, Chris Callison-Burch, Brooke Cowan, Chris Dyer, Hieu Hoang, Richard Zens, Alexandra Constantin, Christine Corbett Moran, and Evan Herbst.
"Open Source Toolkit for Statistical Machine Translation".
Report of the 2006 Summer Workshop at Johns Hopkins University, 2006.
Koehn, Philipp, and Hieu Hoang.
"Factored Translation Models".
EMNLP, 2007.
Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
"Statistical Phrase-Based Translation".
HLT/NAACL, 2003.
Och, Franz Josef.
"Minimum Error Rate Training for Statistical Machine Translation".
ACL, 2003.
Och, Franz Josef, and Hermann Ney.
"A Systematic Comparison of Various Statistical Alignment Models".
Computational Linguistics 29.1 (2003): 19-51.
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
"BLEU: A Method for Automatic Evaluation of Machine Translation".
ACL, 2002.
Shen, Wade, Richard Zens, Nicola Bertoldi, and Marcello Federico.
"The JHU Workshop 2006 Iwslt System".
International Workshop on Spoken Language Translation, 2006.
Stolcke, Andreas.
"SRILM an Extensible Language Modeling Toolkit".
Intl. Conf.
on Spoken Language Processing, 2002.
Zens, Richard, and Hermann Ney.
"Efficient Phrase-Table Representation for Machine Translation with Applications to Online MT and Speech Recognition".
HLT/NAACL, 2007.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 181??84, Prague, June 2007.
c2007 Association for Computational Linguistics Boosting Statistical Machine Translation by Lemmatization and Linear Interpolation Ruiqiang Zhang1,2 and Eiichiro Sumita1,2 1National Institute of Information and Communications Technology 2ATR Spoken Language Communication Research Laboratories 2-2-2 Hikaridai, Seiika-cho, Soraku-gun, Kyoto, 619-0288, Japan {ruiqiang.zhang,eiichiro.sumita}@atr.jp Abstract Data sparseness is one of the factors that degrade statistical machine translation (SMT).
Existing work has shown that using morphosyntactic information is an effective solution to data sparseness.
However, fewer efforts have been made for Chinese-to-English SMT with using English morpho-syntactic analysis.
We found that while English is a language with less inflection, using English lemmas in training can significantly improve the quality of word alignment that leads to yield better translation performance.
We carried out comprehensive experiments on multiple training data of varied sizes to prove this.
We also proposed a new effective linear interpolation method to integrate multiple homologous features of translation models.
1 Introduction
Raw parallel data need to be preprocessed in the modern phrase-based SMT before they are aligned by alignment algorithms, one of which is the wellknown tool, GIZA++ (Och and Ney, 2003), for training IBM models (1-4).
Morphological analysis (MA) is used in data preprocessing, by which the surface words of the raw data are converted into a new format.
This new format can be lemmas, stems, parts-of-speech and morphemes or mixes of these.
One benefit of using MA is to ease data sparseness that can reduce the translation quality significantly, especially for tasks with small amounts of training data.
Some published work has shown that applying morphological analysis improved the quality of SMT (Lee, 2004; Goldwater and McClosky, 2005).
We found that all this earlier work involved experiments conducted on translations from highly inflected languages, such as Czech, Arabic, and Spanish, to English.
These researchers also provided detailed descriptions of the effects of foreign language morpho-syntactic analysis but presented no specific results to show the effect of English morphological analysis.
To the best of our knowledge, there have been no papers related to English morphological analysis for Chinese-to-English (CE) translations even though the CE translation has been the main track for many evaluation campaigns including NIST MT, IWSLT and TC-STAR, where only simple tokenization or lower-case capitalization has been applied to English preprocessing.
One possible reason why English morphological analysis has been neglected may be that English is less inflected to the extent that MA may not be effective.
However, we found this assumption should not be takenfor-granted.
We studied what effect English lemmatization had on CE translation.
Lemmatization is shallow morphological analysis, which uses a lexical entry to replace inflected words.
For example, the three words, doing, did and done, are replaced by one word, do.
They are all mapped to the same Chinese translations.
As a result, it eases the problem with sparse data, and retains word meanings unchanged.
It is not impossible to improve word alignment by using English lemmatization.
We determined what effect lemmatization had in experiments using data from the BTEC (Paul, 2006) CSTAR track.
We collected a relatively large corpus of more than 678,000 sentences.
We conducted comprehensive evaluations and used multiple trans181 lation metrics to evaluate the results.
We found that our approach of using lemmatization improved both the word alignment and the quality of SMT with a small amounts of training data, and, while much work indicates that MA is useless in training large amounts of data (Lee, 2004), our intensive experiments proved that the chance to get a better MT quality using lemmatization is higher than that without it for large amounts of training data.
On the basis of successful use of lemmatization translation, we propose a new linear interpolation method by which we integrate the homologous features of translation models of the lemmatization and non-lemmatization system.
We found the integrated model improved all the components??performance in the translation.
2 Moses
training for system with lemmatization and without We used Moses to carry out the expriments.
Moses is the state of the art decoder for SMT.
It is an extension of Pharaoh (Koehn et al., 2003), and supports factor training and decoding.
Our idea can be easily implemented by Moses.
We feed Moses English words with two factors: surface word and lemma.
The only difference in training with lemmatization from that without is the alignment factor.
The former uses Chinese surface words and English lemmas as the alignment factor, but the latter uses Chinese surface words and English surface words.
Therefore, the lemmatized English is only used in word alignment.
All the other options of Moses are same for both the lemmatization translation and nonlemmatization translation.
We use the tool created by (Minnen et al., 2001) to complete the morphological analysis of English.
We had to make an English part-of-speech (POS) tagger that is compatible with the CLAWS-5 tagset to use this tool.
We use our in-house tagset and English tagged corpus to train a statistical POS tagger by using the maximum entropy principle.
Our tagset contains over 200 POS tags, most of which are consistent to the CLAWS-5.
The tagger achieved 93.7% accuracy for our test set.
We use the default features defined by Pharaoh in the phrase-based log-linear models i.e., a target language model, five translation models, and one distance-based distortion model.
The weighting parameters of these features were optimized in terms of BLEU by the approach of minimum error rate training (Och, 2003).
The data for training and test are from the IWSLT06 CSTAR track that uses the Basic Travel Expression Corpus (BTEC).
The BTEC corpus are relatively larger corpus for travel domain.
We use 678,748 Chinese/English parallel sentences as the training data in the experiments.
The number of words are about 3.9M and 4.4M for Chinese and English respectively.
The number of unique words for English is 28,709 before lemmatization and 24,635 after lemmatization.
A 15%-20% reduction in vocabulary is obtained by the lemmatization.
The test data are the one used in IWSLT06 evaluation.
It contains 500 Chinese sentences.
The test data of IWSLT05 are the development data for tuning the weighting parameters.
Multiple references are used for computing the automatic metrics.
3 Experiments
3.1 Regular test The purpose of the regular tests is to find what effect lemmatization has as the amount of training data increases.
We used the data from the IWSLT06 CSTAR track.
We started with 50,000 (50 K) of data, and gradually added more training data from a 678 K corpus to this.
We applied the methods in Section 2 to train the non-lemmatized translation and lemmatized translation systems.
The results are listed in Table 1.
We use the alignment error rate (AER) to measure the alignment performance, and the two popular automatic metric, BLEU1 and METEOR2 to evaluate the translations.
To measure the word alignment, we manually aligned 100 parallel sentences from the BTEC as the reference file.
We use the ?sure??links and the ?possible??links to denote the alignments.
As shown in Table 1, we found our approach improved word alignment uniformly from small amounts to large amounts of training data.
The maximal AER reduction is up to 27.4% for the 600K.
However, we found some mixed translation results in terms of BLEU.
The lemmatized 1http://domino.watson.ibm.com/library/CyberDig.nsf (keyword=RC22176) 2http://www.cs.cmu.edu/?alavie/METEOR 182 Table 1: Translation results as increasing amount of training data in IWSLT06 CSTAR track System AER BLEU METEOR 50K nonlem 0.217 0.158 0.427 lemma 0.199 0.167 0.431 100K nonlem 0.178 0.182 0.457 lemma 0.177 0.188 0.463 300K nonlem 0.150 0.223 0.501 lemma 0.132 0.217 0.505 400K nonlem 0.136 0.231 0.509 lemma 0.102 0.224 0.507 500K nonlem 0.119 0.235 0.519 lemma 0.104 0.241 0.522 600K nonlem 0.095 0.238 0.535 lemma 0.069 0.248 0.536 Table 2: Statistical significance test in terms of BLEU: sys1=non-lemma, sys2=lemma Data size Diff(sys1-sys2) 50K -0.092 [-0.0176,-0.0012] 100K -0.006 [-0.0155,0.0039] 300K 0.0057 [-0.0046,0.0161] 400K 0.0074 [-0.0023,0.0174] 500K -0.0054 [-0.0139,0.0035] 600K -0.0103 [-0.0201,-0.0006] translations did not outperform the non-lemmatized ones uniformly.
They did for small amounts of data, i.e., 50 K and 100 K, and for large amounts, 500 K and 600 K.
However, they failed for 300 K and 400 K.
The translations were under the statistical significance test by using the bootStrap scripts3.
The results giving the medians and confidence intervals are shown in Table 2, where the numbers indicate the median, the lower and higher boundary at 95% confidence interval.
we found the lemma systems were confidently better than the nonlem systems for the 50K and 600K, but didn?t for other data sizes.
This experiments proved that our proposed approach improved the qualities of word alignments that lead to the translation improvement for the 50K, 100K, 500K and 600K.
In particular, our results revealed large amounts of data of 500 K and 600 3http://projectile.is.cs.cmu.edu/research/public/tools/bootStrap /tutorial.htm Table 3: Competitive scores (BLEU) for non-lemmatization and lemmatization using randomly extracted corpora System 100K 300K 400K 600K total lemma 10/11 5.5/11 6.5/11 5/7 27/40 nonlem 1/11 5.5/11 4.5/11 2/7 13/40 K was improved by the lemmatization while it has been found impossible in most published results.
However, data of 300 K and 400 K worsen translations achieved by the lemmatization4.
In what follows, we discuss a method of random sampling of creating multiple corpora of varied sizes to see robustness of our approach and re-investigate the results of the 300K and 400K.
3.2 Random
sampling test In this section, we use a method of random extraction to generate new multiple training data for each corpus of one definite size.
The new data are extracted from the whole corpus of 678 K randomly.
We generate ten new corpora for 100 K, 300 K, and 400 K data and six new corpora for the 678 K data.
Thus, we create eleven and seven corpora of varied sizes if the corpora in the last experiments are counted.
We use the same method as in Section 2 for each generated corpus to construct systems to compare non-lemmatization and lemmatization.
The systems are evaluated again using the same test data.
The results are listed in Table 3 and Figure 1.
Table 3 shows the ?scoreboard??of non-lemmatized and lemmatized results in terms of BLEU.
If its score for the lemma system is higher than that for the nonlem system, the former earns one point; if equal, each earns 0.5; otherwise, the nonlem earns one point.
As we can see from the table, the results for the lemma system are better than those for the nonlem system for the 100K in 10 of the total 11 corpora.
Of the total 40 random corpora, the lemma systems outperform the nonlem systems in 27 times.
By analyzing the results from Tables 1 and 3, we can arrive at some conclusions.
The lemma systems outperform the nonlem for training corpora less than 4while the results was not confident by statistical significance test, the medians of 300K and 400K were lowered by the lemmatization 183 0.16 0.25 NL-600K L-600K NL-400K L-400K NL-300K L-300K NL-100K L-100K 1110987654321 0.169 0.178 0.187 0.196 0.205 0.214 0.223 0.232 0.241 BLEU Number of randomly extracted corpora Figure 1: Bleu scores for randomly extracted corpora 100 K.
The BLEU score favors the lemma system overwhelmingly for this size.
When the amount of training data is increased up to 600 K, the lemma still beat the nonlem system in most tests while the number of success by the nonlem system increases.
This random test, as a complement to the last experiment, reveals that the lemma either performs the same or better than the nonlem system for training data of any size.
Therefore, the lemma system is slightly better than the nonlem in general.
Figure 1 illustrates the BLEU scores for the ?lemma(L)??and ?nonlem(NL)??systems for randomly extracted corpora.
A higher number of points is obtained by the lemma system than the nonlem for each corpus.
4 Effect
of linear interpolation of features We generated translation models for lemmatization translation and non-lemmatization translation.
We found some features of the translation models could be added linearly.
For example, phrase translation model p(e| f ) can be calculated as, p(e| f ) = 1 pl(e| f ) +2 pnl(e| f ) where pl(e| f ) and pnl(e| f ) is the phrase translation models corresponding to the lemmatization system and non-lemma system.
1 + 2 = 1.
s can be obtained by maximizing likelihood or BLEU scores of a development data.
But we used the same values for all the .
p(e| f ) is the phrase translation model after linear interpolation.
Besides the phrase translation model, we used this approach to integrate Table 4: Effect of linear interpolation lemma nonlemma interpolation open track 0.1938 0.1993 0.2054 the three other features: phrase inverse probability, lexical probability, and lexical inverse probability.
We tested this integration using the open track of IWSLT 2006, a small task track.
The BLEU scores are shown in Table 4.
An improvement over both of the systems were observed.
5 Conclusions
We proposed a new approach of using lemmatization and linear interpolation of homologous features in SMT.
The principal idea is to use lemmatized English for the word alignment.
Our approach was proved effective for the BTEC Chinese to English translation.
It is significant in particular that we have target language, English, as the lemmatized object because it is less usual in SMT.
Nevertheless, we found our approach significantly improved word alignment and qualities of translations.
References Sharon Goldwater and David McClosky.
2005. Improving statistical MT through morphological analysis.
In Proceedings of HLT/EMNLP, pages 676??83, Vancouver, British Columbia, Canada, October.
Philipp Koehn, Franz J.
Och, and Daniel Marcu.
2003. Statistical phrase-based translation.
In HLT-NAACL 2003: Main Proceedings, pages 127??33.
Young-Suk Lee.
2004. Morphological analysis for statistical machine translation.
In HLT-NAACL 2004: Short Papers, pages 57??0, Boston, Massachusetts, USA.
Guido Minnen, John Carroll, and Darren Pearce.
2001. Applied morphological processing of english.
Natural Language Engineering, 7(3):207??23.
Franz Josef Och and Hermann Ney.
2003. A systematic comparison of various statistical alignment models.
Computational Linguistics, 29(1):19??1.
Franz Josef Och.
2003. Minimum error rate training in statistical machine translation.
In ACL 2003, pages 160??67.
Michael Paul.
2006. Overview of the IWSLT 2006 Evaluation Campaign.
In Proc.
of the IWSLT, pages 1??5, Kyoto, Japan.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 185??88, Prague, June 2007.
c2007 Association for Computational Linguistics Extractive Summarization Based on Event Term Clustering Maofu Liu 1,2, Wenjie Li 1, Mingli Wu 1 and Qin Lu 1 1 Department of Computing The Hong Kong Polytechnic University {csmfliu, cswjli, csmlwu, csluqin}@comp.polyu.edu.hk 2 College of Computer Science and Technology Wuhan University of Science and Technology mfliu_china@hotmail.com Abstract Event-based summarization extracts and organizes summary sentences in terms of the events that the sentences describe.
In this work, we focus on semantic relations among event terms.
By connecting terms with relations, we build up event term graph, upon which relevant terms are grouped into clusters.
We assume that each cluster represents a topic of documents.
Then two summarization strategies are investigated, i.e. selecting one term as the representative of each topic so as to cover all the topics, or selecting all terms in one most significant topic so as to highlight the relevant information related to this topic.
The selected terms are then responsible to pick out the most appropriate sentences describing them.
The evaluation of clustering-based summarization on DUC 2001 document sets shows encouraging improvement over the well-known PageRank-based summarization.
1 Introduction
Event-based extractive summarization has emerged recently (Filatova and Hatzivassiloglou, 2004).
It extracts and organizes summary sentences in terms of the events that sentences describe.
We follow the common agreement that event can be formulated as ??Who] did [What] to [Whom] [When] and [Where]??and ?did [What]??denotes the key element of an event, i.e. the action within the formulation.
We approximately define the verbs and action nouns as the event terms which can characterize or partially characterize the event occurrences.
Most existing event-based summarization approaches rely on the statistical features derived from documents and generally associated with single events, but they neglect the relations among events.
However, events are commonly related with one another especially when the documents to be summarized are about the same or very similar topics.
Li et al (2006) report that the improved performance can be achieved by taking into account of event distributional similarities, but it does not benefit much from semantic similarities.
This motivated us to further investigate whether event-based summarization can take advantage of the semantic relations of event terms, and most importantly, how to make use of those relations.
Our idea is grouping the terms connected by the relations into the clusters, which are assumed to represent some topics described in documents.
In the past, various clustering approaches have been investigated in document summarization.
Hatzivassiloglou et al (2001) apply clustering method to organize the highly similar paragraphs into tight clusters based on primitive or composite features.
Then one paragraph per cluster is selected to form the summary by extraction or by reformulation.
Zha (2002) uses spectral graph clustering algorithm to partition sentences into topical groups.
Within each cluster, the saliency scores of terms and sentences are calculated using mutual reinforcement principal, which assigns high salience scores to the sentences that contain many terms with high salience scores.
The sentences and key phrases are selected by their saliency scores to generate the summary.
The similar work based on topic or event is also reported in (Guo and Stylios, 2005).
The granularity of clustering units mentioned above is rather coarse, either sentence or paragraph.
In this paper, we define event term as clustering 185 unit and implement a clustering algorithm based on semantic relations.
We extract event terms from documents and construct the event term graph by linking terms with the relations.
We then regard a group of closely related terms as a topic and make the following two alterative assumptions: (1) If we could find the most significant topic as the main topic of documents and select all terms in it, we could summarize the documents with this main topic.
(2) If we could find all topics and pick out one term as the representative of each topic, we could obtain the condensed version of topics described in the documents.
Based on these two assumptions, a set of cluster ranking, term selection and ranking and sentence extraction strategies are developed.
The remainder of this paper is organized as follows.
Section 2 introduces the proposed extractive summarization approach based on event term clustering.
Section 3 presents experiments and evaluations.
Finally, Section 4 concludes the paper.
2 Summarization
Based on Event Term Clustering 2.1 Event Term Graph We introduce VerbOcean (Chklovski and Pantel, 2004), a broad-coverage repository of semantic verb relations, into event-based summarization.
Different from other thesaurus like WordNet, VerbOcean provides five types of semantic verb relations at finer level.
This just fits in with our idea to introduce event term relations into summarization.
Currently, only the stronger-than relation is explored.
When two verbs are similar, one may denote a more intense, thorough, comprehensive or absolute action.
In the case of change-of-state verbs, one may denote a more complete change.
This is identified as the strongerthan relation in (Timothy and Patrick, 2004).
In this paper, only stronger-than is taken into account but we consider extending our future work with other applicable relations types.
The event term graph connected by term semantic relations is defined formally as, where V is a set of event terms and E is a set of relation links connecting the event terms in V.
The graph is directed if the semantic relation has the characteristic of the asymmetric.
Otherwise, it is undirected.
Figure 1 shows a sample of event term graph built from one DUC 2001 document set.
It is a directed graph as the stronger-than relation in VerbOcean exhibits the conspicuous asymmetric characteristic.
For example, ?fight??means to attempt to harm by blows or with weapons, while ?resist??means to keep from giving in.
Therefore, a directed link from ?fight??to ?resist??is shown in the following Figure 1.
),( EVG = Relations link terms together and form the event term graph.
Based upon it, term significance is evaluated and in turn sentence is judged whether to be extracted in the summary.
Figure 1.
Terms connected by semantic relations 2.2 Event Term Clustering Note that in Figure 1, some linked event terms, such as ?kill??
?rob?? ?threaten??and ?infect?? are semantically closely related.
They may describe the same or similar topic somehow.
In contrast, ?toler??
?resist??and ?fight??are clearly involved in another topic; although they are also reachable from ?kill??
Based on this observation, a clustering algorithm is required to group the similar and related event terms into the cluster of the topic.
In this work, event terms are clustered by the DBSCAN, a density-based clustering algorithm proposed in (Easter et al, 1996).
The key idea behind it is that for each term of a cluster the neighborhood of a given radius has to contain at least a minimum number of terms, i.e. the density in the neighborhood has to exceed some threshold.
By using this algorithm, we need to figure out appropriate values for two basic parameters, namely, Eps (denoting the searching radius from each term) and MinPts (denoting the minimum number of terms in the neighborhood of the term).
We assign one semantic relation step to Eps since there is no clear distance concept in the event term 186 graph.
The value of Eps is experimentally set in our experiments.
We also make some modification on Easter?s DBSCAN in order to accommodate to our task.
Figure 2 shows the seven term clusters generated by the modified DBSCAN clustering algorithm from the graph in Figure 1.
We represent each cluster by the starting event term in bold font.
fight resist consider expect announce offer list public accept honor publish study found place prepare toler pass fear threaten kill feel suffer live survive undergo ambush rob infect endure run moverush report investigate file satisfy please manage accept Figure 2.
Term clusters generated from Figure 1 2.3 Cluster Ranking The significance of the cluster is calculated by ???
??? = CCCt t Ct ti iii ddCsc /)( where is the degree of the term t in the term graph.
C is the set of term clusters obtained by the modified DBSCAN clustering algorithm and is the ith one.
Obviously, the significance of the cluster is calculated from global point of view, i.e. the sum of the degree of all terms in the same cluster is divided by the total degree of the terms in all clusters.
t d i C 2.4 Term Selection and Ranking Representative terms are selected according to the significance of the event terms calculated within each cluster (i.e.
from local point of view) or in all clusters (i.e.
from global point of view) by LOCAL: or ??
?? = i ct tt ddtst /)( GLOBAL: ? ? = Ccct tt ii ddtst /)( Then two strategies are developed to select the representative terms from the clusters.
(1) One Cluster All Terms (OCAT) selects all terms within the first rank cluster.
The selected terms are then ranked according to their significance.
(2) One Term All Cluster (OTAC) selects one most significant term from each cluster.
Notice that because terms compete with each other within clusters, it is not surprising to see )()( 21 tsttst < even when,. To address this problem, the representative terms are ranked according to the significance of the clusters they belong to.
)()( 21 csccsc > ),( 2211 ctct ? 2.5 Sentence Evaluation and Extraction A representative event term may associate to more than one sentence.
We extract only one of them as the description of the event.
To this end, sentences are compared according to the significance of the terms in them.
MAX compares the maximum significance scores, while SUM compares the sum of the significance scores.
The sentence with either higher MAX or SUM wins the competition and is picked up as a candidate summary sentence.
If the sentence in the first place has been selected by another term, the one in the second place is chosen.
The ranks of these candidates are the same as the ranks of the terms they are selected for.
Finally, candidate sentences are selected in the summary until the length limitation is reached.
3 Experiments
We evaluate the proposed approaches on DUC 2001 corpus which contains 30 English document sets.
There are 431 event terms on average in each document set.
The automatic evaluation tool, ROUGE (Lin and Hovy, 2003), is run to evaluate the quality of the generated summaries (200 words in length).
The tool presents three values including unigram-based ROUGE-1, bigram-based ROUGE2 and ROUGE-W which is based on longest common subsequence weighted by the length.
Google?s PageRank (Page and Brin, 1998) is one of the most popular ranking algorithms.
It is also graph-based and has been successfully applied in summarization.
Table 1 lists the result of our implementation of PageRank based on event terms.
We then compare it with the results of the event term clustering-based approaches illustrated in Table 2.
PageRank ROUGE-1 0.32749 187 ROUGE-2 0.05670 ROUGE-W 0.11500 Table 1.
Evaluations of PageRank-based Summarization LOCAL+OTAC MAX SUM ROUGE-1 0.32771 0.33243 ROUGE-2 0.05334 0.05569 ROUGE-W 0.11633 0.11718 GLOBAL+OTAC MAX SUM ROUGE-1 0.32549 0.32966 ROUGE-2 0.05254 0.05257 ROUGE-W 0.11670 0.11641 LOCAL+OCAT MAX SUM ROUGE-1 0.33519 0.33397 ROUGE-2 0.05662 0.05869 ROUGE-W 0.11917 0.11849 GLOBAL+OCAT MAX SUM ROUGE-1 0.33568 0.33872 ROUGE-2 0.05506 0.05933 ROUGE-W 0.11795 0.12011 Table 2.
Evaluations of Clustering-based Summarization The experiments show that both assumptions are reasonable.
It is encouraging to find that our event term clustering-based approaches could outperform the PageRank-based approach.
The results based on the second assumption are even better.
This suggests indeed there is a main topic in a DUC 2001 document set.
4 Conclusion
In this paper, we put forward to apply clustering algorithm on the event term graph connected by semantic relations derived from external linguistic resource.
The experiment results based on our two assumptions are encouraging.
Event term clustering-based approaches perform better than PageRank-based approach.
Current approaches simply utilize the degrees of event terms in the graph.
In the future, we would like to further explore and integrate more information derived from documents in order to achieve more significant results using the event term clusteringbased approaches.
Acknowledgments The work described in this paper was fully supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No.
PolyU5181/03E). References Chin-Yew Lin and Eduard Hovy.
2003. Automatic Evaluation of Summaries using N-gram Cooccurrence Statistics.
In Proceedings of HLT/NAACL 2003, pp71-78.
Elena Filatova and Vasileios Hatzivassiloglou.
2004. Event-based Extractive Summarization.
In Proceedings of ACL 2004 Workshop on Summarization, pp104-111.
Hongyuan Zha.
2002. Generic Summarization and keyphrase Extraction using Mutual Reinforcement Principle and Sentence Clustering.
In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, 2002.
pp113-120. Lawrence Page and Sergey Brin, Motwani Rajeev and Winograd Terry.
1998. The PageRank CitationRanking: Bring Order to the Web.
Technical Report,Stanford University.
Martin Easter, Hans-Peter Kriegel, Jrg Sander, et al.1996. A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise.
In Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, Menlo Park, CA, 1996.
226-231. Lawrence Page, Sergey Brin, Rajeev Motwani and Terry Winograd.
1998. The PageRank CitationRanking: Bring Order to the Web.
Technical Report,Stanford University.
Timothy Chklovski and Patrick Pantel.
2004. VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations.
In Proceedings of Conference on Empirical Methods in Natural Language Processing, 2004.
Vasileios Hatzivassiloglou, Judith L.
Klavans, Melissa L.
Holcombe, et al.2001. Simfinder: A Flexible Clustering Tool for Summarization.
In Workshop on Automatic Summarization, NAACL, 2001.
Wenjie Li, Wei Xu, Mingli Wu, et al.2006. Extractive Summarization using Interand Intra-Event Relevance.
In Proceedings of ACL 2006, pp369-376.
Yi Guo and George Stylios.
2005. An intelligent summarization system based on cognitive psychology.
Journal of Information Sciences, Volume 174, Issue 1-2, Jun.
2005, pp1-36 .
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 189??92, Prague, June 2007.
c2007 Association for Computational Linguistics Machine Translation between Turkic Languages A.
Cuneyd TANTU?G Istanbul Technical University Istanbul, Turkey tantug@itu.edu.tr Esref ADALI Istanbul Technical University Istanbul, Turkey adali@itu.edu.tr Kemal OFLAZER Sabanci University Istanbul, Turkey oflazer@sabanciuniv.edu Abstract We present an approach to MT between Turkic languages and present results from an implementation of a MT system from Turkmen to Turkish.
Our approach relies on ambiguous lexical and morphological transfer augmented with target side rule-based repairs and rescoring with statistical language models.
1 Introduction
Machine translation is certainly one of the toughest problems in natural language processing.
It is generally accepted however that machine translation between close or related languages is simpler than full-fledged translation between languages that differ substantially in morphological and syntactic structure.
In this paper, we present a machine translation system from Turkmen to Turkish, both of which belong to the Turkic language family.
Turkic languages essentially exhibit the same characteristics at the morphological and syntactic levels.
However, except for a few pairs, the languages are not mutually intelligible owing to substantial divergences in their lexicons possibly due to different regional and historical influences.
Such divergences at the lexical level along with many but minor divergences at morphological and syntactic levels make the translation problem rather non-trivial.
Our approach is based on essentially morphological processing, and direct lexical and morphological transfer, augmented with substantial multi-word processing on the source language side and statistical processing on the target side where data for statistical language modelling is more readily available.
2 Related
Work Studies on machine translation between close languages are generally concentrated around certain Slavic languages (e.g., Czech?Slovak, Czech?Polish, Czech?Lithuanian (Hajic et al., 2003)) and languages spoken in the Iberian Peninsula (e.g., Spanish?Catalan (Canals et al., 2000), Spanish?Galician (Corbi-Bellot et al., 2003) and Spanish?Portugese (Garrido-Alenda et al., 2003).
Most of these implementations use similar modules: a morphological analyzer, a part-of-speech tagger, a bilingual transfer dictionary and a morphological generator.
Except for the Czech?Lithuanian system which uses a shallow parser, syntactic parsing is not necessary in most cases because of the similarities in word orders.
Also, the lexical semantic ambiguity is usually preserved so, none of these systems has any module for handling the lexical ambiguity.
For Turkic languages, Hamzao?glu (1993) has developed a system from Turkish to Azerbaijani, and Altntas (2000) has developed a system from Turkish to Crimean Tatar.
3 Turkic
Languages Turkic languages, spoken by more than 180 million people, constitutes subfamily of Ural-Altaic languages and includes languages like Turkish, Azerbaijani, Turkmen, Uzbek, Kyrghyz, Kazakh, Tatar, Uyghur and many more.
All Turkic languages have very productive inflectional and derivational agglutinative morphology.
For example the Turkish word evlerimizden has three inflectional morphemes attached to a noun root ev (house), for the plural form with second person plural possessive agreement and ablative case: 189 evlerimizden (from our houses) ev+ler+imiz+den ev+Noun+A3pl+P1sg+Abl All Turkic languages exhibit SOV constituent order but depending on discourse requirements, constituents can be in any order without any substantial formal constraints.
Syntactic structures between Turkic languages are more or less parallel though there are interesting divergences due to mismatches in multi-word or idiomatic constructions.
4 Approach
Our approach is based on a direct morphological transfer with some local multi-word processing on the source language side, and statistical disambiguation on the target language side.
The main steps of our model are: 1.
Source Language (SL) Morphological Analysis 2.
SL Morphological Disambiguation 3.
Multi-Word Unit (MWU) Recognizer 4.
Morphological Transfer 5.
Root Word Transfer 6.
Statistical Disambiguation and Rescoring (SLM) 7.
Sentence Level Rules (SLR) 8.
Target Language (TL) Morphological Generator Steps other than 3, 6 and 7 are the minimum requirements for a direct morphological translation model (henceforth, the baseline system).
The MWU Recognizer, SLM and SLR modules are additional modules for the baseline system to improve the translation quality.
Sourcelanguagemorphologicalanalysismayproduce multiple interpretation of a source word, and usually, depending on the ambiguities brought about by multiple possible segmentations into root and suffixes, there may be different root words of possibly different parts-of-speech for the same word form.
Furthermore, each root word thus produced may map to multiple target root words due to word sense ambiguity.
Hence, among all possible sentences that can be generated with these ambiguities, the most probable one is selected by using various types of SLMs that are trained on target language corpora annotated with disambiguated roots and morphological features.
MWU processing in Turkic languages involves more than the usual lexicalized collocations and involves detection of mostly unlexicalized intraword morphological patterns (Oflazer et al., 2004).
Source MWUs are recognized and marked during source analysis and the root word transfer module maps these either to target MWU patterns, or directly translates when there is a divergence.
Morphological transfer is implemented by a set of rules hand-crafted using the contrastive knowledge between the selected language pair.
Although the syntactic structures are very similar between Turkic languages, there are quite many minor situations where target morphological features marking features such as subject-verb agreement have to be recovered when such features are not present in the source.
Furthermore, occasionally certain phrases have to be rearranged.
Finally, a morphological generator produces the surface forms of the lexical forms in the sentence.
5 Turkmen
to Turkish MT System The first implementation of our approach is from Turkmen to Turkish.
A general diagram of our MT system is presented in Figure 1.
The morphological analysis on the Turkmen side is performed by a two-level morphological analyzer developed using Xerox finite state tools (Tantu?g et al., 2006).
It takes a Turkmen word and produces all possible morphological interpretations of that word.
A simple experiment on our test set indicates that the average Turkmen word gets about 1.55 analyses.
The multiword recognition module operates on the output of the morphological analyzer and wherever applicable, combines analyses of multiple tokens into a new analysis with appropriate morphological features.
One side effect of multi-word processing is a small reduction in morphological ambiguity, as when such unitsarecombined, theremainingmorphologicalinterpretations for these tokens are deleted.
The actual transfer is carried out by transferring the morphological structures and word roots from the source language to the target language maintaining any ambiguity in the process.
These are implemented with finite state transducers that are compiled from replace rules written in the Xerox regular expressionlanguage.1 Averysimpleexampleofthis transfer is shown in Figure 2.2 1The current implementation employs 28 replace rules for morphological feature transfer and 19 rules for sentence level processing.
2+Pos:Positive polarity, +A3sg: 3rd person singular agreement, +Inf1,+Inf2: infinitive markers, +P3sg, +Pnon: possessive agreement markers, +Nom,+Acc: Nominative and ac190 Figure 1: Main blocks of the translation system osmegi ??
Source Morphological Analysis ??
os+Verb+Pos?DB+Noun+Inf1+A3sg+P3sg+Nom os+Verb+Pos?DB+Noun+Inf1+A3sg+Pnon+Acc ??
Source-to-Target Morphological Feature Transfer ??
os+Verb+Pos?DB+Noun+Inf2+A3sg+P3sg+Nom os+Verb+Pos?DB+Noun+Inf2+A3sg+Pnon+Acc ??
Source-to-Target Root word Transfer ?? ilerle+Verb+Pos?DB+Noun+Inf2+A3sg+P3sg+Nom ilerle+Verb+Pos?DB+Noun+Inf2+A3sg+Pnon+Acc buyu+Verb+Pos?DB+Noun+Inf2+A3sg+P3sg+Nom buyu+Verb+Pos?DB+Noun+Inf2+A3sg+Pnon+Acc ??
Target Morphological Generation ?? ilerlemesi (the progress of (something)) ilerlemeyi (the progress (as direct object)) buyumesi (the growth of (something)) buyumeyi (the growth (as direct object)) Figure 2: Word transfer In this example, once the morphological analysis is produced, first we do a morphological feature transfer mapping.
In this case, the only interesting mapping is the change of the infinitive marker.
The source root verb is then ambiguously mapped to two verbs on the Turkish side.
Finally, the Turkish surface form is generated by the morphological generator.
Note that all the morphological processing details such as vowel harmony resolution (a morphographemic process common to all Turkic languages though not in identical ways) are localized to morphological generation.
Root word transfer is also based on a large transcusative case markers.
ducer compiled from bilingual dictionaries which contain many-to-many mappings.
During mapping this transducer takes into account the source root word POS.3 In some rare cases, mapping the word root is not sufficient to generate a legal Turkish lexical structure, as sometimes a required feature on the target side may not be explicitly available on the source word to generate a proper word.
In order to produce the correct mapping in such cases, some additional lexicalized rules look at a wider context and infer any needed features.
While the output of morphological feature transfermoduleisusuallyunambiguous, ambiguityarises during the root word transfer phase.
We attempt to resolve this ambiguity on the target language side using statistical language models.
This however presents additional difficulties as any statistical language model for Turkish (and possibly other Turkic languages) which is built by using the surface forms suffers from data sparsity problems.
This is due to agglutinative morphology whereby a root word may give rise to too many inflected forms (about a hundred inflected forms for nouns and much more for verbs; when productive derivations are considered these numbers grow substantially!).
Therefore, instead of building statistical language models on full word forms, we work with morphologically analyzed and disambiguated target language corpora.
For example, we use a language model that is only based on the (disambiguated) root words to disambiguate ambiguous root words that arise from root 3Statistics on the test set indicate that on the average each source language root word maps to about 2 target language root words.
191 word transfer.
We also employ a language model which is trained on the last set of inflectional features of morphological parses (hence does not involve any root words.) Although word-by-word translation can produce reasonably high quality translations, but in many cases, it is also the source of many translation errors.
To alleviate the shortcomings of the word-by-word translation approach, we resort to a series of rules that operate across the whole sentence.
Such rules operate on the lexical and surface representation of the output sentence.
For example, when the source language is missing a subject agreement marker on a verb, this feature can not be transferred to the target language and the target language generator will fail to generate the appropriate word.
We use some simple heuristics that try to recover the agreement information from any overt pronominal subject in nominative case, and that failing, set the agreement to 3rd person singular.
Some sentence level rules require surface forms because this set of rules usuallymakeorthographicchangesaffectedbyprevious word forms.
In the following example, suitable variants of the clitics de and mi must be selected so that vowel harmony with the previous token is preserved.
o de gordu mi?
?o da gordu mu?
(did he see too?) A wide-coverage Turkish morphological analyzer (Oflazer, 1994) made available to be used in reverse direction to generate the surface forms of the translations.
6 Results
and Evaluation We have tracked the progress of our changes to our system using the BLEU metric (Papineni et al., 2004), though it has serious drawbacks for agglutinative and free constituent order languages.
The performance of the baseline system (all steps above, except 3, 6, and 7) and systems with additional modules are given in Table 1 for a set of 254 Turkmen sentences with 2 reference translations each.
As seen in the table, each module contributes to the performance of the baseline system.
Furthermore, a manual investigation of the outputs indicates that the actual quality of the translations is higher than the one indicated by the BLEU score.4 The errors mostly stem from the statical language models 4Therearemanytranslationswhichpreservethesamemeaning with the references but get low BLEU scores.
not doing a good job at selecting the right root words and/or the right morphological features.
System BLEU Score Baseline 26.57 Baseline + MWU 28.45 Baseline + MWU + SLM 31.37 Baseline + MWU + SLM + SLR 33.34 Table 1: BLEU Scores 7 Conclusions We have presented an MT system architecture between Turkic languages using morphological transfer coupled with target side language modelling and results from a Turkmen to Turkish system.
The results are quite positive but there is quite some room for improvement.
Our current work involves improving the quality of our current system as well as expanding this approach to Azerbaijani and Uyghur.
Acknowledgments This work was partially supported by Project 106E048 funded by The Scientific and Technical Research Council of Turkey.
Kemal Oflazer acknowledges the kind support of LTI at Carnegie Mellon University, where he was a sabbatical visitor during the academic year 2006 ??2007.
References A.
Cuneyd Tantu?g, Esref Adal, Kemal Oflazer.
2006. Computer Analysis of the Turkmen Language Morphology.
Fin-TAL, Lecture Notes in Computer Science, 4139:186-193.
A. Garrido-Alenda et al.2003. Shallow Parsing for Portuguese-Spanish Machine Translation.
in TASHA 2003: Workshop on Tagging and Shallow Processing of Portuguese, Lisbon, Portugal.
A. M.
Corbi-Bellot et al.2005. An open-source shallow-transfer machine translation engine for the Romance languages of Spain.
in 10th EAMT conference ?Practical applications of machine translation??
Budapest, Hungary.
Jan Hajic, Petr Homola, Vladislav Kubon.
2003. A simple multilingual machine translation system.
MT Summit IX.
?Ilker Hamzao?glu.
1993. Machine translation from Turkish to other Turkic languages and an implementation for the Azeri language.
MSc Thesis, Bogazici University, Istanbul.
Kemal Altntas.
2000. Turkish to Crimean Tatar Machine Translation System.
MSc Thesis, Bilkent University, Ankara.
Kemal Oflazer.
1994. Two-level description of Turkish morphology.
Literary and Linguistic Computing, 9(2).
Kemal Oflazer, Ozlem Cetino?glu, Bilge Say.
2004. Integrating Morphology with Multi-word Expression Processing in Turkish.
The ACL 2004 Workshop on Multiword Expressions: Integrating Processing.
Kishore Papineni et al.2002. BLEU : A Method for Automatic Evaluation of Machine Translation.
Association of Computational Linguistics, ACL??2.
Raul Canals-Marote et al.2000. interNOSTRUM: a Spanish-Catalan Machine Translation System.
Machine Translation Review, 11:21-25.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 193??96, Prague, June 2007.
c2007 Association for Computational Linguistics Measuring Importance and Query Relevance in Topic-focused Multi-document Summarization Surabhi Gupta and Ani Nenkova and Dan Jurafsky Stanford University Stanford, CA 94305 surabhi@cs.stanford.edu, {anenkova,jurafsky}@stanford.edu Abstract The increasing complexity of summarization systems makes it difficult to analyze exactly which modules make a difference in performance.
We carried out a principled comparison between the two most commonly used schemes for assigning importance to words in the context of query focused multi-document summarization: raw frequency (word probability) and log-likelihood ratio.
We demonstrate that the advantages of log-likelihood ratio come from its known distributional properties which allow for the identification of a set of words that in its entirety defines the aboutness of the input.
We also find that LLR is more suitable for query-focused summarization since, unlike raw frequency, it is more sensitive to the integration of the information need defined by the user.
1 Introduction
Recently the task of multi-document summarization in response to a complex user query has received considerable attention.
In generic summarization, the summary is meant to give an overview of the information in the documents.
By contrast, when the summary is produced in response to a user query or topic (query-focused, topic-focused, or generally focused summary), the topic/query determines what information is appropriate for inclusion in the summary, making the task potentially more challenging.
In this paper we present an analytical study of two questions regarding aspects of the topic-focused scenario.
First, two estimates of importance on words have been used very successfully both in generic and query-focused summarization: frequency (Luhn, 1958; Nenkova et al., 2006; Vanderwende et al., 2006) and loglikelihood ratio (Lin and Hovy, 2000; Conroy et al., 2006; Lacatusu et al., 2006).
While both schemes have proved to be suitable for summarization, with generally better results from loglikelihood ratio, no study has investigated in what respects and by how much they differ.
Second, there are many little-understood aspects of the differences between generic and query-focused summarization.
For example, we?d like to know if a particular word weighting scheme is more suitable for focused summarization than others.
More significantly, previous studies show that generic and focused systems perform very similarly to each other in query-focused summarization (Nenkova, 2005) and it is of interest to find out why.
To address these questions we examine the two weighting schemes: raw frequency (or word probability estimated from the input), and log-likelihood ratio (LLR) and two of its variants.
These metrics are used to assign importance to individual content words in the input, as we discuss below.
Word probability R(w) = nN, where n is the number of times the word w appeared in the input and N is the total number of words in the input.
Log-likelihood ratio (LLR) The likelihood ratio  (Manning and Schutze, 1999) uses a background corpus to estimate the importance of a word and it is proportional to the mutual information between a word w and the input to be summarized; (w) is defined as the ratio between the probability (under a binomial distribution) of observing w in the input and the background corpus assuming equal probability of occurrence of w in both and the probability of the data assuming different probabilities for w in the input and the background corpus.
LLR with cut-off (LLR(C)) A useful property of the log-likelihood ratio is that the quantity 193 2 log() is asymptotically well approximated by ?2 distribution.
A word appears in the input significantly more often than in the background corpus when 2 log() > 10.
Such words are called signature terms in Lin and Hovy (2000) who were the first to introduce the log-likelihood weighting scheme for summarization.
Each descriptive word is assigned an equal weight and the rest of the words have a weight of zero: R(w) = 1 if ( 2 log((w)) > 10), 0 otherwise.
This weighting scheme has been adopted in several recent generic and topic-focused summarizers (Conroy et al., 2006; Lacatusu et al., 2006).
LLR(CQ) The above three weighting schemes assign a weight to words regardless of the user query and are most appropriate for generic summarization.
When a user query is available, it should inform the summarizer to make the summary more focused.
In Conroy et al.(2006) such query sensititivity is achieved by augmenting LLR(C) with all content words from the user query, each assigned a weight of 1 equal to the weight of words defined by LLR(C) as topic words from the input to the summarizer.
2 Data
We used the data from the 2005 Document Understanding Conference (DUC) for our experiments.
The task is to produce a 250-word summary in response to a topic defined by a user for a total of 50 topics with approximately 25 documents for each marked as relevant by the topic creator.
In computing LLR, the remaining 49 topics were used as a background corpus as is often done by DUC participants.
A sample topic (d301) shows the complexity of the queries: Identify and describe types of organized crime that crosses borders or involves more than one country.
Name the countries involved.
Also identify the perpetrators involved with each type of crime, including both individuals and organizations if possible.
3 The
Experiment In the summarizers we compare here, the various weighting methods we describe above are used to assign importance to individual content words in the input.
The weight or importance of a sentence S in GENERIC FOCUSED Frequency 0.11972 0.11795 (0.11168??.12735) (0.11010??.12521) LLR 0.11223 0.11600 (0.10627??.11873) (0.10915??.12281) LLR(C) 0.11949 0.12201 (0.11249??.12724) (0.11507??.12950) LLR(CQ) not app 0.12546 (.11884??13247) Table 1: SU4 ROUGE recall (and 95% confidence intervals) for runs on the entire input (GENERIC) and on relevant sentences (FOCUSED).
the input is defined as WeightR(S) = summationdisplay w?S R(w) (1) where R(w) assigns a weight for each word w.
For GENERIC summarization, the top scoring sentences in the input are taken to form a generic extractive summary.
In the computation of sentence importance, only nouns, verbs, adjectives and adverbs are considered and a short list of light verbs are excluded: ?has, was, have, are, will, were, do, been, say, said, says??
For FOCUSED summarization, we modify this algorithm merely by running the sentence selection algorithm on only those sentences in the input that are relevent to the user query.
In some previous DUC evaluations, relevant sentences are explicitly marked by annotators and given to systems.
In our version here, a sentence in the input is considered relevant if it contains at least one word from the user query.
For evaluation we use ROUGE (Lin, 2004) SU4 recall metric1, which was among the official automatic evaluation metrics for DUC.
4 Results
The results are shown in Table 1.
The focused summarizer using LLR(CQ) is the best, and it significantly outperforms the focused summarizer based on frequency.
Also, LLR (using log-likelihood ratio to assign weights to all words) perfroms significantly worse than LLR(C).
We can observe some trends even from the results for which there is no significance.
Both LLR and LLR(C) are sensitive to the introduction of topic relevance, producing somewhat better summaries in the FOCUSED scenario 1-n 2 -x -m -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0 -d 194 compared to the GENERIC scenario.
This is not the case for the frequency summarizer, where using only the relevant sentences has a negative impact.
4.1 Focused
summarization: do we need query expansion?
In the FOCUSED condition there was little (for LLR weighting) or no (for frequency) improvement over GENERIC.
One possible explanation for the lack of clear improvement in the FOCUSED setting is that there are not enough relevant sentences, making it impossible to get stable estimates of word importance.
Alternatively, it could be the case that many of the sentences are relevant, so estimates from the relevant portion of the input are about the same as those from the entire input.
To distinguish between these two hypotheses, we conducted an oracle experiment.
We modified the FOCUSED condition by expanding the topic words from the user query with all content words from any of the human-written summaries for the topic.
This increases the number of relevant sentences for each topic.
No automatic method for query expansion can be expected to give more accurate results, since the content of the human summaries is a direct indication of what information in the input was important and relevant and, moreover, the ROUGE evaluation metric is based on direct n-gram comparison with these human summaries.
Even under these conditions there was no significant improvement for the summarizers, each getting better by 0.002: the frequency summarizer gets R-SU4 of 0.12048 and the LLR(CQ) summarizer achieves R-SU4 of 0.12717.
These results seem to suggest that considering the content words in the user topic results in enough relevant sentences.
Indeed, Table 2 shows the minimum, maximum and average percentage of relevant sentences in the input (containing at least one content words from the user the query), both as defined by the original query and by the oracle query expansion.
It is clear from the table that, on average, over half of the input comprises sentences that are relevant to the user topic.
Oracle query expansion makes the number of relevant sentences almost equivalent to the input size and it is thus not surprising that the corresponding results for content selection are nearly identical to the query independent Original query Oracle query expansion Min 13% 52% Average 57% 86% Max 82% 98% Table 2: Percentage of relevant sentences (containing words from the user query) in the input.
The oracle query expansion considers all content words form human summaries of the input as query words.
runs of generic summaries for the entire input.
These numbers indictate that rather than finding ways for query expansion, it might instead be more important to find techniques for constraining the query, determining which parts of the input are directly related to the user questions.
Such techniques have been described in the recent multi-strategy approach of Lacatusu et al.(2006) for example, where one of the strategies breaks down the user topic into smaller questions that are answered using robust question-answering techniques.
4.2 Why
is log-likelihood ratio better than frequency?
Frequency and log-likelihood ratio weighting for content words produce similar results when applied to rank all words in the input, while the cut-off for topicality in LLR(C) does have a positive impact on content selection.
A closer look at the two weighting schemes confirms that when cut-off is not used, similar weighting of content words is produced.
The Spearman correlation coefficient between the weights for words assigned by the two schemes is on average 0.64.
At the same time, it is likely that the weights of sentences are dominated by only the top most highly weighted words.
In order to see to what extent the two schemes identify the same or different words as the most important ones, we computed the overlap between the 250 most highly weighted words according to LLR and frequency.
The average overlap across the 50 sets was quite large, 70%.
To illustrate the degree of overlap, we list below are the most highly weighted words according to each weighting scheme for our sample topic concerning crimes across borders.
LLR drug, cocaine, traffickers, cartel, police, crime, enforcement, u.s., smuggling, trafficking, arrested, government, seized, year, drugs, organised, heroin, criminal, cartels, last, 195 official, country, law, border, kilos, arrest, more, mexican, laundering, officials, money, accounts, charges, authorities, corruption, anti-drug, international, banks, operations, seizures, federal, italian, smugglers, dealers, narcotics, criminals, tons, most, planes, customs Frequency drug, cocaine, officials, police, more, last, government, year, cartel, traffickers, u.s., other, drugs, enforcement, crime, money, country, arrested, federal, most, now, trafficking, seized, law, years, new, charges, smuggling, being, official, organised, international, former, authorities, only, criminal, border, people, countries, state, world, trade, first, mexican, many, accounts, according, bank, heroin, cartels It becomes clear that the advantage of likelihood ratio as a weighting scheme does not come from major differences in overall weights it assigns to words compared to frequency.
It is the significance cut-off for the likelihood ratio that leads to noticeable improvement (see Table 1).
When this weighting scheme is augmented by adding a score of 1 for content words that appear in the user topic, the summaries improve even further (LLR(CQ)).
Half of the improvement can be attributed to the cut-off (LLR(C)), and the other half to focusing the summary using the information from the user query (LLR(CQ)).
The advantage of likelihood ratio comes from its providing a principled criterion for deciding which words are truly descriptive of the input and which are not.
Raw frequency provides no such cut-off.
5 Conclusions
In this paper we examined two weighting schemes for estimating word importance that have been successfully used in current systems but have not todate been directly compared.
Our analysis confirmed that log-likelihood ratio leads to better results, but not because it defines a more accurate assignment of importance than raw frequency.
Rather, its power comes from the use of a known distribution that makes it possible to determine which words are truly descriptive of the input.
Only when such words are viewed as equally important in defining the topic does this weighting scheme show improved performance.
Using the significance cut-off and considering all words above it equally important is key.
Log-likelihood ratio summarizer is more sensitive to topicality or relevance and produces summaries that are better when it take the user request into account than when it does not.
This is not the case for a summarizer based on frequency.
At the same time it is noteworthy that the generic summarizers perform about as well as their focused counterparts.
This may be related to our discovery that on average 57% of the sentences in the document are relevant and that ideal query expansion leads to a situation in which almost all sentences in the input become relevant.
These facts could be an unplanned side-effect from the way the test topics were produced: annotators might have been influenced by information in the input to be summarizied when defining their topic.
Such observations also suggest that a competitive generic summarizer would be an appropriate baseline for the topicfocused task in future DUCs.
In addition, including some irrelavant documents in the input might make the task more challenging and allow more room for advances in query expansion and other summary focusing techniques.
References J.
Conroy, J.
Schlesinger, and D.
O?Leary. 2006.
Topic-focused multi-document summarization using an approximate oracle score.
In Proceedings of the COLING/ACL??6 (Poster Session).
F. Lacatusu, A.
Hickl, K.
Roberts, Y.
Shi, J.
Bensley, B.
Rink, P.
Wang, and L.
Taylor. 2006.
Lcc?s gistexter at duc 2006: Multi-strategy multi-document summarization.
In Proceedings of DUC??6.
C. Lin and E.
Hovy. 2000.
The automated acquisition of topic signatures for text summarization.
In Proceedings of COLING??0.
C. Lin.
2004. Rouge: a package for automatic evaluation of summaries.
In Proceedings of the Workshop on Text Summarization Branches Out (WAS 2004).
H. P.
Luhn. 1958.
The automatic creation of literature abstracts.
IBM Journal of Research and Development, 2(2):159??65.
C. Manning and H.
Schutze. 1999.
Foundations of Statistical Natural Language Processing.
MIT Press.
A. Nenkova, L.
Vanderwende, and K.
McKeown. 2006.
A compositional context sensitive multi-document summarizer: Exploring the factors that influence summarization.
In Proceedings of ACM SIGIR??6.
A. Nenkova.
2005. Automatic text summarization of newswire: lessons learned from the document understanding conference.
In Proceedings of AAAI??5.
L. Vanderwende, H.
Suzuki, and C.
Brockett. 2006.
Microsoft research at duc 2006: Task-focused summarization with sentence simplification and lexical expansion.
In Proceedings of DUC??6.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 197??00, Prague, June 2007.
c2007 Association for Computational Linguistics Expanding Indonesian-Japanese Small Translation Dictionary Using a Pivot Language Masatoshi Tsuchiya??Ayu Purwarianti??Toshiyuki Wakita??Seiichi Nakagawa??
?Information and Media Center / ?Department of Information and Computer Sciences, Toyohashi University of Technology tsuchiya@imc.tut.ac.jp, {wakita,ayu,nakagawa}@slp.ics.tut.ac.jp Abstract We propose a novel method to expand a small existing translation dictionary to a largetranslationdictionaryusingapivotlanguage.
Our method depends on the assumption that it is possible to nd a pivot language for a given language pair on condition that there are both a large translation dictionary from the source language to the pivot language, and a large translation dictionary from the pivot language to the destination language.
Experiments that expands the Indonesian-Japanese dictionary using the English language as a pivot language shows that the proposed method can improveperformanceofarealCLIRsystem.
1 Introduction
Rich cross lingual resources including large translation dictionaries are necessary in order to realize working cross-lingual NLP applications.
However, it is infeasible to build such resources for all languagepairs, becausetherearemanylanguagesinthe world.
Actually, while rich resources are available for several popular language pairs like the English language and the Japanese language, poor resources are only available for rest unfamiliar language pairs.
In order to resolve this situation, automatic construction of translation dictionary is effective, but it is quite difcult as widely known.
We, therefore, concentrateonthetaskofexpandingasmallexisting translation dictionary instead of it.
Let us consider three dictionaries: a small seed dictionary which consists of headwords in the source language and their translations in the destination language, a large source-pivotdictionarywhichconsistsofheadwords in the source language and their translations in the pivot language, and a large pivot-destination dictionary which consists of headwords in the pivot language and their translations in the destination language.
When these three dictionaries are given, expanding the seed dictionary is to translate words in the source language that meets two conditions: (1) they are not contained in the seed dictionary, and (2) they can be translated to the destination language transitively referring both the source-pivot dictionary and the pivot-destination dictionary.
Obviously, this task depends on two assumptions: (a) the existence of the small seed dictionary, and (b) the existence of the pivot language which meets the condition that there are both a large sourcepivot dictionary and a large pivot-destination dictionary.
Because of the rst assumption, it is true that this task cannot be applied to a brand-new language pair.
However, the number of such brandnew language pairs are decreasing while machinereadable language resources are increasing.
Moreover, The second assumption is valid for many language pairs, when supposing the English language as a pivot.
From these point of view, we think that theexpansiontaskismorepromising,althoughitdepends more assumptions than the construction task.
There are two different points among the expansion task and the construction task.
Previous researches of the construction task can be classied into two groups.
The rst group consists of researchestoconstructanewtranslationdictionaryfor a fresh language pair from existing translation dictionaries or other language resources (Tanaka and Umemura, 1994).
In the rst group, information of the seed dictionary are not counted in them unlike the expansion task, because it is assumed that there is no seed dictionary for such fresh language pairs.
The second group consists of researches to translate 197 xs v(xs) vt(xs) ys zs u(zs) Corpus in the source Source-Pivot Dictionary PivotDestination Dictionary Corpus inthe destination Seed Dictionary Select output words Figure 1: Translation Procedure novel words using both a large existing translation dictionary and other linguistic resources like huge parallel corpora (Tonoike et al., 2005).
Because almost of novel words are nouns, these researches focus into the task of translating nouns.
In the expansion task, however, it is necessary to translate verbs and adjectives as well as nouns, because a seed dictionary will be so small that only basic words will be contained in it if the target language pair is unfamiliar.
We will discuss about this topic in Section 3.2.
The remainder of this paper is organised as follows: Section 2 describes the method to expand a small seed dictionary.
The experiments presented in Section 3 shows that the proposed method can improve performance of a real CLIR system.
This paper ends with concluding remarks in Section 4.
2 Method
of Expanding Seed Dictionary The proposed method roughly consists of two steps shown in Figure 1.
The rst step is to generate a cooccurrence vector on the destination language corresponding to an input word, using both the seed dictionary and a monolingual corpus in the source language.
The second step is to list translation candidatesup,referringboththesource-pivotdictionary and the pivot-destination dictionary, and to calculate their co-occurrence vectors based on a monolingual corpus in the destination.
The seed dictionary is used to convert a cooccurrence vector in the source language into a vector in the destination language.
In this paper, f(wi,wj) represents a co-occurrence frequency of a word wi and a word wj for all languages.
A cooccurrence vector v(xs) of a word xs in the source is: v(xs) = (f(xs,x1),...,f(xs,xn)), (1) where xi(i = 1,2,...,n) is a headword of the seed dictionary D.
A co-occurrence vector v(xs), whose each element is corresponding to a word in the source, is converted into a vector vt(xs), whose each element is corresponding to a word in the destination, referring the dictionary D: vt(xs) = (ft(xs,z1),...,ft(xs,zm)), (2) where zj(j = 1,2,...,m) is a translation word which appears in the dictionary D.
The function ft(xs,zk), whichassignsaco-occurrencedegreebetween a word xs and a word zj in the destination based on a co-occurrence vector of a word xs in the source, is dened as follows: ft(xs,zj) = n?? i=1 f(xs,xi)  (xi,zj).
(3) where (xi,zj) is equal to one when a word zj is included in a translation word set D(xi), which consists of translation words of a word xi, and zero otherwise.
A set of description sentences Ys in the pivot are obtained referring the source-pivot dictionary for a word xs.
After that, a description sentence ys ??Ys in the pivot is converted to a set of description sentences Zs in the destination referring the pivot-destination dictionary.
A co-occurrence vector against a candidate description sentencezs = z1sz2s zls, which is an instance of Zs, is calculated by this equation: u(zs) = parenleftBigg l?? k=1 f(zks,z1),..., l?? k=1 f(zks,zm) ) (4) Finally, the candidate zs which meets a certain condition is selected as an output.
Two conditions are examined in this paper: (1) selecting top-n candidatesfromsortedonesaccordingtoeachsimilarity score, and (2) selecting candidates whose similarity scoresaregreaterthanacertainthreshold.
Inthispaper, cosine distance s(vt(xs),u(zs)) between a vector based on an input word xs and a vector based on 198 acandidatezs isusedasthesimilarityscorebetween them.
3 Experiments
In this section, we present the experiments of the proposed method that the Indonesian language, the English language and the Japanese language are adopted as the source language, the pivot language and the destination language respectively.
3.1 Experimental
Data The proposed method depends on three translation dictionaries and two monolingual corpora as described in Section 2.
Mainichi Newspaper Corpus (1993??995), which contains 3.5M sentences consist of 140M words, is used as the Japanese corpus.
When measuring similarity between words using co-occurrence vectors, it is common that a corpus in the source language for the similar domain to one of the corpus in the source languageismoresuitablethanoneforadifferentdomain.
Unfortunately, becausewecouldnotndsuch corpus, the articles which were downloaded from the Indonesian Newspaper WEB sites1 are used as the Indonesian corpus.
It contains 1.3M sentences, which are tokenized into 10M words.
An online Indonesian-Japanese dictionary2 contains 10,172 headwords, however, only 6,577 headwords of them appear in the Indonesian corpus.
We divide them into two sets: the rst set which consists of 6,077 entries is used as the seed dictionary, and the second set which consists of 500 entries is used to evaluate translation performance.
Moreover, an online Indonesian-English dictionary3, and an English-Japanese dictionary(Michibata, 2002) are also used as the source-pivot dictionary and the pivot-destination dictionary.
3.2 Evaluation
of Translation Performance As described in Section 2, two conditions of selecting output words among candidates are examined.
Table 1 shows their performances and the baseline, 1http://www.kompas.com/, http://www.tempointeraktif.com/ 2http://m1.ryu.titech.ac.jp/?indonesia/ todai/dokumen/kamusjpina.pdf 3http://nlp.aia.bppt.go.id/kebi that is the translation performance when all candidates are selected as output words.
It is revealed that the condition of selecting top-n candidates outperforms the another condition and the baseline.
The maximum F=1 value of 52.5% is achieved when selecting top-3 candidates as output words.
Table2showsthatthelexicaldistributionofheadwordscontainedintheseeddictionaryarequitesimilar to the lexical distribution of headwords contained in the source-pivot dictionary.
This observation means that it is necessary to translate verbs andadjectivesaswellasnouns,whenexpandingthis seed dictionary.
Table 3 shows translation performances against nouns, verbs and adjectives, when selecting top-3 candidates as output words.
The proposed method can be regarded likely because it is effective to verbs and adjectives as well as to nouns, whereas the baseline precision of verbs is considerably lower than the others.
3.3 CLIR
Performance Improved by Expanded Dictionary In this section, performance impact is presented when the dictionary expanded by the proposed method is adopted to the real CLIR system proposed in (Purwarianti et al., 2007).
NTCIR3 Web Retrieval Task(Eguchi et al., 2003) provides the evaluation dataset and denes the evaluation metric.
The evaluation metric consists of four MAP values: PC, PL, RC and RL.
They are corresponding to assessment types respectively.
The dataset consists 100GB Japanese WEB documents and 47 queries of Japanese topics.
The Indonesian queries, which are manually translated from them, are used as inputs of the experiment systems.
The number of unique words which occur in the queries is 301, and the number of unique words which are not contained in the Indonesian-Japanese dictionary is 106 (35%).
It is reduced to 78 (26%), while the existingdictionarythatcontains10,172entriesisexpanded to the dictionary containing 20,457 entries with the proposed method.
Table 4 shows the MAP values achieved by both the baseline systems using the existing dictionary and ones using the expanded dictionary.
The former three systems use existing dictionaries, and the latter three systems use the expanded one.
The 3rd system translates keywords transitively using both 199 Table 1: Comparison between Conditions of Selecting Output Words Selecting top-n candidates Selecting plausible candidates Baseline n = 1 n = 2 n = 3 n = 5 n = 10 x = 0.1 x = 0.16 x = 0.2 x = 0.3 Prec.
55.4% 49.9% 46.2% 40.0% 32.2% 20.8% 23.6% 25.8% 33.0% 18.9% Rec.
40.9% 52.6% 60.7% 67.4% 74.8% 65.3% 50.1% 40.0% 16.9% 82.5% F=1 47.1% 51.2% 52.5% 50.2% 45.0% 31.6% 32.1% 31.4% 22.4% 30.8% Table 2: Lexical Classication of Headwords IndonesianIndonesianJapanese English # of nouns 4085 (57.4%) 15718 (53.5%) # of verbs 1910 (26.8%) 9600 (32.7%) # of adjectives 795 (11.2%) 3390 (11.5%) # of other words 330 (4.6%) 682 (2.3%) Total 7120 (100%) 29390 (100%) Table 3: Performance for Nouns, Verbs and Adjectives Noun Verb Adjective n = 3 Baseline n = 3 Baseline n = 3 Baseline Prec.
49.1% 21.8% 41.0% 14.7% 46.9% 26.7% Rec.
65.6% 80.6% 52.3% 84.1% 59.4% 88.4% F=1 56.2% 34.3% 46.0% 25.0% 52.4% 41.0% Table 4: CLIR Performance PC PL RC RL (1) Existing Indonesian-Japanese dictionary 0.044 0.044 0.037 0.037 (2) Existing Indonesian-Japanese dictionary and Japanese proper name dictionary 0.054 0.052 0.047 0.045 (3) Indonesian-English-Japanese transitive translation with statistic ltering 0.078 0.072 0.055 0.053 (4) Expanded Indonesian-Japanese dictionary 0.061 0.059 0.046 0.046 (5) Expanded Indonesian-Japanese dictionary with Japanese proper name dictionary 0.066 0.063 0.049 0.049 (6) Expanded Indonesian-Japanese dictionary with Japanese proper name dictionary and statistic ltering 0.074 0.072 0.059 0.058 the source-pivot dictionary and the pivot-destination dictionary, and the others translate keywords using either the existing source-destination dictionary or the expanded one.
The 3rd system and the 6th system try to eliminate unnecessary translations based statistic measures calculated from retrieved documents.
These measures are effective as shown in (Purwarianti et al., 2007), but, consume a high runtime computational cost to reduce enormous translation candidates statistically.
It is revealed that CLIR systems using the expanded dictionary outperform ones using the existing dictionary without statistic ltering.
And more, it shows that ones using the expanded dictionary without statistic ltering achieve near performance to the 3rd system without paying a high run-time computational cost.
Once it is paid, the6thsystemachievesalmostsamescoreofthe3rd system.
These observation leads that we can concludethatourproposedmethodtoexpanddictionary is valuable to a real CLIR system.
4 Concluding
Remarks In this paper, a novel method of expanding a small existing translation dictionary to a large translation dictionary using a pivot language is proposed.
Our method uses information obtained from a small existing translation dictionary from the source languagetothedestinationlanguageeffectively.
Experiments that expands the Indonesian-Japanese dictionary using the English language as a pivot language shows that the proposed method can improve performance of a real CLIR system.
References Koji Eguchi, Keizo Oyama, Emi Ishida, Noriko Kando,, and KazukoKuriyama.
2003. Overview of the web retrieval task at the third NTCIR workshop.
In Proceedings of the Third NTCIR Workshop on research in Information Retrieval, Automatic Text Summarization and Question Answering.
Hideki Michibata, editor.
2002. Eijiro.
ALC, 3.
(in Japanese).
Ayu Purwarianti, Masatoshi Tsuchiya, and Seiichi Nakagawa.
2007. Indonesian-Japanese transitive translation using English for CLIR.
Journal of Natural Language Processing, 14(2), Apr.
Kumiko Tanaka and Kyoji Umemura.
1994. Construction of a bilingual dictionary intermediated by a third language.
In Proceedings of the 15th International Conference on Computational Linguistics.
Masatugu Tonoike, Mitsuhiro Kida, Toshihiro Takagi, Yasuhiro Sasaki, Takehito Utsuro, and Satoshi Sato.
2005. Translation estimation for technical terms using corpus collected from the web.
In Proceedings of the Pacic Association for Computational Linguistics, pages 325??31, August.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 201??04, Prague, June 2007.
c2007 Association for Computational Linguistics Shallow Dependency Labeling Manfred Klenner Institute of Computational Linguistics University of Zurich klenner@cl.unizh.ch Abstract We present a formalization of dependency labeling with Integer Linear Programming.
We focus on the integration of subcategorization into the decision making process, where the various subcategorization frames of a verb compete with each other.
A maximum entropy model provides the weights for ILP optimization.
1 Introduction
Machine learning classifiers are widely used, although they lack one crucial model property: they can?t adhere to prescriptive knowledge.
Take grammatical role (GR) labeling, which is a kind of (shallow) dependency labeling, as an example: chunkverb-pairs are classified according to a GR (cf.
(Buchholz, 1999)).
The trials are independent of each other, thus, local decisions are taken such that e.g. a unique GR of a verb might (erroneously) get multiply instantiated etc.
Moreover, if there are alternative subcategorization frames of a verb, they must not be confused by mixing up GR from different frames to a non-existent one.
Often, a subsequent filter is used to repair such inconsistent solutions.
But usually there are alternative solutions, so the demand for an optimal repair arises.
We apply the optimization method Integer Linear Programming (ILP) to (shallow) dependency labeling in order to generate a globally optimized consistent dependency labeling for a given sentence.
A maximum entropy classifier, trained on vectors with morphological, syntactic and positional information automatically derived from the TIGER treebank (German), supplies probability vectors that are used as weights in the optimization process.
Thus, the probabilities of the classifier do not any longer provide (as usually) the solution (i.e.
by picking out the most probable candidate), but count as probabilistic suggestions to a globally consistent solution.
More formally, the dependency labeling problem is: given a sentence with (i) verbs, a0a2a1, (ii) NP and PP chunks1, a3a5a4, label all pairs (a0a6a1a8a7 a3a9a4a11a10a13a12 a14 a0a6a1a15a7 a3a9a4 ) with a dependency relation (including a class for the null assignment) such that all chunks get attached and for each verb exactly one subcategorization frame is instantiated.
2 Integer
Linear Programming Integer Linear Programming is the name of a class of constraint satisfaction algorithms which are restricted to a numerical representation of the problem to be solved.
The objective is to optimize (e.g.
maximize) a linear equation called the objective function (a) in Fig.
1) given a set of constraints (b) in Fig.
1): a16 a10a18a17a20a19a22a21a11a23a25a24 a14a27a26a29a28a31a30a31a32a31a32a31a32a33a30a34a26a36a35 a10a2a23a38a37a40a39 a28a34a26a29a28a42a41a40a32a31a32a31a32a43a41 a39 a35a44a26a36a35 a45 a10 a16a25a46 a28a47a26a29a28a42a41 a16a44a46a49a48 a26 a48 a41a40a32a31a32a31a32a22a41 a16a25a46 a35a44a26a36a35 a50a51 a52a54a53a37a55 a56a58a57 a59 a45 a46 a30 Figure 1: ILP Specification where, a60a61a37a63a62 a30a31a32a31a32a31a32a22a30a34a64 and a26 a28 a32a31a32a31a32a58a26 a35 are variables, a39 a28a65a32a31a32a31a32 a39 a35, a45 a46 and a16a25a46 a28a66a32a31a32a31a32 a16a44a46 a35 are constants.
For dependency labeling we have: a26a67a35 are binary class variables that indicate the (non-) assignment of a chunk a68 to a dependency relation a69 of a subcat frame a24 of a verb a70. Thus, three indices are needed: a69a72a71a74a73a76a75 . If such an indicator variable a69a77a71a74a73a76a75 is set to 1 in the course of the maximization task, then the dependency label a69 between these chunks is said to hold, otherwise (a69a78a71a74a73a58a75a6a37a80a79 ) it doesn?t hold.
a39 a28a5a32a31a32a31a32 a39 a35 from Fig.1 are interpreted as weights that represent the impact of an assignment.
3 Dependency
Labeling with ILP Given the chunks a3a9a4a11a81 (NP, PP and verbs) of a sentence, each pair a3a9a4a11a81a82a12a83a3a9a4a11a81 is formed.
It can 1Note that we use base chunks instead of heads.
201 a0 a37 a1a2a4a3a5a1 a6 a46 a1a2a4a3a5a1 a6 a7a9a8 a46a11a10 a12 a7a14a13a16a15a18a17a20a19a21a23a22a25a24 a46 a7 (1) a26 a37 a1a27a29a28a30a1 a6 a46 a1a31a4a31a30a1 a6 a7 a15a33a32a34a19a21a25a22a23a35 a46 a7 (2) a0 a37 a1a2a4a3a9a36a37a1 a6 a75 a1a27a38a28a39a1 a6 a73 a6 a40a42a41a33a43 a71a45a44a47a46a49a48a51a50a53a52 a41a4a54 a50a56a55 a22 a69 a71a74a73a58a75 (3) a57 a37 a1a27a29a28a39a1 a6 a46 a1a27a38a28a39a1 a6 a7a16a8 a46a11a10 a12 a7a14a13 a15a30a58 a19a21 a22a60a59 a46 a7 (4) a17a20a19a22a21a11a23 a0 a41 a26 a41 a0 a41 a57 (5) Figure 2: Objective Function stand in one of eight dependency relations, including a pseudo relation representing the null class.
We consider the most important dependency labels: subject (a61 ), direct object (a62 ), indirect object (a63 ), clausal complement (a3 ), prepositional complement (a64 ), attributive (NP or PP) attachment ( a24 ) and adjunct ( a35 ).
Although coarse-grained, this set allows us to capture all functional dependencies and to construct a dependency tree for every sentence in the corpus2.
Technically, indicator variables are used to represent attachment decisions.
Together with a weight, they form the addend of the objective function.
In the case of attributive modifiers or adjuncts (the non-governable labels), the indicator variables correspond to triples.
There are two labels of this type: a24 a46 a7 represents that chunk a65 modifies chunk a60 and a35 a46 a7 represents that chunk a65 is in an adjunct relation to chunk a60 . a0 and a26 are defined as the weighted sum of such pairs (cf.
Eq. 1 and Eq 2.
from Fig.
2), the weights (e.g.
a15a30a17a20a19a21 ) stem from the statistical model.
For subcategorized labels, we have quadruples, consisting of a label name a69, a frame index a24, a verb a70 and a chunk a68 (also verb chunks are allowed as a a68 ): a69 a71a74a73a58a75 . We define a0 to be the weighted sum of all label instantiations of all verbs (and their subcat frames), see Eq.
3 in Fig.
2. The subscript a66 a73 is a list of pairs, where each 2Note that we are not interested in dependencies beyond the (base) chunk level pair consists of a label and a subcat frame index.
This way, a66 a73 represents all subcat frames of a verb a70 . For example, a66 of ?to believe??could be: a67a69a68 a61 a30 a62a71a70 a30a71a68 a62 a30 a62a71a70 a30a71a68 a61 a30a73a72 a70 a30a71a68 a3 a30a74a72 a70 a30a75a68 a61 a30a74a76 a70 a30a75a68 a63 a30a73a76 a70a74a77 . There are three frames, the first one requires a a61 and a a62 . Consider the sentence ?He believes these stories??
We have a78a80a79 =a67 believesa77 and a81a83a82 a81 = a67 He, believes, storiesa77 . Assume a66 a28 to be the a66 of ?to believe??as defined above.
Then, e.g. a84 a48 a28a86a85 a37 a62 represents the assignment of ?stories??as the filler of the subject relation a84 of the second subcat frame of ?believes??
To get a dependency tree, every chunk must find a head (chunk), except the root verb.
We define a root verb a65 as a verb that stands in the relation a57 a46a7 to all other verbs a60 . a57 (cf.
Eq.4 from Fig.2) is the weighted sum of all null assignment decisions.
It is part of the maximization task and thus has an impact (a weight).
The objective function is defined as the sum of equations 1 to 4 (Eq.5 from Fig.2).
So far, our formalization was devoted to the maximization task, i.e. which chunks are in a dependency relation, what is the label and what is the impact.
Without any further (co-occurrence) restrictions, every pair of chunks would get related with every label.
In order to assure a valid linguistic model, constraints have to be formulated.
4 Basic
Global Constraints Every chunk a65 from a81a83a82 (a87a37a88a81a89a82 a81 ) must find a head, that is, be bound either as an attribute, adjunct or a verb complement.
This requires all indicator variables with a65 as the dependent (second index) to sum up to exactly 1.
a1a2a4a3a90a1 a6 a75 a24 a75 a7 a41 a1a27a29a28a39a1 a6 a46 a35 a46 a7 a41 a1a27a38a28a39a1 a6 a73 a6 a40a42a41a33a43 a71a45a44a47a46a49a48 a50 a69 a71a74a73 a7 a37 a62 a30 (6) a91 a65a36a23a13a79a93a92a94a65 a53 a95 a81a83a82 a95 A verb is attached to any other verb either as a clausal object a3 (of some verb frame a24 ) or as a57 (null class) indicating that there is no dependency relation between them.
a57 a46 a7 a41 a6 a40a97a96a98a43 a71a45a44a47a46a49a48 a19 a3 a71 a46 a7 a37 a62 a30 a91 a60 a30 a65 a14 a60a99a87a37a88a65a25a10 a23 a79a93a92a8a60 a30 a65 a53 a95 a78a83a79 a95(7) 202 This does not exclude that a verb gets attached to several verbs as a a3 . We capture this by constraint 8: a1a27a38a28a30a1 a6 a46 a6 a40a97a96a98a43 a71a45a44a47a46a49a48 a19 a3 a71 a46 a7 a53 a62 a30 a91 a65a20a23a13a79 a92a94a65 a53 a95 a78 a79 a95 (8) Another (complementary) constraint is that a dependency label a69 of a verb must have at most one filler.
We first introduce a indicator variable a69 a71a74a73 : a69a72a71a31a73a13a37 a1a2a4a3a25a36a37a1 a6 a75 a69a72a71a74a73a58a75 (9) In order to serve as an indicator of whether a label a69 (of a frame a24 of a verb a70 ) is active or inactive, we restrict a69 a71a74a73 to be at most 1: a69a72a71a74a73 a53 a62 a30 a91 a70 a30 a24 a30 a69 a23a44a79 a92 a70 a53 a95 a78a80a79 a95 a1 a68 a69 a30 a24a38a70a3a2 a66 a73 (10) To illustrate this by the example previously given: the subject of the second verb frame of ?to believe?? is defined as a84 a48 a28 a37 a61 a48 a28a34a28 a41 a61 a48 a28a86a85 (with a84 a48 a28 a53 a62 ).
Either a61 a48 a28a34a28 a37 a62 or a61 a48 a28a86a85 a37 a62 or both are zero, but if one of them is set to one, then a84 a48 a28 = 1.
Moreover, as we show in the next section, the selection of the label indicator variable of a frame enforces the frame to be selected as well3.
5 Subcategorization
as a Global Constraint The problem with the selection among multiple subcat frames is to guarantee a valid distribution of chunks to verb frames.
We don?t want to have chunk a68 a28 be labeled according to verb frame a24 a28 and chunk a68 a48 according to verb frame a24 a48 . Any valid attachment must be coherent (address one verb frame) and complete (select all of its labels).
We introduce an indicator variable a4 a71a74a73 with frame and verb indices.
Since exactly one frame of a verb has to be active at the end, we restrict: a5a7a6 a50 a6 a71 a12 a28 a8 a71a74a73a72a37 a62 a30 a91 a70a67a23 a79a93a92 a70 a53 a95 a78 a79 a95 (11) (a9a10a4 a73 is the number of subcat frames of verb a70 ) However, we would like to couple a verb?s (a70 ) frame (a24 ) to the frame?s label set and restrict it to be active (i.e.
set to one) only if all of its labels are active.
To achieve this, we require equivalence, 3There are more constraints, e.g. that no two chunks can be attached to each other symmetrically (being chunk and modifier of each other at the same time).
We won?t introduce them here.
namely that selecting any label of a frame is equivalent to selecting the frame.
As defined in equation 10, a label is active, if the label indicator variable (a69a72a71a74a73 ) is set to one.
Equivalence is represented by identity, we thus get (cf.
constraint 12): a8 a71a74a73a72a37 a69a72a71a74a73 a30 a91 a70 a30 a24 a30 a69 a23 a79 a92 a70 a53 a95 a78a83a79 a95 a1 a68 a69 a30 a24a38a70a11a2 a66 a73 (12) If any a69 a71a74a73 is set to one (zero), then a4 a71a31a73 is set to one (zero) and all other a69a78a71a31a73 of the same subcat frame are forced to be one (completeness).
Constraint 11 ensures that exactly one subcat frame a4 a71a74a73 can be active (coherence).
6 Maximum
Entropy and ILP Weights A maximum entropy approach was used to induce a probability model that serves as the basis for the ILP weights.
The model was trained on the TIGER treebank (Brants et al., 2002) with feature vectors stemming from the following set of features: the part of speech tags of the two candidate chunks, the distance between them in chunks, the number of intervening verbs, the number of intervening punctuation marks, person, case and number features, the chunks, the direction of the dependency relation (left or right) and a passive/active voice flag.
The output of the maxent model is for each pair of chunks a probability vector, where each entry represents the probability that the two chunks are related by a particular label (a61 a30 a62 a32a31a32a31a32 including a57 ).
7 Empirical
Results A 80% training set (32,000 sentences) resulted in about 700,000 vectors, each vector representing either a proper dependency labeling of two chunks, or a null class pairing.
The accuracy of the maximum entropy classifier was 87.46%.
Since candidate pairs are generated with only a few restrictions, most pairings are null class labelings.
They form the majority class and thus get a strong bias.
If we evaluate the dependency labels, therefore, the results drop appreciably.
The maxent precision then is 62.73% (recall is 85.76%, f-measure is 72.46 %).
Our first experiment was devoted to find out how good our ILP approach was given that the correct subcat frame was pre-selected by an oracle.
Only the decision which pairs are labeled with which dependency label was left to ILP (also the selection and assignment of the non subcategorized labels).
203 There are 8000 sentence with 36,509 labels in the test set; ILP retrieved 37,173; 31,680 were correct.
Overall precision is 85.23%, recall is 86.77%, the f-measure is 85.99% (Fa0a2a1a4a3a6a5 in Fig.
3). Fa0a2a1a4a3a6a5 Fa75a8a7a6a9 a0 Prec Rec F-Mea Prec Rec F-Mea a61 91.4 86.1 88.7 90.3 80.9 85.4 a62 90.4 83.3 86.7 81.4 73.3 77.2 a63 88.5 76.9 82.3 75.8 55.5 64.1 a64 79.3 73.7 76.4 77.8 40.9 55.6 a3 98.6 94.1 96.3 91.4 86.7 89.1 a35 76.7 75.6 76.1 74.5 72.3 73.4 a24 75.7 76.9 76.3 74.1 74.2 74.2 Figure 3: Pre-selected versus Competing Frames The results of the governable labels (a61 down to a3 ) are good, except PP complements (a64 ) with a fmeasure of 76.4%.
The errors made with a4a10a0a2a1a4a3a6a5 : the wrong chunks are deemed to stand in a dependency relation or the wrong label (e.g.
a61 instead of a62 ) was chosen for an otherwise valid pair.
This is not a problem of ILP, but one of the statistical model the weights do not discriminate well.
Improvements of the statistical model will push ILP?s precision.
Clearly, performance drops if we remove the subcat frame oracle letting all subcat frames of a verb compete with each other (Fa75a11a7a12a9 a0, Fig.3).
How close can Fa75a8a7a6a9 a0 come to the oracle setting Fa0a2a1a4a3a13a5 . The overall precision of the Fa75a8a7a6a9 a0 setting is 81.8%, recall is 85.8% and the f-measure is 83.7% (f-measure of Fa0a2a1a4a3a6a5 was 85.9%).
This is not too far away.
We have also evaluated how good our model is at finding the correct subcat frame (as a whole).
First some statistics: In the test set are 23 different subcat frames (types) with 16,137 occurrences (token).
15,239 out of these are cases where the underlying verb has more than one subcat frame (only here do we have a selection problem).
The precision was 71.5%, i.e. the correct subcat frame was selected in 10,896 out of 15,239 cases.
8 Related
Work ILP has been applied to various NLP problems including semantic role labeling (Punyakanok et al., 2004), which is similar to dependency labeling: both can benefit from verb specific information.
Actually, (Punyakanok et al., 2004) take into account to some extent verb specific information.
They disallow argument types a verb does not ?subcategorize for??by setting an occurrence constraint.
However, they do not impose co-occurrence restrictions as we do (allowing for competing subcat frames).
None of the approaches to grammatical role labeling tries to scale up to dependency labeling.
Moreover, they suffer from the problem of inconsistent classifier output (e.g.
(Buchholz, 1999)).
A comparison of the empirical results is difficult, since e.g. the number and type of grammatical/dependency relations differ (the same is true wrt.
German dependency parsers, e.g (Foth et al., 2005)).
However, our model seeks to integrate the (probabilistic) output of such systems and in the best case boosts the results, or at least turn it into a consistent solution.
9 Conclusion
and Future Work We have introduced a model for shallow dependency labeling where data-driven and theory-driven aspects are combined in a principled way.
A classifier provides empirically justified weights, linguistic theory contributes well-motivated global restrictions, both are combined under the regiment of optimization.
The empirical results of our approach are promising.
However, we have made idealized assumptions (small inventory of dependency relations and treebank derived chunks) that clearly must be replaced by a realistic setting in our future work.
Acknowledgment. I would like to thank Markus Dreyer for fruitful (?long distance?? discussions and the (steadily improved) maximum entropy models.
References Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius and George Smith.
2002. The TIGER Tree-bank.
Proc. of the Wshp.
on Treebanks and Linguistic Theories Sozopol.
Sabine Buchholz, Jorn Veenstra and Walter Daelemans.
1999. Cascaded Grammatical Relation Assignment.
EMNLP-VLC??9, the Joint SIGDAT Conference on Empirical Methods in NLP and Very Large Corpora.
Kilian Foth, Wolfgang Menzel, and Ingo Schroder.
Robust parsing with weighted constraints.
Natural Language Engineering, 11(1):1-25 2005.
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dave Zimak.
2004. Semantic Role Labeling via Integer Linear Programming Inference.
COLING ??4 .
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 205??08, Prague, June 2007.
c2007 Association for Computational Linguistics Minimally Lexicalized Dependency Parsing Daisuke Kawahara and Kiyotaka Uchimoto National Institute of Information and Communications Technology, 3-5 Hikaridai Seika-cho Soraku-gun, Kyoto, 619-0289, Japan {dk, uchimoto}@nict.go.jp Abstract Dependencystructuresdonothavetheinformation of phrase categories in phrase structure grammar.
Thus, dependency parsing relies heavily on the lexical information of words.
This paper discusses our investigation into the effectiveness of lexicalization in dependency parsing.
Specically, by restricting the degree of lexicalization in the training phase of a parser, we examine the change in the accuracy of dependency relations.
Experimental results indicate that minimal or low lexicalization is sufcient for parsing accuracy.
1 Introduction
In recent years, many accurate phrase-structure parsers have been developed (e.g., (Collins, 1999; Charniak, 2000)).
Since one of the characteristics of these parsers is the use of lexical information in the tagged corpus, they are called ?lexicalized parsers??
Unlexicalized parsers, on the other hand, achieved accuracies almost equivalent to those of lexicalized parsers (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006).
Accordingly, we can say that the state-of-the-art lexicalized parsers are mainly based on unlexical (grammatical) information due to the sparse data problem.
Bikel also indicated that Collins??parser can use bilexical dependencies only 1.49% of the time; the rest of the time, itbacksofftoconditiononewordonjustphrasaland part-of-speech categories (Bikel, 2004).
This paper describes our investigation into the effectiveness of lexicalization in dependency parsing instead of phrase-structure parsing.
Usual dependency parsing cannot utilize phrase categories, and thus relies on word information like parts of speech and lexicalized words.
Therefore, we want to know the performance of dependency parsers that have minimal or low lexicalization.
Dependency trees have been used in a variety of NLP applications, such as relation extraction (Culotta and Sorensen, 2004) and machine translation (Ding and Palmer, 2005).
For such applications, a fast, efcient and accurate dependency parser is required to obtain dependency trees from a large corpus.
From this point of view, minimally lexicalized parsers have advantages over fully lexicalized ones in parsing speed and memory consumption.
We examined the change in performance of dependency parsing by varying the degree of lexicalization.
The degree of lexicalization is specied by giving a list of words to be lexicalized, which appear in a training corpus.
For minimal lexicalization, we used a short list that consists of only high-frequency words, and for maximal lexicalization, the whole list was used.
Consequently, minimally or low lexicalization is sufcient for dependency accuracy.
2 Related
Work Klein and Manning presented an unlexicalized PCFG parser that eliminated all the lexicalized parameters (Klein and Manning, 2003).
They manually split category tags from a linguistic view.
This corresponds to determining the degree of lexicalization by hand.
Their parser achieved an F1 of 85.7% forsection23ofthePennTreebank.
Matsuzakietal. and Petrov et al.proposed an automatic approach to 205 Dependency accuracy (DA) Proportions of words, except punctuation marks, that are assigned the correct heads.
Root accuracy (RA) Proportions of root words that are correctly detected.
Complete rate (CR) Proportions of sentences whose dependency structures are completely correct.
Table 1: Evaluation criteria.
splitting tags (Matsuzaki et al., 2005; Petrov et al., 2006).
In particular, Petrov et al.reported an F1 of 90.2%, which is equivalent to that of state-of-the-art lexicalized parsers.
Dependency parsing has been actively studied in recent years (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Isozaki et al., 2004; McDonald et al., 2005; McDonald and Pereira, 2006; Corston-Oliver et al., 2006).
For instance, Nivre and Scholz presented a deterministic dependency parser trained by memory-based learning (Nivre and Scholz, 2004).
McDonald et al.proposed an online large-margin method for training dependency parsers (McDonald et al., 2005).
All of them performed experiments using section 23 of the Penn Treebank.
Table 2 summarizes their dependency accuracies based on three evaluation criteria shown in Table 1.
These parsers believed in the generalization ability of machine learners and did not pay attention to the issue of lexicalization.
3 Minimally
Lexicalized Dependency Parsing We present a simple method for changing the degree of lexicalization in dependency parsing.
This method restricts the use of lexicalized words, so it is the opposite to tag splitting in phrase-structure parsing.
In the remainder of this section, we rst describe a base dependency parser and then report experimental results.
3.1 Base
Dependency Parser We built a parser based on the deterministic algorithm of Nivre and Scholz (Nivre and Scholz, 2004) as a base dependency parser.
We adopted this algorithm because of its linear-time complexity.
In the algorithm, parsing states are represented by triples ?S,I,A?? where S is the stack that keeps the words being under consideration, I is the list of reDA RA CR (Yamada and Matsumoto, 2003) 90.3 91.6 38.4 (Nivre and Scholz, 2004) 87.3 84.3 30.4 (Isozaki et al., 2004) 91.2 95.7 40.7 (McDonald et al., 2005) 90.9 94.2 37.5 (McDonald and Pereira, 2006) 91.5 N/A 42.1 (Corston-Oliver et al., 2006) 90.8 93.7 37.6 Our Base Parser 90.9 92.6 39.2 Table 2: Comparison of parser performance.
maining input words, and A is the list of determined dependencies.
Given an input word sequence, W, the parser is rst initialized to the triple ?nil,W,???.
The parser estimates a dependency relation between two words (the top elements of stacks S and I).
The algorithm iterates until the list I is empty.
There are four possible operations for a parsing state (where t is the word on top of S, n is the next input word in I, and w is any word): Left In a state ?t|S,n|I,A?? if there is no dependency relation (t ??w) in A, add the new dependency relation (t ??n) into A and pop S (remove t), giving the state ?S,n|I,A ??(t ?? n)??
Right In a state ?t|S,n|I,A?? if there is no dependency relation (n ??w) in A, add the new dependency relation (n ??t) into A and push n onto S, giving the state ?n|t|S,I,A??n ??t)??
Reduce In a state ?t|S,I,A?? if there is a dependency relation (t ??w) in A, pop S, giving the state ?S,I,A??
Shift In a state ?S,n|I,A?? push n onto S, giving the state ?n|S,I,A??
In this work, we used Support Vector Machines (SVMs) to predict the operation given a parsing state.
SinceSVMsarebinaryclassiers, weusedthe pair-wise method to extend them in order to classify our four-class task.
The features of a node are the word?s lemma, the POS/chunk tag and the information of its child node(s).
The lemma is obtained from the word form using a lemmatizer, except for numbers, which are replaced by ?num??
The context features are the two preceding nodes of node t (and t itself), the two succeeding nodes of node n (and n itself), and their 1We use ?nil??to denote an empty list and a|A to denote a list with head a and tail A.
206 87 87.2 87.4 87.6 87.8 88 88.2 88.4 0 1000 2000 3000 4000 5000 Accuracy (%) Number of Lexicalized Words Figure 1: Dependency accuracies on the WSJ while changing the degree of lexicalization.
child nodes (lemmas and POS tags).
The distance between nodes n and t is also used as a feature.
We trained our models on sections 2-21 of the WSJ portion of the Penn Treebank.
We used section 23 as the test set.
Since the original treebank is basedonphrasestructure, weconvertedthetreebank to dependencies using the head rules provided by Yamada 2.
During the training phase, we used intact POS and chunk tags3.
During the testing phase, we used automatically assigned POS and chunk tags by Tsuruoka?s tagger4(Tsuruoka and Tsujii, 2005) and YamCha chunker5(Kudo and Matsumoto, 2001).
We used an SVMs package, TinySVM6,and trained the SVMs classiers using a third-order polynomial kernel.
The other parameters are set to default.
The last row in Table 2 shows the accuracies of our base dependency parser.
3.2 Degree
of Lexicalization vs.
Performance The degree of lexicalization is specied by giving a list of words to be lexicalized, which appear in a training corpus.
For minimal lexicalization, we used a short list that consists of only high-frequency words, and for maximal lexicalization, the whole list was used.
To conduct the experiments efciently, we trained 2http://www.jaist.ac.jp/?h-yamada/ 3In a preliminary experiment, we tried to use automatically assigned POS and chunk tags, but we did not detect signicant difference in performance.
4http://www-tsujii.is.s.u-tokyo.ac.jp/?tsuruoka/postagger/ 5http://chasen.org/?taku-ku/software/yamcha/ 6http://chasen.org/?taku-ku/software/TinySVM/ 83.6 83.8 84 84.2 84.4 84.6 84.8 85 0 1000 2000 3000 4000 5000 Accuracy (%) Number of Lexicalized Words Figure2: DependencyaccuraciesontheBrownCorpus while changing the degree of lexicalization.
our models using the rst 10,000 sentences in sections 2-21 of the WSJ portion of the Penn Treebank.
We used section 24, which is usually used as the development set, to measure the change in performance based on the degree of lexicalization.
We counted word (lemma) frequencies in the training corpus and made a word list in descending order of their frequencies.
The resultant list consists of 13,729 words, and the most frequent word is ?the?? which occurs 13,252 times, as shown in Table 3.
We dene the degree of lexicalization as a threshold of the word list.
If, for example, this threshold is set to 1,000, the top 1,000 most frequently occurring words are lexicalized.
We evaluated dependency accuracies while changing the threshold of lexicalization.
Figure 1 shows the result.
The dotted line (88.23%) represents the dependency accuracy of the maximal lexicalization, that is, using the whole word list.
We can see that the decrease in accuracy is less than 1% at the minimal lexicalization (degree=100) and the accuracy of more than 3,000 degree slightly exceeds that of the maximal lexicalization.
The best accuracy(88.34%)wasachievedat4,500degreeand signicantly outperformed the accuracy (88.23%) of the maximal lexicalization (McNemar?s test; p = 0.017 < 0.05).
These results indicate that maximal lexicalization is not so effective for obtaining accurate dependency relations.
We also applied the same trained models to the Brown Corpus as an experiment of parser adaptation.
We rst split the Brown Corpus portion of 207 rank word freq.
rank word freq.
1 the 13,252 1,000 watch 29 2, 12,858...
... ...
... ...
... 2,000 healthvest 12 100 week 261 ...
... ...
... ...
... 3,000 whoop 7 500 estate 64 ...
... ...
... ...
... Table 3: Word list.
the Penn Treebank into training and testing parts in the same way as (Roark and Bacchiani, 2003).
We further extracted 2,425 sentences at regular intervals from the training part and used them to measure the change in performance while varying the degree of lexicalization.
Figure 2 shows the result.
The dotted line (84.75%) represents the accuracy of maximal lexicalization.
The resultant curve is similar to that of the WSJ experiment7.
We can say that our claim is true even if the testing corpus is outside the domain.
3.3 Discussion
We have presented a minimally or lowly lexicalized dependency parser.
Its dependency accuracy is close or almost equivalent to that of fully lexicalized parsers, despite the lexicalization restriction.
Furthermore, the restriction reduces the time and space complexity.
The minimally lexicalized parser (degree=100) took 12m46s to parse the WSJ development set and required 111 MB memory.
These are 36% of time and 45% of memory reduction, compared to the fully lexicalized one.
The experimental results imply that training corpora are too small to demonstrate the full potential of lexicalization.
We should consider unsupervised or semi-supervised ways to make lexicalized parsers more effective and accurate.
Acknowledgment This research is partially supported by special coordination funds for promoting science and technology.
7In the experiment on the Brown Corpus, the difference between the best accuracy and the baseline was not signicant.
References Daniel M.
Bikel. 2004.
Intricacies of Collins??parsing model.
Computational Linguistics, 30(4):479??11.
Eugene Charniak.
2000. A maximum-entropy-inspired parser.
In Proceedings of NAACL2000, pages 132??39.
Michael Collins.
1999. Head-Driven Statistical Models for Natural Language Parsing.
Ph.D. thesis, University of Pennsylvania.
Simon Corston-Oliver, Anthony Aue, Kevin Duh, and Eric Ringger.
2006. Multilingual dependency parsing using bayes point machines.
In Proceedings of HLT-NAACL2006, pages 160??67.
Aron Culotta and Jeffrey Sorensen.
2004. Dependency tree kernels for relation extraction.
In Proceedings of ACL2004, pages 423??29.
Yuan Ding and Martha Palmer.
2005. Machine translation using probabilistic synchronous dependency insertion grammars.
In Proceedings of ACL2005, pages 541??48.
Hideki Isozaki, Hideto Kazawa, and Tsutomu Hirao.
2004. A deterministic word dependency analyzer enhanced with preference learning.
In Proceedings of COLING2004, pages 275??81.
Dan Klein and Christopher D.
Manning. 2003.
Accurate un-lexicalized parsing.
In Proceedings of ACL2003, pages 423?? 430.
Taku Kudo and Yuji Matsumoto.
2001. Chunking with support vector machines.
In Proceedings of NAACL2001, pages 192??99.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations.
In Proceedings of ACL2005, pages 75??2.
Ryan McDonald and Fernando Pereira.
2006. Online learning of approximate dependency parsing algorithms.
In Proceedings of EACL2006, pages 81??8.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency parsers.
In Proceedings of ACL2005, pages 91??8.
Joakim Nivre and Mario Scholz.
2004. Deterministic dependency parsing of English text.
In Proceedings of COLING2004, pages 64??0.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein.
2006. Learning accurate, compact, and interpretable tree annotation.
In Proceedings of COLING-ACL2006, pages 433??440.
Brian Roark and Michiel Bacchiani.
2003. Supervised and unsupervised PCFG adaptation to novel domains.
In Proceedings of HLT-NAACL2003, pages 205??12.
Yoshimasa Tsuruoka and Jun?ichi Tsujii.
2005. Bidirectional inference with the easiest-rst strategy for tagging sequence data.
In Proceedings of HLT-EMNLP2005, pages 467??74.
Hiroyasu Yamada and Yuji Matsumoto.
2003. Statistical dependency analysis with support vector machines.
In Proceedings of IWPT2003, pages 195??06 .
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 209??12, Prague, June 2007.
c2007 Association for Computational Linguistics HunPos ??an open source trigram tagger Peter Halacsy Budapest U.
of Technology MOKK Media Research H-1111 Budapest, Stoczek u 2 peter@halacsy.com Andras Kornai MetaCarta Inc.
350 Massachusetts Ave.
Cambridge MA 02139 andras@kornai.com Csaba Oravecz Hungarian Academy of Sciences Institute of Linguistics H-1068 Budapest, Benczur u.
33. oravecz@nytud.hu Abstract In the world of non-proprietary NLP software the standard, and perhaps the best, HMM-based POS tagger is TnT (Brants, 2000).
We argue here that some of the criticism aimed at HMM performance on languages with rich morphology should more properly be directed at TnT?s peculiar license, free but not open source, since it is those details of the implementation which are hidden from the user that hold the key for improved POS tagging across a wider variety of languages.
We present HunPos1, a free and open source (LGPL-licensed) alternative, which can be tuned by the user to fully utilize the potential of HMM architectures, offering performance comparable to more complex models, but preserving the ease and speed of the training and tagging process.
0 Introduction
Even without a formal survey it is clear that TnT (Brants, 2000) is used widely in research labs throughout the world: Google Scholar shows over 400 citations.
For research purposes TnT is freely available, but only in executable form (closed source).
Its greatest advantage is its speed, important both for a fast tuning cycle and when dealing with large corpora, especially when the POS tagger is but one component in a larger information retrieval, information extraction, or question answer1http://mokk.bme.hu/resources/hunpos/ ing system.
Though taggers based on dependency networks (Toutanova et al., 2003), SVM (Gimenez and M`arquez, 2003), MaxEnt (Ratnaparkhi, 1996), CRF (Smith et al., 2005), and other methods may reach slightly better results, their train/test cycle is orders of magnitude longer.
A ubiquitous problem in HMM tagging originates from the standard way of calculating lexical probabilities by means of a lexicon generated during training.
In highly inflecting languages considerably more unseen words will be present in the test data than in more isolating languages, which largely accounts for the drop in the performance of n-gram taggers when moving away from English.
To mitigate the effect one needs a morphological dictionary (Haji?c et al., 2001) or a morphological analyzer (Hakkani-Tur et al., 2000), but if the implementation source is closed there is no handy way to incorporate morphological knowledge in the tagger.
The paper is structured as follows.
In Section 1 we present our own system, HunPos, while in Section 2 we describe some of the implementation details of TnT that we believe influence the performance of a HMM based tagging system.
We evaluate the system and compare it to TnT on a variety of tasks in Section 3.
We don?t necessarily consider HunPos to be significantly better than TnT, but we argue that we could reach better results, and so could others coming after us, because the system is open to explore all kinds of fine-tuning strategies.
Some concluding remarks close the paper in Section 4.
209 1 Main features of HunPos HunPos has been implemented in OCaml, a highlevel language which supports a succinct, wellmaintainable coding style.
OCaml has a highperformance native-code compiler (Doligez et al., 2004) that can produce a C library with the speed of a C/C++ implementation.
On the whole HunPos is a straightforward trigram system estimating the probabilities argmax t1...tT P(tT+1|tT) Tproductdisplay i=1 P(ti|ti??,ti??)P(wi|ti??,ti) for a given sequence of words w1...wT (the additional tags t??,t0, and tT+1 are for sentence boundary markers).
Notice that unlike traditional HMM models, we estimate emission/lexicon probabilities based on the current tag and the previous tag as well.
As we shall see in the next Section, using tag bigrams to condition the emissions can lead to as much as 10% reduction in the error rate.
(In fact, HunPos can handle a context window of any size, but on the limited training sets available to us increasing this parameter beyond 2 gives no further improvement.) As for contextualized lexical probabilities, our extension is very similar to Banko and Moore (2004) who use P(wi|ti??,ti,ti+1) lexical probabilities and found, on the Penn Treebank, that ?incorporating more context into an HMM when estimating lexical probabilities improved accuracy from 95.87% to 96.59%??
One difficulty with their approach, noted by Banko and Moore (2004), is the treatment of unseen words: their method requires a full dictionary that lists what tags are possible for each word.
To be sure, for isolating languages such information is generally available from machine readable dictionaries which are often large enough to make the out of vocabulary problem negligible.
But in our situation this amounts to idealized morphological analyzers (MA) that have their stem list extended so as to have no OOV on the test set.
The strong side of TnT is its suffix guessing algorithm that is triggered by unseen words.
From the training set TnT builds a trie from the endings of words appearing less than n times in the corpus, and memorizes the tag distribution for each suffix.2 A 2The parameter n cannot be externally set ??it is documented as 10 but we believe it to be higher.
clear advantage of this approach is the probabilistic weighting of each label, however, under default settings the algorithm proposes a lot more possible tags than a morphological analyzer would.
To facilitate the use of MA, HunPos has hooks to work with a morphological analyzer (lexicon), which might still leave some OOV items.
As we shall see in Section 3, the key issue is that for unseen words the HMM search space may be narrowed down to the alternatives proposed by this module, which not only speeds up search but also very significantly improves precision.
That is, for unseen words the MA will generate the possible labels, to which the weights are assigned by the suffix guessing algorithm.
2 Inside
TnT Here we describe, following the lead of (Jurish, 2003), some non-trivial features of TnT sometimes only hinted at in the user guide, but clearly evident from its behavior on real and experimentally adjusted corpora.
For the most part, these features are clever hacks, and it is unfortunate that neither Brants (2000) nor the standard HMM textbooks mention them, especially as they often yield more significant error reduction than the move from HMM to other architectures.
Naturally, these features are also available in HunPos.
2.1 Cardinals
For the following regular expressions TnT learns the tag distribution of the training corpus separately to give more reliable estimates for open class items like numbers unseen during training: ?[0-9]+$ ?[0-9]+\.$ ?[0-9.,:-]+[0-9]+$ ?[0-9]+[a-zA-Z]{1,3}$ (The regexps are only inferred ??we haven?t attempted to trace the execution).
After this, at test time, if the word is not found in the lexicon (numerals are added to the lexicon like all other items) TnT checks whether the unseen word matches some of the regexps, and uses the distribution learned for this regexp to guess the tag.
210 2.2 Upperand lowercase The case of individual words may carry relevant information for tagging, so it is well worth preserving the uppercase feature for items seen as such in training.
For unseen words TnT builds two suffix tries: if the word begins with uppercase one trie is used, for lowercase words the other trie is applied.
The undocumented trick is to try to lookup the word in sentence initial position from the training lexicon in its lowercase variant, which contributes noticeably to the better performance of the system.
3 Evaluation
English For the English evaluation we used the WSJ data from Penn Treebank II.
We extracted sentences from the parse trees.
We split data into training and test set in the standard way (Table 1).
Set Sect?ns Sent.
Tokens Unseen Train 0-18 38,219 912,344 0 Test 22-24 5,462 129,654 2.81% Table 1: Data set splits used for English As Table 2 shows HunPos achieves performance comparable to TnT for English.
The increase in the emission order clearly improves this performance.
seen unseen overall TnT 96.77% 85.91% 96.46% HunPos 1 96.76% 86.90% 96.49% HunPos 2 96.88% 86.13% 96.58% Table 2: WSJ tagging accuracy, HunPos with first and second order emission/lexicon probabilities If we follow Banko and Moore (2004) and construct a full (no OOV) morphological lexicon from the tagged version of the test corpus, we obtain 96.95% precision where theirs was 96.59%.
For words seen, precision improves by an entirely negligible 0.01%, but for unseen words it improves by 10%, from 86.13% to 98.82%.
This surprising result arises from the fact that there are a plenty of unambiguous tokens (especially the proper names that are usually unseen) in the test corpus.
What this shows is not just that morphology matters (this is actually not that visible for English), but that the difference between systems can only be appreciated once the small (and scantily documented) tricks are factored out.
The reason why Banko and Moore (2004) get less than HunPos is not because their system is inherently worse, but rather because it lacks the engineering hacks built into TnT and HunPos.
Hungarian We evaluated the different models by tenfold cross-validation on the Szeged Corpus (Csendes et al., 2004), with the relevant data in presented Table 3.
Set Sent.
Tokens Unseens OOV Train 63,075 1,044,914 0 N.A Test 7,008 116,101 9.59% 5.64% Table 3: Data set splits used for Hungarian.
Note that the proportion of unseen words, nearly 10%, is more than three times higher than in English.
Most of these words were covered by the morphological analyzer (Tron et al., 2006) but still 28% of unseen words were only guessed.
However, this is just 2.86% of the whole corpus, in the magnitude similar to English.
morph lex order seen unseen overall no 1 98.34% 88.96% 97.27%2 98.58% 87.97% 97.40% yes 1 98.32% 96.01% 98.03%2 98.56% 95.96% 98.24% Table 4: Tagging accuracy for Hungarian of HunPos with and without morphological lexicon and with first and second order emission/lexicon probabilities.
On the same corpus TnT had 97.42% and Halacsy et al.(2006) reached 98.17% with a MaxEnt tagger that used the TnT output as a feature.
HunPos gets as good performance in one minute as this MaxEnt model which took three hours to go through the train/test cycle.
4 Concluding
remarks Though there can be little doubt that the ruling system of bakeoffs actively encourages a degree of oneupmanship, our paper and our software are not offered in a competitive spirit.
As we said at the out211 set, we don?t necessarily believe HunPos to be in any way better than TnT, and certainly the main ideas have been pioneered by DeRose (1988), Church (1988), and others long before this generation of HMM work.
But to improve the results beyond what a basic HMM can achieve one needs to tune the system, and progress can only be made if the experiments are end to end replicable.
There is no doubt many other systems could be tweaked further and improve on our results ??what matters is that anybody could now also tweak HunPos without any restriction to improve the state of the art.
Such tweaking can bring surprising results, e.g. the conclusion, strongly supported by the results presented here, that HMM tagging is actually quite competitive with, and orders of magnitude faster than, the current generation of learning algorithms including SVM and MaxEnt.
No matter how good TnT was to begin with, the closed source has hindered its progress to the point that inherently clumsier, but better tweakable algorithms could overtake HMMs, a situation that HunPos has now hopefully changed at least for languages with more complex morphologies.
Acknowledgement We thank Thorsten Brants for TnT, and Gyorgy Gyepesi for constant help and encouragement.
References Michele Banko and Robert C.
Moore. 2004.
Part of speech tagging in context.
In COLING ??4: Proceedings of the 20th international conference on Computational Linguistics, page 556, Morristown, NJ, USA.Association for Computational Linguistics.
Thorsten Brants.
2000. TnT ??a statistical part-of-speech tagger.
In Proceedings of the Sixth Applied Natural Language Processing Conference (ANLP-2000), Seattle, WA.
Kenneth Ward Church.
1988. A stochastic parts program and noun phrase parser for unrestricted text.
In Proceedings of the second conference on Applied natural language processing, pages 136??43, Morristown, NJ, USA.
Association for Computational Linguistics.
Dora Csendes, Janos Csirik, and Tibor Gyimothy.
2004. The Szeged Corpus: A POS tagged and syntactically annotated Hungarian natural language corpus.
In Karel Pala Petr Sojka, Ivan Kopecek, editor, Text, Speech and Dialogue: 7th International Conference, TSD, pages 41??7.
Steven J.
DeRose. 1988.
Grammatical category disambiguation by statistical optimization.
Computational Linguistics, 14:31??9.
Damien Doligez, Jacques Garrigue, Didier Remy, and Jer?ome Vouillon, 2004.
The Objective Caml system.
Institut National de Recherche en Informatique et en Automatique.
Jesus Gimenez and Llus M`arquez.
2003. Fast and accurate part-of-speech tagging: The svm approach revisited.
In Proceedings of RANLP, pages 153??63.
Jan Haji?c, Pavel Krbec, Karel Oliva, Pavel Kv?eto?n, and Vladimr Petkevi?c.
2001. Serial combination of rules and statistics: A case study in Czech tagging.
In Proceedings of the 39th Association of Computational Linguistics Conference, pages 260??67, Toulouse, France.
Dilek Z.
Hakkani-Tur, Kemal Oflazer, and Gokhan Tur.
2000. Statistical morphological disambiguation for agglutinative languages.
In Proceedings of the 18th conference on Computational linguistics, pages 285?? 291, Saarbrucken, Germany.
Peter Halacsy, Andras Kornai, Csaba Oravecz, Viktor Tron, and Daniel Varga.
2006. Using a morphological analyzer in high precision POS tagging of Hungarian.
In Proceedings of LREC 2006, pages 2245??248.
Bryan Jurish.
2003. A hybrid approach to part-of-speech tagging.
Technical report, Berlin-Brandenburgische Akademie der Wissenschaften.
Adwait Ratnaparkhi.
1996. A maximum entropy model for part-of-speech tagging.
In Karel Pala Petr Sojka, Ivan Kopecek, editor, Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 133??42, University of Pennsylvania.
Noah A.
Smith, David A.
Smith, and Roy W.
Tromble. 2005.
Context-based morphological disambiguation with random fields.
In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, Vancouver.
Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer.
2003. Feature-rich part-of-speech tagging with a cyclic dependency network.
In Proceedings of HLT-NAACL, pages 252??59.
Viktor Tron, Peter Halacsy, Peter Rebrus, Andras Rung, Peter Vajda, and Eszter Simon.
2006. Morphdb.hu: Hungarian lexical database and morphological grammar.
In Proceedings of LREC 2006, pages 1670??673 .
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 213??16, Prague, June 2007.
c2007 Association for Computational Linguistics Extending MARIE: an N-gram-based SMT decoder Josep M.
Crego TALP Research Center Universitat Polit`ecnica de Catalunya Barcelona, 08034 jmcrego@gps.tsc.upc.edu Jose B.
Mari?no TALP Research Center Universitat Polit`ecnica de Catalunya Barcelona,08034 canton@gps.tsc.upc.edu Abstract In this paper we present several extensions of MARIE1, a freely available N-gram-based statistical machine translation (SMT) decoder.
The extensions mainly consist of the ability to accept and generate word graphs and the introduction of two new N-gram models in the loglinear combination of feature functions the decoder implements.
Additionally, the decoder is enhanced with a caching strategy that reduces the number of N-gram calls improving the overall search efficiency.
Experiments are carried out over the Eurpoean Parliament Spanish-English translation task.
1 Introduction
Research on SMT has been strongly boosted in the last few years, partially thanks to the relatively easy development of systems with enough competence as to achieve rather competitive results.
In parallel, tools and techniques have grown in complexity, which makes it difficult to carry out state-of-the-art research without sharing some of this toolkits.
Without aiming at being exhaustive, GIZA++2, SRILM3 and PHARAOH4 are probably the best known examples.
We introduce the recent extensions made to an Ngram-based SMT decoder (Crego et al., 2005), which allowed us to tackle several translation issues (such as reordering, rescoring, modeling, etc).
successfully improving accuracy, as well as efficiency results.
As far as SMT can be seen as a double-sided problem (modeling and search), the decoder emerges as a key component, core module of any SMT system.
Mainly, 1http://gps-tsc.upc.es/soft/soft/marie 2http://www.fjoch.com/GIZA++.html 3http://www.speech.sri.com/projects/srilm/ 4http://www.isi.edu/publications/licensed-sw/pharaoh/ any technique aiming at dealing with a translation problem needs for a decoder extension to be implemented.
Particularly, the reordering problem can be more efficiently (and accurate) addressed when tightly coupled with decoding.
In general, the competence of a decoder to make use of the maximum of information in the global search is directly connected with the likeliness of successfully improving translations.
The paper is organized as follows.
In Section 2 we and briefly review the previous work on decoding with special attention to N-gram-based decoding.
Section 3 describes the extended log-linear combination of feature functions after introduced the two new models.
Section 4 details the particularities of the input and output word graph extensions.
Experiments are reported on section 5.
Finally, conclusions are drawn in section 6.
2 Related
Work The decoding problem in SMT is expressed by the next maximization: argmaxtI 1??
P(tI1|sJ1), where sJ1 is the source sentence to translate and tI1 is a possible translation of the set ?, which contains all the sentences of the language of tI1.
Given that the full search over the whole set of target language sentences is impracticable (? is an infinite set), the translation sentence is usually built incrementally, composing partial translations of the source sentence, which are selected out of a limited number of translation candidates (translation units).
The first SMT decoders were word-based.
Hence, working with translation candidates of single source words.
Later appeared the phrase-based decoders, which use translation candidates composed of sequences of source and target words (outperforming the wordbased decoders by introducing the word context).
In the last few years syntax-based decoders have emerged aiming at dealing with pair of languages with different syntactical structures for which the word context introduced 213 Figure 1: Generative process.
Phrase-based (left) and N-gram-based (right) approaches.
in phrase-based decoders is not sufficient to cope with long reorderings.
Like standard phrase-based decoders, MARIE employs translation units composed of sequences of source and target words.
In contrast, the translation context is differently taken into account.
Whereas phrasebased decoders employ translation units uncontextualized, MARIE takes the translation unit context into account by estimating the translation model as a standard N-gram language model (N-gram-based decoder).
Figure 1 shows that both approaches follow the same generative process, but they differ on the structure of translation units.
In the example, the units ?s1#t1??and ?s2 s3#t2 t3??of the N-gram-based approach are used considering that both appear sequentially.
This fact can be understood as using a longer unit that includes both (longer units are drawn in grey).
MARIE follows the maximum entropy framework, where we can define a translation hypothesis t given a source sentence s, as the target sentence maximizing a log-linear combination of feature functions: ?tI1 = argmax tI1 braceleftBigg Msummationdisplay m=1 mhm(sJ1,tI1) bracerightBigg (1) where m corresponds to the weighting coefficients of the log-linear combination, and the feature functions hm(s,t) to a logarithmic scaling of the probabilities of each model.
See (Mari?no et al., 2006) for further details on the N-gram-based approach to SMT.
3 N-gram Feature Functions Two language models (LM) are introduced in equation 1, aiming at helping the decoder to find the right translations.
Both are estimated as standard N-gram LM.
3.1 Target-side N-gram LM The first additional N-gram LM is destinated to be applied over the target sentence (tagged) words.
Hence, as the original target LM (computed over raw words), it is also used to score the fluency of target sentences, but aiming at achieving generalization power through using a more generalized language (such as a language of Part-of-Speech tags) instead of the one composed of raw words.
Part-Of-Speech tags have successfully been used in several previous experiments.
however, any other tag can be applied.
Several sequences of target tags may apply to any given translation unit (which are passed to the decoder before it starts the search).
For instance, regarding a translation unit with the english word ?general??in its target side, if POS tags were used as target tagged tags, there would exist at least two different tag options: noun and adjective.
In the search, multiple hypotheses are generated concerning different target tagged sides (sequences of tags) of a single translation unit.
Therefore, on the one side, the overall search is extended towards seeking the sequence of target tags that better fits the sequence of target raw words.
On the other side, this extension is hurting the overall efficiency of the decoder as additional hypotheses appear in the search stacks while not additional translation hypotheses are being tested (only differently tagged).
This extended feature may be used toghether with a limitation of the number of target tagged hypotheses per translation unit.
The use of a limited number of these hypotheses implies a balance between accuracy and efficiency.
3.2 Source-side N-gram LM The second N-gram LM is applied over the input sentence tagged words.
Obviously, this model only makes sense when reordering is applied over the source words in order to monotonize the source and target word order.
In such a case, the tagged LM is learnt over the training set with reordered source words.
Hence, the new model is employed as a reordering model.
It scores a given source-side reordering hypothesis according to the reorderings made in the training sentences (from which the tagged LM is estimated).
As for the previous extension, source tagged words are used instead of raw words in order to achieve generalization power.
Additional hypotheses regarding the same translation unit are not generated in the search as all input sentences are uniquely tagged.
Figure 2 illustrates the use of a source POS-tagged N214 gram LM.
The probability of the sequence ?PRN VRB NAME ADJ??is greater than the probability of the sequence ?PRN VRB ADJ NAME??for a model estimated over the training set with reordered source words (with english words following the spanish word order).
Figure 2: Source POS-tagged N-gram LM.
3.3 Caching
N-grams The use of several N-gram LM?s implies a reduction in efficiency in contrast to other models that can be implemented by means of a single lookup table (one access per probability call).
The special characteristics of Ngram LM?s introduce additional memory access to account for backoff probabilities and lower Ngrams fallings.
Many N-gram calls are requested repeatedly, producing multiple calls of an entry.
A simple strategy to reduce additional access consists of keeping a record (cache) for those Ngram entries already requested.
A drawback for the use of a cache consists of the additional memory access derived of the cache maintenance (adding new and checking for existing entries).
Figure 3: Memory access derived of an N-gram call.
Figure 3 illustrates this situation.
The call for a 3-gram probability (requesting for the probability of the sequence of tokens ?a b c?? may need for up to 6 memory access, while under a phrase-based translation model the final probability would always be reached after the first memory access.
The additional access in the N-gram-based approach are used to provide lower N-gram and backoff probabilities in those cases that upper N-gram probabilities do not exist.
4 Word
Graphs Word graphs are successfully used in SMT for several applications.
Basically, with the objective of reducing the redundancy of N-best lists, which very often convey serious combinatorial explosion problems.
A word graph is here described as a directed acyclic graph G = (V,E) with one root node n0 ??V. Edges are labeled with tokens (words or translation units) and optionally with accumulated scores.
We will use (ns(ne ?t??s)), to denote an edge starting at node ns and ending at node ne, with token t and score s.
The file format of word graphs coincides with the graph file format recognized by the CARMEL5 finite state automata toolkit.
4.1 Input
Graph We can mainly find two applications for which word graphs are used as input of an SMT system: the recognition output of an automatic speech recognition (ASR) system; and a reordering graph, consisting of a subset of the whole word permutations of a given input sentence.
In our case we are using the input graph as a reordering graph.
The decoder introduces reordering (distortion of source words order) by allowing only for the distortion encoded in the input graph.
Though, the graph is only allowed to encode permutations of the input words.
In other words, any path in the graph must start at node n0, finish at node nN (where nN is a unique ending node) and cover all the input words (tokens t) in whatever order, without repetitions.
An additional feature function (distortion model) is introduced in the log-linear combination of equation 1: pdistortion(uk) ?? kIproductdisplay i=k1 p(ni|ni)??
(2) where uk refers to the kth partial translation unit covering the source positions [k1,...,kI].
p(ni|ni)?? corresponds to the edge score s encoded in the edge (ns(ne ?t??s)), where ni = ne and ni??
= ns.
One of the decoding first steps consists of building (for each input sentence) the set of translation units to be used in the search.
When the search is extended with reordering abilities the set must be also extended with those translation units that cover any sequence of input words following any of the word orders encoded in the input graph.
The extension of the units set is specially relevant when translation units are built from the tranining set with reordered source words.
Given the example of figure 2, if the translation unit ?translations perfect # traducciones perfectas??is available, the decoder should not discard it, as it provides a right translation.
Notwithstanding that its source side does not follow the original word order of the input sentence.
4.2 Output
Graph The goal of using an output graph is to allow for further rescoring work.
That is, to work with alternative transla5http://www.isi.edu/licensed-sw/carmel/ 215 tions to the single 1-best.
Therefore, our proposed output graph has some peculiarities that make it different to the previously sketched intput graph.
The structure of edges remains the same, but obviously, paths are not forced to consist of permutations of the same tokens (as far as we are interested into multiple translation hypotheses), and there may also exist paths which do not reach the ending node nN.
These latter paths are not useful in rescoring tasks, but allowed in order to facilitate the study of the search graph.
However, a very easy and efficient algorithm (O(n), being n the search size) can be used in order to discard them, before rescoring work.
Additionally, given that partial model costs are needed in rescoring work, our decoder allows to output the individual model costs computed for each translation unit (token t).
Costs are encoded within the token s, as in the next example: (0 (1 "o#or1.5,0.9,0.6,0.2}" 6)) where the token t is now composed of the translation unit ?o#or?? followed by (four) model costs.
Multiple translation hypotheses can only be extracted if hypotheses recombinations are carefully saved.
As in (Koehn, 2004), the decoder takes a record of any recombined hypothesis, allowing for a rigorous N-best generation.
Model costs are referred to the current unit while the global score s is accumulated.
Notice also that translation units (not words) are now used as tokens.
5 Experiments
Experiments are carried out for a Spanish-to-English translation task using the EPPS data set, corresponding to session transcriptions of the European Parliament.
Eff. base +tpos +reor +spos Beam size = 50 w/o cache 1,820 2,170 2,970 3,260 w/ cache ??0 ??10 ??90 ??10 Beam size = 100 w/o cache 2,900 4,350 5,960 6,520 w/ cache ??75 ??10 ??25 ??40 Table 1: Translation efficiency results.
Table 1 shows translation efficiency results (measured in seconds) given two different beam search sizes.
w/cache and w/o cache indicate whether the decoder employs (or not) the cache technique (section 3.3).
Several system configuration have been tested: a baseline monotonous system using a 4-gram translation LM and a 5-gram target LM (base), extended with a target POStagged 5-gram LM (+tpos), further extended by allowing for reordering (+reor), and finally using a source-side POS-tagged 5-gram LM (+spos).
As it can be seen, the cache technique improves the efficiency of the search in terms of decoding time.
Time results are further decreased (reduced time is shown for the w/ cache setting) by using more N-gram LM and allowing for a larger search graph (increasing the beam size and introducing distortion).
Further details on the previous experiment can be seen in (Crego and Mari?no, 2006b; Crego and Mari?no, 2006a), where additionally, the input word graph and extended N-gram tagged LM?s are successfully used to improve accuracy at a very low computational cost.
Several publications can also be found in bibliography which show the use of output graphs in rescoring tasks allowing for clear accuracy improvements.
6 Conclusions
We have presented several extensions to MARIE, a freely available N-gram-based decoder.
The extensions consist of accepting and generating word graphs, and introducing two N-gram LM?s over source and target tagged words.
Additionally, a caching technique is applied over the Ngram LM?s.
Acknowledgments This work has been funded by the European Union under the integrated project TC-STAR (IST-2002-FP65067-38), the Spanish Government under the project AVIVAVOZ (TEC2006-13694-C03) and the Universitat Polit`ecnica de Catalunya under UPC-RECERCA grant.
References J.M.
Crego and J.B.
Mari?no. 2006a.
Integration of postag-based source reordering into smt decoding by an extended search graph.
Proc. of the 7th Conf.
of the Association for Machine Translation in the Americas, pages 29??6, August.
J.M. Crego and J.B.
Mari?no. 2006b.
Reordering experiments for n-gram-based smt.
1st IEEE/ACL Workshop on Spoken Language Technology, December.
J.M. Crego, J.B.
Mari?no, and A.
de Gispert.
2005. Anngram-based statistical machine translation decoder.
Proc. of the 9th European Conference on Speech Communication and Technology, Interspeech??5, pages 3193??196, September.
Ph. Koehn.
2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models.
Proc. of the 6th Conf.
of the Association for Machine Translation in the Americas, pages 115??24, October.
J.B. Mari?no, R.E.
Banchs, J.M.
Crego, A.
de Gispert, P.
Lambert, J.A.R.
Fonollosa, and M.R.
Costa-juss`a. 2006.
N-gram based machine translation.
Computational Linguistics, 32(4):527??49 .
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 217??20, Prague, June 2007.
c2007 Association for Computational Linguistics A Hybrid Approach to Word Segmentation and POS Tagging Tetsuji Nakagawa Oki Electric Industry Co., Ltd.
2???? Honmachi, Chuo-ku Osaka 541??053, Japan nakagawa378@oki.com Kiyotaka Uchimoto National Institute of Information and Communications Technology 3??
Hikaridai, Seika-cho, Soraku-gun Kyoto 619??289, Japan uchimoto@nict.go.jp Abstract In this paper, we present a hybrid method for word segmentation and POS tagging.
The target languages are those in which word boundaries are ambiguous, such as Chinese and Japanese.
In the method, word-based and character-based processing is combined, and word segmentation and POS tagging are conducted simultaneously.
Experimental results on multiple corpora show that the integrated method has high accuracy.
1 Introduction
Part-of-speech (POS) tagging is an important task in natural language processing, and is often necessary for other processing such as syntactic parsing.
English POS tagging can be handled as a sequential labeling problem, and has been extensively studied.
However, in Chinese and Japanese, words are not separated by spaces, and word boundaries must be identified before or during POS tagging.
Therefore, POS tagging cannot be conducted without word segmentation, and how to combine these two processing is an important issue.
A large problem in word segmentation and POS tagging is the existence of unknown words.
Unknown words are defined as words that are not in the system?s word dictionary.
It is difficult to determine the word boundaries and the POS tags of unknown words, and unknown words often cause errors in these processing.
In this paper, we study a hybrid method for Chinese and Japanese word segmentation and POS tagging, in which word-based and character-based processing is combined, and word segmentation and POS tagging are conducted simultaneously.
In the method, word-based processing is used to handle known words, and character-based processing is used to handle unknown words.
Furthermore, information of word boundaries and POS tags are used at the same time with this method.
The following sections describe the hybrid method and results of experiments on Chinese and Japanese corpora.
2 Hybrid
Method for Word Segmentation and POS Tagging Many methods have been studied for Chinese and Japanese word segmentation, which include wordbased methods and character-based methods.
Nakagawa (2004) studied a method which combines a word-based method and a character-based method.
Given an input sentence in the method, a lattice is constructed first using a word dictionary, which consists of word-level nodes for all the known words in the sentence.
These nodes have POS tags.
Then, character-level nodes for all the characters in the sentence are added into the lattice (Figure 1).
These nodes have position-of-character (POC) tags which indicate word-internal positions of the characters (Xue, 2003).
There are four POC tags, B, I, E and S, each of which respectively indicates the beginning of a word, the middle of a word, the end of a word, and a single character word.
In the method, the word-level nodes are used to identify known words, and the character-level nodes are used to identify unknown words, because generally wordlevel information is precise and appropriate for processing known words, and character-level information is robust and appropriate for processing unknown words.
Extended hidden Markov models are used to choose the best path among all the possible candidates in the lattice, and the correct path is indicated by the thick lines in Figure 1.
The POS tags and the POC tags are treated equally in the method.
Thus, the word-level nodes and the character-level nodes are processed uniformly, and known words and unknown words are identified simultaneously.
In the method, POS tags of known words as well as word boundaries are identified, but POS tags of unknown words are not identified.
Therefore, we extend the method in order to conduct unknown word POS tagging too: Hybrid Method The method uses subdivided POC-tags in order to identify not only the positions of characters but also the parts-of-speech of the composing words (Figure 2, A).
In the method, POS tagging of unknown words is conducted at the same time as word segmentation and POS tag217 Figure 1: Word Segmentation and Known Word POS Tagging using Word and Character-based Processing ging of known words, and information of partsof-speech of unknown words can be used for word segmentation.
There are also two other methods capable of conducting unknown word POS tagging (Ng and Low, 2004): Word-based Post-Processing Method This method receives results of word segmentation and known word POS tagging, and predicts POS tags of unknown words using words as units (Figure 2, B).
This approach is the same as the approach widely used in English POS tagging.
In the method, the process of unknown word POS tagging is separated from word segmentation and known word POS tagging, and information of parts-of-speech of unknown words cannot be used for word segmentation.
In later experiments, maximum entropy models were used deterministically to predict POS tags of unknown words.
As features for predicting the POS tag of an unknown word w, we used the preceding and the succeeding two words of w and their POS tags, the prefixes and the suffixes of up to two characters of w, the character types contained in w, and the length of w.
Character-based Post-Processing Method This method is similar to the word-based postprocessing method, but in this method, POS tags of unknown words are predicted using characters as units (Figure 2, C).
In the method, POS tags of unknown words are predicted using exactly the same probabilistic models as the hybrid method, but word boundaries and POS tags of known words are fixed in the postprocessing step.
Ng and Low (2004) studied Chinese word segmentation and POS tagging.
They compared several approaches, and showed that character-based approaches had higher accuracy than word-based approaches, and that conducting word segmentation and POS tagging all at once performed better than conducting these processing separately.
Our hybrid method is similar to their character-based all-atonce approach.
However, in their experiments, only word-based and character-based methods were examined.
In our experiments, the combined method of word-based and character-based processing was examined.
Furthermore, although their experiments were conducted with only Chinese data, we conducted experiments with Chinese and Japanese data, and confirmed that the hybrid method performed well on the Japanese data as well as the Chinese data.
3 Experiments
We used five word-segmented and POS-tagged corpora; the Penn Chinese Treebank corpus 2.0 (CTB), a part of the PFR corpus (PFR), the EDR corpus (EDR), the Kyoto University corpus version 2 (KUC) and the RWCP corpus (RWC).
The first two were Chinese (C) corpora, and the rest were Japanese (J) corpora, and they were split into training and test data.
The dictionary distributed with JUMAN version 3.61 (Kurohashi and Nagao, 1998) was used as a word dictionary in the experiments with the KUC corpus, and word dictionaries were constructed from all the words in the training data in the experiments with other corpora.
Table 1 summarizes statistical information of the corpora: the language, the number of POS tags, the sizes of training and test data, and the splitting methods of them1.
We used the following scoring measures to evaluate performance of word segmentation and POS tagging: R : Recall (The ratio of the number of correctly segmented/POS-tagged words in system?s output to the number of words in test data), P : Precision (The ratio of the number of correctly segmented/POS-tagged words in system?s output to the number of words in system?s output), 1The unknown word rate for word segmentation is not equal to the unknown word rate for POS tagging in general, since the word forms of some words in the test data may exist in the word dictionary but the POS tags of them may not exist.
Such words are regarded as known words in word segmentation, but as unknown words in POS tagging.
218 Figure 2: Three Methods for Word Segmentation and POS Tagging F : F-measure (F = 2RP/(R+P)), Runknown : Recall for unknown words, Rknown : Recall for known words.
Table 2 shows the results2.
In the table, Wordbased Post-Proc., Char.-based Post-Proc.
and Hybrid Method respectively indicate results obtained with the word-based post-processing method, the character-based post-processing method, and the hybrid method.
Two types of performance were measured: performance of word segmentation alone, and performance of both word segmentation and POS tagging.
We first compare performance of both word segmentation and POS tagging.
The F-measures of the hybrid method were highest on all the corpora.
This result agrees with the observation by Ng and Low (2004) that higher accuracy was obtained by conducting word segmentation and POS tagging at the same time than by conducting these processing separately.
Comparing the word-based and the character-based post-processing methods, the F-measures of the latter were higher on the Chinese corpora as reported by Ng and Low (2004), but the F-measures of the former were slightly higher on the Japanese corpora.
The same tendency existed in the recalls for known words; the recalls of the character-based post-processing method were highest on the Chinese corpora, but 2The recalls for known words of the word-based and the character-based post-processing methods differ, though the POS tags of known words are identified in the first common step.
This is because known words are sometimes identified as unknown words in the first step and their POS tags are predicted in the post-processing step.
those of the word-based method were highest on the Japanese corpora, except on the EDR corpus.
Thus, the character-based method was not always better than the word-based method as reported by Ng and Low (2004) when the methods were used with the word and character-based combined approach on Japanese corpora.
We next compare performance of word segmentation alone.
The F-measures of the hybrid method were again highest in all the corpora, and the performance of word segmentation was improved by the integrated processing of word segmentation and POS tagging.
The precisions of the hybrid method were highest with statistical significance on four of the five corpora.
In all the corpora, the recalls for unknown words of the hybrid method were highest, but the recalls for known words were lowest.
Comparing our results with previous work is not easy since experimental settings are not the same.
It was reported that the original combined method of word-based and character-based processing had high overall accuracy (F-measures) in Chinese word segmentation, compared with the state-of-the-art methods (Nakagawa, 2004).
Kudo et al.(2004) studied Japanese word segmentation and POS tagging using conditional random fields (CRFs) and rulebased unknown word processing.
They conducted experiments with the KUC corpus, and achieved Fmeasure of 0.9896 in word segmentation, which is better than ours (0.9847).
Some features we did not used, such as base forms and conjugated forms of words, and hierarchical POS tags, were used in 219 Corpus Number Number of Words (Unknown Word Rate for Segmentation/Tagging) (Lang).
of POS [partition in the corpus] Tags Training Test CTB 34 84,937 7,980 (0.0764 / 0.0939) (C) [sec.
1??70] [sec.
271??00] PFR 41 304,125 370,627 (0.0667 / 0.0749) (C) [Jan.
1?Jan. 9] [Jan.
10?Jan. 19] EDR 15 2,550,532 1,280,057 (0.0176 / 0.0189) (J) [id = 4n+0,id = 4n+1] [id = 4n+2] KUC 40 198,514 31,302 (0.0440 / 0.0517) (J) [Jan.
1?Jan. 8] [Jan.
9] RWC 66 487,333 190,571 (0.0513 / 0.0587) (J) [1??0,000th sentences] [10,001??4,000th sentences] Table 1: Statistical Information of Corpora Corpus Scoring Word Segmentation Word Segmentation & POS Tagging (Lang).
Measure Word-based Char.-based Hybrid Word-based Char.-based Hybrid Post-Proc.
Post-Proc. Method Post-Proc.
Post-Proc. Method R 0.9625 0.9625 0.9639 0.8922 0.8935 0.8944 CTB P 0.9408 0.9408 0.9519* 0.8721 0.8733 0.8832 (C) F 0.9516 0.9516 0.9578 0.8821 0.8833 0.8887 Runknown 0.6492 0.6492 0.7148 0.4219 0.4312 0.4713 Rknown 0.9885 0.9885 0.9845 0.9409 0.9414 0.9382 R 0.9503 0.9503 0.9516 0.8967 0.8997 0.9024* PFR P 0.9419 0.9419 0.9485* 0.8888 0.8917 0.8996* (C) F 0.9461 0.9461 0.9500 0.8928 0.8957 0.9010 Runknown 0.6063 0.6063 0.6674 0.3845 0.3980 0.4487 Rknown 0.9749 0.9749 0.9719 0.9382 0.9403 0.9392 R 0.9525 0.9525 0.9525 0.9358 0.9356 0.9357 EDR P 0.9505 0.9505 0.9513* 0.9337 0.9335 0.9346 (J) F 0.9515 0.9515 0.9519 0.9347 0.9345 0.9351 Runknown 0.4454 0.4454 0.4630 0.4186 0.4103 0.4296 Rknown 0.9616 0.9616 0.9612 0.9457 0.9457 0.9454 R 0.9857 0.9857 0.9850 0.9572 0.9567 0.9574 KUC P 0.9835 0.9835 0.9843 0.9551 0.9546 0.9566 (J) F 0.9846 0.9846 0.9847 0.9562 0.9557 0.9570 Runknown 0.9237 0.9237 0.9302 0.6724 0.6774 0.6879 Rknown 0.9885 0.9885 0.9876 0.9727 0.9719 0.9721 R 0.9574 0.9574 0.9592 0.9225 0.9220 0.9255* RWC P 0.9533 0.9533 0.9577* 0.9186 0.9181 0.9241* (J) F 0.9553 0.9553 0.9585 0.9205 0.9201 0.9248 Runknown 0.6650 0.6650 0.7214 0.4941 0.4875 0.5467 Rknown 0.9732 0.9732 0.9720 0.9492 0.9491 0.9491 (Statistical significance tests were performed for R and P, and * indicates significance at p < 0.05) Table 2: Performance of Word Segmentation and POS Tagging their study, and it may be a reason of the difference.
Although, in our experiments, extended hidden Markov models were used to find the best solution, the performance will be further improved by using CRFs instead, which can easily incorporate a wide variety of features.
4 Conclusion
In this paper, we studied a hybrid method in which word-based and character-based processing is combined, and word segmentation and POS tagging are conducted simultaneously.
We compared its performance of word segmentation and POS tagging with other methods in which POS tagging is conducted as a separated post-processing.
Experimental results on multiple corpora showed that the hybrid method had high accuracy in Chinese and Japanese.
References Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying Conditional Random Fields to Japanese Morphological Analysis.
In Proceedings of EMNLP 2004, pages 230??37.
Sadao Kurohashi and Makoto Nagao.
1998. Japanese Morphological Analysis System JUMAN version 3.61.
Department of Informatics, Kyoto University.
(in Japanese).
Tetsuji Nakagawa.
2004. Chinese and Japanese Word Segmentation Using Word-Level and Character-Level Information.
In Proceedings of COLING 2004, pages 466??72.
Hwee Tou Ng and Jin Kiat Low.
2004. Chinese Part-of-Speech Tagging: One-at-a-Time or All-at-Once?
Word-Based or Character-Based?
In Proceedings of EMNLP 2004, pages 277??84.
Nianwen Xue.
2003. Chinese Word Segmentation as Character Tagging.
International Journal of Computational Linguistics and Chinese, 8(1):29??8.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 221??24, Prague, June 2007.
c2007 Association for Computational Linguistics Automatic Part-of-Speech Tagging for Bengali: An Approach for Morphologically Rich Languages in a Poor Resource Scenario Sandipan Dandapat, Sudeshna Sarkar, Anupam Basu Department of Computer Science and Engineering Indian Institute of Technology Kharagpur India 721302 {sandipan,sudeshna,anupam.basu}@cse.iitkgp.ernet.in Abstract This paper describes our work on building Part-of-Speech (POS) tagger for Bengali.
We have use Hidden Markov Model (HMM) and Maximum Entropy (ME) based stochastic taggers.
Bengali is a morphologically rich language and our taggers make use of morphological and contextual information of the words.
Since only a small labeled training set is available (45,000 words), simple stochastic approach does not yield very good results.
In this work, we have studied the effect of using a morphological analyzer to improve the performance of the tagger.
We find that the use of morphology helps improve the accuracy of the tagger especially when less amount of tagged corpora are available.
1 Introduction
Part-of-Speech (POS) taggers for natural language texts have been developed using linguistic rules, stochastic models as well as a combination of both (hybrid taggers).
Stochastic models (Cutting et al., 1992; Dermatas et al., 1995; Brants, 2000) have been widely used in POS tagging for simplicity and language independence of the models.
Among stochastic models, bi-gram and tri-gram Hidden Markov Model (HMM) are quite popular.
Development of a high accuracy stochastic tagger requires a large amount of annotated text.
Stochastic taggers with more than 95% word-level accuracy have been developed for English, German and other European Languages, for which large labeled data is available.
Our aim here is to develop a stochastic POS tagger for Bengali but we are limited by lack of a large annotated corpus for Bengali.
Simple HMM models do not achieve high accuracy when the training set is small.
In such cases, additional information may be coded into the HMM model to achieve higher accuracy (Cutting et al., 1992).
The semi-supervised model described in Cutting et al.(1992), makes use of both labeled training text and some amount of unlabeled text.
Incorporating a diverse set of overlapping features in a HMM-based tagger is difficult and complicates the smoothing typically used for such taggers.
In contrast, methods based on Maximum Entropy (Ratnaparkhi, 1996), Conditional Random Field (Shrivastav, 2006) etc.
can deal with diverse, overlapping features.
1.1 Previous
Work on Indian Language POS Tagging Although some work has been done on POS tagging of different Indian languages, the systems are still in their infancy due to resource poverty.
Very little work has been done previously on POS tagging of Bengali.
Bengali is the main language spoken in Bangladesh, the second most commonly spoken language in India, and the fourth most commonly spoken language in the world.
Ray et al.(2003) describes a morphologybased disambiguation for Hindi POS tagging.
System using a decision tree based learning algorithm (CN2) has been developed for statistical Hindi POS tagging (Singh et al., 2006).
A reasonably good accuracy POS tagger for Hindi has been developed using Maximum Entropy Markov Model (Dalal et al., 2007).
The system uses linguistic suffix and POS categories of a word along with other contextual features.
2 Our
Approach The problem of POS tagging can be formally stated as follows.
Given a sequence of words w 1 ??w n, we want to find the corresponding sequence of tags t 1 ??t n, drawn from a set of tags T.
We use a tagset of 40 tags 1. In this work, we explore supervised and semi-supervised bi-gram 1 http://www.mla.iitkgp.ernet.in/Tag.html 221 HMM and a ME based model.
The bi-gram assumption states that the POS-tag of a word depends on the current word and the POS tag of the previous word.
An ME model estimates the probabilities based on the imposed constraints.
Such constraints are derived from the training data, maintaining some relationship between features and outcomes.
The most probable tag sequence for a given word sequence satisfies equation (1) and (2) respectively for HMM and ME model: 1 1 ...
1, (|)(| )arg max ii ii ttn in SPwtPt??
= = ??
(1) 11 1, ( ...
| ...
) ( | )nn ii in p ttww pth = = ??
(2) Here, h i is the context for word w i . Since the basic bigram model of HMM as well as the equivalent ME models do not yield satisfactory accuracy, we wish to explore whether other available resources like a morphological analyzer can be used appropriately for better accuracy.
2.1 HMM
and ME based Taggers Three taggers have been implemented based on bigram HMM and ME model.
The first tagger (we shall call it HMM-S) makes use of the supervised HMM model parameters, whereas the second tagger (we shall call it HMM-SS) uses the semi supervised model parameters.
The third tagger uses ME based model to find the most probable tag sequence for a given sequence of words.
In order to further improve the tagging accuracy, we use a Morphological Analyzer (MA) and integrate morphological information with the models.
We assume that the POS-tag of a word w can take values from the set T MA (w), where T MA (w) is computed by the Morphological Analyzer.
Note that the size of T MA (w) is much smaller than T.
Thus, we have a restricted choice of tags as well as tag sequences for a given sentence.
Since the correct tag t for w is always in T MA (w) (assuming that the morphological analyzer is complete), it is always possible to find out the correct tag sequence for a sentence even after applying the morphological restriction.
Due to a much reduced set of possibilities, this model is expected to perform better for both the HMM (HMM-S and HMM-SS) and ME models even when only a small amount of labeled training text is available.
We shall call these new models HMM-S+MA, HMM-SS+ MA and ME+MA.
Our MA has high accuracy and coverage but it still has some missing words and a few errors.
For the purpose of these experiments we have made sure that all words of the test set are present in the root dictionary that an MA uses.
While MA helps us to restrict the possible choice of tags for a given word, one can also use suffix information (i.e., the sequence of last few characters of a word) to further improve the models.
For HMM models, suffix information has been used during smoothing of emission probabilities, whereas for ME models, suffix information is used as another type of feature.
We shall denote the models with suffix information with a ??suf?? marker.
Thus, we have ??HMM-S+suf, HMMS+suf+MA, HMM-SS+suf etc.
2.1.1 Unknown
Word Hypothesis in HMM The transition probabilities are estimated by linear interpolation of unigrams and bigrams.
For the estimation of emission probabilities add-one smoothing or suffix information is used for the unknown words.
If the word is unknown to the morphological analyzer, we assume that the POS-tag of that word belongs to any of the open class grammatical categories (all classes of Noun, Verb, Adjective, Adverb and Interjection).
2.1.2 Features
of the ME Model Experiments were carried out to find out the most suitable binary valued features for the POS tagging in the ME model.
The main features for the POS tagging task have been identified based on the different possible combination of the available word and tag context.
The features also include prefix and suffix up to length four.
We considered different combinations from the following set for obtaining the best feature set for the POS tagging task with the data we have.
{ }112 212,,,,,,, 4, 4iii i i i iFwwwwwtt pre suf+? +?= ? Forty different experiments were conducted taking several combinations from set ?F??to identify the best suited feature set for the POS tagging task.
From our empirical analysis we found that the combination of contextual features (current word and previous tag), prefixes and suffixes of length ??4 gives the best performance for the ME model.
It is interesting to note that the inclusion of prefix and suffix for all words gives better result instead of using only for rare words as is described in Ratnaparkhi (1996).
This can be explained by the fact that due to small amount of annotated data, a significant number of instances 222 are not found for most of the word of the language vocabulary.
3 Experiments
We have a total of 12 models as described in subsection 2.1 under different stochastic tagging schemes.
The same training text has been used to estimate the parameters for all the models.
The model parameters for supervised HMM and ME models are estimated from the annotated text corpus.
For semi-supervised learning, the HMM learned through supervised training is considered as the initial model.
Further, a larger unlabelled training data has been used to re-estimate the model parameters of the semi-supervised HMM.
The experiments were conducted with three different sizes (10K, 20K and 40K words) of the training data to understand the relative performance of the models as we keep on increasing the size of the annotated data.
3.1 Training
Data The training data includes manually annotated 3625 sentences (approximately 40,000 words) for both supervised HMM and ME model.
A fixed set of 11,000 unlabeled sentences (approximately 100,000 words) taken from CIIL corpus 2 are used to re-estimate the model parameter during semi-supervised learning.
It has been observed that the corpus ambiguity (mean number of possible tags for each word) in the training text is 1.77 which is much larger compared to the European languages (Dermatas et al., 1995).
3.2 Test
Data All the models have been tested on a set of randomly drawn 400 sentences (5000 words) disjoint from the training corpus.
It has been noted that 14% words in the open testing text are unknown with respect to the training set, which is also a little higher compared to the European languages (Dermatas et al., 1995) 3.3 Results We define the tagging accuracy as the ratio of the correctly tagged words to the total number of words.
Table 1 summarizes the final accuracies achieved by different learning methods with the varying size of the training data.
Note that the baseline model (i.e., the tag probabilities depends 2 A part of the EMILE/CIIL corpus developed at Central Institute of Indian Languages (CIIL), Mysore.
only on the current word) has an accuracy of 76.8%.
Accuracy Method 10K 20K 40K HMM-S 57.53 70.61 77.29 HMM-S+suf 75.12 79.76 83.85 HMM-S+MA 82.39 84.06 86.64 HMM-S+suf+MA 84.73 87.35 88.75 HMM-SS 63.40 70.67 77.16 HMM-SS+suf 75.08 79.31 83.76 HMM-SS+MA 83.04 84.47 86.41 HMM-SS+suf+MA 84.41 87.16 87.95 ME 74.37 79.50 84.56 ME+suf 77.38 82.63 86.78 ME+MA 82.34 84.97 87.38 ME+suf+MA 84.13 87.07 88.41 Table 1: Tagging accuracies (in %) of different models with 10K, 20K and 40K training data.
3.4 Observations
We find that in both the HMM based models (HMM-S and HMM-SS), the use of suffix information as well as the use of a morphological analyzer improves the accuracy of POS tagging with respect to the base models.
The use of MA gives better results than the use of suffix information.
When we use both suffix information as well as MA, the results is even better.
HMM-SS does better than HMM-S when very little tagged data is available, for example, when we use 10K training corpus.
However, the accuracy of the semi-supervised HMM models are slightly poorer than that of the supervised HMM models for moderate size training data and use of suffix information.
This discrepancy arises due to the over-fitting of the supervised models in the case of small training data; the problem is alleviated with the increase in the annotated data.
As we have noted already the use of MA and/or suffix information improves the accuracy of the POS tagger.
But what is significant to note is that the percentage of improvement is higher when the amount of training data is less.
The HMMS+suf model gives an improvement of around 18%, 9% and 6% over the HMM-S model for 10K, 20K and 40K training data respectively.
Similar trends are observed in the case of the semi-supervised HMM and the ME models.
The use of morphological restriction (HMM-S+MA) gives an improvement of 25%, 14% and 9% respectively over the HMM-S in case of 10K, 20K 223 and 40K training data.
As the improvement due to MA decreases with increasing data, it might be concluded that the use of morphological restriction may not improve the accuracy when a large amount of training data is available.
From our empirical observations we found that both suffix and morphological restriction (HMMS+suf+MA) gives an improvement of 27%, 17% and 12% over the HMM-S model respectively for the three different sizes of training data.
The Maximum Entropy model does better than the HMM models for smaller training data.
But with higher amount of training data the performance of the HMM and ME model are comparable.
Here also we observe that suffix information and MA have positive effect, and the effect is higher with poor resources.
Furthermore, in order to estimate the relative performance of the models, experiments were carried out with two existing taggers: TnT (Brants, 2000) and ACOPOST 3 . The accuracy achieved using TnT are 87.44% and 87.36% respectively with bigram and trigram model for 40K training data.
The accuracy with ACOPOST is 86.3%.
This reflects that the higher order Markov models do not work well under the current experimental setup.
3.5 Assessment
of Error Types Table 2 shows the top five confusion classes for HMM-S+MA model.
The most common types of errors are the confusion between proper noun and common noun and the confusion between adjective and common noun.
This results from the fact that most of the proper nouns can be used as common nouns and most of the adjectives can be used as common nouns in Bengali.
Actual Class (frequency) Predicted Class % of total errors % of class errors NP(251) NN 21.03 43.82 JJ(311) NN 5.16 8.68 NN(1483) JJ 4.78 1.68 DTA(100) PP 2.87 15.0 NN(1483) VN 2.29 0.81 Table 2: Five most common types of errors Almost all the confusions are wrong assignment due to less number of instances in the training corpora, including errors due to long distance phenomena.
3 http://maxent.sourceforge.net 4 Conclusion In this paper we have described an approach for automatic stochastic tagging of natural language text for Bengali.
The models described here are very simple and efficient for automatic tagging even when the amount of available annotated text is small.
The models have a much higher accuracy than the nave baseline model.
However, the performance of the current system is not as good as that of the contemporary POStaggers available for English and other European languages.
The best performance is achieved for the supervised learning model along with suffix information and morphological restriction on the possible grammatical categories of a word.
In fact, the use of MA in any of the models discussed above enhances the performance of the POS tagger significantly.
We conclude that the use of morphological features is especially helpful to develop a reasonable POS tagger when tagged resources are limited.
References A.
Dalal, K.
Nagaraj, U.
Swant, S.
Shelke and P.
Bhattacharyya. 2007.
Building Feature Rich POS Tagger for Morphologically Rich Languages: Experience in Hindi.
ICON, 2007.
A. Ratnaparkhi, 1996.
A maximum entropy part-of-speech tagger.
EMNLP 1996.
pp. 133-142.
D. Cutting, J.
Kupiec, J.
Pederson and P.
Sibun. 1992.
A practical part-of-speech tagger.
In Proc.
of the 3rd Conference on Applied NLP, pp.
133-140. E.
Dermatas and K.
George. 1995.
Automatic stochastic tagging of natural language texts.
Computational Linguistics, 21(2): 137-163.
M. Shrivastav, R.
Melz, S.
Singh, K.
Gupta and P.
Bhattacharyya, 2006.
Conditional Random Field Based POS Tagger for Hindi.
In Proceedings of the MSPIL, pp.
63-68. P.
R. Ray, V.
Harish, A.
Basu and S.
Sarkar, 2003.
Part of Speech Tagging and Local Word Grouping Techniques for Natural Language Processing.
ICON 2003.
S. Singh, K.
Gupta, M.
Shrivastav and P.
Bhattacharyya, 2006.
Morphological Richness Offset Resource Demand ??Experience in constructing a POS Tagger for Hindi.
COLING/ACL 2006, pp.
779-786. T.
Brants. 2000.
TnT ??A statistical part-of-sppech tagger.
In Proc.
of the 6th Applied NLP Conference, pp. 224-231 .
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 225??28, Prague, June 2007.
c2007 Association for Computational Linguistics Japanese Dependency Parsing Using Sequential Labeling for Semi-spoken Language Kenji Imamura and Genichiro Kikui NTT Cyber Space Laboratories, NTT Corporation 1-1 Hikarinooka, Yokosuka-shi, Kanagawa, 239-0847, Japan {imamura.kenji, kikui.genichiro}@lab.ntt.co.jp Norihito Yasuda NTT Communication Science Laboratories, NTT Corporation 2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan n-yasuda@cslab.kecl.ntt.co.jp Abstract The amount of documents directly published by end users is increasing along with the growth of Web 2.0.
Such documents often contain spoken-style expressions, which are difficult to analyze using conventional parsers.
This paper presents dependency parsing whose goal is to analyze Japanese semi-spoken expressions.
One characteristic of our method is that it can parse selfdependent (independent) segments using sequential labeling.
1 Introduction
Dependency parsing is a way of structurally analyzing a sentence from the viewpoint of modification.
In Japanese, relationships of modification between phrasal units called bunsetsu segments are analyzed.
A number of studies have focused on parsing of Japanese as well as of other languages.
Popular parsers are CaboCha (Kudo and Matsumoto, 2002) and KNP (Kurohashi and Nagao, 1994), which were developed to analyze formal written language expressions such as that in newspaper articles.
Generally, the syntactic structure of a sentence is represented as a tree, and parsing is carried out by maximizing the likelihood of the tree (Charniak, 2000; Uchimoto et al., 1999).
Units that do not modify any other units, such as fillers, are difficult to place in the tree structure.
Conventional parsers have forced such independent units to modify other units.
Documents published by end users (e.g., blogs) are increasing on the Internet along with the growth of Web 2.0.
Such documents do not use controlled written language and contain fillers and emoticons.
This implies that analyzing such documents is difficult for conventional parsers.
This paper presents a new method of Japanese dependency parsing that utilizes sequential labeling based on conditional random fields (CRFs) in order to analyze semi-spoken language.
Concretely, sequential labeling assigns each segment a dependency label that indicates its relative position of dependency.
If the label set includes self-dependency, the fillers and emoticons would be analyzed as segments depending on themselves.
Therefore, since it is not necessary for the parsing result to be a tree, our method is suitable for semi-spoken language.
2 Methods
Japanese dependency parsing for written language is based on the following principles.
Our method relaxes the first principle to allow self-dependent segments (c.f.
Section 2.3). 1.
Dependency moves from left to right.
2. Dependencies do not cross each other.
3. Each segment, except for the top of the parsed tree, modifies at most one other segment.
2.1 Dependency
Parsing Using Cascaded Chunking (CaboCha) Our method is based on the cascaded chunking method (Kudo and Matsumoto, 2002) proposed as the CaboCha parser 1. CaboCha is a sort of shiftreduce parser and determines whether or not a segment depends on the next segment by using an 1 http://www.chasen.org/?taku/software/cabocha/ 225 SVM-based classifier.
To analyze long-distance dependencies, CaboCha shortens the sentence by removing segments for which dependencies are already determined and which no other segments depend on.
CaboCha constructs a tree structure by repeating the above process.
2.2 Sequential
Labeling Sequential labeling is a process that assigns each unit of an input sequence an appropriate label (or tag).
In natural language processing, it is applied to, for example, English part-of-speech tagging and named entity recognition.
Hidden Markov models or conditional random fields (Lafferty et al., 2001) are used for labeling.
In this paper, we use linearchain CRFs.
In sequential labeling, training data developers can design labels with no restrictions.
2.3 Cascaded
Chunking Using Sequential Labeling The method proposed in this paper is a generalization of CaboCha.
Our method considers not only the next segment, but also the followingN segments to determine dependencies.
This area, including the considered segment, is called the window, and N is called the window size.
The parser assigns each segment a dependency label that indicates where the segment depends on the segments in the window.
The flow is summarized as follows: 1.
Extract features from segments such as the part-of-speech of the headword in a segment (c.f.
Section 3.1). 2.
Carry out sequential labeling using the above features.
3. Determine the actual dependency by interpreting the labels.
4. Shorten the sentence by deleting segments for which the dependency is already determined and that other segments have never depended on.
5. If only one segment remains, then finish the process.
If not, return to Step 1.
An example of dependency parsing for written language is shown in Figure 1 (a).
In Steps 1 and 2, dependency labels are supplied to each segment in a way similar to that used by Label Description ??Segment depends on a segment outside of window.
0Q Self-dependency 1D Segment depends on next segment.
2D Segment depends on segment after next.
-1O Segment is top of parsed tree.
Table 1: Label List Used by Sequential Labeling (Window Size: 2) other sequential labeling methods.
However, our sequential labeling has the following characteristics since this task is dependency parsing.
??The labels indicate relative positions of the dependent segment from the current segment (Table 1).
Therefore, the number of labels changes according to the window size.
Long-distance dependencies can be parsed by one labeling process if we set a large window size.
However, growth of label variety causes data sparseness problems.
??One possible label is that of self-dependency (noted as ??Q??in this paper).
This is assigned to independent segments in a tree.
??Also possible are two special labels.
Label ??1O?? denotes a segment that is the top of the parsed tree.
Label ??denotes a segment that depends on a segment outside of the window.
When the window size is two, the segment depends on a segment that is over two segments ahead.
??The label for the current segment is determined based on all features in the window and on the label of the previous segment.
In Step 4, segments, which no other segments depend on, are removed in a way similar to that used by CaboCha.
The principle that dependencies do not cross each other is applied in this step.
For example, if a segment depends on a segment after the next, the next segment cannot be modified by other segments.
Therefore, it can be removed.
Similarly, since the ??label indicates that the segment depends on a segment after N segments, all intermediate segments can be removed if they do not have ??labels.
The sentence is shortened by iteration of the above steps.
The parsing finishes when only one segment remains in the sentence (this is the segment 226 (a) Written Language --2D 1D 1D -1O 2D 1D -1O Output Input Label Label kare wa (he) kanojo no (her) atatakai (warm) magokoro ni (heart) kando-shita.
(be moved) (He was moved by her warm heart.) Seg.
No. 1 2 3 4 5 kare wa (he) kanojo no (her) atatakai (warm) magokoro ni (heart) kando-shita.
(be moved) (b) Semi-spoken Language Input Uuuum, kyo wa (today) ...... choshi (condition) yokatta desu.
(be good) 0Q --0Q 1D -1O 1D -1O (Uuuum, my condition .... was good today.) Seg.
No. 1 2 3 4 5 Label Label Uuuum, kyo wa (today) ...... choshi (condition) yokatta desu.
(be good) Output 1st Labeling 2nd Labeling Figure 1: Examples of Dependency Parsing (Window Size: 2) Corpus Type # of Sentences # of Segments Kyoto Training 24,283 234,685 Test 9,284 89,874 Blog Training 18,163 106,177 Test 8,950 53,228 Table 2: Corpus Size at the top of the parsed tree).
In the example in Figure 1 (a), the process finishes in two iterations.
In a sentence containing fillers, the selfdependency labels are assigned by sequential labeling, as shown in Figure 1 (b), and are parsed as independent segments.
Therefore, our method is suitable for parsing semi-spoken language that contains independent segments.
3 Experiments
3.1 Experimental Settings Corpora In our experiments, we used two corpora.
One is the Kyoto Text Corpus 4.0 2, which is a collection of newspaper articles with segment and dependency annotations.
The other is a blog corpus, which is a collection of blog articles taken as semi-spoken language.
The blog corpus is manually annotated in a way similar to that used for the Kyoto text corpus.
The sizes of the corpora are shown in Table 2.
Training We used CRF++ 3, a linear-chain CRF training tool, with eleven features per segment.
All 2 http://nlp.kuee.kyoto-u.ac.jp/nl-resource/corpus.html 3 http://www.chasen.org/?taku/software/CRF++/ of these are static features (proper to each segment) such as surface forms, parts-of-speech, inflections of a content headword and a functional headword in a segment.
These are parts of a feature set that many papers have referenced (Uchimoto et al., 1999; Kudo and Matsumoto, 2002).
Evaluation Metrics Dependency accuracy and sentence accuracy were used as evaluation metrics.
Sentence accuracy is the proportion of total sentences in which all dependencies in the sentence are accurately labeled.
In Japanese, the last segment of most sentences is the top of the parsed trees, and many papers exclude this last segment from the accuracy calculation.
We, in contrast, include the last one because some of the last segments are selfdependent.
3.2 Accuracy
of Dependency Parsing Dependency parsing was carried out by combining training and test corpora.
We used a window size of three.
We also used CaboCha as a reference for the set of sentences trained only with the Kyoto corpus because it is designed for written language.
The results are shown in Table 3.
CaboCha had better accuracies for the Kyoto test corpus.
One reason might be that our method manually combined features and used parts of combinations, while CaboCha automatically finds the best combinations by using second-order polynomial kernels.
For the blog test corpus, the proposed method using the Kyoto+Blog model had the best depen227 Test Corpus Method Training Corpus Dependency Accuracy Sentence Accuracy (Model) Kyoto Proposed Method Kyoto 89.87% (80766 / 89874) 48.12% (4467 / 9284) (Written Language) (Window Size: 3) Kyoto + Blog 89.76% (80670 / 89874) 47.63% (4422 / 9284) CaboCha Kyoto 92.03% (82714 / 89874) 55.36% (5140 / 9284) Blog Proposed Method Kyoto 77.19% (41083 / 53226) 41.41% (3706 / 8950) (Semi-spoken Language) (Window Size: 3) Kyoto + Blog 84.59% (45022 / 53226) 52.72% (4718 / 8950) CaboCha Kyoto 77.44% (41220 / 53226) 43.45% (3889 / 8950) Table 3: Dependency and Sentence Accuracies among Methods/Corpora 88 88.5 89 89.5 90 90.5 91 1 2 3 4 5 0 2e+06 4e+06 6e+06 8e+06 1e+07 Dependency Accuracy (%) # of Features Window Size Dependency Accuracy # of Features Figure 2: Dependency Accuracy and Number of Features According to Window Size (The Kyoto Text Corpus was used for training and testing.) dency accuracy result at 84.59%.
This result was influenced not only by the training corpus that contains the blog corpus but also by the effect of selfdependent segments.
The blog test corpus contains 3,089 self-dependent segments, and 2,326 of them (75.30%) were accurately parsed.
This represents a dependency accuracy improvement of over 60% compared with the Kyoto model.
Our method is effective in parsing blogs because fillers and emoticons can be parsed as selfdependent segments.
3.3 Accuracy
According to Window Size Another characteristic of our method is that all dependencies, including long-distance ones, can be parsed by one labeling process if the window covers the entire sentence.
To analyze this characteristic, we evaluated dependency accuracies in various window sizes.
The results are shown in Figure 2.
The number of features used for labeling increases exponentially as window size increases.
However, dependency accuracy was saturated after a window size of two, and the best accuracy was when the window size was four.
This phenomenon implies a data sparseness problem.
4 Conclusion
We presented a new dependency parsing method using sequential labeling for the semi-spoken language that frequently appears in Web documents.
Sequential labeling can supply segments with flexible labels, so our method can parse independent words as self-dependent segments.
This characteristic affects robust parsing when sentences contain fillers and emoticons.
The other characteristics of our method are using CRFs and that long dependencies are parsed in one labeling process.
SVM-based parsers that have the same characteristics can be constructed if we introduce multi-class classifiers.
Further comparisons with SVM-based parsers are future work.
References Eugene Charniak.
2000. A maximum-entropy-inspired parser.
In Proc.
of NAACL-2000, pages 132??39.
Taku Kudo and Yuji Matsumoto.
2002. Japanese dependency analyisis using cascaded chunking.
In Proc.
of CoNLL-2002, Taipei.
Sadao Kurohashi and Makoto Nagao.
1994. A syntactic analysis method of long Japanese sentences based on the detection of conjunctive structures.
Computational Linguistics, 20(4):507??34.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data.
In Proc.
of ICML-2001, pages 282??89.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara.
1999. Japanese dependency structure analysis based on maximum entropy models.
In Proc.
of EACL??9, pages 196??03, Bergen, Norway .
Proceedings of ACL-08: HLT, pages 8996,
Columbus, Ohio, USA, June 2008. c2008 Association for Computational Linguistics
Measure Word Generation for English-Chinese SMT Systems 
 
 
Dongdong Zhang
1, Mu Li
1, Nan Duan
2, Chi-Ho Li
1, Ming Zhou
1
 
1
Microsoft Research Asia 
2
Tianjin University 
Beijing, China Tianjin, China 
{dozhang,muli,v-naduan,chl,mingzhou}@microsoft.com 
 
 
 
 
 
 
Abstract 
Measure words in Chinese are used to indi-
cate the count of nouns. Conventional sta-
tistical machine translation (SMT) systems do 
not perform well on measure word generation 
due to data sparseness and the potential long 
distance dependency between measure words 
and their corresponding head words. In this 
paper, we propose a statistical model to gen-
erate appropriate measure words of nouns for 
an English-to-Chinese SMT system. We mod-
el the probability of measure word generation 
by utilizing lexical and syntactic knowledge 
from both source and target sentences. Our 
model works as a post-processing procedure 
over output of statistical machine translation 
systems, and can work with any SMT system. 
Experimental results show our method can 
achieve high precision and recall in measure 
word generation. 
1 Introduction

In linguistics, measure words (MW) are words or 
morphemes used in combination with numerals or 
demonstrative pronouns to indicate the count of 
nouns
1, which are often referred to as head words 
(HW). 
Chinese measure words are grammatical units 
and occur quite often in real text. According to our 
survey on the measure word distribution in the 
Chinese Penn Treebank and the test datasets distri-
buted by Linguistic Data Consortium (LDC) for 
Chinese-to-English machine translation evaluation, 
the average occurrence is 0.505 and 0.319 measure 
                                                 
1
 The uncommon cases of verbs are not considered. 
words per sentence respectively. Unlike in Chinese, 
there is no special set of measure words in English. 
Measure words are usually used for mass nouns 
and any semantically appropriate nouns can func-
tion as the measure words. For example, in the 
phrase three bottles of water, the word bottles acts 
as a measure word. Countable nouns are almost 
never modified by measure words
2
. Numerals and 
indefinite articles are directly followed by counta-
ble nouns to denote the quantity of objects.  
Therefore, in the English-to-Chinese machine 
translation task we need to take additional efforts 
to generate the missing measure words in Chinese. 
For example, when translating the English phrase 
three books into the Chinese phrases  , 
where three corresponds to the numeral   and 
books corresponds to the noun  , the Chinese 
measure word   should be generated between 
the numeral and the noun.  
In most statistical machine translation (SMT) 
models (Och et al., 2004; Koehn et al., 2003; 
Chiang, 2005), some of measure words can be 
generated without modification or additional 
processing. For example, in above translation, the 
phrase translation table may suggest the word three 
be translated into  ,  ,  , etc, and 
the word books into  ,  ,   (scroll), 
etc. Then the SMT model selects the most likely 
combination   as the final translation re-
sult. In this example, a measure word candidate set 
consisting of   and   can be generated by 
bilingual phrases (or synchronous translation rules), 
and the best measure word   from the measure  
                                                 
2
 There are some exceptional cases, such as 100 head of cat-
tle. But they are very uncommon. 
89
 
 
 
 
 
 
 
 
 
 
 
 
 
 
word candidate set can be selected by the SMT 
decoder. However, as we will show below, existing 
SMT systems do not deal well with the measure 
word generation in general due to data sparseness 
and long distance dependencies between measure 
words and their corresponding head words.  
Due to the limited size of bilingual corpora, 
many measure words, as well as the collocations 
between a measure and its head word, cannot be 
well covered by the phrase translation table in an 
SMT system. Moreover, Chinese measure words 
often have a long distance dependency to their 
head words which makes language model ineffec-
tive in selecting the correct measure words from 
the measure word candidate set. For example, in 
Figure 1 the distance between the measure word 
  and its head word   (undertaking) is 15. 
In this case, an n-gram language model with n<15 
cannot capture the MW-HW collocation. Table 1 
shows the relative positions distribution of head 
words around measure words in the Chinese Penn 
Treebank, where a negative position indicates that 
the head word is to the left of the measure word 
and a positive position indicates that the head word 
is to the right of the measure word. Although lots 
of measure words are close to the head words they 
modify, more than sixteen percent of measure 
words are far away from their corresponding head 
words (the absolute distance is more than 5). 
To overcome the disadvantage of measure word 
generation in a general SMT system, this paper 
proposes a dedicated statistical model to generate 
measure words for English-to-Chinese translation. 
We model the probability of measure word gen-
eration by utilizing rich lexical and syntactic 
knowledge from both source and target sentences. 
Three steps are involved in our method to generate 
measure words: Identifying the positions to gener-
ate measure words, collecting the measure word 
candidate set and selecting the best measure word. 
Our method is performed as a post-processing pro-
cedure of the output of SMT systems. The advan-
tage is that it can be easily integrated into any SMT 
system. Experimental results show our method can 
significantly improve the quality of measure word 
generation. We also compared the performance of 
our model based on different contextual informa-
tion, and show that both large-scale monolingual 
data and parallel bilingual data can be helpful to 
generate correct measure words. 
Position Occurrence Position Occurrence
1 39.5% -1 0 
2 15.7% -2 0 
3 4.7% -3 8.7% 
4 1.4% -4 6.8% 
5 2.1% -5 4.3% 
>5 8.8% <-5 8.0% 
Table 1. Position distribution of head words 
2 Our
Method 
2.1 Measure
word  generation in Chinese 
In Chinese, measure words are obligatory in cer-
tain contexts, and the choice of measure word 
usually depends on the head words semantics (e.g., 
shape or material). The set of Chinese measure 
words is a relatively close set and can be classified 
into two categories based on whether they have a 
corresponding English translation. Those not hav-
ing an English counterpart need to be generated 
during translation. For those having English trans-
lations, such as   (meter),   (ton), we just 
use the translation produced by the SMT system 
itself. According to our survey, about 70.4% of 
measure words in the Chinese Penn Treebank need 
Figure 1.  Example of long distance dependency between MW and its modified HW 
 / / 
 /  / 
    
Pudong 's de-
velopment and 
opening up is
a 
century-spanning 
/ /
 / 
for vigorously promoting shanghai 
and constructing a modern econom-
ic , trade , and financial center  undertaking
 / /  /   /  /
/  /  /   / /  /  / 
  
. 

90
 
to be explicitly generated during the translation 
process. 
In Chinese, there are generally stable linguistic 
collocations between measure words and their head 
words. Once the head word is determined, the col-
located measure word can usually be selected ac-
cordingly. However, there is no easy way to identi-
fy head words in target Chinese sentences since for 
most of the time an SMT output is not a well 
formed sentence due to translation errors. Mistake 
of head word identification may cause low quality 
of measure word generation. In addition, some-
times the head word itself is not enough to deter-
mine the measure word. For example, in Chinese 
sentences  5   (there are five people 
in his family) and  5   (a 
total of five people attended the meeting), where 
  (people) is the head word collocated with two 
different measure words   and  , we cannot 
determine the measure word just based on the head 
word  .   
2.2 Framework

In our framework, a statistical model is used to 
generate measure words. The model is applied to 
SMT system outputs as a post-processing proce-
dure. Given an English source sentence, an SMT 
decoder produces a target Chinese translation, in 
which positions for measure word generation are 
identified. Based on contextual information con-
tained in both input source sentence and SMT sys-
tems output translation, a measure word candidate 
set M is constructed. Then a measure word selec-
tion model is used to select the best one from M. 
Finally, the selected measure word is inserted into 
previously determined measure word slot in the 
SMT systems output, yielding the final translation 
result. 
2.3 Measure
word position identification 
To identify where to generate measure words in the 
SMT outputs, all positions after numerals are 
marked at first since measure words often follow 
numerals. For other cases in which measure words 
do not follow numerals (e.g.,  / /  
(many computers), where   is a measure word 
and   (computers) is its head word), we just 
mine the set of words which can be followed by 
measure words from training corpus.  Most of 
words in the set are pronouns such as   (this), 
  (that) and   (several). In the SMT out-
put, the positions after these words are also identi-
fied as candidate positions to generate measure 
words.  
2.4 Candidate
measure word generation 
To avoid high computation cost, the measure word 
candidate set only consists of those measure words 
which can form valid MW-HW collocations with 
their head words. We assume that all the surround-
ing words within a certain window size centered on 
the given position to generate a measure word are 
potential head words, and require that a measure 
word candidate must collocate with at least one of 
the surrounding words. Valid MW-HW colloca-
tions are mined from the training corpus and a sep-
arate lexicon resource.  
There is a possibility that the real head word is 
outside the window of given size. To address this 
problem, we also use a source window centered on 
the position p
s, which is aligned to the target meas-
ure word position p
t
. The link between p
s
 and p
t
 
can be inferred from SMT decoding result. Thus, 
the chance of capturing the best measure word in-
creases with the aid of words located in the source 
window. For example, given the window size of 10, 
although the target head word   (undertaking) 
in Figure 1 is located outside the target window, its 
corresponding source head word undertaking can 
be found in the source window. Based on this 
source head word, the best measure word   will 
be included into the candidate measure word set. 
This example shows how bilingual information can 
enrich the measure word candidate set. 
Another special word {NULL} is always in-
cluded in the measure word candidate set. {NULL} 
represents those measure words having a corres-
ponding English translation as mentioned in Sec-
tion 2.1. If {NULL} is selected, it means that we 
need not generate any measure word at the current 
position. Thus, no matter what kinds of measure 
words they are, we can handle the issue of measure 
word generation in a unified framework.  
2.5 Measure
word selection model 
After obtaining the measure word candidate set M, 
a measure word selection model is employed to 
select the best one from M. Given the contextual 
information C in both source window and target 
91
 
window, we model the measure word selection as 
finding the measure word m* with highest post-
erior probability given C: 


=argmax
nullnull
(|)                   (1) 
To leverage the collocation knowledge between 
measure words and head words, we extend (1) by 
introducing a hidden variable h where H represents 
all candidate head words located within the target 
window: 
     

=argmax
nullnull
  ( ,| )
nullnull
 
           = argmax
nullnull
  ( | ) (|,)
nullnull
  (2) 
In (2),  ( | )  is the head word selection proba-
bility and is empirically estimated according to the 
position distribution of head words in Table 1. 
(|,)  is the conditional probability of m given 
both h and C. We use maximum entropy model to 
compute (|,) : 
            (|,) =
exp(  

 

(,)

)
 exp(  

 

(
,)

)



     (3) 
Based on the different features used in the com-
putation of (|,) , we can train two sub-
models  a monolingual model (Mo-ME) which 
only uses monolingual (Chinese) features and a 
bilingual model (Bi-ME) which integrates bilingual 
features. The advantage of the Mo-ME model is 
that it can employ an unlimited monolingual target 
training corpora, while the Bi-ME model leverages 
rich features including both the source and target 
information and may improve the precision. Com-
pared to the Mo-ME model, the Bi-ME model suf-
fers from small scale of parallel training data. To 
leverage advantages of both models, we use a 
combined model Co-ME, by linearly combing the 
monolingual and bilingual sub-models: 


=argmax
nullnull

nullnullnullnullnull
 + ( 1 ) 
nullnullnullnullnull
  
where [0,1]  is a free parameter that can be op-
timized on held-out data and it was set to 0.39 in 
our experiments. 
2.6 Features

The computation of Formula (3) involves the fea-
tures listed in Table 2 where the Mo-ME model 
only employs target features and the Bi-ME model 
leverages both target features and source features.  
For target features, n-gram language model 
score is defined as the sum of log n-gram probabil-
ities within the target window after the measure 
word is filled into the measure word slot. The 
MW-HW collocation feature is defined to be a 
function f
1
 to capture the collocation between a 
measure word and a head word. For features of 
surrounding words, the feature function f
2
 is de-
fined as 1 if a certain word exists at a certain posi-
tion, otherwise 0. For example, f
2
( ,-2)=1 means 
the second word on the left is  . f
2
( ,3)=1 
means the third word on the right is  . For 
punctuation position feature function f
3, the feature 
value is 1 when there is a punctuation following 
the measure word, which indicates the target head 
word may appear to the left of measure word. Oth-
erwise, it is 0. In practice, we can also ignore the 
position part, i.e., a word appears anywhere within 
the window is viewed as the same feature. 
 Target features Source features 
n-gram language model 
score 
MW-HW collocation
MW-HW collocation surrounding words 
surrounding words source head word 
punctuation position POS tags 
Table 2. Features used in our model 
For source language side features, MW-HW col-
location and surrounding words are used in a simi-
lar way as does with target features. The source 
head word feature is defined to be a function f
4
 to 
indicate whether a word e
i
 is the source head word 
in English according to a parse tree of the source 
sentence. Similar to the definition of lexical fea-
tures, we also use a set of features based on POS 
tags of source language. 
3 Model
Training and Application 
3.1 Training

We parsed English and Chinese sentences to get 
training samples for measure word generation 
model. Based on the source syntax parse tree, for 
each measure word, we identified its head word by 
using a toolkit from (Chiang and Bikel, 2002) 
which can heuristically identify head words for 
sub-trees. For the bilingual corpus, we also per-
form word alignment to get correspondences be-
tween source and target words. Then, the colloca-
tion between measure words and head words and 
their surrounding contextual information are ex-
tracted to train the measure word selection models. 
According to word alignment results, we classify 
92
 
measure words into two classes based on whether 
they have non-null translations. We map Chinese 
measure words having non-null translations to a 
unified symbol {NULL} as mentioned in Section 
2.4, indicating that we need not generate these kind 
of measure words since they can be translated from 
English.  
In our work, the Berkeley parser (Petrov and 
Klein, 2007) was employed to extract syntactic 
knowledge from the training corpus. We ran GI-
ZA++ (Och and Ney, 2000) on the training corpus 
in both directions with IBM model 4, and then ap-
plied the refinement rule described in (Koehn et al., 
2003) to obtain a many-to-many word alignment 
for each sentence pair. We used the SRI Language 
Modeling Toolkit (Stolcke, 2002) to train a five-
gram model with modified Kneser-Ney smoothing 
(Chen and Goodman, 1998). The Maximum Entro-
py training toolkit from (Zhang, 2006) was em-
ployed to train the measure word selection model. 
3.2 Measure
word generation 
As mentioned in previous sections, we apply our 
measure word generation module into SMT output 
as a post-processing step. Given a translation from 
an SMT system, we first determine the position p
t
 
at which to generate a Chinese measure word. Cen-
tered on p
t, a surrounding word window with spe-
cified size is determined. From translation align-
ments, the corresponding source position p
s
 aligned 
to p
t
 can be referred.  In the same way, a source 
window centered on p
s
 is determined as well. Then, 
contextual information within the windows in the 
source and the target sentence is extracted and fed 
to the measure word selection model. Meanwhile, 
the candidate set is obtained based on words in 
both windows. Finally, each measure word in the 
candidate set is inserted to the position p
t, and its 
score is calculated based on the models presented 
in Section 2.5. The measure word with the highest 
probability will be chosen.  
There are two reasons why we perform measure 
word generation for SMT systems as a post-
processing step. One is that in this way our method 
can be easily applied to any SMT system. The oth-
er is that we can leverage both source and target 
information during the measure word generation 
process. We do not integrate our measure word 
generation module into the SMT decoder since 
there is only little target contextual information 
available during SMT decoding. Moreover, as we 
will show in experiment section, a pre-processing 
method does not work well when only source in-
formation is available. 
4 Experiments

4.1 Data

In the experiments, the language model is a Chi-
nese 5-gram language model trained with the Chi-
nese part of the LDC parallel corpus and the Xin-
hua part of the Chinese Gigaword corpus with 
about 27 million words. We used an SMT system 
similar to Chiang (2005), in which FBIS corpus is 
used as the bilingual training data. The training 
corpus for Mo-ME model consists of the Chinese 
Peen Treebank and the Chinese part of the LDC 
parallel corpus with about 2 million sentences. The 
Bi-ME model is trained with FBIS corpus, whose 
size is smaller than that used in Mo-ME model 
training. 
We extracted both development and test data set 
from years of NIST Chinese-to-English evaluation 
data by filtering out sentence pairs not containing 
measure words. The development set is extracted 
from NIST evaluation data from 2002 to 2004, and 
the test set consists of sentence pairs from NIST 
evaluation data from 2005 to 2006. There are 759 
testing cases for measure word generation in our 
test data consisting of 2746 sentence pairs. We use 
the English sentences in the data sets as input to 
the SMT decoder, and apply our proposed method 
to generate measure words for the output from the 
decoder. Measure words in Chinese sentences of 
the development and test sets are used as refer-
ences. When there are more than one measure 
words acceptable at some places, we manually 
augment the references with multiple acceptable 
measure words. 
4.2 Baseline

Our baseline is the SMT output where measure 
words are generated by a Hiero-like SMT decoder 
as discussed in Section 1. Due to noises in the Chi-
nese translations introduced by the SMT system, 
we cannot correctly identify all the positions to 
generate measure words. Therefore, besides preci-
sion we examine recall in our experiments. 
4.3 Evaluation
over SMT output 
Table 3 and Table 4 show the precision and recall 
of our measure word generation method. From the 
93
 
experimental results, the Mo-ME, Bi-ME and Co-
ME models all outperform the baseline. Compared 
with the baseline, the Mo-ME method takes advan-
tage of a large size monolingual training corpus 
and reduces the data sparseness problem. The ad-
vantage of the Bi-ME model is being able to make 
full use of rich knowledge from both source and 
target sentences. Also as shown in Table 3 and Ta-
ble 4, the Co-ME model always achieve the best 
results when using the same window size since it 
leverages the advantage of both the Mo-ME and 
the Bi-ME models. 
Wsize Baseline Mo-ME Bi-ME Co-ME
6  
 
54.82% 
64.29% 67.15%  67.66% 
8 64.93% 68.50% 69.00%
10 64.72% 69.40% 69.58%
12 65.46% 69.40% 69.76%
14 65.61% 69.69% 70.03% 
Table 3. Precision over SMT output 
Wsize Baseline Mo-ME Bi-ME Co-ME
6  
 
45.61% 
51.48% 53.69%  54.09% 
8 51.98% 54.75% 55.14%
10 51.81% 55.44% 55.58%
12 52.38% 55.44% 55.72%
14 52.50% 55.67% 55.93% 
Table 4. Recall over SMT output 
We can see that the Bi-ME model can achieve 
better results than the Mo-ME model in both recall 
and precision metrics although only a small sized 
bilingual corpus is used for Bi-ME model training. 
The reason is that the Mo-ME model cannot cor-
rectly handle the cases where head words are lo-
cated outside the target window. However, due to 
word order differences between English and Chi-
nese, when target head words are outside the target 
window, their corresponding source head words 
might be within the source window. The capacity 
of capturing head words is improved when both 
source and target windows are used, which demon-
strates that bilingual knowledge is useful for meas-
ure word generation. 
We compare the results for each model with dif-
ferent window sizes. Larger window size can lead 
to better results as shown in Table 3 and Table 4 
since more contextual knowledge is used to model 
measure word generation. However, enlarging the 
window size does not bring significant improve-
ments, The major reason is that even a small win-
dow size is already able to cover most of measure 
word collocations, as indicated by the position dis-
tribution of head words in Table 1.  
The quality of the SMT output also affects the 
quality of measure word generation since our me-
thod is performed in a post-processing step over 
the SMT output. Although translation errors de-
grade the measure word generation accuracy, we 
achieve about 15% improvement in precision and a 
10% increase in recall over baseline. We notice 
that the recall is relatively lower. Part of the reason 
is some positions to generate measure words are 
not successfully identified due to translation errors. 
In addition to precision and recall, we also evaluate 
the Bleu score (Papineni et al., 2002) changes be-
fore and after applying our measure word genera-
tion method to the SMT output. For our test data, 
we only consider sentences containing measure 
words for Bleu score evaluation. Our measure 
word generation step leads to a Bleu score im-
provement of 0.32 where the window size is set to 
10, which shows that it can improve the translation 
quality of an English-to-Chinese SMT system. 
4.4 Evaluation
over reference data 
To isolate the impact of the translation errors in 
SMT output on the performance of our measure 
word generation model, we conducted another ex-
periment with reference bilingual sentences in 
which measure words in Chinese sentences are 
manually removed. This experiment can show the 
performance upper bound of our method without 
interference from an SMT system. Table 5 shows 
the results. Compared to the results in Table 3, the 
precision improvement in the Mo-ME model is 
larger than that in the Bi-ME model, which shows 
that noisy translation of the SMT system has more 
serious influence on the Mo-ME model than the 
Bi-ME model. This also indicates that source in-
formation without noises is helpful for measure 
word generation. 
Wsize Mo-ME Bi-ME Co-ME 
6 71.63% 74.92% 75.72% 
8 73.80% 75.48% 76.20% 
10 73.80% 74.76% 75.48% 
12 73.80% 75.24% 75.96% 
14 73.56% 75.48% 76.44% 
Table 5. Results over reference data 
94
 
4.5 Impacts
of features 
In this section, we examine the contribution of 
both target language based features and source 
language based features in our model. Table 6 and 
Table 7 show the precision and recall when using 
different features. The window size is set to 10. In 
the tables, Lm denotes the n-gram language model 
feature, Tmh denotes the feature of collocation be-
tween target head words and the candidate measure 
word, Smh denotes the feature of collocation be-
tween source head words and the candidate meas-
ure word, Hs denotes the feature of source head 
word selection, Punc denotes the feature of target 
punctuation position, Tlex denotes surrounding 
word features in translation, Slex denotes surround-
ing word features in source sentence, and Pos de-
notes Part-Of-Speech feature. 
Feature setting Precision Recall 
Baseline 54.82% 45.61% 
Lm 51.11% 41.24% 
+Tmh 61.43% 49.22% 
+Punc 62.54% 50.08% 
+Tlex 64.80% 51.87% 
Table 6. Feature contribution in Mo-ME model 
Feature setting Precision Recall 
Baseline 54.82% 45.61% 
Lm 51.11% 41.24% 
+Tmh+Smh 64.50% 51.64% 
+Hs 65.32% 52.26% 
+Punc 66.29% 53.10% 
+Pos 66.53% 53.25% 
+Tlex 67.50% 54.02% 
+Slex 69.52% 55.54% 
Table 7. Feature contribution in Bi-ME model 
The experimental results show that all the fea-
tures can bring incremental improvements. The 
method with only Lm feature performs worse than 
the baseline. However, with more features inte-
grated, our method outperforms the baseline, 
which indicates each kind of features we selected 
is useful for measure word generation. According 
to the results, the feature of MW-HW collocation 
has much contribution to reducing the selection 
error of measure words given head words. The 
contribution of Slex feature explains that other sur-
rounding words in source sentence are also helpful 
since head word determination in source language 
might be incorrect due to errors in English parse 
trees. Meanwhile, the contribution from Smh, Hs 
and Slex features demonstrates that bilingual 
knowledge can play an important role for measure 
word generation. Compared with lexicalized fea-
tures, we do not get much benefit from the Pos 
features. 
4.6 Error
analysis 
We conducted an error analysis on 100 randomly 
selected sentences from the test data. There are 
four major kinds of errors as listed in Table 8. 
Most errors are caused by failures in finding posi-
tions to generate measure words. The main reason 
for this is some hint information used to identify 
measure word positions is missing in the noisy 
output of SMT systems. Two kinds of errors are 
introduced by incomplete head word and MW-HW 
collocation coverage, which can be solved by en-
larging the size of training corpus. There are also 
head word selection errors due to incorrect syntax 
parsing. 
Error type Ratio 
unseen head word  32.14% 
unseen MW-HW collocation 10.71% 
missing MW position 39.29% 
incorrect HW selection 10.71% 
others 7.14%
Table 8. Error distribution 
4.7 Comparison
with other methods 
In this section we compare our statistical methods 
with the pre-processing method and the rule-based 
methods for measure word generation in a transla-
tion task.  
In pre-processing method, only source language 
information is available. Given a source sentence, 
the corresponding syntax parse tree T
s
 is first con-
structed with an English parser. Then the pre-
processing method chooses the source head word 
h
s
 based on T
s
. The candidate measure word with 
the highest probability collocated with h
s
 is se-
lected as the best result, where the measure word 
candidate set corresponding to each head word is 
mined over a bilingual training corpus in advance. 
We achieved precision 58.62% and recall 49.25%, 
which are worse than the results of our post-
processing based methods. The weakness of the 
pre-processing method is twofold. One problem is 
data sparseness with respect to collocations be-
95
 
tween English head words and Chinese measure 
words. The other problem comes from the English 
head word selection error introduced by using 
source parse trees.  
We also compared our method with a well-
known rule-based machine translation system  
SYSTRAN
3
. We translated our test data with SY-
STRANs English-to-Chinese translation engine. 
The precision and recall are 63.82% and 51.09% 
respectively, which are also lower than our method.  
5 Related
Work  
Most existing rule-based English-to-Chinese MT 
systems have a dedicated module handling meas-
ure word generation. In general a rule-based me-
thod uses manually constructed rule patterns to 
predict measure words. Like most rule based ap-
proaches, this kind of system requires lots of hu-
man efforts of experienced linguists and usually 
cannot easily be adapted to a new domain. The 
most relevant work based on statistical methods to 
our research might be statistical technologies em-
ployed to model issues such as morphology gener-
ation (Minkov et al., 2007). 
6 Conclusion
and Future Work 
In this paper we propose a statistical model for 
measure word generation for English-to-Chinese 
SMT systems, in which contextual knowledge 
from both source and target sentences is involved. 
Experimental results show that our method not on-
ly achieves high precision and recall for generating 
measure words, but also improves the quality of 
English-to-Chinese SMT systems. 
In the future, we plan to investigate more fea-
tures and enlarge coverage to improve the quality 
of measure word generation, especially reduce the 
errors found in our experiments. 
Acknowledgements 
Special thanks to David Chiang, Stephan Stiller 
and the anonymous reviewers for their feedback 
and insightful comments. 

References 

Stanley F. Chen and Joshua Goodman. 1998. An Empir-
ical study of smoothing techniques for language 
                                                 
3
 http://www.systransoft.com/ 
modeling. Technical Report TR-10-98, Harvard Uni-
versity Center for Research in Computing Technolo-
gy, 1998. 

David Chiang and Daniel M. Bikel. 2002. Recovering 
latent information in treebanks. Proceedings of COL-
ING '02, 2002.  

David Chiang. 2005. A hierarchical phrase-based mod-
el for statistical machine translation. In Proceedings 
of ACL 2005, pages 263-270. 

Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. 
Statistical phrase-based translation. In Proceedings of 
HLT-NAACL 2003, pages 127-133.  

Einat Minkov, Kristina Toutanova, and Hisami Suzuki. 
2007. Generating complex morphology for machine 
translation. In Proceedings of 45th Annual Meeting 
of the ACL, pages 128-135. 

Franz J. Och and Hermann Ney. 2000. Improved statis-
tical alignment models. In Proceedings of 38th An-
nual Meeting of the ACL, pages 440-447.  

Franz J. Och and Hermann Ney. 2004. The alignment 
template approach to statistical machine translation. 
Computational Linguistics, 30:417-449. 

Kishore Papineni, Salim Roukos, ToddWard, and WeiJ-
ing Zhu. 2002. BLEU: a method for automatic evalu-
ation of machine translation. In Proceedings of 40th 
Annual Meeting of the ACL, pages 311-318. 

Slav Petrov and Dan Klein. 2007. Improved inference 
for unlexicalized parsing. In Proceedings of HLT-
NAACL, 2007. 

Andreas Stolcke. 2002. SRILM an extensible language 
modeling toolkit. In Proceedings of International 
Conference on Spoken Language Processing, volume 
2, pages 901-904.  

Le Zhang. MaxEnt toolkit. 2006. http://homepages.inf. 
ed.ac.uk/s0450736/maxent_toolkit.html  
96
Proceedings of ACL-08: HLT, pages 595603,
Columbus, Ohio, USA, June 2008. c2008 Association for Computational Linguistics
Simple Semi-supervised Dependency Parsing
Terry Koo, Xavier Carreras, and Michael Collins
MIT CSAIL, Cambridge, MA 02139, USA
{maestro,carreras,mcollins}@csail.mit.edu
Abstract
We present a simple and effective semi-
supervised method for training dependency
parsers. We focus on the problem of lex-
ical representation, introducing features that
incorporate word clusters derived from a large
unannotated corpus. We demonstrate the ef-
fectiveness of the approach in a series of de-
pendency parsing experiments on the Penn
Treebank and Prague Dependency Treebank,
and we show that the cluster-based features
yield substantial gains in performance across
a wide range of conditions. For example, in
the case of English unlabeled second-order
parsing, we improve from a baseline accu-
racy of 92.02% to 93.16%, and in the case
of Czech unlabeled second-order parsing, we
improve from a baseline accuracy of 86.13%
to 87.13%. In addition, we demonstrate that
our method also improves performance when
small amounts of training data are available,
and can roughly halve the amount of super-
vised data required to reach a desired level of
performance.
1 Introduction
In natural language parsing, lexical information is
seen as crucial to resolving ambiguous relationships,
yet lexicalized statistics are sparse and difficult to es-
timate directly. It is therefore attractive to consider
intermediate entities which exist at a coarser level
than the words themselves, yet capture the informa-
tion necessary to resolve the relevant ambiguities.
In this paper, we introduce lexical intermediaries
via a simple two-stage semi-supervised approach.
First, we use a large unannotated corpus to define
word clusters, and then we use that clustering to
construct a new cluster-based feature mapping for
a discriminative learner. We are thus relying on the
ability of discriminative learning methods to identify
and exploit informative features while remaining ag-
nostic as to the origin of such features. To demon-
strate the effectiveness of our approach, we conduct
experiments in dependency parsing, which has been
the focus of much recent researche.g., see work
in the CoNLL shared tasks on dependency parsing
(Buchholz and Marsi, 2006; Nivre et al., 2007).
The idea of combining word clusters with dis-
criminative learning has been previously explored
by Miller et al. (2004), in the context of named-
entity recognition, and their work directly inspired
our research. However, our target task of depen-
dency parsing involves more complex structured re-
lationships than named-entity tagging; moreover, it
is not at all clear that word clusters should have any
relevance to syntactic structure. Nevertheless, our
experiments demonstrate that word clusters can be
quite effective in dependency parsing applications.
In general, semi-supervised learning can be mo-
tivated by two concerns: first, given a fixed amount
of supervised data, we might wish to leverage ad-
ditional unlabeled data to facilitate the utilization of
the supervised corpus, increasing the performance of
the model in absolute terms. Second, given a fixed
target performance level, we might wish to use un-
labeled data to reduce the amount of annotated data
necessary to reach this target.
We show that our semi-supervised approach
yields improvements for fixed datasets by perform-
ing parsing experiments on the Penn Treebank (Mar-
cus et al., 1993) and Prague Dependency Treebank
(Hajic, 1998; Hajic et al., 2001) (see Sections 4.1
and 4.3). By conducting experiments on datasets of
varying sizes, we demonstrate that for fixed levels of
performance, the cluster-based approach can reduce
the need for supervised data by roughly half, which
is a substantial savings in data-annotation costs (see
Sections 4.2 and 4.4).
The remainder of this paper is divided as follows:
595
Ms. Haag plays Elianti .*
obj
prootnmod sbj
Figure 1: An example of a labeled dependency tree. The
tree contains a special token * which is always the root
of the tree. Each arc is directed from head to modifier and
has a label describing the function of the attachment.
Section 2 gives background on dependency parsing
and clustering, Section 3 describes the cluster-based
features, Section 4 presents our experimental results,
Section 5 discusses related work, and Section 6 con-
cludes with ideas for future research.
2 Background
2.1 Dependency
parsing
Recent work (Buchholz and Marsi, 2006; Nivre
et al., 2007) has focused on dependency parsing.
Dependency syntax represents syntactic informa-
tion as a network of head-modifier dependency arcs,
typically restricted to be a directed tree (see Fig-
ure 1 for an example). Dependency parsing depends
critically on predicting head-modifier relationships,
which can be difficult due to the statistical sparsity
of these word-to-word interactions. Bilexical depen-
dencies are thus ideal candidates for the application
of coarse word proxies such as word clusters.
In this paper, we take a part-factored structured
classification approach to dependency parsing. For a
given sentence x, letY(x) denote the set of possible
dependency structures spanning x, where each y 
Y(x) decomposes into a set of parts ry. In the
simplest case, these parts are the dependency arcs
themselves, yielding a first-order or edge-factored
dependency parsing model. In higher-order parsing
models, the parts can consist of interactions between
more than two words. For example, the parser of
McDonald and Pereira (2006) defines parts for sib-
ling interactions, such as the trio plays, Elianti,
and . in Figure 1. The Carreras (2007) parser
has parts for both sibling interactions and grandpar-
ent interactions, such as the trio *, plays, and
Haag in Figure 1. These kinds of higher-order
factorizations allow dependency parsers to obtain a
limited form of context-sensitivity.
Given a factorization of dependency structures
into parts, we restate dependency parsing as the fol-
apple pear Apple IBM bought run of in
01
100 101 110 111000 001 010 011
00
0
10
1
11
Figure 2: An example of a Brown word-cluster hierarchy.
Each node in the tree is labeled with a bit-string indicat-
ing the path from the root node to that node, where 0
indicates a left branch and 1 indicates a right branch.
lowing maximization:
PARSE(x;w) = argmax
yY(x)
summationdisplay
ry
wf(x,r)
Above, we have assumed that each part is scored
by a linear model with parameters w and feature-
mapping f(). For many different part factoriza-
tions and structure domains Y(), it is possible to
solve the above maximization efficiently, and several
recent efforts have concentrated on designing new
maximization algorithms with increased context-
sensitivity (Eisner, 2000; McDonald et al., 2005b;
McDonald and Pereira, 2006; Carreras, 2007).
2.2 Brown
clustering algorithm
In order to provide word clusters for our exper-
iments, we used the Brown clustering algorithm
(Brown et al., 1992). We chose to work with the
Brown algorithm due to its simplicity and prior suc-
cess in other NLP applications (Miller et al., 2004;
Liang, 2005). However, we expect that our approach
can function with other clustering algorithms (as in,
e.g., Li and McCallum (2005)). We briefly describe
the Brown algorithm below.
The input to the algorithm is a vocabulary of
words to be clustered and a corpus of text containing
these words. Initially, each word in the vocabulary
is considered to be in its own distinct cluster. The al-
gorithm then repeatedly merges the pair of clusters
which causes the smallest decrease in the likelihood
of the text corpus, according to a class-based bigram
language model defined on the word clusters. By
tracing the pairwise merge operations, one obtains
a hierarchical clustering of the words, which can be
represented as a binary tree as in Figure 2.
Within this tree, each word is uniquely identified
by its path from the root, and this path can be com-
pactly represented with a bit string, as in Figure 2.
In order to obtain a clustering of the words, we se-
lect all nodes at a certain depth from the root of the
596
hierarchy. For example, in Figure 2 we might select
the four nodes at depth 2 from the root, yielding the
clusters {apple,pear}, {Apple,IBM}, {bought,run},
and{of,in}. Note that the same clustering can be ob-
tained by truncating each words bit-string to a 2-bit
prefix. By using prefixes of various lengths, we can
produce clusterings of different granularities (Miller
et al., 2004).
For all of the experiments in this paper, we used
the Liang (2005) implementation of the Brown algo-
rithm to obtain the necessary word clusters.
3 Feature
design
Key to the success of our approach is the use of fea-
tures which allow word-cluster-based information to
assist the parser. The feature sets we used are simi-
lar to other feature sets in the literature (McDonald
et al., 2005a; Carreras, 2007), so we will not attempt
to give a exhaustive description of the features in
this section. Rather, we describe our features at a
high level and concentrate on our methodology and
motivations. In our experiments, we employed two
different feature sets: a baseline feature set which
draws upon normal information sources such as
word forms and parts of speech, and a cluster-based
feature set that also uses information derived from
the Brown cluster hierarchy.
3.1 Baseline
features
Our first-order baseline feature set is similar to the
feature set of McDonald et al. (2005a), and consists
of indicator functions for combinations of words and
parts of speech for the head and modifier of each
dependency, as well as certain contextual tokens.1
Our second-order baseline features are the same as
those of Carreras (2007) and include indicators for
triples of part of speech tags for sibling interactions
and grandparent interactions, as well as additional
bigram features based on pairs of words involved
these higher-order interactions. Examples of base-
line features are provided in Table 1.
1We augment the McDonald et al. (2005a) feature set with
backed-off versions of the Surrounding Word POS Features
that include only one neighboring POS tag. We also add binned
distance features which indicate whether the number of tokens
between the head and modifier of a dependency is greater than
2, 5, 10, 20, 30, or 40 tokens.
Baseline Cluster-based
ht,mt hc4,mc4
hw,mw hc6,mc6
hw,ht,mt hc*,mc*
hw,ht,mw hc4,mt
ht,mw,mt ht,mc4
hw,mw,mt hc6,mt
hw,ht,mw,mtht,mc6
 hc4,mw
hw,mc4

ht,mt,st hc4,mc4,sc4
ht,mt,gt hc6,mc6,sc6
 ht,mc4,sc4
hc4,mc4,gc4

Table 1: Examples of baseline and cluster-based feature
templates. Each entry represents a class of indicators for
tuples of information. For example, ht,mt represents
a class of indicator features with one feature for each pos-
sible combination of head POS-tag and modifier POS-
tag. Abbreviations: ht = head POS, hw = head word,
hc4 = 4-bit prefix of head, hc6 = 6-bit prefix of head,
hc* = full bit string of head; mt,mw,mc4,mc6,mc* =
likewise for modifier; st,gt,sc4,gc4,... = likewise
for sibling and grandchild.
3.2 Cluster-based features
The firstand second-order cluster-based feature sets
are supersets of the baseline feature sets: they in-
clude all of the baseline feature templates, and add
an additional layer of features that incorporate word
clusters. Following Miller et al. (2004), we use pre-
fixes of the Brown cluster hierarchy to produce clus-
terings of varying granularity. We found that it was
nontrivial to select the proper prefix lengths for the
dependency parsing task; in particular, the prefix
lengths used in the Miller et al. (2004) work (be-
tween 12 and 20 bits) performed poorly in depen-
dency parsing.2 After experimenting with many dif-
ferent feature configurations, we eventually settled
on a simple but effective methodology.
First, we found that it was helpful to employ two
different types of word clusters:
1. Short bit-string prefixes (e.g., 46 bits), which
we used as replacements for parts of speech.
2One possible explanation is that the kinds of distinctions
required in a named-entity recognition task (e.g., Alice versus
Intel) are much finer-grained than the kinds of distinctions
relevant to syntax (e.g., apple versus eat).
597
2. Full bit strings,3 which we used as substitutes
for word forms.
Using these two types of clusters, we generated new
features by mimicking the template structure of the
original baseline features. For example, the baseline
feature set includes indicators for word-to-word and
tag-to-tag interactions between the head and mod-
ifier of a dependency. In the cluster-based feature
set, we correspondingly introduce new indicators for
interactions between pairs of short bit-string pre-
fixes and pairs of full bit strings. Some examples
of cluster-based features are given in Table 1.
Second, we found it useful to concentrate on
hybrid features involving, e.g., one bit-string and
one part of speech. In our initial attempts, we fo-
cused on features that used cluster information ex-
clusively. While these cluster-only features provided
some benefit, we found that adding hybrid features
resulted in even greater improvements. One possible
explanation is that the clusterings generated by the
Brown algorithm can be noisy or only weakly rele-
vant to syntax; thus, the clusters are best exploited
when anchored to words or parts of speech.
Finally, we found it useful to impose a form of
vocabulary restriction on the cluster-based features.
Specifically, for any feature that is predicated on a
word form, we eliminate this feature if the word
in question is not one of the top-N most frequent
words in the corpus. When N is between roughly
100 and 1,000, there is little effect on the perfor-
mance of the cluster-based feature sets.4 In addition,
the vocabulary restriction reduces the size of the fea-
ture sets to managable proportions.
4 Experiments
In order to evaluate the effectiveness of the cluster-
based feature sets, we conducted dependency pars-
ing experiments in English and Czech. We test the
features in a wide range of parsing configurations,
including first-order and second-order parsers, and
labeled and unlabeled parsers.5
3As in Brown et al. (1992), we limit the clustering algorithm
so that it recovers at most 1,000 distinct bit-strings; thus full bit
strings are not equivalent to word forms.
4We used N = 800 for all experiments in this paper.
5In an unlabeled parser, we simply ignore dependency la-
bel information, which is a common simplification.
The English experiments were performed on the
Penn Treebank (Marcus et al., 1993), using a stan-
dard set of head-selection rules (Yamada and Mat-
sumoto, 2003) to convert the phrase structure syn-
tax of the Treebank to a dependency tree represen-
tation.6 We split the Treebank into a training set
(Sections 221), a development set (Section 22), and
several test sets (Sections 0,7 1, 23, and 24). The
data partition and head rules were chosen to match
previous work (Yamada and Matsumoto, 2003; Mc-
Donald et al., 2005a; McDonald and Pereira, 2006).
The part of speech tags for the development and test
data were automatically assigned by MXPOST (Rat-
naparkhi, 1996), where the tagger was trained on
the entire training corpus; to generate part of speech
tags for the training data, we used 10-way jackknif-
ing.8 English word clusters were derived from the
BLLIP corpus (Charniak et al., 2000), which con-
tains roughly 43 million words of Wall Street Jour-
nal text.9
The Czech experiments were performed on the
Prague Dependency Treebank 1.0 (Hajic, 1998;
Hajic et al., 2001), which is directly annotated
with dependency structures. To facilitate compar-
isons with previous work (McDonald et al., 2005b;
McDonald and Pereira, 2006), we used the train-
ing/development/test partition defined in the corpus
and we also used the automatically-assigned part of
speech tags provided in the corpus.10 Czech word
clusters were derived from the raw text section of
the PDT 1.0, which contains about 39 million words
of newswire text.11
We trained the parsers using the averaged percep-
tron (Freund and Schapire, 1999; Collins, 2002),
which represents a balance between strong perfor-
mance and fast training times. To select the number
6We used Joakim Nivres Penn2Malt conversion tool
(http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html). Depen-
dency labels were obtained via the Malt hard-coded setting.
7For computational reasons, we removed a single 249-word
sentence from Section 0.
8That is, we tagged each fold with the tagger trained on the
other 9 folds.
9We ensured that the sentences of the Penn Treebank were
excluded from the text used for the clustering.
10Following Collins et al. (1999), we used a coarsened ver-
sion of the Czech part of speech tags; this choice also matches
the conditions of previous work (McDonald et al., 2005b; Mc-
Donald and Pereira, 2006).
11This text was disjoint from the training and test corpora.
598
Sec dep1 dep1c MD1 dep2 dep2c MD2 dep1-L dep1c-L dep2-L dep2c-L
00 90.48 91.57(+1.09)  91.76 92.77(+1.01)  90.29 91.03(+0.74) 91.33 92.09(+0.76)
01 91.31 92.43(+1.12)  92.46 93.34(+0.88)  90.84 91.73(+0.89) 91.94 92.65(+0.71)
23 90.84 92.23(+1.39) 90.9 92.02 93.16(+1.14) 91.5 90.32 91.24(+0.92) 91.38 92.14(+0.76)
24 89.67 91.30(+1.63)  90.92 91.85(+0.93)  89.55 90.06(+0.51) 90.42 91.18(+0.76)
Table 2: Parent-prediction accuracies on Sections 0, 1, 23, and 24. Abbreviations: dep1/dep1c = first-order parser with
baseline/cluster-based features; dep2/dep2c = second-order parser with baseline/cluster-based features; MD1 = Mc-
Donald et al. (2005a); MD2 = McDonald and Pereira (2006); suffix -L = labeled parser. Unlabeled parsers are scored
using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions. Improvements of
cluster-based features over baseline features are shown in parentheses.
of iterations of perceptron training, we performed up
to 30 iterations and chose the iteration which opti-
mized accuracy on the development set. Our feature
mappings are quite high-dimensional, so we elimi-
nated all features which occur only once in the train-
ing data. The resulting models still had very high
dimensionality, ranging from tens of millions to as
many as a billion features.12
All results presented in this section are given
in terms of parent-prediction accuracy, which mea-
sures the percentage of tokens that are attached to
the correct head token. For labeled dependency
structures, both the head token and dependency label
must be correctly predicted. In addition, in English
parsing we ignore the parent-predictions of punc-
tuation tokens,13 and in Czech parsing we retain
the punctuation tokens; this matches previous work
(Yamada and Matsumoto, 2003; McDonald et al.,
2005a; McDonald and Pereira, 2006).
4.1 English
main results
In our English experiments, we tested eight differ-
ent parsing configurations, representing all possi-
ble choices between baseline or cluster-based fea-
ture sets, first-order (Eisner, 2000) or second-order
(Carreras, 2007) factorizations, and labeled or unla-
beled parsing.
Table 2 compiles our final test results and also
includes two results from previous work by Mc-
Donald et al. (2005a) and McDonald and Pereira
(2006), for the purposes of comparison. We note
a few small differences between our parsers and the
12Due to the sparsity of the perceptron updates, however,
only a small fraction of the possible features were active in our
trained models.
13A punctuation token is any token whose gold-standard part
of speech tag is one of {  : , .}.
parsers evaluated in this previous work. First, the
MD1 and MD2 parsers were trained via the MIRA
algorithm (Crammer and Singer, 2003; Crammer et
al., 2004), while we use the averaged perceptron. In
addition, the MD2 model uses only sibling interac-
tions, whereas the dep2/dep2c parsers include both
sibling and grandparent interactions.
There are some clear trends in the results of Ta-
ble 2. First, performance increases with the order of
the parser: edge-factored models (dep1 and MD1)
have the lowest performance, adding sibling rela-
tionships (MD2) increases performance, and adding
grandparent relationships (dep2) yields even better
accuracies. Similar observations regarding the ef-
fect of model order have also been made by Carreras
(2007).
Second, note that the parsers using cluster-based
feature sets consistently outperform the models us-
ing the baseline features, regardless of model order
or label usage. Some of these improvements can be
quite large; for example, a first-order model using
cluster-based features generally performs as well as
a second-order model using baseline features. More-
over, the benefits of cluster-based feature sets com-
bine additively with the gains of increasing model
order. For example, consider the unlabeled parsers
in Table 2: on Section 23, increasing the model or-
der from dep1 to dep2 results in a relative reduction
in error of roughly 13%, while introducing cluster-
based features from dep2 to dep2c yields an addi-
tional relative error reduction of roughly 14%. As a
final note, all 16 comparisons between cluster-based
features and baseline features shown in Table 2 are
statistically significant.14
14We used the sign test at the sentence level. The comparison
between dep1-L and dep1c-L is significant at p < 0.05, and all
other comparisons are significant at p < 0.0005.
599
Tagger always trained on full Treebank Tagger trained on reduced dataset
Size dep1 dep1c  dep2 dep2c 
1k 84.54 85.90 1.36 86.29 87.47 1.18
2k 86.20 87.65 1.45 87.67 88.88 1.21
4k 87.79 89.15 1.36 89.22 90.46 1.24
8k 88.92 90.22 1.30 90.62 91.55 0.93
16k 90.00 91.27 1.27 91.27 92.39 1.12
32k 90.74 92.18 1.44 92.05 93.36 1.31
All 90.89 92.33 1.44 92.42 93.30 0.88
Size dep1 dep1c  dep2 dep2c 
1k 80.49 84.06 3.57 81.95 85.33 3.38
2k 83.47 86.04 2.57 85.02 87.54 2.52
4k 86.53 88.39 1.86 87.88 89.67 1.79
8k 88.25 89.94 1.69 89.71 91.37 1.66
16k 89.66 91.03 1.37 91.14 92.22 1.08
32k 90.78 92.12 1.34 92.09 93.21 1.12
All 90.89 92.33 1.44 92.42 93.30 0.88
Table 3: Parent-prediction accuracies of unlabeled English parsers on Section 22. Abbreviations: Size = #sentences in
training corpus;  = difference between cluster-based and baseline features; other abbreviations are as in Table 2.
4.2 English
learning curves
We performed additional experiments to evaluate the
effect of the cluster-based features as the amount
of training data is varied. Note that the depen-
dency parsers we use require the input to be tagged
with parts of speech; thus the quality of the part-of-
speech tagger can have a strong effect on the per-
formance of the parser. In these experiments, we
consider two possible scenarios:
1. The tagger has a large training corpus, while
the parser has a smaller training corpus. This
scenario can arise when tagged data is cheaper
to obtain than syntactically-annotated data.
2. The same amount of labeled data is available
for training both tagger and parser.
Table 3 displays the accuracy of firstand second-
order models when trained on smaller portions of
the Treebank, in both scenarios described above.
Note that the cluster-based features obtain consistent
gains regardless of the size of the training set. When
the tagger is trained on the reduced-size datasets,
the gains of cluster-based features are more pro-
nounced, but substantial improvements are obtained
even when the tagger is accurate.
It is interesting to consider the amount by which
cluster-based features reduce the need for supervised
data, given a desired level of accuracy. Based on
Table 3, we can extrapolate that cluster-based fea-
tures reduce the need for supervised data by roughly
a factor of 2. For example, the performance of the
dep1c and dep2c models trained on 1k sentences is
roughly the same as the performance of the dep1
and dep2 models, respectively, trained on 2k sen-
tences. This approximate data-halving effect can be
observed throughout the results in Table 3.
When combining the effects of model order and
cluster-based features, the reductions in the amount
of supervised data required are even larger. For ex-
ample, in scenario 1 the dep2c model trained on 1k
sentences is close in performance to the dep1 model
trained on 4k sentences, and the dep2c model trained
on 4k sentences is close to the dep1 model trained on
the entire training set (roughly 40k sentences).
4.3 Czech
main results
In our Czech experiments, we considered only unla-
beled parsing,15 leaving four different parsing con-
figurations: baseline or cluster-based features and
first-order or second-order parsing. Note that our
feature sets were originally tuned for English pars-
ing, and except for the use of Czech clusters, we
made no attempt to retune our features for Czech.
Czech dependency structures may contain non-
projective edges, so we employ a maximum directed
spanning tree algorithm (Chu and Liu, 1965; Ed-
monds, 1967; McDonald et al., 2005b) as our first-
order parser for Czech. For the second-order pars-
ing experiments, we used the Carreras (2007) parser.
Since this parser only considers projective depen-
dency structures, we projectivized the PDT 1.0
training set by finding, for each sentence, the pro-
jective tree which retains the most correct dependen-
cies; our second-order parsers were then trained with
respect to these projective trees. The development
and test sets were not projectivized, so our second-
order parser is guaranteed to make errors in test sen-
tences containing non-projective dependencies. To
overcome this, McDonald and Pereira (2006) use a
15We leave labeled parsing experiments to future work.
600
dep1 dep1c dep2 dep2c
84.49 86.07(+1.58) 86.13 87.13(+1.00)
Table 4: Parent-prediction accuracies of unlabeled Czech
parsers on the PDT 1.0 test set, for baseline features and
cluster-based features. Abbreviations are as in Table 2.
Parser Accuracy
NivreandNilsson(2005) 80.1
McDonaldetal.(2005b) 84.4
HallandNovak(2005) 85.1
McDonaldandPereira(2006) 85.2
dep1c 86.07
dep2c 87.13
Table 5: Unlabeled parent-prediction accuracies of Czech
parsers on the PDT 1.0 test set, for our models and for
previous work.
Size dep1 dep1c  dep2 dep2c 
1k 72.79 73.66 0.87 74.35 74.63 0.28
2k 74.92 76.23 1.31 76.63 77.60 0.97
4k 76.87 78.14 1.27 78.34 79.34 1.00
8k 78.17 79.83 1.66 79.82 80.98 1.16
16k 80.60 82.44 1.84 82.53 83.69 1.16
32k 82.85 84.65 1.80 84.66 85.81 1.15
64k 84.20 85.98 1.78 86.01 87.11 1.10
All 84.36 86.09 1.73 86.09 87.26 1.17
Table 6: Parent-prediction accuracies of unlabeled Czech
parsers on the PDT 1.0 development set. Abbreviations
are as in Table 3.
two-stage approximate decoding process in which
the output of their second-order parser is deprojec-
tivized via greedy search. For simplicity, we did
not implement a deprojectivization stage on top of
our second-order parser, but we conjecture that such
techniques may yield some additional performance
gains; we leave this to future work.
Table 4 gives accuracy results on the PDT 1.0
test set for our unlabeled parsers. As in the En-
glish experiments, there are clear trends in the re-
sults: parsers using cluster-based features outper-
form parsers using baseline features, and second-
order parsers outperform first-order parsers. Both of
the comparisons between cluster-based and baseline
features in Table 4 are statistically significant.16 Ta-
ble 5 compares accuracy results on the PDT 1.0 test
set for our parsers and several other recent papers.
16We used the sign test at the sentence level; both compar-
isons are significant at p < 0.0005.
N dep1 dep1c dep2 dep2c
100 89.19 92.25 90.61 93.14
200 90.03 92.26 91.35 93.18
400 90.31 92.32 91.72 93.20
800 90.62 92.33 91.89 93.30
1600 90.87  92.20 
All 90.89  92.42 
Table 7: Parent-prediction accuracies of unlabeled En-
glish parsers on Section 22. Abbreviations: N = thresh-
old value; other abbreviations are as in Table 2. We
did not train cluster-based parsers using threshold values
larger than 800 due to computational limitations.
dep1-P dep1c-P dep1 dep2-P dep2c-P dep2
77.19 90.69 90.89 86.73 91.84 92.42
Table 8: Parent-prediction accuracies of unlabeled En-
glish parsers on Section 22. Abbreviations: suffix -P =
model without POS; other abbreviations are as in Table 2.
4.4 Czech
learning curves
As in our English experiments, we performed addi-
tional experiments on reduced sections of the PDT;
the results are shown in Table 6. For simplicity, we
did not retrain a tagger for each reduced dataset,
so we always use the (automatically-assigned) part
of speech tags provided in the corpus. Note that
the cluster-based features obtain improvements at all
training set sizes, with data-reduction factors simi-
lar to those observed in English. For example, the
dep1c model trained on 4k sentences is roughly as
good as the dep1 model trained on 8k sentences.
4.5 Additional
results
Here, we present two additional results which fur-
ther explore the behavior of the cluster-based fea-
ture sets. In Table 7, we show the development-set
performance of second-order parsers as the thresh-
old for lexical feature elimination (see Section 3.2)
is varied. Note that the performance of cluster-based
features is fairly insensitive to the threshold value,
whereas the performance of baseline features clearly
degrades as the vocabulary size is reduced.
In Table 8, we show the development-set perfor-
mance of the firstand second-order parsers when
features containing part-of-speech-based informa-
tion are eliminated. Note that the performance ob-
tained by using clusters without parts of speech is
close to the performance of the baseline features.
601
5 Related
Work
As mentioned earlier, our approach was inspired by
the success of Miller et al. (2004), who demon-
strated the effectiveness of using word clusters as
features in a discriminative learning approach. Our
research, however, applies this technique to depen-
dency parsing rather than named-entity recognition.
In this paper, we have focused on developing new
representations for lexical information. Previous re-
search in this area includes several models which in-
corporate hidden variables (Matsuzaki et al., 2005;
Koo and Collins, 2005; Petrov et al., 2006; Titov
and Henderson, 2007). These approaches have the
advantage that the model is able to learn different
usages for the hidden variables, depending on the
target problem at hand. Crucially, however, these
methods do not exploit unlabeled data when learn-
ing their representations.
Wang et al. (2005) used distributional similarity
scores to smooth a generative probability model for
dependency parsing and obtained improvements in
a Chinese parsing task. Our approach is similar to
theirs in that the Brown algorithm produces clusters
based on distributional similarity, and the cluster-
based features can be viewed as being a kind of
backed-off version of the baseline features. How-
ever, our work is focused on discriminative learning
as opposed to generative models.
Semi-supervised phrase structure parsing has
been previously explored by McClosky et al. (2006),
who applied a reranked parser to a large unsuper-
vised corpus in order to obtain additional train-
ing data for the parser; this self-training appraoch
was shown to be quite effective in practice. How-
ever, their approach depends on the usage of a
high-quality parse reranker, whereas the method de-
scribed here simply augments the features of an ex-
isting parser. Note that our two approaches are com-
patible in that we could also design a reranker and
apply self-training techniques on top of the cluster-
based features.
6 Conclusions
In this paper, we have presented a simple but effec-
tive semi-supervised learning approach and demon-
strated that it achieves substantial improvement over
a competitive baseline in two broad-coverage depen-
dency parsing tasks. Despite this success, there are
several ways in which our approach might be im-
proved.
To begin, recall that the Brown clustering algo-
rithm is based on a bigram language model. Intu-
itively, there is a mismatch between the kind of
lexical information that is captured by the Brown
clusters and the kind of lexical information that is
modeled in dependency parsing. A natural avenue
for further research would be the development of
clustering algorithms that reflect the syntactic be-
havior of words; e.g., an algorithm that attempts to
maximize the likelihood of a treebank, according to
a probabilistic dependency model. Alternately, one
could design clustering algorithms that cluster entire
head-modifier arcs rather than individual words.
Another idea would be to integrate the cluster-
ing algorithm into the training algorithm in a limited
fashion. For example, after training an initial parser,
one could parse a large amount of unlabeled text and
use those parses to improve the quality of the clus-
ters. These improved clusters can then be used to
retrain an improved parser, resulting in an overall
algorithm similar to that of McClosky et al. (2006).
Setting aside the development of new clustering
algorithms, a final area for future work is the exten-
sion of our method to new domains, such as con-
versational text or other languages, and new NLP
problems, such as machine translation.
Acknowledgments
The authors thank the anonymous reviewers for
their insightful comments. Many thanks also to
Percy Liang for providing his implementation of
the Brown algorithm, and Ryan McDonald for his
assistance with the experimental setup. The au-
thors gratefully acknowledge the following sources
of support. Terry Koo was funded by NSF grant
DMS-0434222 and a grant from NTT, Agmt. Dtd.
6/21/1998. Xavier Carreras was supported by the
Catalan Ministry of Innovation, Universities and
Enterprise, and a grant from NTT, Agmt. Dtd.
6/21/1998. Michael Collins was funded by NSF
grants 0347631 and DMS-0434222.
602

References

P.F. Brown, V.J. Della Pietra, P.V. deSouza, J.C. Lai, and R.L. Mercer. 1992. Class-Based n-gram Models of Natural Language. Computational Linguistics,
18(4):467479.

S. Buchholz and E. Marsi. 2006. CoNLL-X Shared Task on Multilingual Dependency Parsing. In Proceedings of CoNLL, pages 149164.
X. Carreras. 2007. Experiments with a Higher-Order Projective Dependency Parser. In Proceedings of EMNLP-CoNLL, pages 957961.

E. Charniak, D. Blaheta, N. Ge, K. Hall, and M. Johnson. 2000. BLLIP 198789 WSJ Corpus Release 1, LD No. LDC2000T43. Linguistic Data Consortium.

Y.J. Chu and T.H. Liu. 1965. On the shortest arborescence of a directed graph. Science Sinica, 14:1396 1400.

M. Collins, J. Hajic, L. Ramshaw, and C. Tillmann. 1999 A Statistical Parser for Czech. In Proceedings of ACL pages 505512.

M. Collins. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In Proceedings of EMNLP, pages 18.

K. Crammer and Y. Singer. 2003. Ultraconservative Online Algorithms for Multiclass Problems. Journal of Machine Learning Research, 3:951991.

K. Crammer, O. Dekel, S. Shalev-Shwartz, and Y. Singer. 2004. Online Passive-Aggressive Algorithms. In S. Thrun, L. Saul, and B. Scholkopf, editors, NIPS 16, pages 12291236.

J. Edmonds. 1967. Optimum branchings. Journal of Research of the National Bureau of Standards, 71B:233 240.

J. Eisner. 2000. Bilexical Grammars and Their Cubic-Time Parsing Algorithms. In H. Bunt and A. Nijholt, editors, Advances in Probabilistic and Other Parsing Technologies, pages 2962. Kluwer Academic Pulishers.

Y. Freund and R. Schapire. 1999. Large Margin Classification Using the Perceptron Algorithm. Machine Learning, 37(3):277296.

J. Hajic, E. Hajicova, P. Pajas, J. Panevova, and P. Sgall. 2001. The Prague Dependency Treebank 1.0, LDC No. LDC2001T10. Linguistics Data Consortium.

J. Hajic. 1998. Building a Syntactically Annotated Corpus: The Prague Dependency Treebank. In E. Hajicova, editor, Issues of Valency and Meaning. Studies in Honor of Jarmila Panevova, pages 1219.

K. Hall and V. Novak. 2005. Corrective Modeling for Non-Projective Dependency Parsing. In Proceedings of IWPT, pages 4252.

T. Koo and M. Collins. 2005. Hidden-Variable Models for Discriminative Reranking. In Proceedings of HLTEMNLP, pages 507514.

W. Li and A. McCallum. 2005. Semi-Supervised Sequence Modeling with Syntactic Topic Models. In Proceedings of AAAI, pages 813818.

P. Liang. 2005. Semi-Supervised Learning for Natural Language. Masters thesis, Massachusetts Institute of Technology.

M.P. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. ComputationalLinguistics, 19(2):313330.

T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilistic CFG with Latent Annotations. In Proceedings of ACL, pages 7582.

D. McClosky, E. Charniak, and M. Johnson. 2006. Effective Self-Training for Parsing. In Proceedings of HLT-NAACL, pages 152159.

R. McDonald and F. Pereira. 2006. Online Learning of Approximate Dependency Parsing Algorithms. In Proceedings of EACL, pages 8188.

R. McDonald, K. Crammer, and F. Pereira. 2005a. Online Large-Margin Training of Dependency Parsers. In Proceedings of ACL, pages 9198.

R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005b. Non-Projective Dependency Parsing using Spanning Tree Algorithms. In Proceedings of HLT-EMNLP, pages 523530.

S. Miller, J. Guinness, and A. Zamanian. 2004. Name Tagging with Word Clusters and Discriminative Training. In Proceedings of HLT-NAACL, pages 337342.

J. Nivre and J. Nilsson. 2005. Pseudo-Projective Dependency Parsing. In Proceedings of ACL, pages 99106.

J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson, S. Riedel, and D. Yuret. 2007. The CoNLL 2007 Shared Task on Dependency Parsing. In Proceedings of EMNLP-CoNLL 2007, pages 915932.

S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning Accurate, Compact, and Interpretable Tree Annotation. In Proceedings of COLING-ACL, pages 433440.

A. Ratnaparkhi. 1996. A Maximum Entropy Model for Part-Of-Speech Tagging. In Proceedings of EMNLP, pages 133142.

I. Titov and J. Henderson. 2007. Constituent Parsing with Incremental Sigmoid Belief Networks. In Proceedings of ACL, pages 632639.

Q.I. Wang, D. Schuurmans, and D. Lin. 2005. Strictly Lexical Dependency Parsing. In Proceedings of IWPT, pages 152159.

H. Yamada and Y. Matsumoto. 2003. Statistical Dependency Analysis With Support Vector Machines. In Proceedings of IWPT, pages 195206. 603
Proceedings of ACL-08: HLT, pages 950958,
Columbus, Ohio, USA, June 2008. c2008 Association for Computational Linguistics
Integrating Graph-Based and Transition-Based Dependency Parsers
Joakim Nivre
Vaxjo University Uppsala University
Computer Science Linguistics and Philology
SE-35195 Vaxjo SE-75126 Uppsala
nivre@msi.vxu.se
Ryan McDonald
Google Inc.
76 Ninth Avenue
New York, NY 10011
ryanmcd@google.com
Abstract
Previous studies of data-driven dependency
parsing have shown that the distribution of
parsing errors are correlated with theoretical
properties of the models used for learning and
inference. In this paper, we show how these
results can be exploited to improve parsing
accuracy by integrating a graph-based and a
transition-based model. By letting one model
generate features for the other, we consistently
improve accuracy for both models, resulting
in a significant improvement of the state of
the art when evaluated on data sets from the
CoNLL-X shared task.
1 Introduction
Syntactic dependency graphs have recently gained
a wide interest in the natural language processing
community and have been used for many problems
ranging from machine translation (Ding and Palmer,
2004) to ontology construction (Snow et al., 2005).
A dependency graph for a sentence represents each
word and its syntactic dependents through labeled
directed arcs, as shown in figure 1. One advantage
of this representation is that it extends naturally to
discontinuous constructions, which arise due to long
distancedependenciesorinlanguageswheresyntac-
tic structure is encoded in morphology rather than in
word order. This is undoubtedly one of the reasons
for the emergence of dependency parsers for a wide
range of languages. Many of these parsers are based
on data-driven parsing models, which learn to pro-
duce dependency graphs for sentences solely from
an annotated corpus and can be easily ported to any
Figure 1: Dependency graph for an English sentence.
language or domain in which annotated resources
exist.
Practically all data-driven models that have been
proposed for dependency parsing in recent years can
be described as either graph-based or transition-
based (McDonald and Nivre, 2007). In graph-based
parsing, we learn a model for scoring possible de-
pendency graphs for a given sentence, typically by
factoring the graphs into their component arcs, and
perform parsing by searching for the highest-scoring
graph. This type of model has been used by, among
others, Eisner (1996), McDonald et al. (2005a), and
Nakagawa (2007). In transition-based parsing, we
instead learn a model for scoring transitions from
one parser state to the next, conditioned on the parse
history, and perform parsing by greedily taking the
highest-scoring transition out of every parser state
until we have derived a complete dependency graph.
This approach is represented, for example, by the
models of Yamada and Matsumoto (2003), Nivre et
al. (2004), and Attardi (2006).
Theoretically, these approaches are very different.
Thegraph-basedmodelsaregloballytrainedanduse
exactinferencealgorithms, butdefinefeaturesovera
limited history of parsing decisions. The transition-
based models are essentially the opposite. They use
local training and greedy inference algorithms, but
950
define features over a rich history of parsing deci-
sions. This is a fundamental trade-off that is hard
to overcome by tractable means. Both models have
been used to achieve state-of-the-art accuracy for a
wide range of languages, as shown in the CoNLL
shared tasks on dependency parsing (Buchholz and
Marsi, 2006; Nivre et al., 2007), but McDonald and
Nivre (2007) showed that a detailed error analysis
reveals important differences in the distribution of
errors associated with the two models.
In this paper, we consider a simple way of inte-
grating graph-based and transition-based models in
order to exploit their complementary strengths and
thereby improve parsing accuracy beyond what is
possible by either model in isolation. The method
integrates the two models by allowing the output
of one model to define features for the other. This
method is simple  requiring only the definition of
new features  and robust by allowing a model to
learn relative to the predictions of the other.
2 Two
Models for Dependency Parsing
2.1 Preliminaries
Given a set L = {l1,...,l|L|} of arc labels (depen-
dency relations), a dependency graph for an input
sentence x = w0,w1,...,wn (where w0 = ROOT) is
a labeled directed graph G = (V,A) consisting of a
set of nodes V = {0,1,...,n}1 and a set of labeled
directed arcs A  V V L, i.e., if (i,j,l)  A for
i,j  V and l  L, then there is an arc from node
i to node j with label l in the graph. A dependency
graphGforasentencexmustbeadirectedtreeorig-
inating out of the root node 0 and spanning all nodes
in V , as exemplified by the graph in figure 1. This
is a common constraint in many dependency parsing
theories and their implementations.
2.2 Graph-Based Models
Graph-based dependency parsers parameterize a
model over smaller substructures in order to search
the space of valid dependency graphs and produce
the most likely one. The simplest parameterization
is the arc-factored model that defines a real-valued
score function for arcs s(i,j,l) and further defines
the score of a dependency graph as the sum of the
1We use the common convention of representing words by
their index in the sentence.
score of all the arcs it contains. As a result, the de-
pendency parsing problem is written:
G = argmax
G=(V,A)
summationdisplay
(i,j,l)A
s(i,j,l)
This problem is equivalent to finding the highest
scoring directed spanning tree in the complete graph
over the input sentence, which can be solved in
O(n2) time (McDonald et al., 2005b). Additional
parameterizations are possible that take more than
one arc into account, but have varying effects on
complexity (McDonald and Satta, 2007). An advan-
tage of graph-based methods is that tractable infer-
ence enables the use of standard structured learning
techniques that globally set parameters to maximize
parsing performance on the training set (McDonald
et al., 2005a). The primary disadvantage of these
models is that scores  and as a result any feature
representations  are restricted to a single arc or a
small number of arcs in the graph.
The specific graph-based model studied in this
work is that presented by McDonald et al. (2006),
which factors scores over pairs of arcs (instead of
just single arcs) and uses near exhaustive search for
unlabeled parsing coupled with a separate classifier
to label each arc. We call this system MSTParser, or
simply MST for short, which is also the name of the
freely available implementation.2
2.3 Transition-Based Models
Transition-based dependency parsing systems use a
model parameterized over transitions of an abstract
machine for deriving dependency graphs, such that
every transition sequence from the designated initial
configuration to some terminal configuration derives
a valid dependency graph. Given a real-valued score
function s(c,t) (for transition t out of configuration
c), parsingcanbeperformedbystartingfromtheini-
tial configuration and taking the optimal transition
t = argmaxtT s(c,t) out of every configuration
c until a terminal configuration is reached. This can
be seen as a greedy search for the optimal depen-
dency graph, based on a sequence of locally optimal
decisions in terms of the transition system.
Many transition systems for data-driven depen-
dency parsing are inspired by shift-reduce parsing,
2http://mstparser.sourceforge.net
951
where each configuration c contains a stack c for
storing partially processed nodes and a buffer c
containingtheremaininginput. Transitionsinsucha
system add arcs to the dependency graph and mani-
pulate the stack and buffer. One example is the tran-
sition system defined by Nivre (2003), which parses
a sentence x = w0,w1,...,wn in O(n) time.
To learn a scoring function on transitions, these
systems rely on discriminative learning methods,
such as memory-based learning or support vector
machines, using a strictly local learning procedure
where only single transitions are scored (not com-
plete transition sequences). The main advantage of
these models is that features are not restricted to a
limited number of graph arcs but can take into ac-
count the entire dependency graph built so far. The
major disadvantage is that the greedy parsing strat-
egy may lead to error propagation.
The specific transition-based model studied in
this work is that presented by Nivre et al. (2006),
which uses support vector machines to learn transi-
tion scores. We call this system MaltParser, or Malt
for short, which is also the name of the freely avail-
able implementation.3
2.4 Comparison
and Analysis
These models differ primarily with respect to three
properties: inference, learning, and feature repre-
sentation. MaltParser uses an inference algorithm
that greedily chooses the best parsing decision based
on the current parser history whereas MSTParser
uses exhaustive search algorithms over the space of
all valid dependency graphs to find the graph that
maximizes the score. MaltParser trains a model
to make a single classification decision (choose the
next transition) whereas MSTParser trains a model
to maximize the global score of correct graphs.
MaltParsercanintroducea richfeaturehistorybased
on previous parser decisions, whereas MSTParser is
forced to restrict features to a single decision or a
pair of nearby decisions in order to retain efficiency.
These differences highlight an inherent trade-off
between global inference/learning and expressive-
ness of feature representations. MSTParser favors
theformerattheexpenseofthelatterandMaltParser
the opposite. This difference was highlighted in the
3http://w3.msi.vxu.se/jha/maltparser/
studyofMcDonaldandNivre(2007), whichshowed
that the difference is reflected directly in the error
distributions of the parsers. Thus, MaltParser is less
accurate than MSTParser for long dependencies and
those closer to the root of the graph, but more accu-
rate for short dependencies and those farthest away
from the root. Furthermore, MaltParser is more ac-
curate for dependents that are nouns and pronouns,
whereas MSTParser is more accurate for verbs, ad-
jectives, adverbs, adpositions, and conjunctions.
Given that there is a strong negative correlation
between dependency length and tree depth, and
given that nouns and pronouns tend to be more
deeply embedded than (at least) verbs and conjunc-
tions, these patterns can all be explained by the same
underlying factors. Simply put, MaltParser has an
advantage in its richer feature representations, but
this advantage is gradually diminished by the nega-
tive effect of error propagation due to the greedy in-
ference strategy as sentences and dependencies get
longer. MSTParser has a more even distribution of
errors, which is expected given that the inference al-
gorithm and feature representation should not prefer
one type of arc over another. This naturally leads
one to ask: Is it possible to integrate the two models
in order to exploit their complementary strengths?
This is the topic of the remainder of this paper.
3 Integrated
Models
There are many conceivable ways of combining the
two parsers, including more or less complex en-
semble systems and voting schemes, which only
perform the integration at parsing time. However,
given that we are dealing with data-driven models,
it should be possible to integrate at learning time, so
that the two complementary models can learn from
one another. In this paper, we propose to do this by
letting one model generate features for the other.
3.1 Feature-Based Integration
As explained in section 2, both models essentially
learn a scoring function s : X  R, where the
domain X is different for the two models. For the
graph-based model, X is the set of possible depen-
dency arcs (i,j,l); for the transition-based model,
X isthesetofpossibleconfiguration-transitionpairs
(c,t). But in both cases, the input is represented
952
MSTMalt  defined over (i,j,l) ( = any label/node)
Is (i,j,) in GMaltx ?
Is (i,j,l) in GMaltx ?
Is (i,j,) not in GMaltx ?
Is (i,j,l) not in GMaltx ?
Identity of lprime such that (,j,lprime) is in GMaltx ?
Identity of lprime such that (i,j,lprime) is in GMaltx ?
MaltMST  defined over (c,t) ( = any label/node)
Is (0c,0c,) in GMSTx ?
Is (0c,0c,) in GMSTx ?
Head direction for 0c in GMSTx (left/right/ROOT)
Head direction for 0c in GMSTx (left/right/ROOT)
Identity of l such that (,0c,l) is in GMSTx ?
Identity of l such that (,0c,l) is in GMSTx ?
Table 1: Guide features for MSTMalt and MaltMST.
by a k-dimensional feature vector f : X  Rk.
In the feature-based integration we simply extend
the feature vector for one model, called the base
model, with a certain number of features generated
by the other model, which we call the guide model
in this context. The additional features will be re-
ferred to as guide features, and the version of the
base model trained with the extended feature vector
will be called the guided model. The idea is that the
guided model should be able to learn in which situ-
ations to trust the guide features, in order to exploit
the complementary strength of the guide model, so
that performance can be improved with respect to
the base parser. This method of combining classi-
fiers is sometimes referred to as classifier stacking.
The exact form of the guide features depend on
properties of the base model and will be discussed
in sections 3.23.3 below, but the overall scheme for
thefeature-based integrationcanbedescribed asfol-
lows. To train a guided version BC of base model B
with guide model C and training set T, the guided
model is trained, not on the original training set T,
but on a version of T that has been parsed with the
guide model C under a cross-validation scheme (to
avoid overlap with training data for C). This means
that, for every sentence x  T, BC has access at
training time to both the gold standard dependency
graph Gx and the graph GCx predicted by C, and it is
the latter that forms the basis for the additional guide
features. When parsing a new sentence xprime with BC,
xprime is first parsed with model C (this time trained on
the entire training set T) to derive GCxprime, so that the
guide features can be extracted also at parsing time.
3.2 The
Guided Graph-Based Model
The graph-based model, MSTParser, learns a scor-
ing function s(i,j,l)  R over labeled dependen-
cies. More precisely, dependency arcs (or pairs of
arcs) are first represented by a high dimensional fea-
ture vector f(i,j,l)  Rk, where f is typically a bi-
nary feature vector over properties of the arc as well
as the surrounding input (McDonald et al., 2005a;
McDonald et al., 2006). The score of an arc is de-
fined as a linear classifier s(i,j,l) = w  f(i,j,l),
where w is a vector of feature weights to be learned
by the model.
For the guided graph-based model, which we call
MSTMalt, this feature representation is modified to
include an additional argument GMaltx , which is the
dependency graph predicted by MaltParser on the
input sentence x. Thus, the new feature represen-
tation will map an arc and the entire predicted Malt-
Parser graph to a high dimensional feature repre-
sentation, f(i,j,l,GMaltx )  Rk+m. These m ad-
ditional features account for the guide features over
the MaltParser output. The specific features used by
MSTMalt are given in table 1. All features are con-
joined with the part-of-speech tags of the words in-
volved in the dependency to allow the guided parser
to learn weights relative to different surface syntac-
tic environments. Though MSTParser is capable of
defining features over pairs of arcs, we restrict the
guide features over single arcs as this resulted in
higher accuracies during preliminary experiments.
3.3 The
Guided Transition-Based Model
The transition-based model, MaltParser, learns a
scoring function s(c,t)  Rover configurations and
transitions. The set of training instances for this
learning problem is the set of pairs (c,t) such that
t is the correct transition out of c in the transition
sequence that derives the correct dependency graph
Gx for some sentence x in the training set T. Each
training instance (c,t) is represented by a feature
vector f(c,t)  Rk, where features are defined in
terms of arbitrary properties of the configuration c,
including the state of the stack c, the input buffer
c, and the partially built dependency graph Gc. In
particular, many features involve properties of the
two target tokens, the token on top of the stack c
(0c) and the first token in the input buffer c (0c),
953
which are the two tokens that may become con-
nected by a dependency arc through the transition
out of c. The full set of features used by the base
model MaltParser is described in Nivre et al. (2006).
For the guided transition-based model, which we
call MaltMST, training instances are extended to
triples (c,t,GMSTx ), where GMSTx is the dependency
graph predicted by the graph-based MSTParser for
the sentence x to which the configuration c belongs.
We define m additional guide features, based on
properties of GMSTx , and extend the feature vector
accordingly to f(c,t,GMSTx )  Rk+m. The specific
features used by MaltMST are given in table 1. Un-
like MSTParser, features are not explicitly defined
to conjoin guide features with part-of-speech fea-
tures. These features are implicitly added through
the polynomial kernel used to train the SVM.
4 Experiments
In this section, we present an experimental evalua-
tion of the two guided models based on data from
the CoNLL-X shared task, followed by a compar-
ative error analysis including both the base models
and the guided models. The data for the experiments
are training and test sets for all thirteen languages
from the CoNLL-X shared task on multilingual de-
pendency parsing with training sets ranging in size
from from 29,000 tokens (Slovene) to 1,249,000 to-
kens (Czech). The test sets are all standardized to
about 5,000 tokens each. For more information on
the data sets, see Buchholz and Marsi (2006).
The guided models were trained according to the
scheme explained in section 3, with two-fold cross-
validation when parsing the training data with the
guide parsers. Preliminary experiments suggested
that cross-validation with more folds had a negli-
gible impact on the results. Models are evaluated
by their labeled attachment score (LAS) on the test
set, i.e., the percentage of tokens that are assigned
both the correct head and the correct label, using
the evaluation software from the CoNLL-X shared
task with default settings.4 Statistical significance
was assessed using Dan Bikels randomized pars-
ing evaluation comparator with the default setting of
10,000 iterations.5
4http://nextens.uvt.nl/conll/software.html
5http://www.cis.upenn.edu/dbikel/software.html
Language MST MSTMalt Malt MaltMST
Arabic 66.91 68.64 (+1.73) 66.71 67.80 (+1.09)
Bulgarian 87.57 89.05 (+1.48) 87.41 88.59 (+1.18)
Chinese 85.90 88.43 (+2.53) 86.92 87.44 (+0.52)
Czech 80.18 82.26 (+2.08) 78.42 81.18 (+2.76)
Danish 84.79 86.67 (+1.88) 84.77 85.43 (+0.66)
Dutch 79.19 81.63 (+2.44) 78.59 79.91 (+1.32)
German 87.34 88.46 (+1.12) 85.82 87.66 (+1.84)
Japanese 90.71 91.43 (+0.72) 91.65 92.20 (+0.55)
Portuguese 86.82 87.50 (+0.68) 87.60 88.64 (+1.04)
Slovene 73.44 75.94 (+2.50) 70.30 74.24 (+3.94)
Spanish 82.25 83.99 (+1.74) 81.29 82.41 (+1.12)
Swedish 82.55 84.66 (+2.11) 84.58 84.31 (0.27)
Turkish 63.19 64.29 (+1.10) 65.58 66.28 (+0.70)
Average 80.83 82.53 (+1.70) 80.74 82.01 (+1.27)
Table 2: Labeled attachment scores for base parsers and
guided parsers (improvement in percentage points).
10 20 30 40 50 60
Sentence Length
0.7
0.75
0.8
0.85
0.9
Accuracy
Malt
MST
Malt+MST
MST+Malt
Figure 2: Accuracy relative to sentence length.
4.1 Results
Table 2 shows the results, for each language and on
average, for the two base models (MST, Malt) and
for the two guided models (MSTMalt, MaltMST).
First of all, we see that both guided models show
a very consistent increase in accuracy compared to
their base model, even though the extent of the im-
provement varies across languages from about half
a percentage point (MaltMST on Chinese) up to al-
most four percentage points (MaltMST on Slovene).6
It is thus quite clear that both models have the capa-
city to learn from features generated by the other
model. However, it is also clear that the graph-based
MST model shows a somewhat larger improvement,
both on average and for all languages except Czech,
6The only exception to this pattern is the result for MaltMST
on Swedish, where we see an unexpected drop in accuracy com-
pared to the base model.
954
2 4 6 8 10 12      14   15+
Dependency Length
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
Recall
Malt
MST
Malt+MST
MST+Malt
2 4 6 8 10 12      14   15+
Dependency Length
0.5
0.6
0.65
0.7
0.75
0.8
0.85
Precision
Malt
MST
Malt+MST
MST+Malt
1 2 3 4 5 6 7+
Distance to Root
0.8
0.82
0.84
0.86
0.8
0.9
Recall
Malt
MST
Malt+MST
MST+Malt
1 2 3 4 5 6 7+
Distance to Root
0.78
0.8
0.82
0.84
0.86
0.8
0.9
0.92
Precision
Malt
MST
Malt+MST
MST+Malt
(a) (b)
Figure 3: Dependency arc precision/recall relative to predicted/gold for (a) dependency length and (b) distance to root.
German, Portuguese and Slovene. Finally, given
that the two base models had the previously best
performance for these data sets, the guided models
achieve a substantial improvement of the state of the
art. While there is no statistically significant differ-
ence between the two base models, they are both
outperformed by MaltMST (p < 0.0001), which in
turn has significantly lower accuracy than MSTMalt
(p < 0.0005).
Anextensiontothemodelsdescribedsofarwould
be to iteratively integrate the two parsers in the
spirit of pipeline iteration (Hollingshead and Roark,
2007). For example, one could start with a Malt
model, use it to train a guided MSTMalt model, then
use that as the guide to train a MaltMSTMalt model,
etc. We ran such experiments, but found that accu-
racy did not increase significantly and in some cases
decreasedslightly. Thiswastrueregardlessofwhich
parser began the iterative process. In retrospect, this
result is not surprising. Since the initial integration
effectively incorporates knowledge from both pars-
ing systems, there is little to be gained by adding
additional parsers in the chain.
4.2 Analysis
The experimental results presented so far show that
feature-based integration is a viable approach for
improving the accuracy of both graph-based and
transition-based models for dependency parsing, but
theysayverylittleabouthowtheintegrationbenefits
the two models and what aspects of the parsing pro-
cess are improved as a result. In order to get a better
understanding of these matters, we replicate parts of
the error analysis presented by McDonald and Nivre
(2007), where parsing errors are related to different
structural properties of sentences and their depen-
dency graphs. For each of the four models evalu-
ated, we compute error statistics for labeled attach-
ment over all twelve languages together.
Figure 2 shows accuracy in relation to sentence
length, binned into ten-word intervals (110, 11-20,
etc.). As expected, Malt and MST have very simi-
lar accuracy for short sentences but Malt degrades
more rapidly with increasing sentence length be-
cause of error propagation (McDonald and Nivre,
2007). The guided models, MaltMST and MSTMalt,
behave in a very similar fashion with respect to each
other but both outperform their base parser over the
entire range of sentence lengths. However, except
for the two extreme data points (010 and 5160)
there is also a slight tendency for MaltMST to im-
prove more for longer sentences and for MSTMalt to
improve more for short sentences, which indicates
that the feature-based integration allows one parser
to exploit the strength of the other.
Figure 3(a) plots precision (top) and recall (bot-
tom) for dependency arcs of different lengths (pre-
dicted arcs for precision, gold standard arcs for re-
call). With respect to recall, the guided models ap-
pear to have a slight advantage over the base mod-
955
Part of Speech MST MSTMalt Malt MaltMST
Verb 82.6 85.1 (2.5) 81.9 84.3 (2.4)
Noun 80.0 81.7 (1.7) 80.7 81.9 (1.2)
Pronoun 88.4 89.4 (1.0) 89.2 89.3 (0.1)
Adjective 89.1 89.6 (0.5) 87.9 89.0 (1.1)
Adverb 78.3 79.6 (1.3) 77.4 78.1 (0.7)
Adposition 69.9 71.5 (1.6) 68.8 70.7 (1.9)
Conjunction 73.1 74.9 (1.8) 69.8 72.5 (2.7)
Table 3: Accuracy relative to dependent part of speech
(improvement in percentage points).
els for short and medium distance arcs. With re-
spect to precision, however, there are two clear pat-
terns. First, the graph-based models have better pre-
cision than the transition-based models when pre-
dicting long arcs, which is compatible with the re-
sults of McDonald and Nivre (2007). Secondly, both
the guided models have better precision than their
base model and, for the most part, also their guide
model. InparticularMSTMalt outperformsMSTand
is comparable to Malt for short arcs. More inter-
estingly, MaltMST outperforms both Malt and MST
for arcs up to length 9, which provides evidence that
MaltMST has learned specifically to trust the guide
features from MST for longer dependencies. The
reasonthataccuracydoesnotimprovefordependen-
cies of length greater than 9 is probably that these
dependencies are too rare for MaltMST to learn from
the guide parser in these situations.
Figure 3(b) shows precision (top) and recall (bot-
tom) for dependency arcs at different distances from
the root (predicted arcs for precision, gold standard
arcs for recall). Again, we find the clearest pat-
terns in the graphs for precision, where Malt has
very low precision near the root but improves with
increasing depth, while MST shows the opposite
trend (McDonald and Nivre, 2007). Considering
the guided models, it is clear that MaltMST im-
proves in the direction of its guide model, with a
5-point increase in precision for dependents of the
root and smaller improvements for longer distances.
Similarly, MSTMalt improves precision in the range
where its base parser is inferior to Malt and for dis-
tances up to 4 has an accuracy comparable to or
higher than its guide parser Malt. This again pro-
vides evidence that the guided parsers are learning
from their guide models.
Table 3 gives the accuracy for arcs relative to de-
pendent part-of-speech. As expected, we see that
MST does better than Malt for all categories except
nouns and pronouns (McDonald and Nivre, 2007).
But we also see that the guided models in all cases
improve over their base parser and, in most cases,
also over their guide parser. The general trend is that
MSTimprovesmorethanMalt, exceptforadjectives
and conjunctions, where Malt has a greater disad-
vantage from the start and therefore benefits more
from the guide features.
Consideringtheresultsforpartsofspeech, aswell
as those for dependency length and root distance, it
is interesting to note that the guided models often
improve even in situations where their base parsers
are more accurate than their guide models. This sug-
gests that the improvement is not a simple function
of the raw accuracy of the guide model but depends
on the fact that labeled dependency decisions inter-
act in inference algorithms for both graph-based and
transition-based parsing systems. Thus, if a parser
can improve its accuracy on one class of dependen-
cies, e.g., longer ones, then we can expect to see im-
provements on all types of dependencies  as we do.
The interaction between different decisions may
also be part of the explanation why MST benefits
more from the feature-based integration than Malt,
with significantly higher accuracy for MSTMalt than
for MaltMST as a result. Since inference is global
(or practically global) in the graph-based model,
an improvement in one type of dependency has a
good chance of influencing the accuracy of other de-
pendencies, whereas in the transition-based model,
where inference is greedy, some of these additional
benefits will be lost because of error propagation.
This is reflected in the error analysis in the following
recurrent pattern: Where Malt does well, MaltMST
does only slightly better. But where MST is good,
MSTMalt is often significantly better.
Another part of the explanation may have to do
with the learning algorithms used by the systems.
Although both Malt and MST use discriminative
algorithms, Malt uses a batch learning algorithm
(SVM) and MST uses an online learning algorithm
(MIRA). If the original rich feature representation
of Malt is sufficient to separate the training data,
regularization may force the weights of the guided
features to be small (since they are not needed at
training time). On the other hand, an online learn-
956
ing algorithm will recognize the guided features as
strong indicators early in training and give them a
high weight as a result. Features with high weight
early in training tend to have the most impact on the
final classifier due to both weight regularization and
averaging. This is in fact observed when inspecting
the weights of MSTMalt.
5 Related
Work
Combinations of graph-based and transition-based
models for data-driven dependency parsing have
previously been explored by Sagae and Lavie
(2006), who report improvements of up to 1.7 per-
centage points over the best single parser when
combining three transition-based models and one
graph-based model for unlabeled dependency pars-
ing, evaluated on data from the Penn Treebank. The
combined parsing model is essentially an instance of
the graph-based model, where arc scores are derived
from the output of the different component parsers.
Unlike the models presented here, integration takes
place only at parsing time, not at learning time, and
requires at least three different base parsers. The
same technique was used by Hall et al. (2007) to
combine six transition-based parsers in the best per-
forming system in the CoNLL 2007 shared task.
Feature-based integration in the sense of letting a
subset of the features for one model be derived from
the output of a different model has been exploited
for dependency parsing by McDonald (2006), who
trained an instance of MSTParser using features
generated by the parsers of Collins (1999) and Char-
niak (2000), which improved unlabeled accuracy by
1.7 percentage points, again on data from the Penn
Treebank. In addition, feature-based integration has
been used by Taskar et al. (2005), who trained a
discriminative word alignment model using features
derived from the IBM models, and by Florian et al.(2004), who trained classifiers on auxiliary data to
guide named entity classifiers.
Feature-based integration also has points in com-
mon with co-training, which have been applied to
syntactic parsing by Sarkar (2001) and Steedman et
al. (2003), among others. The difference, of course,
is that standard co-training is a weakly supervised
method, where guide features replace, rather than
complement, the gold standard annotation during
training. Feature-based integration is also similar to
parse re-ranking (Collins, 2000), where one parser
produces a set of candidate parses and a second-
stage classifier chooses the most likely one. How-
ever, feature-based integration is not explicitly con-
strained to any parse decisions that the guide model
might make and only the single most likely parse is
used from the guide model, making it significantly
more efficient than re-ranking.
Finally, there are several recent developments in
data-driven dependency parsing, which can be seen
as targeting the specific weaknesses of graph-based
and transition-based models, respectively, though
without integrating the two models. Thus, Naka-
gawa (2007) and Hall (2007) both try to overcome
the limited feature scope of graph-based models by
adding global features, in the former case using
Gibbs sampling to deal with the intractable infer-
ence problem, in the latter case using a re-ranking
scheme. For transition-based models, the trend is
to alleviate error propagation by abandoning greedy,
deterministic inference in favor of beam search with
globally normalized models for scoring transition
sequences, either generative (Titov and Henderson,
2007a; Titov and Henderson, 2007b) or conditional
(Duan et al., 2007; Johansson and Nugues, 2007).
6 Conclusion
In this paper, we have demonstrated how the two
dominant approaches to data-driven dependency
parsing, graph-based models and transition-based
models, can be integrated by letting one model learn
from features generated by the other. Our experi-
mental results show that both models consistently
improve their accuracy when given access to fea-
tures generated by the other model, which leads to
a significant advancement of the state of the art in
data-driven dependency parsing. Moreover, a com-
parative error analysis reveals that the improvements
are largely predictable from theoretical properties of
the two models, in particular the tradeoff between
global learning and inference, on the one hand, and
rich feature representations, on the other. Directions
for future research include a more detailed analysis
of the effect of feature-based integration, as well as
the exploration of other strategies for integrating dif-
ferent parsing models.
957

References

Giuseppe Attardi. 2006. Experiments with a multilanguage non-projective dependency parser. In Proceedings of CoNLL, pages 166170.

Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of CoNLL, pages 149164.

Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of NAACL, pages 132139.

Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.

Michael Collins. 2000. Discriminative reranking for natural language parsing. In Proceedings of ICML, pages 175182.

Yuan Ding and Martha Palmer. 2004. Synchronous dependency insertion grammars: A grammar formalism for syntax based statistical MT. In Proceedings of the Workshop on Recent Advances in Dependency Grammar, pages 9097.

Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Probabilistic parsing action models for multi-lingual dependency parsing. In Proceedings of EMNLP-CoNLL, pages 940946.

Jason M. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of COLING, pages 340345.

Radu Florian, Hany Hassan, Abraham Ittycheriah, Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo, Nicolas Nicolov, and Salim Roukos. 2004. A statistical model for multilingual entity detection and tracking. In Proceedings of NAACL/HLT.

Johan Hall, Jens Nilsson, Joakim Nivre, Gulsen EryigitBeata Megyesi, Mattias Nilsson, and Markus Saers. 2007. Single malt or blended? A study in multilingual parser optimization. In Proceedings of EMNLP-CoNLL.

Keith Hall. 2007. K-best spanning tree parsing. In Proceedings of ACL, pages 392399.

Kristy Hollingshead and Brian Roark. 2007. Pipeline iteration. In Proceedings of ACL, pages 952959.

Richard Johansson and Pierre Nugues. 2007. Incremental dependency parsing using online learning. In Proceedings of EMNLP-CoNLL, pages 11341138.

Ryan McDonald and Joakim Nivre. 2007. Characterizing the errors of data-driven dependency parsing mod-els. In Proceedings of EMNLP-CoNLL, pages 122131.

Ryan McDonald and Giorgio Satta. 2007. On the complexityofnon-projectivedata-drivendependencyparsing. In Proceedings of IWPT, pages 122131.

Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005a. Online large-margin training of dependency parsers. In Proceedings of ACL, pages 9198.

Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajic. 2005b. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of HLT/EMNLP, pages 523530.

Ryan McDonald, Kevin Lerman, and Fernando Pereira. 2006. Multilingual dependency analysis with a two-stage discriminative parser. In Proceedings of CoNLL, pages 216220.

Ryan McDonald. 2006. Discriminative Learning and Spanning Tree Algorithms for Dependency Parsing. Ph.D. thesis, University of Pennsylvania.

Tetsuji Nakagawa. 2007. Multilingual dependency parsing using global features. In Proceedings of EMNLP-CoNLL, pages 952956.

Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. Memory-based dependency parsing. In Proceedings of CoNLL, pages 4956.

Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigitand Svetoslav Marinov. 2006. Labeled pseudo-projective dependency parsing with support vector machines. In Proceedings of CoNLL, pages 221225.

Joakim Nivre, Johan Hall, Sandra Kubler, Ryan McDoald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In Proceedings of EMNLP-CoNLL, pages 915932.

Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of IWPT, pages 149160.

Kenji Sagae and Alon Lavie. 2006. Parser combination byreparsing. InProceedingsofNAACL:ShortPapers, pages 129132.

Anoop Sarkar. 2001. Applying co-training methods to statistical parsing. In Proceedings of NAACL, pages 175182.

Rion Snow, Dan Jurafsky, and Andrew Y. Ng. 2005. Learning syntactic patterns for automatic hypernym discovery. In Proceedings of NIPS.

Mark Steedman, Rebecca Hwa, Miles Osborne, and Anoop Sarkar. 2003. Corrected co-training for statistical parsers. In Proceedings of ICML, pages 95102.

Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005. A discriminative matching approach to word alignment. In Proceedings of HLT/EMNLP, pages 7380.

Ivan Titov and James Henderson. 2007a. Fast and robust multilingual dependency parsing with a generative latent variable model. In Proceedings of EMNLP-CoNLL, pages 947951.

Ivan Titov and James Henderson. 2007b. A latent variable model for generative dependency parsing. In Proceedings of IWPT, pages 144155.

Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings of IWPT, pages 195206
Proceedings of ACL-08: HLT, pages 959967,
Columbus, Ohio, USA, June 2008. c2008 Association for Computational Linguistics
Efficient, Feature-based, Conditional Random Field Parsing
Jenny Rose Finkel, Alex Kleeman, Christopher D. Manning
Department of Computer Science
Stanford University
Stanford, CA 94305
jrfinkel@cs.stanford.edu, akleeman@stanford.edu, manning@cs.stanford.edu
Abstract
Discriminative feature-based methods are
widely used in natural language processing,
but sentence parsing is still dominated by gen-
erative methods. While prior feature-based
dynamic programming parsers have restricted
training and evaluation to artificially short sen-
tences, we present the first general, feature-
rich discriminative parser, based on a condi-
tional random field model, which has been
successfully scaled to the full WSJ parsing
data. Our efficiency is primarily due to the
use of stochastic optimization techniques, as
well as parallelization and chart prefiltering.
On WSJ15, we attain a state-of-the-art F-score
of 90.9%, a 14% relative reduction in error
over previous models, while being two orders
of magnitude faster. On sentences of length
40, our system achieves an F-score of 89.0%,
a 36% relative reduction in error over a gener-
ative baseline.
1 Introduction
Over the past decade, feature-based discriminative
models have become the tool of choice for many
natural language processing tasks. Although they
take much longer to train than generative models,
they typically produce higher performing systems,
in large part due to the ability to incorporate ar-
bitrary, potentially overlapping features. However,
constituency parsing remains an area dominated by
generative methods, due to the computational com-
plexity of the problem. Previous work on discrim-
inative parsing falls under one of three approaches.
One approach does discriminative reranking of the
n-best list of a generative parser, still usually de-
pending highly on the generative parser score as
a feature (Collins, 2000; Charniak and Johnson,
2005). A second group of papers does parsing by a
sequence of independent, discriminative decisions,
either greedily or with use of a small beam (Ratna-
parkhi, 1997; Henderson, 2004). This paper extends
the third thread of work, where joint inference via
dynamic programming algorithms is used to train
models and to attempt to find the globally best parse.
Work in this context has mainly been limited to use
of artificially short sentences due to exorbitant train-
ing and inference times. One exception is the re-
cent work of Petrov et al. (2007), who discrimina-
tively train a grammar with latent variables and do
not restrict themselves to short sentences. However
their model, like the discriminative parser of John-
son (2001), makes no use of features, and effectively
ignores the largest advantage of discriminative train-
ing. It has been shown on other NLP tasks that mod-
eling improvements, such as the switch from gen-
erative training to discriminative training, usually
provide much smaller performance gains than the
gains possible from good feature engineering. For
example, in (Lafferty et al., 2001), when switching
from a generatively trained hidden Markov model
(HMM) to a discriminatively trained, linear chain,
conditional random field (CRF) for part-of-speech
tagging, their error drops from 5.7% to 5.6%. When
they add in only a small set of orthographic fea-
tures, their CRF error rate drops considerably more
to 4.3%, and their out-of-vocabulary error rate drops
by more than half. This is further supported by John-
son (2001), who saw no parsing gains when switch-
959
ing from generative to discriminative training, and
by Petrov et al. (2007) who saw only small gains of
around 0.7% for their final model when switching
training methods.
In this work, we provide just such a framework for
training a feature-rich discriminative parser. Unlike
previous work, we do not restrict ourselves to short
sentences, but we do provide results both for training
and testing on sentences of length  15 (WSJ15) and
for training and testing on sentences of length  40,
allowing previous WSJ15 results to be put in context
with respect to most modern parsing literature. Our
model is a conditional random field based model.
For a rule application, we allow arbitrary features
to be defined over the rule categories, span and split
point indices, and the words of the sentence. It is
well known that constituent length influences parse
probability, but PCFGs cannot easily take this infor-
mation into account. Another benefit of our feature
based model is that it effortlessly allows smooth-
ing over previously unseen rules. While the rule
may be novel, it will likely contain features which
are not. Practicality comes from three sources. We
made use of stochastic optimization methods which
allow us to find optimal model parameters with very
few passes through the data. We found no differ-
ence in parser performance between using stochastic
gradient descent (SGD), and the more common, but
significantly slower, L-BFGS. We also used limited
parallelization, and prefiltering of the chart to avoid
scoring rules which cannot tile into complete parses
of the sentence. This speed-up does not come with a
performance cost; we attain an F-score of 90.9%, a
14% relative reduction in errors over previous work
on WSJ15.
2 The
Model
2.1 A
Conditional Random Field Context Free
Grammar (CRF-CFG)
Our parsing model is based on a conditional ran-
dom field model, however, unlike previous TreeCRF
work, e.g., (Cohn and Blunsom, 2005; Jousse et al.,
2006), we do not assume a particular tree structure,
and instead find the most likely structure and la-
beling. This is similar to conventional probabilis-
tic context-free grammar (PCFG) parsing, with two
exceptions: (a) we maximize conditional likelihood
of the parse tree, given the sentence, not joint like-
lihood of the tree and sentence; and (b) probabil-
ities are normalized globally instead of locally 
the graphical models depiction of our trees is undi-
rected.
Formally, we have a CFG G, which consists of
(Manning and Schutze, 1999): (i) a set of termi-
nals {wk},k = 1,...,V ; (ii) a set of nonterminals
{Nk},k = 1,...,n; (iii) a designated start symbol
ROOT ; and (iv) a set of rules, { = Ni   j}, where
 j is a sequence of terminals and nonterminals. A
PCFG additionally assigns probabilities to each rule
 such that ij P(Ni   j) = 1. Our conditional
random field CFG (CRF-CFG) instead defines local
clique potentials (r|s;), where s is the sentence,
and r contains a one-level subtree of a tree t, corre-
sponding to a rule , along with relevant information
about the span of words which it encompasses, and,
if applicable, the split position (see Figure 1). These
potentials are relative to the sentence, unlike a PCFG
where rule scores do not have access to words at the
leaves of the tree, or even how many words they
dominate. We then define a conditional probabil-
ity distribution over entire trees, using the standard
CRF distribution, shown in (1). There is, however,
an important subtlety lurking in how we define the
partition function. The partition function Zs, which
makes the probability of all possible parses sum to
unity, is defined over all structures as well as all la-
belings of those structures. We define (s) to be the
set of all possible parse trees for the given sentence
licensed by the grammar G.
P(t|s;) = 1Z
s rt
(r|s;) (1)
where
Zs = t(s)rt (r|s;)
The above model is not well-defined over all
CFGs. Unary rules of the form Ni  N j can form
cycles, leading to infinite unary chains with infinite
mass. However, it is standard in the parsing liter-
ature to transform grammars into a restricted class
of CFGs so as to permit efficient parsing. Binariza-
tion of rules (Earley, 1970) is necessary to obtain
cubic parsing time, and closure of unary chains is re-
quired for finding total probability mass (rather than
just best parses) (Stolcke, 1995). To address this is-
sue, we define our model over a restricted class of
960
S
NP
NN
Factory
NNS
payrolls
VP
VBD
fell
PP
IN
in
NN
September
Phrasal rules
r1 = S0,5  NP0,2 VP2,5 | Factory payrolls fell in September
r3 = VP2,5  VBD2,3 PP3,5 | Factory payrolls fell in September
. . .
Lexicon rules
r5 = NN0,1  Factory | Factory payrolls fell in September
r6 = NNS1,2  payrolls | Factory payrolls fell in September
. . .
(a) PCFG Structure (b) Rules r
Figure 1: A parse tree and the corresponding rules over which potentials and features are defined.
CFGs which limits unary chains to not have any re-
peated states. This was done by collapsing all al-
lowed unary chains to single unary rules, and dis-
allowing multiple unary rule applications over the
same span.1 We give the details of our binarization
scheme in Section 5. Note that there exists a gram-
mar in this class which is weakly equivalent with any
arbitrary CFG.
2.2 Computing
the Objective Function
Our clique potentials take an exponential form. We
have a feature function, represented by f(r,s), which
returns a vector with the value for each feature. We
denote the value of feature fi by fi(r,s) and our
model has a corresponding parameter i for each
feature. The clique potential function is then:
(r|s;) = expi i fi(r,s) (2)
The log conditional likelihood of the training data
D, with an additional L2 regularization term, is then:
L(D;) =parenleftBigg

(t,s)D
parenleftbigg

rt

i
i fi(r,s)
parenrightbigg
Zs
parenrightBigg
+
i
2i
2 2 (3)
And the partial derivatives of the log likelihood, with
respect to the model weights are, as usual, the dif-
ference between the empirical counts and the model
expectations:
L
i =
parenleftBigg

(t,s)D
parenleftbigg

rt
fi(r,s)
parenrightbigg
E[fi|s]
parenrightBigg
+ i 2 (4)
1In our implementation of the inside-outside algorithm, we
then need to keep two inside and outside scores for each span:
one from before and one from after the application of unary
rules.
The partition function Zs and the partial derivatives
can be efficiently computed with the help of the
inside-outside algorithm.2 Zs is equal to the in-
side score of ROOT over the span of the entire sen-
tence. To compute the partial derivatives, we walk
through each rule, and span/split, and add the out-
side log-score of the parent, the inside log-score(s)
of the child(ren), and the log-score for that rule and
span/split. Zs is subtracted from this value to get the
normalized log probability of that rule in that posi-
tion. Using the probabilities of each rule applica-
tion, over each span/split, we can compute the ex-
pected feature values (the second term in Equation
4), by multiplying this probability by the value of
the feature corresponding to the weight for which we
are computing the partial derivative. The process is
analogous to the computation of partial derivatives
in linear chain CRFs. The complexity of the algo-
rithm for a particular sentence is O(n3), where n is
the length of the sentence.
2.3 Parallelization
Unlike (Taskar et al., 2004), our algorithm has the
advantage of being easily parallelized (see footnote
7 in their paper). Because the computation of both
the log likelihood and the partial derivatives involves
summing over each tree individually, the compu-
tation can be parallelized by having many clients
which each do the computation for one tree, and one
central server which aggregates the information to
compute the relevant information for a set of trees.
Because we use a stochastic optimization method,
as discussed in Section 3, we compute the objec-
tive for only a small portion of the training data at
a time, typically between 15 and 30 sentences. In
2In our case the values in the chart are the clique potentials
which are non-negative numbers, but not probabilities.
961
this case the gains from adding additional clients
decrease rapidly, because the computation time is
dominated by the longest sentences in the batch.
2.4 Chart
Prefiltering
Training is also sped up by prefiltering the chart. On
the inside pass of the algorithm one will see many
rules which cannot actually be tiled into complete
parses. In standard PCFG parsing it is not worth fig-
uring out which rules are viable at a particular chart
position and which are not. In our case however this
can make a big difference.We are not just looking
up a score for the rule, but must compute all the fea-
tures, and dot product them with the feature weights,
which is far more time consuming. We also have to
do an outside pass as well as an inside one, which
is sped up by not considering impossible rule appli-
cations. Lastly, we iterate through the data multi-
ple times, so if we can compute this information just
once, we will save time on all subsequent iterations
on that sentence. We do this by doing an inside-
outside pass that is just boolean valued to determine
which rules are possible at which positions in the
chart. We simultaneously compute the features for
the possible rules and then save the entire data struc-
ture to disk. For all but the shortest of sentences,
the disk I/O is easily worth the time compared to re-
computation. The first time we see a sentence this
method is still about one third faster than if we did
not do the prefiltering, and on subsequent iterations
the improvement is closer to tenfold.
3 Stochastic
Optimization Methods
Stochastic optimization methods have proven to be
extremely efficient for the training of models involv-
ing computationally expensive objective functions
like those encountered with our task (Vishwanathan
et al., 2006) and, in fact, the on-line backpropagation
learning used in the neural network parser of Hen-
derson (2004) is a form of stochastic gradient de-
scent. Standard deterministic optimization routines
such as L-BFGS (Liu and Nocedal, 1989) make little
progress in the initial iterations, often requiring sev-
eral passes through the data in order to satisfy suffi-
cient descent conditions placed on line searches. In
our experiments SGD converged to a lower objective
function value than L-BFGS, however it required far
0 5 10 15 20 25 30 35 40 45 503.5
3
2.5
2
1.5
1
0.5
0 x 10
5
Passes
Log Likelihood
SGD
LBFGS
Figure 2: WSJ15 objective value for L-BFGS and SGD
versus passes through the data. SGD ultimately con-
verges to a lower objective value, but does equally well
on test data.
fewer iterations (see Figure 2) and achieved compa-
rable test set performance to L-BFGS in a fraction of
the time. One early experiment on WSJ15 showed a
seven time speed up.
3.1 Stochastic
Function Evaluation
Utilization of stochastic optimization routines re-
quires the implementation of a stochastic objective
function. This function, L is designed to approx-
imate the true function L based off a small subset
of the training data represented by Db. Here b, the
batch size, means that Db is created by drawing b
training examples, with replacement, from the train-
ing set D. With this notation we can express the
stochastic evaluation of the function as L(Db;).
This stochastic function must be designed to ensure
that:
E
bracketleftBig
ni L(D(i)b ;)
bracketrightBig
=L(D;)
Note that this property is satisfied, without scaling,
for objective functions that sum over the training
data, as it is in our case, but any priors must be
scaled down by a factor of b/|D|. The stochastic
gradient, L(D(i)b ;), is then simply the derivative
of the stochastic function value.
3.2 Stochastic
Gradient Descent
SGD was implemented using the standard update:
k+1 = k kL(D(k)b ;k)
962
And employed a gain schedule in the form
k = 0  +k
where parameter  was adjusted such that the gain is
halved after five passes through the data. We found
that an initial gain of 0 = 0.1 and batch size be-
tween 15 and 30 was optimal for this application.
4 Features
As discussed in Section 5 we performed experi-
ments on both sentences of length  15 and length
 40. All feature development was done on the
length 15 corpus, due to the substantially faster
train and test times. This has the unfortunate effect
that our features are optimized for shorter sentences
and less training data, but we found development
on the longer sentences to be infeasible. Our fea-
tures are divided into two types: lexicon features,
which are over words and tags, and grammar fea-
tures which are over the local subtrees and corre-
sponding span/split (both have access to the entire
sentence). We ran two kinds of experiments: a dis-
criminatively trained model, which used only the
rules and no other grammar features, and a feature-
based model which did make use of grammar fea-
tures. Both models had access to the lexicon fea-
tures. We viewed this as equivalent to the more
elaborate, smoothed unknown word models that are
common in many PCFG parsers, such as (Klein and
Manning, 2003; Petrov et al., 2006).
We preprocessed the words in the sentences to ob-
tain two extra pieces of information. Firstly, each
word is annotated with a distributional similarity tag,
from a distributional similarity model (Clark, 2000)
trained on 100 million words from the British Na-
tional Corpus and English Gigaword corpus. Sec-
ondly, we compute a class for each word based on
the unknown word model of Klein and Manning
(2003); this model takes into account capitaliza-
tion, digits, dashes, and other character-level fea-
tures. The full set of features, along with an expla-
nation of our notation, is listed in Table 1.
5 Experiments
For all experiments, we trained and tested on the
Penn treebank (PTB) (Marcus et al., 1993). We used
Binary Unary
Model States Rules Rules
WSJ15 1,428 5,818 423
WSJ15 relaxed 1,428 22,376 613
WSJ40 7,613 28,240 823
Table 2: Grammar size for each of our models.
the standard splits, training on sections 2 to 21, test-
ing on section 23 and doing development on section
22. Previous work on (non-reranking) discrimina-
tive parsing has given results on sentences of length
 15, but most parsing literature gives results on ei-
ther sentences of length  40, or all sentences. To
properly situate this work with respect to both sets
of literature we trained models on both length 
15 (WSJ15) and length  40 (WSJ40), and we also
tested on all sentences using the WSJ40 models. Our
results also provide a context for interpreting previ-
ous work which used WSJ15 and not WSJ40.
We used a relatively simple grammar with few ad-
ditional annotations. Starting with the grammar read
off of the training set, we added parent annotations
onto each state, including the POS tags, resulting in
rules such as S-ROOT  NP-S VP-S. We also added
head tag annotations to VPs, in the same manner as
(Klein and Manning, 2003). Lastly, for the WSJ40
runs we used a simple, right branching binarization
where each active state is annotated with its previous
sibling and first child. This is equivalent to children
of a state being produced by a second order Markov
process. For the WSJ15 runs, each active state was
annotated with only its first child, which is equiva-
lent to a first order Markov process. See Table 5 for
the number of states and rules produced.
5.1 Experiments
For both WSJ15 and WSJ40, we trained a genera-
tive model; a discriminative model, which used lexi-
con features, but no grammar features other than the
rules themselves; and a feature-based model which
had access to all features. For the length 15 data we
also did experiments in which we relaxed the gram-
mar. By this we mean that we added (previously un-
seen) rules to the grammar, as a means of smoothing.
We chose which rules to add by taking existing rules
and modifying the parent annotation on the parent
of the rule. We used stochastic gradient descent for
963
Table 1: Lexicon and grammar features. w is the word and t the tag. r represents a particular rule along with span/split
information;  is the rule itself, rp is the parent of the rule; wb, ws, and we are the first, first after the split (for binary
rules) and last word that a rule spans in a particular context. All states, including the POS tags, are annotated with
parent information; b(s) represents the base label for a state s and p(s) represents the parent annotation on state s.
ds(w) represents the distributional similarity cluster, and lc(w) the lower cased version of the word, and unk(w) the
unknown word class.
Lexicon Features Grammar Features
t Binary-specific features
b(t) 
t,w b(p(rp)),ds(ws) b(p(rp)),ds(ws1,dsws)
t,lc(w) b(p(rp)),ds(we) PP feature:
b(t),w unary? if right child is a PP then r,ws
b(t),lc(w) simplified rule: VP features:
t,ds(w) base labels of states if some child is a verb tag, then rule,
t,ds(w1) dist sim bigrams: with that child replaced by the word
t,ds(w+1) all dist. sim. bigrams below
b(t),ds(w) rule, and base parent state Unaries which span one word:
b(t),ds(w1) dist sim bigrams:
b(t),ds(w+1) same as above, but trigrams r,w
p(t),w heavy feature: r,ds(w)
t,unk(w) whether the constituent is big b(p(r)),w
b(t),unk(w) as described in (Johnson, 2001) b(p(r)),ds(w)
these experiments; the length 15 models had a batch
size of 15 and we allowed twenty passes through
the data.3 The length 40 models had a batch size
of 30 and we allowed ten passes through the data.
We used development data to decide when the mod-
els had converged. Additionally, we provide gener-
ative numbers for training on the entire PTB to give
a sense of how much performance suffered from the
reduced training data (generative-all in Table 4).
The full results for WSJ15 are shown in Table 3
and for WSJ40 are shown in Table 4. The WSJ15
models were each trained on a single Dual-Core
AMD OpteronTM using three gigabytes of RAM and
no parallelization. The discriminatively trained gen-
erative model (discriminative in Table 3) took ap-
proximately 12 minutes per pass through the data,
while the feature-based model (feature-based in Ta-
ble 3) took 35 minutes per pass through the data.
The feature-based model with the relaxed grammar
(relaxed in Table 3) took about four times as long
as the regular feature-based model. The discrimina-
3Technically we did not make passes through the data, be-
cause we sampled with replacement to get our batches. By this
we mean having seen as many sentences as are in the data, de-
spite having seen some sentences multiple times and some not
at all.
tively trained generative WSJ40 model (discrimina-
tive in Table 4) was trained using two of the same
machines, with 16 gigabytes of RAM each for the
clients.4 It took about one day per pass through
the data. The feature-based WSJ40 model (feature-
based in Table 4) was trained using four of these
machines, also with 16 gigabytes of RAM each for
the clients. It took about three days per pass through
the data.
5.2 Discussion
The results clearly show that gains came from both
the switch from generative to discriminative train-
ing, and from the extensive use of features. In Fig-
ure 3 we show for an example from section 22 the
parse trees produced by our generative model and
our feature-based discriminative model, and the cor-
rect parse. The parse from the feature-based model
better exhibits the right branching tendencies of En-
glish. This is likely due to the heavy feature, which
encourages long constituents at the end of the sen-
tence. It is difficult for a standard PCFG to learn this
aspect of the English language, because the score it
assigns to a rule does not take its span into account.
4The server does almost no computation.
964
Model P R F1 Exact Avg CB 0 CB P R F1 Exact Avg CB 0 CB
development set  length  15 test set  length  15
Taskar 2004 89.7 90.2 90.0    89.1 89.1 89.1   
Turian 2007       89.6 89.3 89.4   
generative 86.9 85.8 86.4 46.2 0.34 81.2 87.6 85.8 86.7 49.2 0.33 81.9
discriminative 89.1 88.6 88.9 55.5 0.26 85.5 88.9 88.0 88.5 56.6 0.32 85.0
feature-based 90.4 89.3 89.9 59.5 0.24 88.3 91.1 90.2 90.6 61.3 0.24 86.8
relaxed 91.2 90.3 90.7 62.1 0.24 88.1 91.4 90.4 90.9 62.0 0.22 87.9
Table 3: Development and test set results, training and testing on sentences of length  15 from the Penn treebank.
Model P R F1 Exact Avg CB 0 CB P R F1 Exact Avg CB 0 CB
test set  length  40 test set  all sentences
Petrov 2007   88.8      88.3   
generative 83.5 82.0 82.8 25.5 1.57 53.4 82.8 81.2 82.0 23.8 1.83 50.4
generative-all 83.6 82.1 82.8 25.2 1.56 53.3      
discriminative 85.1 84.5 84.8 29.7 1.41 55.8 84.2 83.7 83.9 27.8 1.67 52.8
feature-based 89.2 88.8 89.0 37.3 0.92 65.1 88.2 87.8 88.0 35.1 1.15 62.3
Table 4: Test set results, training on sentences of length  40 from the Penn treebank. The generative-all results were
trained on all sentences regardless of length
6 Comparison
With Related Work
The most similar related work is (Johnson, 2001),
which did discriminative training of a generative
PCFG. The model was quite similar to ours, except
that it did not incorporate any features and it re-
quired the parameters (which were just scores for
rules) to be locally normalized, as with a genera-
tively trained model. Due to training time, they used
the ATIS treebank corpus , which is much smaller
than even WSJ15, with only 1,088 training sen-
tences, 294 testing sentences, and an average sen-
tence length of around 11. They found no signif-
icant difference in performance between their gen-
eratively and discriminatively trained parsers. There
are two probable reasons for this result. The training
set is very small, and it is a known fact that gener-
ative models tend to work better for small datasets
and discriminative models tend to work better for
larger datasets (Ng and Jordan, 2002). Additionally,
they made no use of features, one of the primary
benefits of discriminative learning.
Taskar et al. (2004) took a large margin approach
to discriminative learning, but achieved only small
gains. We suspect that this is in part due to the gram-
mar that they chose  the grammar of (Klein and
Manning, 2003), which was hand annotated with the
intent of optimizing performance of a PCFG. This
grammar is fairly sparse  for any particular state
there are, on average, only a few rules with that state
as a parent  so the learning algorithm may have suf-
fered because there were few options to discriminate
between. Starting with this grammar we found it dif-
ficult to achieve gains as well. Additionally, their
long training time (several months for WSJ15, ac-
cording to (Turian and Melamed, 2006)) made fea-
ture engineering difficult; they were unable to really
explore the space of possible features.
More recent is the work of (Turian and Melamed,
2006; Turian et al., 2007), which improved both the
training time and accuracy of (Taskar et al., 2004).
They define a simple linear model, use boosted de-
cision trees to select feature conjunctions, and a line
search to optimize the parameters. They use an
agenda parser, and define their atomic features, from
which the decision trees are constructed, over the en-
tire state being considered. While they make exten-
sive use of features, their setup is much more com-
plex than ours and takes substantially longer to train
 up to 5 days on WSJ15  while achieving only
small gains over (Taskar et al., 2004).
The most recent similar research is (Petrov et al.,
2007). They also do discriminative parsing of length
40 sentences, but with a substantially different setup.
Following up on their previous work (Petrov et al.,
2006) on grammar splitting, they do discriminative
965
S
S
NP
PRP
He
VP
VBZ
adds
NP
DT
This
VP
VBZ
is
RB
nt
NP
NP
CD
1987
VP
VBN
revisited
S
NP
PRP
He
VP
VBZ
adds
S
NP
DT
This
VP
VBZ
is
RB
nt
NP
CD
1987
VP
VBN
revisited
S
NP
PRP
He
VP
VBZ
adds
S
NP
DT
This
VP
VBZ
is
RB
nt
NP
NP
CD
1987
VP
VBN
revisited
(a) generative output (b) feature-based discriminative output (c) gold parse
Figure 3: Example output from our generative and feature-based discriminative models, along with the correct parse.
parsing with latent variables, which requires them
to optimize a non-convex function. Instead of us-
ing a stochastic optimization technique, they use L-
BFGS, but do coarse-to-fine pruning to approximate
their gradients and log likelihood. Because they
were focusing on grammar splitting they, like (John-
son, 2001), did not employ any features, and, like
(Taskar et al., 2004), they saw only small gains from
switching from generative to discriminative training.
7 Conclusions
We have presented a new, feature-rich, dynamic pro-
gramming based discriminative parser which is sim-
pler, more effective, and faster to train and test than
previous work, giving us new state-of-the-art per-
formance when training and testing on sentences of
length  15 and the first results for such a parser
trained and tested on sentences of length  40. We
also show that the use of SGD for training CRFs per-
forms as well as L-BFGS in a fraction of the time.
Other recent work on discriminative parsing has ne-
glected the use of features, despite their being one of
the main advantages of discriminative training meth-
ods. Looking at how other tasks, such as named
entity recognition and part-of-speech tagging, have
evolved over time, it is clear that greater gains are to
be gotten from developing better features than from
better models. We have provided just such a frame-
work for improving parsing performance.
Acknowledgments
Many thanks to Teg Grenager and Paul Heymann
for their advice (and their general awesomeness),
and to our anonymous reviewers for helpful com-
ments.
This paper is based on work funded in part by
the Defense Advanced Research Projects Agency
through IBM, by the Disruptive Technology Office
(DTO) Phase III Program for Advanced Question
Answering for Intelligence (AQUAINT) through
Broad Agency Announcement (BAA) N61339-06-
R-0034, and by a Scottish Enterprise Edinburgh-
Stanford Link grant (R37588), as part of the EASIE
project.

References

Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-best parsing and maxent discriminative reranking. In ACL 43, pages 173180.

Alexander Clark. 2000. Inducing syntactic categories by context distribution clustering. In Proc. of Conference on Computational Natural Language Learning, pages 9194, Lisbon, Portugal.

Trevor Cohn and Philip Blunsom. 2005. Semantic role labelling with tree conditional random fields. In CoNLL 2005.

Michael Collins. 2000. Discriminative reranking for natural language parsing. In ICML 17, pages 175182.

Jay Earley. 1970. An efficient context-free parsing algorithm. Communications of the ACM, 6(8):451455.

James Henderson. 2004. Discriminative training of a neural network statistical parser. In ACL 42, pages 96 103.

Mark Johnson. 2001. Joint and conditional estimation of tagging and parsing models. In Meeting of the Association for Computational Linguistics, pages 314321.

Florent Jousse, Remi Gilleron, Isabelle Tellier, and Marc Tommasi. 2006. Conditional Random Fields for XML trees. In ECML Workshop on Mining and Learning in Graphs.

Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the Association of Computational Linguistics (ACL).

John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional Random Fields: Probabilistic models for segmenting and labeling sequence data. In ICML 2001, pages 282289. Morgan Kaufmann, Sa Francisco, CA.

Dong C. Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Math. Programming, 45(3, (Ser. B)):503528.

Christopher D. Manning and Hinrich Schutze. 1999Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, Massachusetts.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313330.

Andrew Ng and Michael Jordan. 2002. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. In Advances in Neural Information Processing Systems (NIPS). 

Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In ACL 44/COLING 21, pages 433440.

Slav Petrov, Adam Pauls, and Dan Klein. 2007. Discriminative log-linear grammars with latent variables. In NIPS.

Adwait Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum entropy models. In EMNLP 2, pages 110.

Andreas Stolcke. 1995. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. Computational Linguistics, 21:165202.

Ben Taskar, Dan Klein, Michael Collins, Daphne Koller, and Christopher D. Manning. 2004. Max-margin parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).

Joseph Turian and I. Dan Melamed. 2006. Advances in discriminative parsing. In ACL 44, pages 873880. 

Joseph Turian, Ben Wellington, and I. Dan Melamed. 2007. Scalable discriminative learning for natural language parsing and translation. In Advances in Neural Information Processing Systems 19, pages 14091416. MIT Press.

S. V. N. Vishwanathan, Nichol N. Schraudolph, Mark W. Schmidt, and Kevin P. Murphy. 2006. Accelerated training of conditional random fields with stochastic gradient methods. In ICML 23, pages 969976. 967
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 261264,
Columbus, Ohio, USA, June 2008. c2008 Association for Computational Linguistics
Evaluating Word Prediction: Framing Keystroke Savings
Keith Trnka and Kathleen F. McCoy
University of Delaware
Newark, DE 19716
trnka@cis.udel.edu
Abstract
Researchers typically evaluate word predic-
tion using keystroke savings, however, this
measure is not straightforward. We present
several complications in computing keystroke
savings which may affect interpretation and
comparison of results. We address this prob-
lem by developing two gold standards as a
frame for interpretation. These gold standards
measure the maximum keystroke savings un-
der two different approximations of an ideal
language model. The gold standards addition-
ally narrow the scope of deficiencies in a word
prediction system.
1 Introduction
Word prediction is an application of language mod-
eling to speeding up text entry, especially to entering
utterances to be spoken by an Augmentative and Al-
ternative Communication (AAC) device. AAC de-
vices seek to address the dual problem of speech and
motor impairment by attempting to optimize text in-
put. Even still, communication rates with AAC de-
vices are often below 10 words per minute (Newell
et al., 1998), compared to the common 130-200
words per minute speech rate of speaking people.
Word prediction addresses these issues by reducing
the number of keystrokes required to produce a mes-
sage, which has been shown to improve communi-
cation rate (Trnka et al., 2007). The reduction in
keystrokes also translates into a lower degree of fa-
tigue from typing all day (Carlberger et al., 1997).
Word prediction systems present multiple com-
pletions of the current word to the user. Systems
generate a list of W predictions on the basis of the
word being typed and a language model. The vo-
cabulary is filtered to match the prefix of the current
word and the language model ranks the words ac-
cording to their likelihood. In the case that no letters
of the current word have been entered, the language
model is the sole factor in generating predictions.
Systems often use a touchscreen or function/number
keys to select any of the predicted words.
Because the goal of word prediction systems is
to reduce the number of keystrokes, the primary
evaluation for word prediction is keystroke savings
(Garay-Vitoria and Abascal, 2006; Newell et al.,
1998; Li and Hirst, 2005; Trnka and McCoy, 2007;
Carlberger et al., 1997). Keystroke savings (KS)
measures the percentage reduction in keys pressed
compared to letter-by-letter text entry.
KS = keysnormal keyswith predictionkeys
normal
100%
A word prediction system that offers higher savings
will benefit a user more in practice.
However, the equation for keystroke savings has
two major deficiencies. Firstly, the equation alone
is not enough to compute keystroke savings  actu-
ally computing keystroke savings requires a precise
definition of a keystroke and also requires a method
fordetermininghowmanykeystrokesareusedwhen
predictionsareavailable, discussedinSection2. Be-
yond simply computing keystroke savings, the equa-
tionalonedoesnotprovidemuchinthewayofinter-
pretationis60%keystrokesavingsgood? Canwe
do better? Section 3 will present two gold standards
to allow better interpretation of keystroke savings.
261
2 Computing
Keystroke Savings
We must have a way to determine how many
keystrokes a user would take under both letter-
by-letter entry and word prediction to compute
keystroke savings. The common trend in research
is to simulate a perfect user that will never make
typing mistakes and will select a word from the pre-
dictions as soon as it appears.
Implementation of perfect utilization of the pre-
dictions is not always straightforward. For exam-
ple, consider the predictive interface in Microsoft
WordTM: a single prediction is offered as an inline
completion. If the prediction is selected, the user
may backspace and edit the word. However, this
freedom makes finding the minimum sequence of
keys more difficult  now the user may select a
prediction with the incorrect suffix and correct the
suffix as the optimal action. We feel that a more in-
tuitive interface would allow a user to undo the pre-
diction selection by pressing backspace, an interface
which does not support backspace-editing. In addi-
tion to backspacing, future research in multi-word
prediction will face a similar problem, analogous to
the garden-path problem in parsing, where a greedy
approach does not always give the optimal result.
The keystrokes used for training and testing word
prediction systems can affect the results. We at-
tempt to evaluate word prediction as realistically as
possible. Firstly, many corpora have punctuation
marks, but an AAC user in a conversational setting
is unlikely to use punctuation due to the high cost
of each key press. Therefore, we remove punctua-
tion on the outside of words, such as commas and
periods, but leave word-internal punctuation intact.
Also, we treat capital letters as a single key press,
reflecting the trend of many AAC users to avoid cap-
italization. Another problem occurs for a newline or
speak key, which the user would press after com-
pleting an utterance. In pilot studies, including the
simulation of a speak key lowered keystroke savings
by 0.81.0% for window sizes 110, because new-
linesarenotabletobepredictedinthesystem. How-
ever, we feel that the simulation of a speak key will
produce an evaluation metric that is closer to the ac-
tual users experience, therefore we include a speak
key in our evaluations.
An evaluation of word prediction must address
these issues, if only implicitly. The effect of these
potentially implicit decisions on keystroke savings
can make comparison of results difficult. However,
if results are presented in reference to a gold stan-
dard under the sameassumptions, wecan draw more
reliable conclusions from results.
3 Towards
a Gold Standard
In trying to improve the state of word prediction,
several researchers have noted that it seems ex-
tremely difficult to improve keystroke savings be-
yond a certain point. Copestake (1997) discussed
the entropy of English to conclude that 5060%
keystroke savings may be the most we can expect
in practice. Lesher et al. (2002) replaced the lan-
guage model in a word prediction system with a
human to try and estimate the limit of keystroke
savings. They found that humans could achieve
59% keystroke savings with access to their ad-
vanced language model and that their advanced lan-
guage model alone achieved 54% keystroke savings.
They noted that one subject achieved nearly 70%
keystroke savings on one particular text, and con-
cluded that further improvements on current meth-
ods are possible. Garay-Vitoria and Abascal (2006)
survey many prediction systems, showing a wide
spectrum of savings, but no system offers more than
70% keystroke savings.
We investigated the problem of the limitations
of keystroke savings first from a theoretical per-
spective, seeking a clearly defined upper boundary.
Keystrokesavingscanneverreach100%itwould
mean that the system divined the entire text they in-
tended without a single key.
3.1 Theoretical
keystroke savings limit
The minimum amount of input required corresponds
to a perfect system  one that predicts every word
as soon as possible. In a word completion sys-
tem, the predictions are delayed until after the first
character of the word is entered. In such a sys-
tem, the minimum amount of input using a perfect
language model is two keystrokes per word  one
for the first letter and one to select the prediction.
The system would also require one keystroke per
sentence. In a word prediction system, the predic-
tions are available immediately, so the minimal in-
262
put for a perfect system is one keystroke per word
(to select the prediction) and one keystroke per sen-
tence. Weaddedtheabilitytomeasuretheminimum
number of keystrokes and maximum savings to our
simulation software, which we call the theoretical
keystroke savings limit.
We evaluated a baseline trigram model under two
conditions with different keystroke requirements on
the Switchboard corpus. The simulation software
was modified to output the theoretical limit in ad-
dition to actual keystroke savings at various window
sizes. To demonstrate the effect of the theoretical
keystroke savings limit on actual savings, we eval-
uated the trigram model under conditions with two
different limits  word prediction and word com-
pletion. The evaluation of the trigram model using
word completion is shown in Figure 1. The actual
keystroke savings is graphed by window size in ref-
erence to the theoretical limit. As noted by other re-
searchers, keystroke savings increases with window
size, but with diminishing returns (this is the effect
of placing the most probable words first). One of
0%
10%
20%
30%
40%
50%
60%
1 2 3 4 5 6 7 8 9 10
Keystroke savings
Window size
Word completion
Theoretical limit
Figure 1: Keystroke savings and the limit vs. window
size for word completion.
the problems with word completion is that the the-
oretical limit is so close to actual performance 
around 58.5% keystroke savings compared to 50.8%
keystroke savings with five predictions. At only five
predictions, the system has already realized 87% of
the possible keystroke savings. Under these circum-
stances, it would take a drastic change in the lan-
guage model to impact keystroke savings.
We repeated this analysis for word prediction,
showninFigure2alongsidewordcompletion. Word
prediction is much higher than completion, both the-
oretically (the limit) and in actual keystroke savings.
0%
10%
20%
30%
40%
50%
60%
70%
80%
1 2 3 4 5 6 7 8 9 10
Keystroke savings
Window size
Word prediction
Word prediction limit
Word completion
Word completion limit
Figure 2: Keystroke savings and the limit vs. window
size for word prediction compared to word completion.
Word prediction offers much more headroom in
terms of improvements in keystroke savings. There-
fore our ongoing research will focus on word pre-
diction over word completion.
This analysis demonstrates a limit to keystroke
savings, but this limit is slightly different than
Copestake (1997) and Lesher et al. (2002) seek to
describe  beyond the limitations of the user in-
terface, there seems to be a limitation on the pre-
dictability of English. Ideally, we would like to have
a gold standard that is a closer estimate of an ideal
language model.
3.2 Vocabulary
limit
We can derive a more practical limit by simulating
word prediction using a perfect model of all words
that occur in the training data. This gold standard
will predict the correct word immediately so long as
it occurs in the training corpus. Words that never oc-
curred in training require letter-by-letter entry. We
call this measure the vocabulary limit and apply it to
evaluatewhetherthedifferencebetweentrainingand
testing vocabulary is significant. Previous research
has focused on the percentage of out-of-vocabulary
(OOV) terms to explain changes in keystroke sav-
ings (Trnka and McCoy, 2007; Wandmacher and
Antoine, 2006). In contrast, the vocabulary limit
gives more guidance for research by translating the
problem of OOVs into keystroke savings.
Expanding the results from the theoretical limit,
the vocabulary limit is 77.6% savings, compared to
78.4% savings for the theoretical limit and 58.7%
actual keystroke savings with 5 predictions. The
practical limit is very close to the theoretical limit
263
in the case of Switchboard. Therefore, the remain-
ing gap between the practical limit and actual per-
formance must be due to other differences between
testing and training data, limitations of the model,
and limitations of language modeling.
3.3 Application
to corpus studies
Weappliedthegoldstandardstoourcorpusstudy, in
which a trigram model was individually trained and
tested on several different corpora (Trnka and Mc-
Coy, 2007). In contrast to the actual trigram model
Corpus Trigram Vocab.
limit
Theor.
limit
AAC Email 48.92% 61.94% 84.83%
Callhome 43.76% 54.62% 81.38%
Charlotte 48.30% 65.69% 83.74%
SBCSAE 42.30% 60.81% 79.86%
Micase 49.00% 69.18% 84.08%
Switchboard 60.35% 80.33% 82.57%
Slate 53.13% 81.61% 85.88%
Table 1: A trigram model compared to the limits.
performance, the theoretical limits all fall within a
relatively narrow range, suggesting that the achiev-
able keystroke savings may be similar even across
different domains. The more technical and formal
corpora (Micase, Slate, AAC) show higher limits, as
the theoretical limit is based on the length of words
and sentences in each corpus. The practical limit
exhibits much greater variation. Unlike the Switch-
board analysis, many other corpora have a substan-
tial gap between the theoretical and practical limits.
Although the practical measure seems to match the
actual savings similarly to OOVs testing with cross-
validation (Trnka and McCoy, 2007), this measure
more concretely illustrates the effect of OOVs on
actual keystroke savings  60% keystroke savings
when training and testing on AAC Email would be
extraordinary.
4 Conclusions
Although keystroke savings is the predominant eval-
uation for word prediction, this evaluation is not
straightforward, exacerbating the problem of inter-
preting and comparing results. We have presented
a novel solution  interpreting results alongside
gold standards which capture the difficulty of the
evaluation. These gold standards are also applica-
ble to drive future research  if actual performance
is very close to the theoretical limit, then relaxing
the minimum keystroke requirements should be the
most beneficial (e.g., multi-word prediction). Sim-
ilarly, if actual performance is very close to the
vocabulary limit, then the vocabulary of the lan-
guage model must be improved (e.g., cache mod-
eling, adding general-purpose training data). In the
case that keystroke savings is far from either limit,
then research into improving the language model is
likely to be the most beneficial.
Acknowledgments
This work was supported by US Department of Ed-
ucation grant H113G040051.

References

Alice Carlberger, John Carlberger, Tina Magnuson,
M. Sharon Hunnicutt, Sira Palazuelos-Cagigas, and
Santiago Aguilera Navarro. 1997. Profet, a new gen-
eration of word prediction: An evaluation study. In
ACL-97 workshop on Natural Language Processing
for Communication Aids.

Ann Copestake. 1997. Augmented and alternative NLP
techniques for augmentative and alternative commu-
nication. In ACL-97 workshop on Natural Language
Processing for Communication Aids, pages 3742.

Nestor Garay-Vitoria and Julio Abascal. 2006. Text pre-
diction systems: a survey. Univ Access Inf Soc, 4:183
203.

Gregory W. Lesher, Bryan J. Moulton, D Jeffery Higgin-
botham, and Brenna Alsofrom. 2002. Limits of hu-
man word prediction performance. In CSUN.

Jianhua Li and Graeme Hirst. 2005. Semantic knowl-
edge in word completion. In ASSETS, pages 121128.

Alan Newell,Stefan Langer,and Marianne Hickey. 1998.
The role of natural language processing in alternative
and augmentative communication. Natural Language
Engineering, 4(1):116.

Keith Trnka and Kathleen F. McCoy. 2007. Corpus Stud-
ies in Word Prediction. In ASSETS, pages 195202.
Keith Trnka, Debra Yarrington, John McCaw, Kathleen F.
McCoy, and Christopher Pennington. 2007. The Ef-
fects of Word Prediction on Communication Rate for
AAC. In NAACL-HLT; Companion Volume: Short Papers, pages 173176.

Tonio Wandmacher and Jean-Yves Antoine. 2006.
Training Language Models without Appropriate Lan-
guage Resources: Experiments with an AAC System
for Disabled People. In Eurospeech.
Proceedings of the ACL-08: HLT Student Research Workshop (Companion Volume), pages 6166,
Columbus, June 2008. c2008 Association for Computational Linguistics
Adaptive Language Modeling for Word Prediction
Keith Trnka
University of Delaware
Newark, DE 19716
trnka@cis.udel.edu
Abstract
We present the development and tuning of a
topic-adapted language model for word pre-
diction, which improves keystroke savings
over a comparable baseline. We outline our
plans to develop and integrate style adap-
tations, building on our experience in topic
modeling to dynamically tune the model to
both topically and stylistically relevant texts.
1 Introduction
PeoplewhouseAugmentativeandAlternativeCom-
munication (AAC) devices communicate slowly, of-
ten below 10 words per minute (wpm) compared to
150 wpm or higher for speech (Newell et al., 1998).
AAC devices are highly specialized keyboards with
speech synthesis, typically providing single-button
input for common words or phrases, but requiring a
user to type letter-by-letter for other words, called
fringe vocabulary. Many commercial systems (e.g.,
PRCs ECO) and researchers (Li and Hirst, 2005;
Trnka et al., 2006; Wandmacher and Antoine, 2007;
Matiasek and Baroni, 2003) have leveraged word
prediction to help speed AAC communication rate.
While the user is typing an utterance letter-by-letter,
the system continuously provides potential comple-
tions of the current word to the user, which the user
may select. The list of predicted words is generated
using a language model.
At best, modern devices utilize a trigram model
and very basic recency promotion. However, one of
the lamented weaknesses of ngram models is their
sensitivity to the training data. They require sub-
stantialtrainingdatato beaccurate,andincreasingly
more data as more of the context is utilized. For ex-
ample, Lesher et al. (1999) demonstrate that bigram
and trigram models for word predictionare not satu-
rated even when trained on 3 million words, in con-
trast to a unigram model. In addition to the prob-
lem of needing substantial amounts of training text
to build a reasonable model, ngrams are sensitive
to the difference between training and testing/user
texts. An ngram model trained on text of a differ-
ent topic and/orstyle may performvery poorlycom-
pared to a model trained and tested on similar text.
Trnka and McCoy (2007) and Wandmacher and An-
toine(2006)have demonstratedthe domainsensitiv-
ity of ngram models for word prediction.
The problem of utilizing ngram models for con-
versational AAC usage is that no substantial cor-
pora of AAC text are available (much less conver-
sational AAC text). The most similar available cor-
pora are spoken language, but are typically much
smaller than written corpora. The problem of cor-
pora for AAC is that similarity and availability are
inversely related, illustrated in Figure 1. At one ex-
treme,a verylargeamountofformalwrittenEnglish
is available, however, it is very dissimilar from con-
versational AAC text, making it less useful for word
prediction. At the other extreme, logged text from
the current conversation of the AAC user is the most
highly related text, but it is extremely sparse. While
this trend is demonstratedwith a variety of language
modeling applications, the problem is more severe
for AAC due to the extremely limited availability of
AAC text. Even if we train our models on both a
large number of general texts in addition to highly
related in-domain texts to address the problem, we
61
Figure 1: The most relevant text available is often the smallest, while the largest corpora are often the least relevant
for AAC word prediction. This problem is exaggerated for AAC.
must focus the models on the most relevant texts.
We addressthe problemof balancingtrainingsize
and similarityby dynamicallyadaptingthe language
model to the most topically relevant portions of the
training data. We present the results of experiment-
ing with different topic segmentationsand relevance
scores in order to tune existing methods to topic
modeling. Our approach is designed to seamlessly
degrade to the baseline model when no relevant top-
ics are found, by interpolatingfrequenciesas well as
ensuringthatalltrainingdocumentscontributesome
non-zero probabilities to the model. We also out-
line our plans to adapt ngram models to the style of
discourse and then combine the topical and stylistic
adaptations.
1.1 Evaluating
Word Prediction
Word prediction is evaluated in terms of keystroke
savings  the percentage of keystrokes saved by
taking full advantage of the predictions compared to
letter-by-letter entry.
KS = keysletter-by-letter keyswith predictionkeys
letter-by-letter
100%
Keystroke savings is typically measured automati-
callyby simulatinga usertypingthe testingdataof a
corpus, where any prediction is selected with a sin-
gle keystroke and a space is automatically entered
after selecting a prediction. The results are depen-
dent on the quality of the language model as well as
the number of words in the prediction window. We
focus on 5-word prediction windows. Many com-
mercialdevicesprovideoptimizedinputforthemost
common words (called core vocabulary) and offer
word prediction for all other words (fringe vocabu-
lary). Therefore, we limit our evaluation to fringe
words only, based on a core vocabulary list from
conversations of young adults.
WefocusourtrainingandtestingonSwitchboard,
which we feel is similar to conversational AAC text.
Our overall evaluation varies the training data from
Switchboard training to training on out-of-domain
datato estimatethe effectsof topicmodelingin real-
world usage.
2 Topic
Modeling
Topic models are language models that dynamically
adapt to testing data, focusing on the most related
topics in the training data. It can be viewed as a
two stage process: 1) identifying the relevant topics
by scoring and 2) tuning the language model based
on relevant topics. Various other implementations
of topic adaptation have been successful in word
prediction (Li and Hirst, 2005; Wandmacher and
Antoine, 2007) and speech recognition (Bellegarda,
2000; Mahajan et al., 1999; Seymore and Rosen-
feld, 1997). The main difference of the topic mod-
eling approach compared to Latent Semantic Anal-
ysis (LSA) models (Bellegarda, 2000) and trigger
pair models (Lau et al., 1993; Matiasek and Baroni,
2003) is that topic models perform the majority of
generalizationabouttopicrelatednessat testingtime
rather than training time, which potentially allows
user text to be added to the training data seamlessly.
Topic modeling follows the framework below
Ptopic(w | h) =
summationdisplay
ttopics
P(t | h)P(w | h,t)
where w is the word being predicted/estimated, h
represents all of the document seen so far, and t rep-
resents a single topic. The linear combination for
topic modeling shows the three main areas of vari-
ation in topic modeling. The posterior probability,
62
P(w | h,t) represents the sort of model we have;
how topic will affect the adapted language model in
theend. Theprior,P(t | h),representsthewaytopic
is identified. Finally, the meaning of t  topics, re-
quires explanation  what is a topic?
2.1 Posterior
Probability  Topic Application
The topic modeling approach complicates the esti-
mation of probabilities from a corpus because the
additional conditioning information in the posterior
probability P(w | h,t) worsens the data sparseness
problem. This section will present our experience in
lessening the data sparseness problem in the poste-
rior, using examples on trigram models.
The posterior probability requires more data
thanatypicalngrammodel,potentiallycausingdata
sparseness problems. We have explored the pos-
sibility of estimating it by geometrically combin-
ing a topic-adapted unigram model (i.e., P(w | t))
with a context-adapted trigram model (i.e., P(w |
w1,w2)), compared to straightforward measure-
ment (P(w | w1,w2,t)). Although the first
approach avoids the additional data sparseness, it
makes an assumption that the topic of discourse
onlyaffectsthevocabularyusage. Bellegarda(2000)
usedthis approachfor LSA-adaptedmodeling,how-
ever, we found this approach to be inferior to di-
rect estimation of the posterior probability for word
prediction (Trnka et al., 2006). Part of the reason
for the lesser benefit is that the overall model is
only affected slightly by topic adaptations due to
the tuned exponential weight of 0.05 on the topic-
adapted unigram model. We extended previous re-
search by forcing trigram predictions to occur over
bigrams and so on (rather than backoff) and using
the topic-adapted model for re-ranking within each
set of predictions, but found that the forced ordering
of the ngram components was overly detrimental to
keystroke savings.
Backoff models for topic modeling can be con-
structed either before or after the linear interpola-
tion. If the backoff is performed after interpolation,
we must also choose whether smoothing (a prereq-
uisite for backoff) is performed before or after the
interpolation. If we smooth before the interpolation,
then the frequencies will be overly discounted, be-
cause the smoothing method is operating on a small
fraction of the training data, which will reduce the
benefit of higher-order ngrams in the overall model.
Also, if we combine probability distributions from
each topic, the combination approach may have dif-
ficulties with topics of varying size. We address
these issues by instead combining frequencies and
performing smoothing and backoff after the combi-
nation, similar to Adda et al. (1999), although they
used corpus-sized topics. The advantage of this ap-
proach is that the held-out probability for each dis-
tribution is appropriatefor the training data, because
the smoothing takes place knowing the number of
words that occurred in the whole corpus, rather than
for each small segment. This is especially important
when dealing with small and different sized topics.
The linear interpolation affects smoothing
methodsnegatively  because the weights are less
than one, the combination decreases the total sum
of each conditional distribution. This will cause
smoothing methods to underestimate the reliability
of the models, because smoothing methods estimate
the reliability of a distribution based on the absolute
number of occurrences. To correct this, after inter-
polating the frequencies we found it useful to scale
the distribution back to its original sum. The scal-
ing approach improved keystroke savings by 0.2%
0.4% for window size 210 and decreased savings
by 0.1% for window size 1. Becausemost AAC sys-
tems provide 57 predictions, we use this approach.
Also, because some smoothing methods operate on
frequencies, but the combination model produces
real-valued weights for each word, we found it nec-
essarytobucketthecombinedfrequenciestoconvert
them to integers.
Finally, we required an efficient smoothing
method that could discount each conditional distri-
bution individually to facilitate on-demand smooth-
ing for each conditional distribution, in contrast to
a method like Katz backoff (Katz, 1987) which
smoothes an entire ngram model at once. Also,
Good-Turing smoothingproved too cumbersome,as
wewereunabletorelyontheratiobetweenwordsin
given bins and also unable to reliably apply regres-
sion. Instead, we used an approximation of Good-
Turing smoothing that performed similarly, but al-
lowed for substantial optimization.
63
2.2 Prior
Probability  Topic Identification
Thetopicmodelingapproachusesthecurrenttesting
document to tune the language model to the most
relevant training data. The benefit of adaptation is
dependentonthequalityofthesimilarityscores. We
will first present our representation of the current
document, which is compared to unigram models of
each topic using a similarityfunction. We determine
the weight of each word in the current document us-
ing frequency, recency, and topical salience.
The recency of use of a word contributes to the
relevance of the word. If a word was used somewhat
recently, we would expect to see the word again. We
follow Bellegarda (2000) in using an exponentially
decayed cache with weight of 0.95 to model this ef-
fect of recency on importance at the current position
in the document. The weight of 0.95 represents a
preservation in topic, but with a decay for very stale
words, whereas a weight of 1 turns the exponen-
tial model into a pure frequency model and lower
weights represent quick shifts in topic.
The importance of each word occurrence in the
current document is a factor of not just its frequency
and recency, but also its topical salience  how
well the word discriminatesbetween topics. For this
reason, we decided to use a technique like Inverse
Document Frequency (IDF) to boost the weight of
words that occur in only a few documents and de-
press the weights of words that occur in most docu-
ments. However, instead of using IDF to measure
topical salience, we use Inverse Topic Frequency
(ITF), which is more specifically tailored to topic
modeling and the particular kinds of topics used.
We evaluated several similarity functions for
topic modeling, initially using the cosine measure
for similarity scoring and scaling the scores to be
a probability distribution, following Florian and
Yarowsky (1999). The intuition behind the co-
sine measure is that the similarity between two dis-
tributions of words should be independent of the
length of either document. However, researchers
have demonstrated that cosine is not the best rele-
vance metric for other applications, so we evaluated
two other topical similarity scores: Jacquards coef-
ficient, which performed better than most other sim-
ilarity measures in a different task for Lee (1999)
and Nave Bayes, which gave better results than co-
sine in topic-adapted language models for Seymore
and Rosenfeld (1997). We evaluated all three simi-
larity metrics using Switchboard topics as the train-
ing data and each of our corpora for testing us-
ing cross-validation. We found that cosine is con-
sistently better than both Jacquards coefficient and
Nave Bayes, across all corpora tested. The differ-
ences between cosine and the other methods are sta-
tisticallysignificantatp < 0.001. Itmaybepossible
that the ITF or recency weighting in the cache had a
negative interaction with Nave Bayes; traditionally
raw frequencies are used.
We found it useful to polarize the similarity
scores, following Florian and Yarowsky (1999),
who found that transformations on cosine similarity
reduced perplexity. We scaled the scores such that
the maximumscore was one and the minimumscore
was zero, which improved keystroke savings some-
what. This helps fine-tune topic modelingby further
boosting the weights of the most relevant topics and
depressing the weights of the less relevant topics.
Smoothing the scores helps prevent some scores
from being zero due to lack of word overlap. One of
themotivationsbehindusingalinearinterpolationof
all topics is that the resulting ngram model will have
the same coverage of ngrams as a model that isnt
adapted by topic. However, the similarity score will
be zero when no words overlap between the topic
and history. Therefore we decided to experiment
with similarity score smoothing, which records the
minimum nonzero score and then adds a fraction of
that score to all scores, then only apply upscaling,
where the maximumis scaled to 1, but the minimum
is not scaled to zero. In pilot experiments, we found
that smoothing the scores did not affect topic mod-
eling with traditional topic clusters, but gave minor
improvements when documentswere used as topics.
Stemming is another alternative to improving the
similarity scoring. This helps to reduce problems
with data sparseness by treating different forms of
the same word as topically equivalent. We found
that stemming the cache representations was very
useful when documentswere treated as topics (0.2%
increaseacrosswindow sizes), but detrimentalwhen
larger topics were used (0.10.2% decrease across
window sizes). Therefore, we only use stemming
when documents are treated as topics.
64
2.3 Whats
in a Topic  Topic Granularity
We adapt a language model to the most relevant top-
ics in training text. But what is a topic? Tradition-
ally, document clusters are used for topics, where
some researchers use hand-crafted clusters (Trnka
et al., 2006; Lesher and Rinkus, 2001) and oth-
ers use automatic clustering (Florian and Yarowsky,
1999). However, other researchers such as Mahajan
et al. (1999) have used each individual document as
a topic. On the other end of the spectrum, we can
use whole corpora as topics when training on mul-
tiple corpora. We call this spectrum of topic defini-
tionstopicgranularity, wheremanualandautomatic
document clusters are called medium-grained topic
modeling. When topics are individual documents,
wecalltheapproachfine-grained topicmodeling. In
fine-grained modeling, topics are very specific, such
as seasonal clothing in the workplace, compared to
a medium topic for clothing. When topics are whole
corpora, we call the approach coarse-grained topic
modeling. Coarse-grained topics model much more
high-level topics, such as research or news.
The results of testing on Switchboard across dif-
ferent topic granularities are showin in Table 1. The
in-domain test is trained on Switchboard only. Out-
of-domain training is performed using all other cor-
pora in our collection (a mix of spoken and writ-
ten language). Mixed-domain training combines the
two data sets. Medium-grained topics are only pre-
sented for in-domain training, as human-annotated
topics were only available for Switchboard. Stem-
ming was used for fine-grained topics, but similarity
score smoothing was not used due to lack of time.
The topic granularity experiment confirms our
earlier findings that topic modeling can significantly
improve keystroke savings. However, the variation
of granularity shows that the size of the topics has
a strong effect on keystroke savings. Human anno-
tatedtopicsgive thebestresults,thoughfine-grained
topicmodelinggivessimilarresultswithouttheneed
for annotation, making it applicable to training on
not just Switchboard but other corpora as well. The
coarse grained topic approach seems to be limited
to finding acceptable interpolation weights between
very similar and very dissimilar data, but is poor at
selectingthe mostrelevant corporafroma collection
of very different corpora in the out-of-domain test.
Another problem may be that many of the corpora
are only homogeneous in style but not topic. We
would like to extend our work in topic granularity to
testing on other corpora in the future.
3 Future
Work  Style and Combination
Topic modeling balances the similarity of the train-
ing data against the size by tuning a large training
set to the most topically relevant portions. However,
keystroke savings is not only affected by the topical
similarity of the training data, but also the stylistic
similarity. Therefore, we plan to also adapt models
to the style of text. Our success in adapting to the
topic of conversation leads us to believe that a sim-
ilar process may be applicable to style modeling 
splitting the model into style identification and style
application. Because we are primarily interested in
syntactic style, we will focus on part of speech as
the mechanism for realizing grammatical style. As
a pilot experiment, we compared a collection of our
technical writings on word prediction with a collec-
tion of our research emails on word prediction, find-
ing that we could observe traditional trends in the
POS ngram distributions (e.g., more pronouns and
phrasal verbs in emails). Therefore, we expect that
distributional similarity of POS tags will be useful
for style identification. We envision a single style s
affecting the likelihood of each part of speech p in a
POS ngram model like the one below:
P(w | w1,w2,s) =summationdisplay
pPOS(w)
P(p | p1,p2,s)P(w | p)
In this reformulation of a POS ngram model, the
prior is conditioned on the style and the previous
couple tags. We will use the overall framework to
combine style identification and modeling:
Pstyle(w | h) =
summationdisplay
sstyles
P(s | h)P(w | w1,w2,s)
The topical and stylistic adaptations can be com-
bined by adding topic modeling into the style model
shown above. The POS posterior probability P(w |
p) can be additionally conditioned on the topic of
discourse. Topic identification and the topic sum-
mation would be implemented consistently with the
standalone topic model. Also, the POS framework
65
Model type In-domain Out-of-domain Mixed-domain
Trigram baseline 60.35% 53.88% 59.80%
Switchboard topics (medium grained) 61.48% (+1.12%)  
Document as topic (fine grained) 61.42% (+1.07%) 54.90% (+1.02%) 61.17% (+1.37%)
Corpus as topic (coarse grained)  52.63% (-1.25%) 60.62% (+0.82%)
Table 1: Keystroke savings across different granularity topics and training domains, tested on Switchboard. Improve-
ment over baseline is shown in parentheses. All differences from baseline are significant at p < 0.001
facilitates cache modeling in the posterior, allowing
direct adaptation to the current text, but with less
sparseness than other context-aware models.
4 Conclusions
Wehavecreatedatopicadaptedlanguagemodelthat
utilizesthefulltrainingdata,butwithfocusedtuning
on the most relevant portions. The inclusion of all
the training data as well as the usage of frequencies
addresses the problem of sparse data in an adaptive
model. We have demonstrated that topic modeling
can significantly increase keystroke savings for tra-
ditional testing as well as testing on text from other
domains. We have also addressed the problem of
annotated topics through fine-grained modeling and
found that it is also a significantimprovement over a
baseline ngram model. We plan to extend this work
to build models that adapt to both topic and style.
Acknowledgments
This work was supported by US Department of Ed-
ucation grant H113G040051. I would like to thank
my advisor, Kathy McCoy, for her help as well as
the many excellent and thorough reviewers.

References

Gilles Adda, Mich`ele Jardino, and Jean-Luc Gauvain.
1999. Language modeling for broadcast news tran-
scription. In Eurospeech, pages 17591762.

Jerome R. Bellegarda. 2000. Large vocabulary
speech recognition with multispan language models.
IEEE Transactions on Speech and Audio Processing,
8(1):7684.

Radu Florian and David Yarowsky. 1999. Dynamic
Nonlocal Language Modeling via Hierarchical Topic-
Based Adaptation. In ACL, pages 167174.

Slava M. Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE Transactions on Acoustics
Speech and Signal Processing, 35(3):400401.

R. Lau, R. Rosenfeld, and S. Roukos. 1993. Trigger-
based language models: a maximum entropy ap-
proach. In ICASSP, volume 2, pages 4548.
Lillian Lee. 1999. Measures of distributional similarity.
In ACL, pages 2532.

Gregory Lesher and Gerard Rinkus. 2001. Domain-
specific word prediction for augmentative communication. In RESNA, pages 6163.

Gregory W. Lesher, Bryan J. Moulton, and D. Jeffery
Higgonbotham. 1999. Effects of ngram order and
trainingtextsizeonwordprediction. InRESNA, pages
5254.

Jianhua Li and Graeme Hirst. 2005. Semantic knowl-
edge in word completion. In ASSETS, pages 121128.

Milind Mahajan, Doug Beeferman, and X. D. Huang.
1999. Improved topic-dependent language modeling
using information retrieval techniques. In ICASSP,
volume 1, pages 541544.

Johannes Matiasek and Marco Baroni. 2003. Exploiting
long distance collocational relations in predictive typ-
ing. InEACL-03WorkshoponLanguage Modeling for
Text Entry, pages 18.

Alan Newell,Stefan Langer,and Marianne Hickey. 1998.
The role of natural language processing in alternative
and augmentative communication. Natural Language
Engineering, 4(1):116.

Kristie Seymore and Ronald Rosenfeld. 1997. Using
Story Topics for Language Model Adaptation. In Eu-
rospeech, pages 19871990.
Keith Trnka and KathleenF.McCoy. 2007. Corpus Studies in Word Prediction. In ASSETS, pages 195202.

Keith Trnka, Debra Yarrington, Kathleen McCoy, and
Christopher Pennington. 2006. Topic Modeling in
Fringe Word Prediction for AAC. In IUI, pages 276278.

Tonio Wandmacher and Jean-Yves Antoine. 2006.
Training Language Models without Appropriate Language Resources: Experiments with an AAC System
for Disabled People. In LREC.

T. Wandmacher and J.Y. Antoine. 2007. Methods to integrate a language model with semantic information
for a word prediction component. In EMNLP, pages 506513
! META-COMPILING TEXT GRAMMARS AS A MODEL FOR HUMAN BEHAVIOR Sheldon Klein Computer Sciences Department University of Wisconsin I.
BACKGROUND Folktale (Propp 1968) which generated 50 Russian fairytales, according to the r u l e s of his text grammar, at an average speed of 128 words a second, again including plot computations and specification of deep structure as well as surface syntax (Klein et al 1974, Klein et all 1975).
Our earliest automatic text generation work used syntactic dependency network/graphs with 2-valued labelling of edges as an approximation to semantic network/graphs with multi-valued labelling of edges (Klein & Simmons 1963, Klein 1965a, 1965b).
Our work on automatic inference of grammars includes the world's first program for learning context free, phrase structure grammars, for both natural and artificial languages, and the first program for learning transformational grammars (Klein 1967, Klein et al 1968, Klein & Kuppin 1970).
More recent inference work includes the formulation of techniques for automatic inference of generative semantic grammars (Klein 1973) and for the ontogeny of Pidgin and Creole languages (Klein & Rozencvejg 1974).
In formulating components for automatic inference of rules in the meta-symbolic simulation system, we find that the common notation for the semantics of the non-verbal behavioral simulation r u l e s and natural language means that the same learning heuristics may be used to infer behavioral rules as well as linguistic rules.
The implication is that the totality of human verbal and non-verbal behavior, in complex social groups, both synchronically and diachronically, may now be modelled within the same notational framework.
What for us started as a generalized device for testing varying theoretical models as part of an effort to model language change and variation (Klein 1974a, Klein & Rozencvejg 1974) now appears as the basis for a higher level theory of the linguistic basis of human behavior (Klein 1974b).
 i i I  In our efforts to model the totality of synchronic and diachronic language behavior in complex social groups, we developed a meta-symbolic simulation system that includes a powerful behavioral simulation programming language that models, generates and manipulates events in the notation of a semantic network that changes through time, and a generalized, semantics-to-surface structure generation mechanism that can describe changes in the semantic universe in the syntax of any natural language for which a grammar is supplied.
Because the system is a meta-theoretical device, it can handle generative semantic grammars formulated within a variety of theoretical frameworks.
A key feature of the system is that the semantic deep structure of the non-verbal, behavioral rules may be represented in the same network notation as the semantics for natural language grammars, and, as a consequence, provide non-verbal context for linguistic rules.
We are also experimenting with a natural language meta-compiling capability, that is, the use of the semantic network to generate productions in the simulation language itself -productions in the form of "texts" that may themselves be compiled as new behavioral rules during the flow of the simulation -rules that may themselves control the process of deriving new rules.
This feature permits non-verbal behavioral rules to be derived from natural language conversational inputs, and through inference techniques identical with those for inferring natural language generative semantic grammars.
The total system has the power of at least the 2nd order predicate calculus, and will facilitate the formulation of highly abstract meta-models of discourse, including the logical quantification of such models.
Achievements with the generative portion of the system include a text grammar model that generates 2100 word murder mystery stories in less than 19 seconds each, complete with calculation of the plot and specification of the deep structure as well as the surface syntax (Klein et al 1973).
The speed of this generation is 100 to 1000 times faster than other existing programs using transformational grammars.
(The algorithm for the semantics-to-surface structure generative component is such that processing time increases only linearly as a f u n c t i o n of sentence length and syntactic complexity).
More recent achievements include models of portions of Levi-Strauss" mythology work in The Raw & the Cooked (Levi-Strauss 1969) and a model for Propp's Morphology of the I I I I II.
WHAT IS A TEXT GRAMMAR?
The text grammarian movement, centered in Germany and Holland, includes work such as that of van Dijk, Ihwe, Pet~fi and Rieser (1972), Pet6fi and Rieser (1973), Pet~fi (1973), van Dijk (1973), and van Dijk and Pet6fi (1974).
The underlying motivation of this group is the belief that Chomskian derived linguistic theories are inadequate to handle the complexities of complex narrative and discourse -that more powerful logical devices are needed.
An attempted refutation of the text grammarian position appeared in Dascal & Margalit (1974).
Our own work on Propp and Levi-Strauss models refutes the refutation by demonstration (Klein et al 1974).
To provide the reader with an intuitive view of the nature of a text grammar, we offer the following two Russian fairytales generated by our automated model of Propp (Klein et al 1974).
The same text grammar figenerated both stories from a structural model at a level of abstraction that provided a semantic unification of the a ppa r e n t surface diversity.
Tale THE BORISIEVICHES LIVE IN A DISTANCE PROVINCE.
THE FATHER IS EMELYA.
THE ONLY SON IS BORIS.
MARTHA IS THE ONLY DAUGHTER.
EMELYA HAS THE SHEEP.
BORIS, MARTHA AND THE SHEEP ARE IN THE WOODS.
BORIS SAYS MARTHA, DO NOT LEAVE THE WOODS.
BORIS LEAVES TO GO BERRY GATHERING.
MARTHA LEAVES THE WOODS.
A WOLF APPEARS IN THE D I S T A N T PROVINCE.
EMELYA ASKS THE WOLF WHERE IS YOUR WISDOM.
THE WOLF SAYS THAT MY WISDOM IS IN A MAGIC EGG.
THE WOLF P L U N D E R S THE SHEEP.
EMELYA SENDS MARTHA TO SEARCH FOR THE WOLF.
MARTHA DECIDES TO SEARCH FOR THE WOLF.
MARTHA LEAVES ON A SEARCH.
MARTHA MEETS A WITCH ALONG THE WAY.
THE WITCH P R O P O S E S THAT MARTHA LISTEN TO THE GUSLA W I T H O U T FALLING ASLEEP.
MARTHA RESPONDS BY STAYING AWAKE WHILE L I S T E N I N G TO THE GUSLA.
A MAGIC WAFER IS C O N S U M E D BY MARTHA.
M A R T H A O B T A I N S S U P E R H U M A N STRENGTH.
MARTHA T R A V E L S TO THE L O C A T I O N OF THE WOLF IN ANOTHER KINGDOM.
MARTHA IS D I R E C T E D BY A HEDGEHOG.
MARTHA FINDS THE WOLF THEY FIGHT IN AN OPEN FIELD.
MARTHA IS WOUNDED.
MARTHA D E F E A T S THE WOLF WITH THE AID OF S U P E R H U M A N STRENGTH.
THE WOLF IS CAUGHT BY MARTHA.
MARTHA STARTS BACK HOME.
MARTHA RETURNS HOME.
THE MAGIC BOW UNPROTECTED.
THE MAGIC BOW, A MAGIC CARPET AND A MAGIC BOX ARE SEIZED BY NICHOLAS.
NICHOLAS TRAVELS TO THE LOCATION OF THE MAGIC STEED IN ANOTHER KINGDOM.
N I C H O L A S BY THE MAGIC CARPET.
N I C H O L A S FINDS THE BEAR.
N I C H O L A S S U R P R I S E S THE BEAR.
N I C H O L A S KILLS THE BEAR WITH THE AID OF THE MAGIC BOW.
THE MAGIC STEED A P P E A R S FROM THE MAGIC BOX.
N I C H O L A S STARTS BACK HOME.
THE B E A R ' S FATHER CHASES AFTER NICHOLAS.
N I C H O L A S E S C A P E S BY FLYING ON A FALCON.
N I C H O L A S RETURNS HOME.
III. THE KEY Q U E S T I O N We perceive the locus of theoretical interest to be the process of verbal and non-verbal behavior transmission across generations.
Our work on m o d e l l i n g speech c o m m u n i t i e s i n c l u d e s designs for s i m u l a t i o n s in which many modelled individuals, each with his own semantic network, his own grammar(s), his own b e h a v i o r rules, interact with each other a c c o r d i n g to the modelled rules of the social s t r u c t u r e of the s o c i e t y (Klein 1974a).
It is our hope to be able to model the t r a n s m i s s i o n process of all the rules in the system.
This means that newly born m o d e l l e d individuals will infer rules for natural language and also for n o n v e r b a l behavioral s i m u l a t i o n rules, as a function of inputs of texts supplied by other modelled individuals.
The texts may be verbal discourse, or non-verbal sequences of behavior.
The learning individual will a c t u a l l y c o m p i l e and r e c o m p i l e new versions of his own behavioral rules as the s i m u l a t i o n process proceeds.
His own test p r o d u c t i o n s of b e h a v i o r s c e n a r i o s as well as natural language d i s c o u r s e will be subject to evaluation and possible c o r r e c t i o n by o t h e r m e m b e r s of the m o d e l l e d community, and their r e a c t i o n s as well as the c o n s e q u e n c e s of the p r o d u c t i o n s, will serve as a control on the entire learning process.
And, as i n d i c a t e d earlier, the rules to be inferred, compiled and r e c o m p i l e d will include rules that govern the process of inference and c o m p i l a t i o n itself.
Tale THE M O R E V N A S LIVE IN A D I S T A N T PROVINCE.
THE FATHER IS EREMA.
THE MOTHER IS VASILISA.
THE O L D E S T SON IS BALDAK.
THE Y O U N G E R SON IS MARCO.
THE Y O U N G E S T SON IS BORIS.
THE O L D E S T D A U G H T E R IS MARIA.
THE Y O U N G E R D A U G H T E R IS KATRINA.
THE Y O U N G E S T D A U G H T E R IS MARTHA.
N I C H O L A S ALSO L I V E S IN THE SAME LAND.
N I C H O L A S IS OF M I R A C U L O U S BIRTH.
B A L D A K HAS A MAGIC STEED.
A BEAR A P P E A R S IN THE D I S T A N T PROVINCE.
THE BEAR S E I Z E S THE MAGIC STEED.
B A L D A K CALLS FOR HELP FROM NICHOLAS.
N I C H O L A S D E C I D E S TO SEARCH FOR THE MAGIC STEED.
N I C H O L A S LEAVES ON A SEARCH.
N I C H O L A S MEETS A JUG ALONG THE WAY.
THE JUG IS F I G H T I N G WITH ELENA OVER A MAGIC BOW.
THE JUG ASKS N I C H O L A S TO DIVIDE THE MAGIC BOW.
N I C H O L A S TRICKS THE D I S P U T A N T S INTO LEAVING 85 IV.
LOGICAL QUANTIFICATION, PARSING, P R E S U P P O S I T I O N A L A N A L Y S I S SEMANTIC We have mentioned the 2nd order or higher predicate calculus.
For our purposes, the e s s e n t i a l feature is that the logical quantification of the rules may be q u a n t i f i e d by the contents of the rules themselves.
Meta-compiling of rules g o v e r n i n g m e t a c o m p i l i n g is an example of this process.
There are other The b e h a v i o r a l rules classes that make it rules that can treat complex actions as techniques available.
operate with high-level possible to formulate objects, c h a r a c t e r s and manifestations of the fisame abstract semantic Unit.
A major type of b e h a v i o r rule m o d i f i c a t i o n and extension is the a b i l i t y to requantify the rules as a heuristic function of experience.
The process does not involve r e c o m p i l a t i o n -rather m o d i f i c a t i o n of the domain of a p p l i c a b i l i t y of an existing rule.
One of the types of semantic parsing possible in the system is the d e t e r m i n a t i o n of the presuppositions of the semantic content of input text.
The scenario rules that could have generated the text have preconditions, and these p r e c o n d i t i o n s also have their own p r e c o n d i t i o n s as s p e c i f i e d by other rules.
In cases where the semantic content of an input text is not potentially derivable from existing b e h a v i o r a l rules, the system can posit requantification (assignments and r e a s s i g n m e n t s to semantic classes) to make the input text derivable.
Or, if necessary, the same end can be a c h i e v e d by c o m p i l i n g new rules that would make the text plausible.
G e n e r a l i z a t i o n of the method makes it possible to build complex learning models for highly abstract, semantically driven text grammars.
Perhaps the u l t i m a t e test is the m o d e l l i n g of the h e u r i s t i c processes of Levi-Strauss.
We hope to be able to build a model that learns text grammars with arbitrarily a b s t r a c t semantics such as that m a n i f e s t e d in L e v i S t r a u s s (1969).
At the moment, we are w o r k i n g on m o d e l l i n g the text grammar he himself has derived (Klein et al 1975).
The potential of our work is to handle a degree and kind of abstraction in semantics heretofore untouched by linguistics, i n c l u d i n g the m o d e l l i n g of the automatic creation of text grammars for dreams and myths as a function of cultural rules.
GENERALITY OF THE META-SYMBOLIC S I M U L A T I O N SYSTEM AS A THEORY TESTING DEVICE V.
2. The theoretical foundations of Computer Science are identical with those of Linguistics.
3. T h e o r e t i c a l linguistic models that are not strongly linked to objective tasks are meaningless.
No semantics is m e a n i n g f u l except in terms of the o b j e c t i v e tasks it facilitates.
4. The future of Linguistics, Computational Linguistics, Artificial Intelligence, Psychological models of human behavior, are in the future of the Foundations of Programming Languages and the Theory of Operating Systems.
The human mind is at least as complicated as an operating system for a 4th g e n e r a t i o n computer.
5. An a d e q u a t e linguistic theory must account for the function of language in social groups and its transmission through time and space.
At the same time, such a theory must account for the highest semantic a t t a i n m e n t s of the human mind, i n c l u d i n g l i t e r a t u r e and art, and, in fact, the totality of symbolic processes.
6. I n p u t / o u t p u t e q u i v a l e n c e of model and modelled does not imply isomorphism between model and modelled.
(Chomskian beliefs to the c o n t r a r y have their roots in Leibniz" Theory of Monads and its required ontological argument).
There are no models of performance, only models of c o m p e t e n c e which can be compared, one against the other, for accuracy in predicting relations between input and output in real w o r l d systems.
Our m e t h o d o l o g y and programming style have y i e l d e d a system w h e r e i n all the rules, and even the form of the theories in which they are cast, are input as data.
As far as we can determine, this permits us to encode in our system v i r t u a l l y all the t h e o r e t i c a l models c u r r e n t l y p r e v a l e n t in linguistics, plus heretofore unformulated models of vastly greater power.
(Preliminary work in the classroom, for example, indicates that models of the work of Schank and his students may easily be i m p l e m e n t e d in our system, with an i n c r e a s e d speed of e x e c u t i o n of about 50 to I in favor of our versions.) VII.
THEORETICAL IMPLICATIONS of Human Mental A.
The Structures Non-inateness Our work c o n s t i t u t e s a refutation by counter example of the necessity for a c o r r e l a t i o n between models of human mental structures and the s t r u c t u r e of the human brain.
(A s o f t w a r e system can o p e r a t e with no inherent i s o m o r p h i s m s with a p a r t i c u l a r computer).
N o t h i n g need be innate except the meta-compiling capacity and the p e r c e p t i o n of time.
Our work suggests the logical p o s s i b i l i t y that the human mind can learn to learn, and learn how to learn to learn, and that each human may do it differently.
The basic principles of language inference, which can be derived from a b e h a v i o r i s t i c p s y c h o l o g i c a l framework, can alone account for the s t r u c t u r i n g of mental p r o c e s s e s as a software phenomenon, independent of physiological reality.
It follows that humans can have d i f f e r e n t rules, different data structures, different hierarchical 86 VI.
THE M E T H O D O L O G I C A L WORK S I G N I F I C A N C E OF OUR Our work over the years has suggested and r e i n f o r c e d the f o l l o w i n g m e t h o d o l o g i c a l principles: I.
No significant theories formulated in L i n g u i s t i c s not c o m p u t e d based.
can be that are fiorganizations, where the only controlling factor is the requirement that the i n t e r n a l i z e d models permit the individuals to function and interact with the inputs and outputs of other individuals in a social group.
B. History History as the Meta-language of des Linguistics.
Bucharest Implicit in our approach is an alternative to the concept of an infinite h i e r a r c h y of m e t a l a n g u a g e s, as formulated by B e r t r a n d Russell in his Theory of Types in Principia Mathematica (Whitehead & Russell 1911-1913).
The concept of successive states of time, each linked with the p o s s i b i l i t y of defining (meta-compiling) new rules of the universe for the next state, (including the rules for defining new rules), suggests that there need be only a single meta-language and a single language in any state at any point in time, and that each serves, in turn, as the m e t a l a n g u a g e for the other in successive time frames.
This is not a s t o c h a s t i c process.
It is the concepts of time and meta-compiling that appear to be the f u n d a m e n t a l aspects of human cognition.
The principle may be universal for all human b e h a v i o r a l / s y m b o l i c processes, and students of the p h i l o s o p h y of history will now recognize our meta-symbolic simulation system as equivalent to an automated Hegelian dialectic philosophy which specifies that each successive state of h i s t o r i c a l d e v e l o p m e n t is c o n t r o l l e d by the meta-language of its previous state, and becomes the m e t a l a n g u a g e of its successor state.
REFERENCES Klein, S.,. 1973.
Automatic inference of semantic deep structure rules ingenerative semantic Grammars.
Univ. if W i s c o n s i n Comp.
Sci. Tech Report 180.
Also in 1974.
Computational and Mathematical Linguistics, Proceedings of the Int.
Conf. on Computational Linguistics, Pisa, 1973.
A. Zampolli, ed., Florence: Olschki Klein, S., . 1974a.
C o m p u t e r simulation of language contact models.
In Towards Tomorrow's Linguistics, Shuy & Bailey, editors, Washington, D.C.: Georgetown University Press.
Klein, S., . 1974b.
A computer model for the linguistic basis of the t r a n s m i s s i o n of culture.
Presented at 1974 Meeting of American Anthropological Association, Mexico City, Nov.
1974. (Final draft in preparation) Klein, S., J.
F. Aeschlimann, D.
F. Balsiger, S.
L. Converse, C.
Court, M.
Foster, R.
Lao, J.
D. Oakley, and J.
Smith . 1973.
A U T O M A T I C NOVEL WRITING: a status report.
Univ. of Wisc.
Comp. Sci.
Dept. Tech Report 186.
Presented at 1973 Int.
Conf. on Computers in the Humanities.
Klein, S., J.
F. Aeschlimann, M.
A. Appelbaum, D.
F. Balsiger, E.
J. Curtis, M.
Foster, S.
D. Kalish, S.
J. Kamin, Y-D.
Lee, L.
A. Price, D.
F. Salsieder.
1974. M o d e l l i n g Propp and Levi-Strauss in a Meta-symbolic Simulation System.
Univ. of Wisc.
Comp. Sci.
Tech Report 226.
In press in Patterns in Oral Literature, edited by Heda Jason and Dimitri Segal as a retroactive contribution to this volume of the 1973 Wolrd Conference of Anthropological and Ethnological Sciences.
Chicago. Klein, S., W.
Fabens, R.
Herriot, W.
Katke, M.A.
Kuppin, A.
Towster . 1968.
The AUTOLING system.
Univ. of Wisc.
Comp. Sci.
Dept. Tech Report 43.
Klein, S.
and M.
A. Kuppin.
1970. An interactive, heuristic program for learning transformational grammars.
Computer Studies in the Humanities and Verbal Behavior, 3:144-162.
Klein, S., L.
A. Price, J.
F. Aeschlimann, D.
A. Balsiger and E.
J. Curtis.
1975. A Meta-symbolic Simulation Model for Five Myths from Levi-Strauss " The Raw and the Cooked.
Univ. of Wisc.
Comp. Sci.
Dept. Tech Report P r e s e n t e d at 2nd Int.
Conf. on Computers in the Humanities, Chicago, April 1975.
Klein, S.
and V.
Rozencvejg . 1974.
A Computer Model for the Ontogeny of Pidgin and Creole Languages, Univ.
of Wisc.
Comp. Sci.
Dept. Tech Report 238.
Presented at the 1975 Int.
Conf. on Pidgins and Creoles, Hawaii, January 1975.
87 Levi-Strauss, C . 1969, The Raw and the Cooked.
(English translation) New York: Harper & Row.
PetSfi, J.
S . 1973.
Toward an empirically motivated grammatical theory of verbal texts.
In Studies in Text Grammar, edited by J.S.
Petofi & H.
Rieser. Dordrecht: Reidel.
Pet~fi, J.
S. & H.
Rieser. 1973.
Probleme der modelltheoretischen Interpretation yon Texten.
Hamburg: Buske Verlag.
Propp, V . 1968.
Morphology of the Folkt~le (English translation) 2nd Edition, Austin: University of Texas Press.
Whitehead, A.
N. and B.
Russell. 1911-1913.
Princip~a Mathematica.
(3 volumes London: Cambridge University Press .
Limitations of Co-Training for Natural Language Learning from Large Datasets by David Pierce and Claire Cardie References S.
Argamon, I.
Dagan, and Y.
Krymolowski. 1999.
A memory-based approach to learning shallow natural language patterns.
Journal of Experimental and Theoretical Artificial Intelligence, 11(3).
Available as cmp-lg/9806011.
A. Blum and T.
Mitchell. 1998.
Combining labeled and unlabeled data with co-training.
In Proceedings of the 11th Annual Conference on Computational Learning Theory (COLT-98).
C. Cardie and D.
Pierce. 1998.
Error-driven pruning of treebank grammars for base noun phrase identification.
In Proceedings of the 36th Annual Meeting of the ACL and COLING-98, pages 218224.
Available as cmp-lg/9808015.
C. Cardie, V.
Ng, D.
Pierce, and C.
Buckley. 2000.
Examining the role of statistical and linguistic knowledge sources in a general-knowledge question answering system.
In Proceedings of the Sixth Applied Natural Language Processing Conference (ANLP-NAACL 2000), pages 180187.
K. Church.
1988. A stochastic parts programs and noun phrase parser for unrestricted text.
In Proceedings of the Second Conference on Applied Natural Language Processing, pages 136143.
D. Cohn, L.
Atlas, and R.
Ladner. 1994.
Improving generalization with active learning.
Machine Learning, 15(2):201221.
M. Collins and Y.
Singer. 1999.
Unsupervised models for named entity classification.
In Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-99).
A. Dempster, N.
Laird, and D.
Rubin. 1977.
Maximum likelihood from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society, Series B, 39(1):138.
Y. Freund and R.
Shapire. 1997.
A decisiontheoretic generalization of on-line learning and an application to boosting.
Journal of Computer and System Sciences, 55(1):119 139.
D. Lewis and J.
Catlett. 1994.
Heterogeneous uncertainty sampling for supervised learning.
In Proceedings of the Eleventh International Conference on Machine Learning, pages 148 156.
M.Marcus, M.
Marcinkiewicz, andB.
Santorini. 1993.
Building a large annotated corpus of English: The Penn Treebank.
Computational Linguistics, 19(2):313330.
M. Mitra, C.
Buckley, A.
Singhal, and C.
Cardie. 1997.
An analysis of statistical and syntactic phrases.
In 5TH RIAO Conference, Computer-Assisted Information Searching On the Internet, pages 200214.
I. Muslea, S.
Minton, and C.
Knoblock. 2000.
Selective sampling with redundant views.
In Proceedings of the Seventeenth National Conference on Artificial Intelligence, pages 621 626.
K. Nigam and R.
Ghani. 2000.
Analyzing the effectiveness and applicability of co-training.
In Ninth International Conference on Information and Knowledge Management (CIKM2000).
K. Nigam, A.
McCallum, S.
Thrun, and T.
Mitchell. 2000.
Text classification from labeled and unlabeled documents using EM.
Machine Learning, 39(2/3):103134.
L. Ramshaw and M.
Marcus. 1998.
Text chunking using transformation-based learning.
In Natural Language Processing Using Very Large Corpora.
Kluwer. Originally appeared in WVLC95.
E. Riloff and R.
Jones. 1999.
Learning dictionaries for information extraction by multilevel bootstrapping.
In Proceedings of the Sixteenth National Conference on Artificial Intelligence, pages 474479.
E. Tjong Kim Sang and J.
Veenstra. 1999.
Representing text chunks.
In Proceedings of EACL99.
Available as cs.CL/9907006.
D. Yarowsky.
1995. Unsupervised word sense disambiguation rivaling supervised methods.
In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 189196.
References 1 Thorsten Brants, TnT: a statistical part-of-speech tagger, Proceedings of the sixth conference on Applied natural language processing, p.224-231, April 29-May 04, 2000, Seattle, Washington 2 Eric Brill, Transformation-based error-driven learning and natural language processing: a case study in part-of-speech tagging, Computational Linguistics, v.21 n.4, p.543-565, December 1995 3 Masaru Tomita, Harry C.
Bunt, Recent Advances in Parsing Technology, Kluwer Academic Publishers, Norwell, MA, 1996 4 Rich Caruana, Multitask Learning, Machine Learning, v.28 n.1, p.41-75, July 1997 5 C.
Chang and C.
Chen. 1993.
A study on integrating chinese word segmentation and part-of-speech tagging.
Communications of COLIPS, 3(1).
6 Radu
Florian, John C.
Henderson, Grace Ngai, Coaxing confidences from an old friend: probabilistic classifications from transformation rule lists, Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora: held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics, p.26-34, October 07-08, 2000, Hong Kong 7 Jan Hajic, Barbora Hladk, Tagging inflective languages: prediction of morphological categories for a rich, structured tagset, Proceedings of the 17th international conference on Computational linguistics, August 10-14, 1998, Montreal, Quebec, Canada 8 J.
Hockenmeier and C.
Brew. 1998.
Error-driven segmentation of chinese.
Communications of COLIPS, 8(1):69--84.
9 Mitchell
P.
Marcus, Mary Ann Marcinkiewicz, Beatrice Santorini, Building a large annotated corpus of English: the penn treebank, Computational Linguistics, v.19 n.2, June 1993 10 M.
Munoz, V.
Punyakanok, D.
Roth, and D.
Zimak. 1999.
A learning approach to shallow parsing.
In Proceedings of EMNLP-WVLC'99.
Association for Computational Linguistics.
11 Grace
Ngai, Radu Florian, Transformation-based learning in the fast lane, Second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies 2001, p.1-8, June 01-07, 2001, Pittsburgh, Pennsylvania 12 David D.
Palmer, A trainable rule-based algorithm for word segmentation, Proceedings of the 35th annual meeting on Association for Computational Linguistics, p.321-328, July 07-12, 1997, Madrid, Spain 13 L.
Ramshaw and M.
Marcus. 1994.
Exploring the statistical derivation of transformational rule sequences for part-of-speech tagging.
In The Balancing Act: Proceedings of the ACL Workshop on Combining Symbolic and Statistical Approaches to Language, New Mexico State University, July.
14 L.
Ramshaw and M.
Marcus, 1999.
Natural Language Processing Using Very Large Corpora, chapter Text Chunking Using Transformation-based Learning.
Kluwer. 15 A.
Ratnaparkhi. 1996.
A maximum entropy model for part of speech tagging.
In Proceedings of the First Conference on Empirical Methods in Natural Language Processing, Philadelphia.
16 Richard
Sproat, William Gale, Chilin Shih, Nancy Chang, A stochastic finite-state word-segmentation algorithm for Chinese, Computational Linguistics, v.22 n.3, p.377-404, September 1996 17 Erik F.
Tjong Kim Sang, Sabine Buchholz, Introduction to the CoNLL-2000 shared task: chunking, Proceedings of the 2nd workshop on Learning language in logic and the 4th conference on Computational natural language learning, September 13-14, 2000, Lisbon, Portugal 18 Erik F.
Tjong Kim Sang, Noun phrase recognition by system combination, Proceedings of the first conference on North American chapter of the Association for Computational Linguistics, p.50-55, April 29-May 04, 2000, Seattle, Washington 19 Dekai Wu, Pascale Fung, Improving Chinese tokenization with linguistic filters on statistical lexical acquisition, Proceedings of the fourth conference on Applied natural language processing, October 13-15, 1994, Stuttgart, Germany 20 Fei Xia, Martha Palmer, Nianwen Xue, Mary Ellen Okurowski, John Kovarik, Fu-Dong Chiou, Shizhe Huang, Tony Kroch, and Mitch Marcus.
2000. Developing guidelines and ensuring consistency for chinese text annotation.
In Proceedings of the second International Conference on Language Resources and Evaluation (LREC-2000), Athens, Greece.
21 Endong Xun, Changning Huang, Ming Zhou, A unified statistical model for the identification of English baseNP, Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, p.109-116, October 03-06, 2000, Hong Kong
References Rod Ellis.
1997. Second Language Acquisition.
Oxford University Press, Oxford.
Pascale Fung.
1998. A statistical view on bilingual lexicon extraction: from parallel corpora to non-parallel corpora.
In Lecture Notes in Artificial Intelligence, pages 117.
Springer Publisher.
Rena Helms-Park.
1997. Building an L2 Lexicon: The Acquisition of Verb Classes Relevant to Causativization in English by Speakers of Hindi-Urdu and Vietnamese.
Ph.D. thesis, University of Toronto, Toronto, Canada.
Nancy Ide.
2000. Cross-lingual sense determination: Can it work?
Computers and the Humanities, 34:223234.
Shunji Inagaki.
1997. Japanese and Chinese learners acquisition of the narrow-range rules for the dative alternation in English.
Language Learning, 47(4):637669.
Alan Juffs.
2000. An overview of the second language acquisition of links between verb semantics and morpho-syntax.
In John Archibald, editor, Second Language Acquisition and Linguistic Theory, pages 170179.
Blackwell Publishers.
Maria Lapata and Chris Brew.
1999. Using subcategorization to resolve verb class ambiguity.
In Proceedings of Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 266274, College Park, MD.
Beth Levin.
1993. English Verb Classes and Alternations: A Preliminary Investigation.
University of Chicago, Chicago.
Diana McCarthy and Anna-Leena Korhonen.
1998. Detecting verbal participation in diathesis alternations.
In Proceedings of the 36th Annual Meeting of the ACL and the 17th International Conference on Computational Linguistics (COLING-ACL 1998), pages 14931495, Montreal, Canada.
I. Dan Melamed and Mitchell P.
Marcus. 1998.
Automatic construction of Chinese-English translation lexicons.
Technical Report 98-28, University of Pennsylvania, Philadelphia, PA.
I. Dan Melamed.
1997. A portable algorithm for mapping bitext correspondence.
In Proceedings of the 35th Conference of the Association for Computational Linguistics, Madrid, Spain.
Paola Merlo and Suzanne Stevenson.
2001. Automatic verb classification based on statistical distributions of argument structure.
Computational Linguistics.
To appear.
Adwait Ratnaparkhi.
1996. A maximum entropy partofspeech tagger.
In Proceedings of The Empirical Methods in Natural Language Processing Conference, Philadelphia, PA.
Philip Resnik and David Yarowsky.
2000. Distinguishing systems and distinguishing senses: New evaluation methods for word sense disambiguation.
Natural Language Engineering, 5(2):113133.
Sabine Schulte im Walde.
2000. Clustering verbs semantically according to their alternation behaviour.
In Proceedings of COLING 2000, pages 747753, Saarbrucken, Germany.
Eric V.
Siegel and Kathleen R.
McKeown. 2000.
Learning methods to combine linguistic indicators: Improving aspectual classification and revealing linguistic insights.
Journal of Computational Linguistics, 26(4):595628, December.
Vivian Tsang.
2001. Second language information transfer in automatic verb classification: A preliminary investigation.
Masters thesis, University of Toronto, Toronto, Canada.
References 1 Sabine Buchholz.
1999. Distinguishing complements from adjuncts using memory-based learning.
ILK, Computational Linguistics, Tilburg University.
2 Michael
Collins and James Brooks.
1995. Prepositional phrase attachment through a backed-off model.
In Proceedings of the Third Workshop on Very Large Corpora, pages 27--38.
3 Bonnie
J.
Dorr, Large-Scale Dictionary Construction for ForeignLanguage Tutoring and Interlingual Machine Translation, Machine Translation, v.12 n.4, p.271-322, 1997 4 Jane Grimshaw.
1990. Argument Structure.
MIT Press.
5 Donald
Hindle, Mats Rooth, Structural ambiguity and lexical relations, Computational Linguistics, v.19 n.1, March 1993 6 Ray Jackendoff.
1977. X' Syntax: A Study of Phrase Structure.
MIT Press, Cambridge, MA.
7 Beth
Levin.
1993. English Verb Classes and Alternations.
University of Chicago Press, Chicago, IL.
8 Matthias
Leybold.
2001. Automatic distinction of pp-arguments and pp-modifiers based on statistic implementations of linguistic criteria: a contribution to the problem of pp-attachment disambiguation.
Master's thesis, University of Geneva.
9 A.
Marantz. 1984.
On the Nature of Grammatical Relations.
MIT Press, Cambridge, MA.
10 Mitchell
P.
Marcus, Mary Ann Marcinkiewicz, Beatrice Santorini, Building a large annotated corpus of English: the penn treebank, Computational Linguistics, v.19 n.2, June 1993 11 Paola Merlo, Matt Crocker, and Cathy Berthouzoz.
1997. Attaching multiple prepositional phrases: Generalized backed-off estimation.
In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, pages 145--154, Providence, RI.
12 George
Miller, R.
Beckwith, C.
Fellbaum, D.
Gross, and K.
Miller. 1990.
Five papers on Wordnet.
Technical report, Cognitive Science Lab, Princeton University.
13 Carl
Pollard, Ivan A.
Sag, Information-based syntax and semantics: Vol.
1: fundamentals, Center for the Study of Language and Information, Stanford, CA, 1988 14 J.
Ross Quinlan, C4.5: programs for machine learning, Morgan Kaufmann Publishers Inc., San Francisco, CA, 1993 15 Adwait Ratnaparkhi, Jeff Reynar, Salim Roukos, A maximum entropy model for prepositional phrase attachment, Proceedings of the workshop on Human Language Technology, March 08-11, 1994, Plainsboro, NJ 16 Ellen Riloff and Mark Schmelzenbach.
1998. An empirical approach to conceptual case frame acquisition.
In Proceedings of the Sixth Workshop on Very Large Corpora, pages 49--56.
17 Carson
T.
Schtze. 1995.
PP Attachment and Argumenthood.
MIT Working Papers in Linguistics, 26:95--151.
18 Srinivas
Bangalore, Aravind K.
Joshi, Supertagging: an approach to almost parsing, Computational Linguistics, v.25 n.2, p.237-265, June 1999 19 Manfred Stede, A generative perspective on verb alternations, Computational Linguistics, v.24 n.3, September 1998
References 1 Chinatsu Aone, Scott Bennett, Applying machine learning to anaphora resolution, Connectionist, Statistical, and Symbolic Approaches to Learning for Natural Language Processing, p.302-314, January 1996 2 Breck Baldwin.
1997. CogNIAC: High precision coreference with limited knowledge and linguistic resources.
In Proceedings of the ACL-EACL'97 workshop on operational factors in practical, robust anaphora resolution for unrestricted texts, pages 38--45, Madrid, Spain.
3 Eric
T.
Bell. 1934.
Exponential numbers.
American Mathematical Monthly, 41:411--419.
4 Claire
Cardie and Kiri Wagstaff.
1999. Noun phrase coreference as clustering.
In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-99), pages 82--89, College Park, Maryland.
5 Michael
Collins and James Brooks.
1995. Prepositional phrase attachment through a backed-off model.
In Proceedings of the 3rd Workshop on Very Large Corpora (WVLC-3), pages 27--38, Cambridge, Massachusetts.
http://xxx.lanl.gov/abs/cmp-lg/9506021. 6 Dennis Connolly, John D.
Burger, and David S.
Day. 1994.
A machine learning approach to anaphoric reference.
In Proceedings of the International Conference on New Methods in Language Processing, pages 255--261, Manchester, England.
7 Ido
Dagan, Alon Itai, Automatic processing of large corpora for the resolution of anaphora references, Proceedings of the 13th conference on Computational linguistics, p.330-332, August 20-25, 1990, Helsinki, Finland 8 Barbara J.
Grosz, Scott Weinstein, Aravind K.
Joshi, Centering: a framework for modeling the local coherence of discourse, Computational Linguistics, v.21 n.2, p.203-225, June 1995 9 Sanda M.
Harabagiu, Steven J.
Maiorano, Multilingual coreference resolution, Proceedings of the sixth conference on Applied natural language processing, p.142-149, April 29-May 04, 2000, Seattle, Washington 10 Sven Hartrumpf.
1999. Hybrid disambiguation of prepositional phrase attachment and interpretation.
In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-99), pages 111--120, College Park, Maryland.
11 Hermann
Helbig and Sven Hartrumpf.
1997. Word class functions for syntactic-semantic analysis.
In Proceedings of the 2nd International Conference on Recent Advances in Natural Language Processing (RANLP'97), pages 312--317, Tzigov Chark, Bulgaria, September.
12 Hermann
Helbig.
2001. Die semantische Struktur natrlicher Sprache: Wissensreprsentation mit MultiNet.
Springer, Berlin.
13 Lynette
Hirschman and Nancy Chinchor.
1997. MUC-7 coreference task definition (version 3.0).
In Proceedings of the 7th Message Understanding Conference (MUC-7).
http://www.itl.nist.gov/iaui/894.02/related_projects/muc/. 14 Nancy Ide, Greg Priest-Dorman, and Jean Vronis, 1996.
Corpus Encoding Standard.
http://www.cs.vassar.edu/CES/. 15 Slava M.
Katz. 1987.
Estimation of probabilities from sparse data for the language model component of a speech recognizer.
IEEE Transactions on Acoustics, Speech and Signal Processing, 35(3):400--401, March.
16 Christopher
Kennedy, Branimir Boguraev, Anaphora for everyone: pronominal anaphora resoluation without a parser, Proceedings of the 16th conference on Computational linguistics, August 05-09, 1996, Copenhagen, Denmark 17 Alois Knoll, Christian Altenschmidt, Joachim Biskup, H.-M.
Blthgen, Ingo Glckner, Sven Hartrumpf, Hermann Helbig, C.
Henning, Reinhard Lling, Burkhard Monien, Thomas Noll, Norbert Sensen, An Integrated Approach to Semantic Evaluation and Content-Based Retrieval of Multimedia Documents, Proceedings of the Second European Conference on Research and Advanced Technology for Digital Libraries, p.409-428, September 21-23, 1998 18 Shalom Lappin, Herbert J.
Leass, An algorithm for pronominal anaphora resolution, Computational Linguistics, v.20 n.4, p.535-561, December 1994 19 Ruslan Mitkov.
1995. An uncertainty reasoning approach for anaphora resolution.
In Proceedings of the Natural Language Processing Pacific Rim Symposium (NLPRS'95), pages 149--154, Seoul, Korea.
20 Ruslan Mitkov.
1997. Two engines are better than one: Generating more power and confidence in the search for the antecedent.
In Ruslan Mitkov and Nicolas Nicolov, editors, Recent Advances in Natural Language Processing: Selected Papers from RANLP'95, pages 225--234.
John Benjamins, Amsterdam.
21 Ruslan Mitkov.
1998a. Evaluating anaphora resolution approaches.
In Proceedings of the Second Colloquium on Discourse Anaphora and Anaphor Resolution (DAARC 2), pages 164--177, Lancaster, England.
22 Ruslan Mitkov, Robust pronoun resolution with limited knowledge, Proceedings of the 36th annual meeting on Association for Computational Linguistics, p.869-875, August 10-14, 1998, Montreal, Quebec, Canada 23 Ruslan Mitkov, Multilingual Anaphora Resolution, Machine Translation, v.14 n.3-4, p.281-299, December 1999 24 Wee Meng Soon, Hwee Tou Ng., and Chung Yong Lim.
1999. Corpus-based learning for noun-phrase coreference resolution.
In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-99), pages 285--291, College Park, Maryland.
25 Kees van Deemter, Rodger Kibble, On coreferring: coreference in MUC and related annotation schemes, Computational Linguistics, v.26 n.4, December 2000 26 Renata Vieira, Massimo Poesio, An empirically based system for processing definite descriptions, Computational Linguistics, v.26 n.4, p.539-593, December 2000 27 Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, Lynette Hirschman, A model-theoretic coreference scoring scheme, Proceedings of the 6th conference on Message understanding, November 06-08, 1995, Columbia, Maryland

