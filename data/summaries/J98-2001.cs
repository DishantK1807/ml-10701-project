Doubt 3 0 4 0 0 7 Total A 294 160 546 39 1 1,040 In order to measure the agreement in a more precise way, we used the Kappa statistic (Siegel and Castellan 1988), recently proposed by Carletta as a measure of agreement for discourse analysis (Carletta 1996).
J96-2004
The presence of such a large number of discourse-new definite descriptions is also problematic for the idea that definite descriptions are interpreted with respect to the global focus (Grosz 1977; Grosz and Sidner 1986).
J86-3001
We evaluated the strength of these correlations by means of a computer simulation (Vieira and Poesio 1997).
W97-1301
developed two versions of the system: one that only attempts to classify subsequent-mention and discourse-new definite descriptions (Vieira and Poesio 1997), and one that also attempts to classify bridging references (Poesio, Vieira, and Teufel 1997).
W97-1301
Our study is also different from these previous ones in that measuring the agreement among annotators became an issue (Carletta 1996).
J96-2004
Carletta, Jean 1996.
J96-2004
In the past two or three years, this kind of verification has been attempted for other aspects of semantic interpretation: by Passonneau and Litman (1993) for segmentation and by Kowtko, Isard, and Doherty (1992) and Carletta et al.(1997) for dialogue act annotation.
J97-1002
A second reason was that we intended to use computer simulations of the classification task to supplement the results of our experiments, and we needed a parsed corpus for this purpose; the articles we chose were all part of the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993).
J93-2004
And indeed, the agreement figures went up from K = 0.63 to K = 0.68 (ignoring doubts) when we did so, i.e., within the "tentative" margins of agreement according to Carletta (1996) (0.68 <_ x < 0.8).
J96-2004
