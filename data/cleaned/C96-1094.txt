Translating into Free Word Order Languages Beryl Hoffman Centre for Cognitive Science University of Edinburgh 2 Buccleuch Place Edinburgh, EH8 9LW, U.K. hoffman~cogsci, ed.
ac. uk Abstract In this paper, I discuss machine translation of English text into a relatively "free" word order language, specifically Turkish.
I present algorithms that use contextual information to determine what the topic and the focus of each sentence should be, in order to generate the contextually appropriate word orders in the target language.
1 Introduction
Languages such as Catalan, Czech, Finnish, German, Hindi, Hungarian, Japanese, Polish, Russian, Turkish, etc.
have much freer word order than English.
For example, all six permutations of a transitive sentence are grammatical in Turkish (although SOV is the most common).
When we translate an English text into a "free" word order language, we are faced with a choice between many different word orders that are all syntactically grammatical but are not all felicitous or contextually appropriate.
In this paper, I discuss machine translation (MT) of English text into 2hrkish and concentrate on how to generate the appropriate word order in the target language based on contextual information.
The most comprehensive project of this type is presented in (Stys/Zemke, 1995) for MT into Polish.
They use the referential form and repeated mention of items in the English text in order to predict the salience of discourse entities and order the Polish sentence according to this salience ranking.
They also rely on statistical data, choosing the most frequently used word orders.
I argue for a more generative approach: a particular information structure (IS) can be determined from the contextual information and then can be used to generate the felicitous word order.
This paper concentrates on how to determine the IS from contextual information using centering, old vs.
new information, and contrastiveness.
(Hajifiov£/etal, 1993; Steinberger, 1994) present approaches that determine the IS by using cues such as word order, definiteness, and complement semantic types (e.g.
temporal adjuncts vs arguments) in the som:cc language, English.
I believe that we cannot rely upon cues in the source language in order to determine the IS of the translated text.
Instead, I use contextual informati<)n in the target language to determine the IS of sentences in the target language.
In section 2, I discuss the Information Structure, and specifically th<~ topic and the focus in naturally occurring Turkish data.
Then, in section 3, I present algorithms for determining the topic and the focus, and show that we can generate contextually appropriate word orders in '\[~/rkish using these algorithms in a simple MT implementation.
2 Information
Structure \]n the Information Structure (IS) that I use for Turkish, a sentence is first divided into a topic and a comment.
The topic is the maiu element that the sentence is about, and the comment is the information conveyed about this toI)ic.
Within the comment, we tind the focus, the most information-bearing const.itnent in the senten(:e, and the ground, the rest of the sentence.
The fo cus is the new or important information in the sentence and receives prosodic prominence in speech.
In Turkish, the pragmatic fimction of topic is assigned to the sentence-initial position mM the focus to the immediately preverbM position, following (Erguvanh, 11984).
The rest of the sentence forms the ground.
In (Iloffman, 1995; Iloffman, 1995b), I show that the information structure components of topic and focus can be suecessfiflly used in generating the context-appropriate answer to database queries.
Determining the topic and focns is fairly easy in the context of a simple question, however it is much more complica.ted in a text.
In the fol556 The Cb in SOY seiitences.
Cb = Subject 14 (47%) Cb = Object 6 (20%) Cb = Subj or Obj? 6 (20%) Cb = Subj or Other Obj? 0 (0%) No Cb 4 (1.3%) TOTAL 3O Tim Cb in OSV sento, nco.s.
Cb = Subject 4 (13%) C'b = Object :t6 (53%) Cb = Sub,i or Ob.i? 6 (2()%)Cb = Sul)j or Other ()b.i'? 2 U%) No Cb 2 (7%) TO'rn l, 30 Figure 1: The Cb it, SOV a,nd OSV Sentences.
lowing sections, I will describe the characteristics of topic, focus, and ground components of the 1S in naturally occurring texts analyzed in (lloffman, 1995b) and allude to possible algorithms for determining them.
The algorithms will then be spelled out in section 3.
An example text from the cortms 1 is shown below.
The noncanonical OSV word order in (1)b is contextually appropriate because the object t)ronoun is a discourse-old topic that links the se.ntence to the previous context, and the sul)jeet, "your father", is a discourse-new focus that is being contrasted with other relatives.
Discourse-old entities are those that were previously mentioned in the discourse while discourse-new entil, ics are those that were not (Prince, 1992).
O) a.
b. Bu defteri de gok say(lira ben.
This notebk-acc too much like-l)st-lS I.
'As for this notebook, I like it very much'.
Bunu da baban ml verdi?
(OSV) This-Ace too father-2S Quest give-Past?
'Did your FATHER, give this to you'?
(CHILDES lba.cha) Many people have suggested that "free" word order languages order information from old to new information.
However, the Old-to-New ordering prim:iple is a generalization to which exceptions can be found.
1 believe that the order in which speakers place old vs.
new items in a sentence reflects the information structures that are awdlable to the speakers.
The ordering is actually tile 'Ibpic followed by the Focus.
Tile qbpic tends to be discourse-old inlbrmation and the focus disconrsenew.
However, it is possible to have a disconrseNEW topic and a discourse-OLD focus, as wc will see in the following sections, which explains the exceptions to the Old-To-New ordering principle.
1The data was collected fi'om transcribed conversations, contemporary novels, and adult speedl from the CHILDES corpus.
2.1 Topic
Although humans can intuitively determine whal, the tol)ic of a sentence is, the traditional delinition (what tim sentence is about) is too vague to be implemented in a COmlml, ational system, l propose heuristics based on familiarit,y and salience to determine discourse-old seal;ante topics, ~tt~¢l heu ris~ ties based on grammatical reb~tions Ibr discou rsenew t.opics.
Speakers can shill; Loa new topic at the start, of a new discourse sag/ileal., ;ts iH (2)a.
Or they can continue ta.lking about Lh(~ sam(, (liscours(>o\[(I tot)it, as iu (2)1).
(2) a.
\[Mary\]m went to lhe I,ookstore.
b. \[She\]./.
I)ought a new book on linguistics.
A discourse-old topic often serves 1.o liuk the sentence to the previous context l)y evoking a familiar and sMient discourse entity.
(~enteriug Theory ((~rosz/etal, 1)95) provides a measure of saliency based on the obserwrtions t;hat salient discourse entities are often mentioned rel)ea.1;edly within a discourse segment and are oft.an r(mlized as pronouns.
(rl~lran, 1995) provides a.
(:OUlprehensive study of null and overt subjects in Turkish using Centering Theory, and \[ inw~stigate the interaction between word order and (',catering in Turkish in (Iloffman, 1996).
In the Centering Algoritl.n, each nt,l, era.nce in a discom:se is associated with a ranked list of discourse entities called the forward-lookiug eent.ers (Cf list;) that contains every (lis(:ours(~ entity that is reMized in thai; utteraltce.
The Cf list is usually ranked according to a hierarchy of granmmtica\] relal, ions, e.g. subjects are aSSllllled to \])e lllore salient than objects.
The backward looking center (Cb) is the most salient member of t,he Cf list that links the era'rent utterance to the iwevious utterance.
The Cb of an utterance is delined as the highest ranke(l element of the previous u tterance's Cf list that also occurs iu the curren(, utterance.
If there is a pronoun in the sentence, it ia likely to be the (Jb.
As we.
will see, the (~,b has much in common with a sentencetol)ic.
557 Discourse-Old Inferrable D-New, Hearer-Old S-init sov,osv 55 (85%) 8 (13%) i (2%) IPV Post-V so ov,os_v ovs, svoo_ 43 (67%) 56 (93%) 10 06%) 4 (7%) 1 (2%) 0 * D-New, Hearer-New 0 10 (15%) 0 TOTAL 64 64 60 Figure 2: Given/New Status in Different Sentence Positions The Cb analyses of the canonical SOV and the noncanonical OSV word orders in 251rkish are summarized in Figure 1 (forthcoming study in (Hoffman, 1996)).
As expected, the subject is often the Cb in the SOV sentences.
However, in the OSV sentences, the object, not the subject, is most often the Cb of the utterance.
A comparison of the 20 discourses in the first two rows 2 of the tables in Figure 1 using the chisquare test shows that the association between sentence-position and Cb is statistically significant (X 2 = 10.10, p < 0.001). a Thus, the Cb, when it is not dropped, is often placed in the sentence initial topic position in Turkish regardless of whether it is the subject or the object of the sentence.
The intditive reason for this is that speakers want to form a coherent discourse by immediately linking each sentence to the previous ones by placing the Cb and discourse-old topic in the sentence-initial position.
There are also situations where no Cb or discourse-old topic can be found.
Then, a discourse-new topic can be placed in the sentenceinitial position to start a new discourse segment.
Discourse-new topics are often subjects or situation-setting adverbs (e.g.
yesterday, in the morning, in the garden) in 3Mrkish.
2.2 Focus
The term focus has been used with many different meanings.
Focusing is often associated with new information, but it is well-known that old information, for example pronouns, can be focused as well.
I think part of the confusion lies in the distinction between contrastive and presentational 2The centering analysis is inconclusive in some cases because the subject and the object in the sentence are realized with the same referential form (e.g.
both as overt pronouns or as full NPs).
ZAlternatively, using the canonical SOV sentences as the expected frequencies, the observed frequencies for the noncanonical OSV sentences significantly diverge from the expected frequencies (X 2 = 8.8, p < 0.005). focus.
Focusing discourse-new information is often called presentational or informational focus as shown in (3)a.
Broad/wide focus (focus projection) is also possible where the rightmost element in the phrase is accented, but the whole phrase is in focus.
However, we can also use focusing in order to contrast one item with another, and in this case the focus can be discourse-old or discoursenew, e.g.
(3)b. (3) a.
What did Mary do this summer?
She \[wandered around TURKEY\]F.
b. It wasn't \[ME\],., It was \[HF, R\]e.
(VMlduvf, 1992) defines fbcns as the most information-bearing constituent, and this definition encompasses both contrastive and presentational focusing.
I use this definition of focus as well.
However, as will see, we still need two different algorithms in order to determine which items are in focus in the target sentence in MT.
We must check to see if they are discourse-new information as well as checking if they are being contrasted with another item in the discourse model.
In Turkish, items that are presentationally or contrastively focused are placed in the immediately preverbM (IPV) position and receive the primary accent of the phrase.
4 As
seen in Figure 2, brand-new discourse entities are found in the,,IPV position, but never in other positions in the sentence in my Turkish corpus.
The distribution of brand-new (the starred line of the table) versus discourse-old information (the rest of the table 5) is statistically significant, (X 2 = 10.847, p <.001).
This supports the association of discourse-new \[bcus with the IPV position.
4Some languages such as Greek and Russian treat presentational and contrastive focus differently in word order.
5 lnferrables refer to entities that the hearer can easily accmnmodate based on entities already in the dis-.
course model or the situation.
Hearer-old entities are well-known to the speaker and hearer but not necessarily mentioned in the prior discourse (Prince, 1992).
They both behave like discourse-oM entities.
558 However, as can be seen in Figure 2, most of the focused subjects in the OSV sentences in my corpus were actually discourse-old information.
Discourse-old entities that occur in the IPV position are contrastively focused.
In (Rooth, 1985)'s alternative-set theory, a contrastively focused item is interpreted by constructing a set of alternatives from which the focused item must be distinguished.
Generalizing from his work, we can determine whether an entity should be contrastively focused by seeing if we can construct an alternative set from the discourse model.
2.3 Ground
Those items that do not play a role in IS of the sentence as the topic or the focus form the ground of the sentence.
In Turkish, discourse-old information that is not the topic or focus can be (4) a.
dropped, b.
postposed to the right of the verb, c.
or placed unstressed between the topic and the focus.
Postposing plays a backgrounding fnnction in Turkish, and it is very common.
Often, speakers will drop only those items that are very salient (e.g.
mentioned just in the previous sentence) and postpose the rest of the discourse-old items, lIowever, the conditions for dropping arguments can be very complex.
(Turan, 1995) shows that there are semantic considerations; for instance, generic objects are often dropped, but specific objects are often realized as overt pronouns and fronted.
Thus, the conditions governing dropping and postposing are areas that require more research.
3 The
Implementation In order to simplify the MT implementation, I concentrate on translating short and simple English texts into Turkish, using an interlingua representation where concepts in the semantic representation map onto at most one word in the English or Turkish lexicons.
The translation proceeds sentence by sentence (leaving aside questions of aggregation, etc.), but contextual information is used during the incremental generation of the target text.
These simplifications allow me to test out the algorithms for determining the topic and the focus presented in this section.
In the implementation, first, an English sentence is parsed with a Combinatory Categorial Grammar, CCG, (Steedman, 1985).
The semantic representation is then sent to the sentence planner for Turkish.
The Sentence Planner uses the algorithms in the following subsections to determine the topic, focus, and ground from the given semantic representation ~md the discourse model.
Then, the sentence planner sends the semantic representation and the information strncture it has determined to the sentence realization component for Turkish.
This component consists of a head-driven bottom up generation algorithm that uses the semantic as well as the information strncture features given by the planner to choose an appropriate head in the lexicon.
The grammar used for the generation of 3hlrkish is a lexicalist formalism called Multiset-CCG (Hoffman, 1995; Iloffman, 1995b), an extension of CCGs.
MultisetCCG was developed in order to capture formal and descriptive properties of "free" and restricted word order in simple and complex sentences (with discontinuous constituents and long distance dependencies).
Mnltiset-CCG captures the contextdependent meaning of word order in 'Fnrkish by compositionally deriving the predicate-argument structure and the information strnctm'e of a sentence in parallel.
The following sections describe the algorithms used by the sentence plauner to determine the IS of the 'lSlrkish sentence, given the semantic representation of a parsed English sentence.
3.1 The
Topic Algorithm As each sentence is translated, we update the discourse model, and keep track of the forward looking centers list (Cflist) of the last processed sentence.
This is simply a list of all the discourse enities realized in that sentence ranked according to the theta-role hierarchy found in the semantic representation.
Thus, the Cf list for the reI)resentation give(Pat, Chris, book) is the ranked list \[Pat,Chris,book\], where the subject is assmned to be more salient than the objects.
Given the semantic representation for the sentence, the discourse model of the text processe(l so far, and the ranked C\[ lists of the current and previous sentences in the discourse, the following algorithm determines the topic of (;he sentence.
First, the algorithm tries to choose the most salient discourse-old entity as the sentence topicf If there is no discourse-old entity realized in the sentence, then a situation-setting adverb o, the subject is chosen as the discourse-new topic.
l. Compare the current Cf list with the previous sentence's Cf list; and choose the firs( item that is a member of both of the ranked lists (the Cb).
6(Stys/Zemke, 1995) use the saliency ranking to order the whole sentence in Polish.
tIowever, \[ I)clieve that there is a distinct notion of topic and fo(:as in Turkish.
559 2.
If 1 fails: Choose the first item in the current sentence's Cf list that is discourse-old (i.e.
is already in the discourse model).
3. If 2 fails: If there is a situation-setting adverb in the semantic representation (i.e.
a predicate modifying the main event, in representation), choose it as the discourse-new topic.
4. If 3 fails: choose the first item in the Cf list (i.e.
the subject) as the discourse-new topic.
Note that the determination of the sentence topic is distinct from the question of how to realize the salient Cb/topic (e.g.
as a dropped or overt pronoun or full NP).
In the MT domain, this can be determined by the referential form in the source text.
This trick can also be used for accommodating inferrable or hearer-old entities that behave as if they are discourse-old even though they are literally discourse-new.
If an item that is not; in the discourse model is nonetheless realized as a definite NP in the source text, the speaker is treating the entity as discourse-old.
This is very similar to (Stys/Zemke, 1995)'s MT system which uses the referential form in the source text to predict the topicality of a phrase in the target text.
3.2 The
Focus Algorithm Given the rest of the semantic representation for the sentence and the discourse model of the text processed so far, the following algorithm determines the focus of the sentence.
The first step is to determine presentational focusing of discoursenew information.
Note that the focus, unlike the topic, can contain more than one element; this allows broad focus as well as narrow focusing.
If there is no discourse-new information, the second step in the algorithm allows contrastive focusing of discourse-old information.
In order to construct the alternative sets, a small knowledge base is used to determine the semantic type (agent, object, or event) of the entities in the discourse model.
1. If there are any discourse-new entities (i.e.
not in the discourse model) in the sentence, put their semantic representations into focus, 2.
Else for each discourse entity realized in the sentence, (a) Look up its semantic type in the KB and construct an alternative set that consists of all objects of that type in the discourse model, (b) If the constructed alternative set is not empty, put the discourse entity's semantic representation into the focus.
Once the topic and focus are determined, the remainder of the semantic representation is assigned as the gronnd.
For now, items in the ground are either generated in between the topic and the focus or post-posed behind the verb as backgrounded information.
Further research is needed to disa.mbiguate the use of the two possible word orders.
Further research is also needed on the exact role of verbs in the IS.
Verbs can be in the focus or the ground in Turkish; this cannot be seen in the word order, but it is distinguished by sentential stress for narrow focus readings.
The algorithm above works for verbs since I place events that are realized as verbs in the sentence into the discourse model as well.
ltowever, verbs are usually not in focus unless they are surprising or contrastive or in a discourse-initiM context.
Thus, the algorithm needs to be extended to a(:comnaodate discourse-new verbs that are nonetheless expected in some way into the ground component.
In addition, verbs often participate in broad focus readings, and fllrther research is needed to account for the observation that broad focus readings are only available in canonical word orders.
3.3 Examples
The English text in (5) is translated using the word orders in (6) following the Mgorithrns given above.
In (6), the numbers following T and F indicate the step in the respective algorithm which determined the topic or focus for that sentence.
Note that the inappropriate word orders (indicated by #) cannot be generated by the algorithm.
(5) a.
Pat will meet Chris today.
b. There is a tMk at four.
c. Chris is giving the talk.
d. Pat cannot come.
(6) a.
b. Bugiin Pat Chris'le bulu~acak.
(AdvSOV) Today Pat Chris-with meet-flit.
(T:3,F~I) D6rtde bir konu~ma vat.
(AdvSV,#SAdvV) Four-Lot one talk exist.
(T:3,F:I) c.
Konu~mayl Chris w'.riyor.
(OSV,#SOV) Talk-Ace Chris give-Prog.
(T:I,F:2) d.
Pat gelemiyecek.
(SV,@VS) Pat come-Neg-Fu|;.
('F:2,F:I for the verb) The algorithms can also utilize long distance scrambling in 3~rkish, i.e. constructions where an element of an embedded clause has been ex560 tracted and scrambled into the matrix clause in order to play a role in the IS of the matrix clause.
For example the b sentence in the following text is translated using long distance scrambling because "the talk" is the Cb of the utterance and therefore the best sentence topic, even though it is the argument of an embedded clause.
(7) a.
There is a talk at four.
b. Pat thinks that Chris will give the talk.
(8) a.
D6rtde bir konu~ma var.
(AdvSV) Four-Lot one talk exist.
b. Konu+mayh Pat \[Chris'in ei verecegini\] Taik-Acci Pat \[Chris-gen ci givc-ger-as-a<:c\] samyor.
(O281 \[S2V2\]V1) think-Prog.
(T:I,F:I) 4 Conclusions In the machine translation task from Fnglish into a "free" word order language, it is crucial to choose the contextnally appropriate word order in the target language.
In this paper, I discussed how to determine the appropriate word order using contextual information in translating into Turkish.
I presented algorithms for deterndning the topic and the focus of the sentence.
These algorithms are sensitive to whether the information is old or new in the discourse model (incrementally constructed from the translated text); whether they refer to salient entities (using Centering Theory); and whether they can be contrasted with other entities in the discourse model.
Once the im formation structure for a semantic representation is constructed using these algorithms, the sentence with the contextually appropriate word order is generated in the target language using Multiset CCG, a grammar which integrates syntax and information structure.
References Eser Emine Erguvanh.
1984. The l,'uuction of Word Order in Turkish Grammar.
University of California Press.
Barbara Grosz and Aravind K.
Joshi and Scott Weinstein.
1995. Centering: A Framework for Modelling the Local Coherence of Discourse.
Computational Linguistics.
Haji~ov& Eva, Petr Sgall, and liana Skounm,lowt 11993.
Identifying Topi(: and Focus 1)y an Auto= marie Procedure.
l'rocccdings of the,%,:th Coatference of the Eurolwan Chapter of the As.sociali(m for Computational Linguistic.< Beryl tIott'man.
1995. Integrating Frec Word O> der Syntax and Information Structure.
t'roecedings of the European A ssoeiation for Com.putatiou,I Linguistics (I';A CL).
Beryl Hoffman.
1995. 7he Computational Anah, lsis of the Syntax and Inte~Tnvtatimt of "\[i','ee'; Word Order in Turki.~h.
Ph.I). dissertation.
1RCS q>ch Report 95-17.
l)ept, of (~on,puter and Information Science.
\[ Miversil;y of I'eJmsylvania.
Beryl lloffman, to appear 1996.
Word Order, infbrmation Structure, and Centering in Turkish.
Centering in Discourse.
eds. F, llcn I'rin<:e, Aravind .loshi, and Marilyn Walker.
Oxford (halversify I)ress.
Ellen F.
Prince. The ZPG Letter: Subjects, l)efiniteness and Information Status.
Discourse descro~tion: diw'rse analyses of a \])rod raising t,e.vt, eds.
Thonrl)son, S.
and Mann, W.
Philadelphia:,lohn Beujamins ILV.
pl),2(,)5 325.
1992. Mats l.ooth. 1985.
Association with l,'ocus.
Ph.D. Dissertation.
lJniversity of Massachusel;t,s.
Amherst. Mark Steedman.
1985. Dependencies mid <:oordination in the grammar of l)uteh and Englislr, Language, 61:523 568.
lalfSteinberger. 1994.
Tr<mting Free Word Order in Ma<:hine Translation.
Coling, Kyol,o, Jal0nl\].
Malgorzata E.
Stys and Stefan S.
Zemke. \] 995.
In: corporating l)iscourse Aspects in Englishl>olish MT: Towards Robust Implementation.
Icccnl Advanees in NLI'.
{)rnit Turan.
1995. Null vs.
Ow'~rt,5'ubjer:ls i~ 7}lrkish Discourse: g Centering An~dysis.
Uni: versil,y of Pennsylvania, Linguistics l>h.l), dissertation.
Fmric Va.llduvL 1992.
The l'nformational Corn.portent.
New York: Garla,d. 561

