This data has since been used in studies by (Mooney, 1996), (Towell and Voorhees, 1998), and (Leacock et al., 1998).
J98-1005 J98-1006 W96-0208
(Ng and Lee, 1996)), shallow lexical features such as co-occurrences and collocations prove to be stronger contributors to accuracy than do deeper, linguistically motivated features such as part-of-speech and verb-object relationships.
P96-1006
(Towell and Voorhees, 1998) report accuracy of 87% while (Leacock et al., 1998) report accuracy of 84%.
J98-1005 J98-1006
For example, (Ng and Lee, 1996) report that local collocations alone achieve 80% accuracy disambiguating interest, while their full set of features result in 87%.
P96-1006
Despite making such an assumption, this proves to be among the most accurate techniques in comparative studies of corpus-based word sense disambiguation methodologies (e.g., (Leacock et al., 1993), (Mooney, 1996), (Ng and Lee, 1996), (Pealersen and Bruce, 1997)).
H93-1051 P96-1006 W96-0208
This data set was subsequently used for word sense disambiguation experiments by (Ng and Lee, 1996), (Pedersen et al., 1997), and (Pedersen and Bruce, 1997).
A97-1056 P96-1006
In these experiments words from a stopNaive Bayesian Ensemble Ng ~: Lee, 1996 Bruce & Wiebe, 1994 Pedersen & Bruce, 1997 accuracy 89% 87% 78% 78% 74% method ensemble of 9 nearest neighbor model selection decision tree naive bayes feature set varying left & right b-o-w p-o-s, morph, co-occur collocates, verb-obj p-o-s, morph, co-occur p-o-s, morph, co-occur Table 5: Comparison to previous results for interest Naive Bayesian Ensemble Towell & Voorhess, 1998 Leacock, Chodor0w, & Miller, 1998 Leacock, Towell, & Voorhees, 1993 Mooney, 1996 accuracy 88% 87% 84% 76% 72% 71% 72% 71% method ensemble oi ~ 9 feature set varying left & right bSo-w neural net local ~z topical b-o-w, p-o-s naive bayes local & topical b-o-w, p.-o-s neural net 2 sentence b-o-w content vector naive bayes naive bayes 2 sentence b-o-w perceptron Table 6: Comparison to previous results for line list are removed, capitalization is ignored, and words are stemmed.
A97-1056 H93-1051 J98-1005 J98-1006 P94-1020 P96-1006 W96-0208
This is discussed in the context of combining part-of-speech taggers in (Brill and Wu, 1998).
P98-1029
The line data was recently revisited by both (Towell and Voorhees, 1998) and (Leacock et al., 1998).
J98-1005 J98-1006
For example, in this work five-fold cross validation is employed to assess accuracy while (Ng and Lee, 1996) train and test using 100 randomly sampled sets of data.
P96-1006
A similar finding has emerged in word sense disambiguation, where a number of comparative studies have all reported that no method achieves significantly greater accuracy than the Naive Bayesian classifier (e.g., (Leacock et al., 1993), (Mooney, 1996), (Ng and Lee, 1996), (Pedersen and Bruce, 1997)).
A97-1056 H93-1051 P96-1006 W96-0208
66 4.1.2 Line The line data was first studied by (Leacock et al., 1993).
H93-1051
The interest data was first studied by (Bruce and Wiebe, 1994).
P94-1020
(Pedersen et al., 1997) and (Pedersen and Bruce, 1997) present studies that utilize the original Bruce and Wiebe feature set and include the interest data.
A97-1056
The line data was studied again by (Mooney, 1996), where seven different machine learning methodologies are compared.
W96-0208
Data The line data was created by (Leacock et al., 1993) by tagging every occurrence of line in the ACL/DCI Wall Street Journal corpus and the American Printing House for the Blind corpus with one of six possible WordNet senses.
H93-1051
The interest data was created by (Bruce and Wiebe, 1994) by tagging all occurrences of interest in the ACL/DCI Wall Street Journal corpus with senses from the Longman Dictionary of Contemporary English.
P94-1020
In natural language processing, ensemble techniques have been successfully applied to partof-speech tagging (e.g., (Brill and Wu, 1998)) and parsing (e.g., (Henderson and Brill, 1999)).
P98-1029 W99-0623
