Proceedings of NAACL HLT 2009: Tutorials, pages 3â€“4,
Boulder, Colorado, June 2009. c 2009 Association for Computational Linguistics
Distributed Language Models 
 
Thorsten Brants and Peng Xu, Gogle Inc. 
 
  
Language models are used in a wide variety of natural anguage applications, including 
mchine transltion, spech recognition, speling corrction, optical character 
reognition, etc. Recnt studies have shown that more data is beter dat, nd bigger 
language models ar beter language models: the authors found nearly constant machine 
trnslation iprovements with each doubling of the training dat size ven t 2 trilion 
toke (resulting in 400 bilion n-grams). Training nd usisuch large models is a 
chalenge. This tutorial shows eficent ethods for distributed training of large language 
models based on the MpReduc computing model. We also show eficent wys of using 
distributd models in which requesting individual n-grams is expensive because they 
require comunication betn difernt machines. 
 
 
Tutorial Outline 
 
1) Training Distributed Models 
 
* N-gram collection 
   Use of the MapReduce model; compresing intermediate data; minimizng 
   communiction overhead with good sharding functions. 
 
* Smoothing 
   Chalenges of Katz Backoff and Knesr-Ney Smoothing in a distributed system; 
   Smoothing techniques that are asy to compute in a distributed systm: 
   Stupid Backof, Linear Intrpoltion; minimizng communication by sharding 
   and aggregtion. 
 
2) Model Size Reduction 
 
* Pruning 
   Reducing the size of the model by removing n-grams that don't have much impact. 
   Entropy pruning is simpl to comput for Stupid Bckoff, requires some fort for Katz 
   and Kneser-Ney in a distributed system. Efects of extrem pruning. 
 
* Quantization 
   Reducing the memory size of the model by storing approximations of the values. We 
   discuss several quantirs; typicaly 4 to 8 bits are sufficent to store a floating point 
   value. 
 
* Randomized Dat Structures 
   Reducing the memory siz of the model by changing the set of n-grams that is stored. 
   This typicaly lts us store models in 3 bytes per n-gram, independent of the n-gram 
3
   order without significant impact on quality. At the same tie it provides very fast 
   aces to the n-grams. 
 
3) Using Distributed Models 
 
* Serving 
   Rquesting a single n-gram in a distributed setup is expensive because it requires 
   communication betwen mchines. W show ho to use a distributd language model 
   in the first-pas of a dcoder by batching up n-gram rqust. 
 
 
Target Audience 
 
Target audience are rsearchers in al ares that focus on or use large n-gram language 
models. 
 
 
Presnters 
 
Thorsten Brants recived his Ph.D. in 1999 at he Sarland University, Germany, on part-
of-spech tagging and parsing. From 2000 to 2003, he worked at he Palo Alto Resarch 
Centr (PARC) on staistical methods for topic and event detection. Thorsten is now a 
Rsarch Scientist t Google working on large, distributed language models with focus 
on ppliations in machine translation. Other esarch interst include information 
retrieval, nmed entity detection, and speech recognition. 
 
Peng Xu joined Google as  Resarch Scientist hortly after geting a Ph.D. in April 2005 
from the Johns Hopkins University. Whil his resarch is focused on staistical machine 
translation at Google, he is also intersted in stistial machine lerning, infortion 
retrieval, nd speech rcognition. 
 
4

