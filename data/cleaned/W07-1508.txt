Proceedings of the Linguistic Annotation Workshop, pages 49–52, Prague, June 2007.
c©2007 Association for Computational Linguistics Criteria for the Manual Grouping of Verb Senses Cecily Jil Dufield, Jena D.
Hwang, Susan Windisch Brown, Dmitriy Dligach, Sarah E.Vieweg, Jeny Davis, Martha Palmer Departments of Linguistics and Computer Science University of Colorado Boulder, C0 80039-0295, USA {cecily.duffield, hwangd, susan.brown, dmitry.dligach, sarah.vieweg, jennifer.davis, martha.palmer}@colorado.edu Abstract In this paper, we argue that clustering WordNet senses into more coarse-grained groupings results in higher inter-annotator agreement and increased system performance.
Clustering of verb senses involves examining syntactic and semantic features of verbs and arguments on a caseby-case basis rather than applying a strict methodology.
Determining appropriate criteria for clustering is based primarily on the needs of annotators.
1 Credits
We gratefuly acknowledge the suport of the National Science Foundation Grant NSF-0415923, Word Sense Disambiguation, and the GALE program of the Defense Advanced Research Projects Agency, Contract No.
HR001-06-C02, a subcontract from the BBN-AGILE Team.
2 Introduction
Word sense ambiguity poses significant obstacles to accurate and efficient information extraction and automatic translation.
Successful disambiguation of polysemous words in NLP applications depends on determining an appropriate level of granularity of sense distinctions, perhaps more so for distinguishing between multiple senses of verbs than for any other grammatical category.
WordNet, an important and widely used lexical resource, uses fine-grained distinctions that provide subtle information about the particular usages of various lexical items (Felbaum, 198).
When used as a resource for annotation of various genres of text, this fine level of granularity has not been conducive to high rates of inter-annotator agreement (ITA) or high automatic tagging performance.
Anotation of verb senses as described by coarse-grained Proposition Bank framesets may result in higher ITA scores, but the blurring of distinctions between verb senses with similar argument structures may fail to alleviate the problems posed by ambiguity.
Our goal in this project is to create verb sense distinctions at a midle level of granularity that allow us to capture as much information as posible from a lexical item while stil attaining high ITA scores and high system performance in automatic sense disambiguation.
We have demonstrated that clear sense distinctions improve annotator productivity and accuracy.
System performance typically lags around 10% behind ITA rates.
ITA scores of at least 90% for a majority of our sense-groupings result in the expected corresponding improvement in system performance.
Training on this new data, Chen et al., (206) report 86.7% accuracy for verbs using a smoothed maximum entropy model and rich linguistic features.
(Also Semeval07 1 ) They also report state-of-the-art performance on finegrained senses, but the results are more than 16% lower.
We begin by describing the overall process.
3 The
Grouping and Anotation Proces The process for building our database with the appropriate level of verb sense distinctions 1 Task 17, htp:/nlp.cs.swarthmore.edu/semeval/.
49 involves two steps: sense grouping and annotation (Figure 1).
During our sense grouping process, linguists (henceforth, “groupers”) cluster finegrained sense distinctions listed in WordNet 2.1 into more coarse-grained groupings.
These rough clusters of WordNet entries are based on speaker intuition.
Other resources, including PropBank, VerbNet (based on Levin’s verb classes (Levin, 193)), and online dictionaries are consulted in further refining the distinctions between senses (Palmer, et.
al., 205, Kiper et al., 206).
To aid annotators in understanding the distinctions, sense groupings are ordered according to saliency and frequency.
Detailed information, including syntactic frames and semantic features, is provided as commentary for the groupings.
We also provide the annotators with simple example sentences from WordNet as well as syntactically complex and ambiguous attested usages from Gogle search results.
These examples are intended to guide annotators faced with similar challenges in the data to be tagged.
Completed verb sense groupings are sent through sample-annotation and tagged by two annotators.
Groupings that receive an ITA score of 90% or above are then used to annotate all instances of that verb in our corpora in actualannotation.
Groupings that receive less than 90% ITA scores are regrouped (Hovy et al., 206).
Revisions are made based on a second grouper’s evaluation of the original grouping, as well as patterns of annotator disagreement.
Verb groupings receiving ITA scores of 85% or above are sent through actual-annotation.
Verbs scoring below 85% are regrouped by a third grouper, and in some cases, by the entire grouping team.
It is sometimes imposible to get ITA scores over 85% for high frequency verbs that also have high entropy.
These have to be carefuly adjudicated to produce a gold standard.
Revised verbs are then evaluated and either deemed ready for actual-annotation or are sent for a third and final round of sampleannotation.
Verbs subject to the re-annotation process are tagged by different annotators.
Data from actual-annotation is examined by an adjudicator who resolves remaining disagreements between annotators.
The adjudicated data is then used as the gold standard for automatic annotation.
The final versions of the sense groupings are mapped to VerbNet and FrameNet and linked to the Omega Ontology (Philpot et al., 205).
Verbs are selected based on frequency of appearance in the WSJ corpus.
As the most frequent verbs are also the most polysemous, the number of sense distinctions per verb as well as the number of instances to be tagged decreases as the project continues.
The 740 most frequent verbs in the WSJ corpus were grouped in order of frequency.
They have an average polysemy of 7 senses in WordNet; our sense groups have reduced the polysemy to 3.75 senses.
Of these, 307 verb groupings have undergone regrouping to some extent.
A total of 670 verbs have completed actualannotation and adjudication.
The next 60 verbs have been divided into rough semantic domains based on VerbNet classes, and grouping wil proceed according to these semantic domains rather than by verb frequency.
As groupers create sense groupings for new verbs, old verb sense groupings in the same semantic domain are consulted.
This organization allows for more consistent grouping methodologies, as well as more efficiency in integrating our sense groupings into the Ontology.
Figure 1: The grouping and annotation process.
50 4 Grouping Methodology Various criteria are considered when disambiguating senses and creating sense groupings for the verbs, including frequent lexical usages and colocations, syntactic features and alternations, and semantic features, similarly to Senseval2 (Palmer, et.
al. 206).
Because these criteria do not apply uniformly to every verb, groupers take various approaches when creating sense groupings.
Groupers recognize that there are many alternate ways to cluster senses at this level of granularity; each grouping represents only one posible clustering as a midle ground between PropBank and WordNet senses for each verb.
Our highest priority is to then create clear distinctions among sense groupings that wil be easily understod by the annotators and consequently result in high ITA scores.
Initial clustering is based on groupers’ intuitions of the most salient categories.
Many verb groupings, such as that for the verb kil, provide litle detailed syntactic or semantic analysis and yet have received high ITA scores.
The success of these intuitive sense groupings is not due to lack of polysemy; kil has 15 WordNet senses and 2 multi-word expressions clustered into 9 sense groupings, yet it received 94% ITA in first round sample-annotation.
While annotators have litle trouble tagging text with verb senses that fall neatly into intuitive categories, many verbs have fine-grained WordNet senses that fall on a continum between two distinct lexical usages.
In such cases, syntactic and semantic aspects of the verb and its arguments help groupers cluster senses in such a way that annotators can make consistent decisions in tagging the text.
Syntactic criteria: Anotators have found syntactic frames, such as those defining VerbNet classes, to be useful in understanding boundaries between sense groupings.
For example, split was originally grouped with consideration for the units resulting from a spliting event (i.e.
whether a whole unit had been split into incomplete portions of the whole, or into smaller, but complete, individual units).
This grouping proved difficult for annotators to distinguish, with an ITA of 42%.
Using the causative/inchoative alternation for verbs in the “break-45.1” class to regroup resulted in higher consistency among annotators, increasing the ITA score to 95%.
Semantic criteria: When senses of a verb have similar syntactic frames, and usages fall along a continum between these senses, semantic features of the arguments, or less often, of the verb itself, can clarify these senses and help groupers draw clear distinctions between them.
Argument features that are considered when creating sense groupings include [+/-attribute], [+/-patient], and [+/locative].
It is most common for groupers to mark these features on nominal arguments, but a prepositional phrase may also be described in semantic terms.
Semantic features of the verb that are considered include aspectual features, as ilustrated by the use of [+/-punctual] in sense groupings for make (Figure 2).
However, it may be argued that this feature is unecessary for annotators to be able to distinguish between the sense groupings, as the prepositional phrase in sense 9 is a more salient feature for annotators.
Other features of the verb that were used earlier in the project include concrete/abstract, continuative, stative, and others.
However, these features proved less useful than those Sense group Description and Comentary WordNet 2.1 senses Examples 8 Atain or reach something desired NP1[+agent] MAKE[+punctual] NP2[desired goal, destination, state] This sense implies the goal has ben met.
Includes: MAKE IT make 13, 2, 38 He made the basketbal team.
We barely made the plane.
I made the opening act in plenty of time.
Can you believe it?
We made it! 9 Move toward or away from a location NP1[+agent] MAKE[-punctual] (pronoun+way) P/INFP make 30, 37 make of 1 make way 1 As the enemy aproached our town, we made for the hils.
He made his way carefuly acros the icy parking lot.
They made of with the jewels.
Figure 2: Sense groupings 8 and 9 for “make.” Senses are distinguished in part by aspectual features marked on the verb.
51 described above, and annotators not familiar with linguistic theory found them to be confusing.
Therefore, they are now rarely used to label sense groupings.
Such concepts, when used, are more likely to be described in prose commentary for the sake of the annotators.
Certain compositional features of verbs have also proven to be confusing for annotators.
In several cases, attempts to distinguish sense groupings based on maner and path have resulted in increased annotator disagreement.
In the first attempt at grouping rol, syntactic and semantic information, as well as prose commentary, was presented to help annotators distinguish the manner and path sense groupings.
Despite this, the admisibility of certain prepositions in both senses (“The baby roled over,” vs “She roled over to the wall,”) may have blurred the distinction.
In two rounds of sample-annotation, the greatest number of disagreements occurred with respect to these two senses for rol, which were then merged in the final version of the sense groupings.
5 Conclusion
Building on results in grouping fine-grained WordNet senses into more coarse-grained senses that led to improved inter-annotator agreement (ITA) and system performance (Palmer et al., 204; Palmer et al., 207), we have developed a process for rapid sense inventory creation and annotation of verbs that also provides critical links between the grouped word senses and the ontology (Philpot et al., 205).
This process is based on recognizing that sense distinctions can be represented by linguists in a hierarchical structure, that is rooted in very coarse-grained distinctions which become increasingly finegrained until reaching WordNet (or similar) senses at the leaves.
Sets of senses under specific nodes of the tree are grouped together into single entries, along with the syntactic and semantic criteria for their groupings, to be presented to the annotators.
Criteria are applied on a case-by-case basis, considering syntactic and semantic features as consistently as posible when grouping verbs in similar semantic domains as defined by VerbNet.
By using this approach when creating sense groupings, we are able to provide annotators with clear and reliable descriptions of senses, resulting in improved accuracy and performance.
References Chen, J., A.
Schein, L.
Ungar and M.
Palmer. 206.
An Empirical Study of the Behavior of Word Sense Disambiguation.
Proceedings of HLT-NACL 206.
New York, NY.
Fellbaum, C.
(ed). 198.
WordNet: An On-line Lexical Database and Some of its Aplications.
MIT Press, Cambridge, MA.
Kiper, K., A.
Korhonen, N.
Ryant, and M.
Palmer. 206.
Extensive Classifications of English Verbs.
Proceedings of the 12th EURALEX International Congress.
Turin, Italy.
Levin, B.
193. English Verb Clases and Alternations.
The University of Chicago Press, Chicago, IL.
OntoNotes, 206.
Hovy, E.H., M.
Marcus, M.
Palmer, S.
Pradhan, L.
Ramshaw, and R.
Weischedel. 206.
OntoNotes: The 90% Solution.
Short paper.
Proceedings of HLT-NACL 206.
New York, NY.
Palmer, M., O.
Babko-Malaya, and H.T.
Dang. 204.
Different Sense Granularities for Different Aplications.
Proceedings of the 2nd Workshop on Scalable Natural Language Understanding Systems (HLT-NACL 204).
Boston, MA.
Palmer, M., Dang, H.T., and Fellbaum, C., Making Fine-grained and Coarse-grained sense distinctions, both manually and automatically, Journal of Natural Language Engineering (to appear, 207).
Palmer, M., Gildea, D., Kingsbury, P., The Proposition Bank: A Corpus Anotated with Semantic Roles, Computational Linguistics Journal, 31:1, 205.
Philpot, A., E.H.
Hovy, and P.
Pantel. 205.
The Omega Ontology.
Proceedings of the ONTOLEX Workshop at the International Conference on Natural Language Processing (IJCNLP).
Jeju Island, Korea.

