In this framework, the source language, let-s say English, is assumed to be generated by a noisy probabilistic source.1 Most of the current statistical MT systems treat this source as a sequence of words (Brown et al., 1993).
J93-2003
Sato (1992), for example, stores complete parse trees in the TMEM and selects and generates new translations by performing similarity matchings on these trees.
C92-4203
From a theoretical perspective, it appears though that the two approaches are complementary: Vogel and Ney (2000) identify abstract patterns of usage and then use them during translation.
C00-2172
First, we show how one can use an existing statistical translation model (Brown et al., 1993) in order to automatically derive a statistical TMEM.
J93-2003
As discussed by Germann et al.(2001), the word-for-word gloss is constructed by aligning each French word fa76 with its most likely English translation ef a107 (ef a107a109a108 argmaxa110 t(e a0 fa76 )).
P01-1030
A few of the errors were genuine, and could be explained by failures of the sentence alignment program that was used to create the corpus (Melamed, 1999).
J99-1003
(Alternative approaches exist, in which the source is taken to be, for example, a sequence of aligned templates/phrases (Wang, 1998; Och et al., 1999) or a syntactic tree (Yamada and Knight, 2001)).
P01-1067 W99-0604
With a few exceptions (Wu and Wong, 1998), most SMT systems are couched in the noisy channel framework (see Figure 1).
P98-2230
The decoding algorithm that we use is a greedy one — see (Germann et al., 2001) for details.
P01-1030
See (Brown et al., 1993) or (Germann et al., 2001) for a detailed discussion of this translation model and a description of its parameters.
J93-2003 P01-1030
The work that comes closest to using a statistical TMEM similar to the one we propose here is that of Vogel and Ney (2000), who automatically derive from a parallel corpus a hierarchical TMEM.
C00-2172
We modified the greedy decoder described by Germann et al.(2001) so that it attempts to find good translation starting from two distinct points in the space of possible translations: one point corresponds to a word-for-word “gloss” of the French input; the other point corresponds to a translation that resembles most closely translations stored in the TMEM.
P01-1030
The bias introduced by TMEMs is a practical alternative to finding optimal translations, which is NP-complete (Knight, 1999).
J99-4005
Vogel and Ney (2000) do not evaluate their TMEM-based system, so it is difficult to empirically compare their approach with ours.
C00-2172
IBM Model 4 For the work described in this paper we used a modified version of the statistical machine translation tool developed in the context of the 1999 Johns HopkinsSummer Workshop (Al-Onaizan et al., 1999), which implements IBM translation model 4 (Brown et al., 1993).
J93-2003
