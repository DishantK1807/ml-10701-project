American Journal of Computational Linguistics ~i crof i che 45 SYNTAX i N AUTOMATI c SPEECH UNDERSTANDLNG Boston University and Bolt Beranek and Newman Inc.
50 Moulton Street Cambridge, Massachusetts 02138 This research was principally supported by the Advanced Reeearch Projeclts Agency of the Department of Defense (ARPA Order No.
2904) and was monitored by ONR under Contract No.
N0001.4-75-c0533.
Partial support of the author by NSF grant GS-39834 to Harvard University is gratefully acknowledged.
Copyright Q 1976 Association for Computational Linguistics Table of Contents Sect ion 1 Introduction 2 Thz BBN Speech Undarvtandinp System 3 The Grammar 4 Overview of SPARSER PraYninaries Be~innin~ tn Parse an Island Parsinp Through an Xslaqd Ending an Island Endinr a Theorv Prooessins Multiple Theorids Processins Events 5 &ore Details of the Parsln~ Process Depth vs.
Breadth Scorinp Paths ~Eorinp Predictions 6 Examples and Results Example 1 Examole 2 Example 3 7 Conclus~ons Strengths anb 'ikaknesses of SPARSER Prasodics Extensions and Further Research Conclusion Appendix I MINIGRAMMAR Ap~andix I1 The Vocabulary and Svntax Classes Section 1 Introduction Understanding speech is an extremely complex process which requires tho use of many types of kn~wledea, one of which is syntax.
This report presents a system called SPARSER which is designed to provide and use the syntactic knowledge necessary to support an artificial speech understanding system.
(We will assume for the remainder of this paper that unless explicitly stated otherwise "speechw means grammatical speech spoken at a mo4erate rate with natural inflections and pauses, spontaneously produced but similar to the type of speech produced by reading text.
) We will make the following assumptions about the characteristics of speech and a speech processor: 1.
There is not enough infopmation in the spech signal to uniquely identify the phonemes or words in a normally spoken utterance.
2. The acoustic processing component of any artificial speech understanding system will introduce additional errors and ambiguity as it attempts to identify the phonemes-or words in the utterance.
3. As a consequence of 1 and 2, when an utterance is scanned to try to identify the words, it is reasonable to suppose that a number of (perhaps overlapping) candidates will be found.
Thls is illustrate in Fipure 1.1 by a structure called a word lattice which shows schematically that many Words may initially appear to be present.
In this representation, the numbers along the horizontal scale are se~ment boundary points in the utterance which rouphly correspond to points in time This word lattice was prodused by the lexical retrieval component of the BBN speech understanding system from an utterance which had been begmented and labeled by hand under conditions desipned to simulate the performance of an automatic sepmrntar and labcler.
ten I people lard Qlass som~le I s l I and I d Figure 1.I mognetite 1 A Word Lattice Sentence: Give me all glass samples 'with magnetite.
lead not In the system described here, such a word lattice can be been . did represented by a collection of word matches, each of which is composed of a word, the boundary points at the left and right, ond I ends of the portion of the utterance where it appears to match y@ well, and a score indicating how well it matches the ideal I done did done phonemic representation of the word, We also make a number of assumptidns about the nature of the speech unde,rstanding process and the charactelristic,s of a system to carry out that process: 1.
People can understand ua speaker even when the speech .is fairly ung~amm~tical, so a syntax-driven system which would accept only input marbing rigid syntactic requirements would not be adequate for natural, converstional speech.
2. Since a number oY word candidates are likely to be found throu~hout the uttarkne, it may be fruitful to be able to select a subset of them on semantic, pragmatic, or prosodic grounds as well as syntactic, depending on which cues seem most robust.
3. Syntax must interact with semantics in order to cut down the combinatorial explosion of syntactically correct but meaningless subsets of the utterance.
Even in the small word lattice of Figure 1.1 It can be seen that there are numerous short sequences which are syntactically but not semantically valid (tr .g.
"Ten people are glass samples with magnetiteff, "glas-s samples give magnetitew, tllunar samples give magnetitetf, "samples give leadn, "people are percentif, etc.
) . 4.
The input to a speech parser will be similar to the word lattice described above, thus the parser will have to face not only the problem that one or more words in its input mipht be incorrect, but that gaps may appear in the input as well.
5. The parser will have to have the ability to predict words and syntactic classes which are consistent with partial hypotheses about the content of the sentence in order to help fill gaps in the lattice.
6. Because of the combinatorial explosion of syntactic Page 6 alternatives which occurs when all syntactic possibilities are explored for small sections of an utterance, the syntactic component must limit the number of such alternatives which are actually generated, or at least factor them or treat them implicitly rather than explicitly.
One way of partially solving this problem is to order the alternatives in such a way that only the best alternatives ape extended.
Section 2 The BBN Speech Understandin3 System In the past few years there has been a flurry of activity in the field of automatic speech understanding, resulting in a number of different systems.
For surveys of a number of these systems the reader is recommended to wolf 1311, Bates[4], and Hyde [lo].
Fgr more specific details on soma of the individual systems, sea [I, 2, 7, 8, 16, 19, 20, 21, 22, 28, 29, 33, 351.
Since SPARSER was implemented as part of a speech understanding system called SPEECHLIS which is under development at Bolt beranek and Newman Inc., that system is briefly described hare and is further documeatad in [3, 4, 5, 6, 15, 23, 24, 26, 33 9 351.
SPEECHLIS has used two task domains; that of the LUNAR text question-answering system 1361 which deals with chemical analyses of Apollo 11 moon rocks and one dealing with travel budget management.
The overall design of the systefn is illustrated in Figure 2.4.
The acoustics component anaJyzes the acoustic signal to extract features and segment the utterance into a iattice of alternative possible sequences of phonemes t~chwartz and Makhoul [26]), phonological rules augment the output of the acoustic component to include sequences of phonemes which could have reaul ted in the observed phonemes; the lexical retrieval componant retrieve8 words from the lexicon on tho basis of this information (Rovner, et.al.
[24]); the word matcher determines the dr~rea to which the ideal phonetic spelling of a given word matches the acoustic analysis at a particular looation [24].
All of these components structure their output in such a way bs to represent the ambiguity whi;*l is inherent in their analyses.
For example, they can be used to produce word lattices such as that which was shown in Figure 1.1.
The syntactic component is SPARSER, the system comprising the body of this paper (see also Bates [3, 41).
Acceptable utterances are not restricted to context-free syntax, since the grammar which SPARSER uses is a rnoqifiad ATN grammar, capable of handling a large, natural subset of English.
The remaining sections of this thesis detail the structure and operation of SPARSER.
The semantic component uses a semantic network to associate semantically related words and to judge the meaningfulness of a hypothesized interpretation (See Nash-Wabber [15]).
This semantic formalism is very-geqoral although a new network must be constructed for each new task domain.
MATCH I Page 8 SYNTAX (SPARSER) J LEXICAL RETRIEVAL CONTROL SEMANTICS ACOUSTICS u Figure 2.4 Design of BBN SPEECHLIS PRAGMATICS n The pragmatics component is current19 being implemented, but is projected to contain information about the past dialogue, a model of the user, and other pragmatic data (sat?
Bruce [6]).
control component contains an overall strategy for employing the other components in order to obtain an interpretation of an utterance (see Rovnsr, et a1 D31) . It decides which csmponent is to be called, what input it is to be given, and what is to be done with the output.
It sets thresholds on word match quality.
It combines the scores produced by the other components in order to rank competing hypothesies, and is the primary interface to all other components.
Section 3 The Grammar We have chosen the Augmented Transition Network formalism [32] for the grammar which drives SPARSER because it is a representation which allows merging of common portions of the analysis, it is amenable to both bottom up and top down parsina techniques, it fairly clearly separates the use of local information from infoflmation which was obtained from a distant portion of the utterance and, the author s previous experience with a large ATW .grammar for parsing text laid the grouudwgrk for the development of a similar grammar for speech.
We have tried as much as possible to keep the formalism which was developed by Woods intact, but some chances have been necessary or desirable to make the grammar more amenable to the speech parser.
We call the formalism a Modifi& Au~mentdd Transitiom Network (MATN), and assert that it has the same power as the original ATN formalism.
The changes, are briefly indicated here.
For a fuller discussion, see Bates [4].
Every arc of an ordinary ATN has a test component, which may be any predicate.
It is usually a boolean combination of tests on the current input word (its features, etc).
and the contents of registers which have been sat by actions on previous arcs.
In the MATN formalism, the test component of each arc is, on all but the PUSH arc, a list of two teats.
The first is a test on the Page 10 current word and its features, i.e. a local, context-free test.
The second is a test on the register contents, i.e. a context-sensitive teat.
Both tests must succeed for the arc to be taken.
The reason for splitting up the testa in this wav is that register checking tests cannot be made unless the repisters are sat, apd in mally situations in the speech environment there may not be enou~h left context to puarantee that the prcper r*egisttbrs would be set.
Thua it is useful to be a to evaluate the context-free test on an arc at a different time in the parsing Rrocess from the context-sensitive one.
On PUSH arcs, there are the types of tests which a used.
It is useful and efficient to test the next word of input before actually doing the PUSH, to sea, for example, if the next word can begin a constituent of the ty~e being PUSHed for.
This test is called a look-ahead test, and takes the place of the normal context-free test in the test component of the arc.
Them is also the usual context-sensitive test on repisters ;rhich were set before the PUSH arc was el-]countered.
And finally, when the PIISH arc returns with a constituent, another context-free tedt may be done on the structure of the entire constituent.
Therefore, the test component of a PUSH arc is a list o.f the three tests iust desaribed.
SENDR s were an efficient mechanism for text parsing because they allowed tests to be made on a lower level which involved information obtained somewhere (possibly fqr) to the left in the input string -information which would normally be inaccessible Page 11 beoause it would be hidden on the stack during the parsing of sub-constituents.
There are apveral reasons for not allowing this mechanism in the speech parser Suppose, in the input that 1-oaks like If..
. " the word "personff is not the word the person who.
travels . . ., which was really uttered.
If it were allowed to be passed down it would become an integral part of the analysis at the lower level, and if another word were to be hypothesized in its place, the lower level the analysis would have to be redone even if none of the words in the relative clause had been ch'anged.
Thfs is a process which would be extremely wasteful, especially in tho speech environment where one wants to be able to take as much advantage a3 possible of information which was gained at one point and slightly altered at another.
In particular, it is advantageous to consider as constituents such constructions as relative clauses so that they can be placed in a well-formed-substring table for use by other probesses.
Another reason i that some typdv of verifications (semantic, prosodic, and pragmatic, at least) can be done most conveniently on portions of an utterance which have been assigned a syntactic structure, i.e. on constituents.
If a portion of an utterance is parsed (e.g!
"that I gave youff from the complete utterance "The book that I gave youf1) but doas not form a complete constituent because it is missing a piece of information from a higher constituent to the left which would have bean sent down had it been available, then these verifications may not be made until the missing word or words are identified.
Yet it may Page 12 be important to build and verify the constituent in order to predict the missing word to tho left.
Therefore, it is better to allow constituents to be built without information which would normally have been passed down When parsing possibly inoorrect fragments with little or no left context, it is brat to keep constituents a3 small and as independent as possible.
The conversion process from an ATN eramrnar to a MATN prammar with rapard to SENDR s is straiffhtforwardand infalvrs the use of a dummy symbol which is used in the construction of the lower Level constituent.
When the structure is popped, the PUSH arc examines it for agreement and may replace the dummy node by the appropriate item which would have baeb sent down.
The structure returned by the PUSH for a relative clause on the fragment "that I gave youw might look like Figure 3.1 (where thd structure is shown in both the usual tree diagram form and a corresponding form more amenable to computer output).
S REL S NP PRO I FEATS NU SG AUX TNS PAST VP V GIVE NP **NP** PP PREP TO NP PRO YOU FEATS S+NP** YOU NU I Figure 3.1 SG Two Representations of a Parse Tree Page 13 The fourth element of every arc in a MATN is a small integer whlch is called the wei~ht of the arc.
This weight was originally conceived of as a rough measure of either (a) how likely the arc is to be taken when the parser is in that state or (b) how much information is likely to be gained from taking this arc, i.e. whether the parse path will block quickly if the arc is wronn.
That these two schemes are not equivalent can be seen by the following example.
In a given state, say just after the maln verb of the sentence has bean found, the arc which accepts a particle may be much less likely than the arc whdch Jumps to another state to look for complements.
However if a particle which agrees with the verb is found in the input stream at this point, then the particle arc is more likely to be correct.
Since it is not at all clear how to measure or even inbuit how much inforrnatiop is likely to be gained from taking an arc, it was decided that the weights would reflect relative likelihoods.
The actual weights which have been used in the speech grammar reflect an Tntuitive, though experienced guess as to how likely the arc is to be correct if it is taken, assuming the state itself is on the corr2ct path.
Two grammars which will figure predominantly in the remainder of this paper have been written in the MATN formalism.
One is an extensive grammar which can handle,many questions, declaratives, noun phrase utterances, imperatives, active and passive forms, relative clauses (reduced and unreducad), complements, simple quantifiers, noun-noun modifiers, varb-particle constructions, numbers, and dates (but not conjunctions).
It began as a modification of the grammar for the Page 14 LUNAR system [361 but has been considerably adapted and expanded.
This grammar is.
called SPEECHGRAMMAR, and is listed ina[4].
Exampled are given below which ware produced using this pramnar.
For some illustrative purposes, SPEECHGRAMMAR ip too hir nnJ complex, so we have produced a UINIGHAEIMAR which 1 be ~IF~V to show the basic operation of the speech parser.
A detailed 3 listing is given in Appendix I, but the diagram In Flpu~e ?... probably shows the structure mor-c clearlv.
The serious rrodrr is encouraged to sketch n ccpy of this grammar for r-rfrrencc later on.
CAT AD3 CAT N PUSH PP/ PUSH NP/ POP PP/PREP PP/ NP Since the work reported here was finished, the author has written another grammar, called SMALLGRAM which uses the 1IATN formalism but which embodies a great deal of semantic and pragmatic, information specific to the domain of discourse currently baing used by the BBN speech understand in^ project.
Page 15 In or for the parser der to move from right to left (to predict what could precede that first given word), it must be able to determine for any state which arcs can enter it, and for any arc which state it comes from.
Since the ramm mar is organized for normal parsing in just the oppoeite fashion, i.e. for any state one can determine what arcs leave it and for any arc (except POP) one can determine which state it terminates on, it was necessary to build an index into the granlmar.
This index consists of a number of tables centaining pre-computed informationwhich in effect inverts the grammar.
Section 4 Uverview of SPARSER The input to SPARSER is assumed to be a set of words together with their boundary points (which may or may not be related to points in time).
A word together with its boundaries Is termed a word match.
A word match also includes a score which indicates how well the ideal phonemic representation of the word matched the acoustic analysib of the utterance (but as we shall see the parser has little need of this information).
Since the same word may match at several sets of boundary points or may match in deveral ways between the same boundary points, each word ~t~~ is also given a unique number to help identify it.
Thus the structure for a basic word match is: (number word leftboundary rightboundary lexicalscore) e.8.
(4 TRAVEL 5 11 94), or (4 TRAVEL 5 11 (94 110)) where the score is given as a pair of numbers representing the actual and maximum scores, or (4 TRAVEL 5 11) where the score is omitted.
How is the input to the parser to be constructed?
We assume that acoustic processing and lexical scanning components can operate on a digitized waveform to produce a number of word matches such as prev.iously shown in the word lattice of Figure 1.1.
(That this is possible has bean demonstrated by Woods [33]).
Allowing the parser to operate unrestricted on the entire word lattice would probably not be fruitful because of the large numbe~ of locally syntactically correct combinations of words, but one possibility for input to the parser would be to take a set of the best-matching, non-overlapping word matches in the lattice, such as those in Figure 4.1.
A set of non-overlapping word matches is a hypothesis about the content of the utterance.
In order to avoid creating large numbers of such sets which are put together combinatorially with no basis except local acoustic match, semantic or pragmatic processes can be used to group word matches based on what is meaningful or likely to be heard.
For example, if a dialogue has been about various nickel compounds, the combination "nickel analysesw may be more likely than "chemical analysesff even though the word match for 'tchemicalff has a higher score than that for mnickelfff'.
We will not attempt to detail here how this semantic grouping could.be done and how the sets could be scored, since it has been described elsewhere [15].
DO MANY PEOPLE DONE CHEMICAL ANALYSES ROCK 0 2 6 11 14 22 30 35 38 GIVE EIGHTY PEOPLE DONE TEN MODAL DETERMINATION ROCK 0 3 6 11 14 15 18 21 26 35 38 WERE ANY PEOPLE METAL SEVEN 0 3 6 11 17 21 27 32 Fijiure 4.1 Sample Word Match Sets Using more terminology from the BBN speech system, the word theorv to denotes a set of word matches such as we have just described together with (possibly empty) slots for information from each of the possible knowledge sources in the system.
From the point of view.of SPARSER, usually only the word match portion of a theory is of fnterest, hence we shall fall into the habit of using the word "theoryv to refer to the word match set it contains.
When speaking of the syntactic component of a theory, however, we are refering to the information slot for syntax whicn accompanies each word match set.
Theories have the fallowing characteristics: 1) They contain a set of basic, nondverlapping word matches.
2) They tend at first to contain long content words and not many shdrt function words.
This is because long words are more reliably acoustically verified and content words are easier to Page 18 relate semantically and pragmatically.
Since small words such as tam r~d~v, rrtherl, rr~nall, "have", rr Of 11 11 in rr I T, etc.
may be reprssented by very little acaQstic information, they would tend to match at many places in the utterance where they do not really occur.
Consequently they ase not searched for? by the initial word match scan, nor are they proposed in the semantic stages of hypothesis formation.
3) They need not (and generally do not) completely span the utterance, but have numerous gaps of va~ving sizes (a.p.
for the function words).
4) They tend to contain some sequences of contiguous word matches.
Such a sequence is called an island.
That such a set of theories can be created has been demonstrated by the BBN SPEECHLIS system.
?he syntactic component, SPARSER, is expected to process these theories one at a time.
In certain circumstances which will be detailed later, the input to SPARSER will be a theory together with one or more word matches which are to be added in order to create a new larger theory which is then to be syntactically analyzed.
We will assume that there exists a cantrol component which presents SPARSER with theories to process and to which SPARSER can communicate predictions and results.
Preliminaries Given a theory, what is to be done with it?
We begin by considering a subset of the question: Given an island of word matches, what is to be done with it?
The answer is to create one Page 19 or more parse patbn through tho island and to predict what words or syntactic classes could surround the island.
A parse path is tho Sequehce of arcs in the grammar which would be usad by a conventional ATN parser to process the words in the island, if the island were embedded in a complete sentence.
For example, consider the way a parser might process an island of word match'es such as (1 CHEMICAL 14 22) (2 ANALYSES 22 30) using the MINIGRAMMAR of the previous section.
Beginning in state NP/ of the grammar (omitting for the moment the problem of how it is known that NP/ is the rieht place to begin) the sequence of s arcs which would bh taken to parse "chemical analysesw as a noun phrase is that shown below in Figure 4.2.
JUMP JUMP POP Figure 4.2 Portion of MINIGRAMMAR needed to parse %hemica1 analysesm Let us define a confiatmation to be a representation of the parser being in a given state (say NP/QUANT) at a given point in the utterance (say 14).
We will write configurations as STATE:POSITION in text (e.g.
NP/QUANT:14) and schematically as a box within yhich are written the state and the position.
If a configuration represents a state which is either the initial state of the grammar or a stake which can be PUSHed to (i.e a 4 state which can begin the parsing of a constituent), it is called Page 20 8rl initiah configuration, and is indicated schematically by a filled-id semi-circle attached to the left edge of the box.
Note that a confi~uration N~/QUANT:I~ is quite distinct from a configuration NP/QUANT:22 since they are at different positions in the input.
In SPARSER, each configuration .is also assigned a unique number which is a convenient inte~nal pointer.
The process of traversin~ an arc of the grammar using a particular word is represented by a transition from one config.uration to another.
A transition ean be made only if the arc type is compatible with the current item of input and if the context-free test on the arc is satisfied.
(The context-sensitive tests are evaluated later).
A transition carries with it information about the arc which it represents and the item of input it uses.
The item of input is usually the word match which the arc uses, but it is NIL in cases such as JUMF arcs which do not use input, and it is a complete constituent fop PUSH ams.
A unique identifying number and the list of features, if any, which is associated with the input word or constituent are el30 recorded on the transition in SPARSER, but they are not shown schematically.
A transition is represented schematically by an arrow from one configuration to a~~other with an abbreviated form of the arc written above the arrow and the item of input under it.
The syntactic part of any theory which SPARSER processes contains, among other things, lists of the transitions and configurations which are created or used by the theory.
Thus wheh we talk about creating a configuration or transition it is implicitly understood that SPARSER also adds it to the appropriate list, and when we talk of adding an ax is tin^ configuration or transition to a theory we mean adding it to the npprbpriate list.
Therefore, removing a confi~uration or transition from a theory means removing it from the syntactic part of the thebry, not removing it entirely from SPARSER s data base.
Like confi~urations, transitions are unique, so only one transition is ever constructed from point A to point B for arc X and input Y.
We will frequently speak of creating a transition or a configuration, but the reader must bear in mind that if such a confi~uration or transition already exists, this fact will be recognized and the pre-existing configuration or transition will be used.
(Timing, measurements indicate that it takes about,052 seconds to create a configuration and only .01 seconds to test if a particular configuration already exists.
For transitions, creation takes about .54 seconds and recognition .012 seconds.
The sequence of configurations and transitions which would parse the above example is displayed in Figure 4.3.
A conpected sequence of transitions and configurations is called a at.
If the sequence begins with an initial configuration and ends with a transition representing a POP arc, POP NIL Figure 4.3 Path for parsing lfchemical analysesv I NP/ADJ 14 NP/ JUMP NP/ART JUMP 14 NIL CAT N CHEMICAL CAT N ANALYSES' NP/ADJ 22 NP/QUANT 14 1 NP/N 30 JUMP, NIL Page 22 it is a complete path, otherwise it is a partiaL path.
Paths are assumed to be partial unless otherwise specified.
darrinnina Paraa a Island SPARSER processes an island of words by beginning with the leftmost word and determining its possible parts of speech.
Then the arcs of the grammar which can process the word arc fpund (by looking in the previoRdly constructed Rrammar index).
For each arc, two confi~urations are constructed one for the state at the tail of the arc and one for the state at the head, using th$ left and right boundary positions of the word match, respectively, and a transition for that arc using the current word match is also built.
Schematically, we have for our example a situation which looks like that of Figure 4.4 (such a display of all or some of the transitions and co~fi.qul*at ions which the parser has constructed is called a map).
Notice that a configuration may have any number of transitions entering or leaving it.
Figure 4.4 Initial map for parsing llchemical analysestr Page 23 The idea of this process is to begin t6 set up paths which my be used to parse the island.
However it is not necessarily the case that the only donfigurations which could start paths throu~h the island are those which have just been obtained, since it may be possible to oreatr transitions which enter them via JUMP arcs or TST ahca.
For each state, the sequence of arcs which can reach it without using the previous word of input have bean be pre-calculated by the grammar indexing package so the appropriate configurations and transitions may be constructed.
These transitions ape cplled lead-in transitions.
Thus the map becomes that in Figure 4.5 Note that any of the configurations (except for NP/ADJ:22 22 J J* and NP/N:22) could actually be the correct leftmost configuration for this island, depending upon what the (currently unknown) left NP/ JUMP NPIART JUMP NWQUANT 14 NIL* 14 14 b context of the island is.
'NP/N Figure 4.5 22 b Lead-in transitions for parsing llchemical analysesn JUMP NIL NP/ADJ 14 By looking in the grammar index, SPARSER can determine, for t~c each configuration which could start the island, just what sort of left context could be appropriate.
For example, th'e CAT ADJ arc in MINIGRAMMAR which enters state NP/QUANT implies that an adjective could precede the island and, if it did, the tra~sition which would proceas it would terminate on configuration NP/ADJ:14, Because the initial configuration NP/:14 could start the isiand, anything which could precede n noun phrase could occur to the left; again the grammar index provides the information that the CAT PREP arc could lead to a configuration which could accept a noun phrase (via the PUSH NP/ arc), so a preposition could also prefix the island.
If the index functions indicate that a constituent could be picked up by a PUSH arc which could terminate on the configuration under consideration, an indication is made in tho WFST so that any time a constituent of the desired type is built which ends at the proper location, it may be tried here.
Because of the highly recursive nature of ATN grammars, it is vary likely that as we chain back through the possible sequences of PUSHas which could lead to tho beginning of tho current constituent (or the seauence of POPS which could be initiated by the completion of the current constituent) a large number of predictions will be made.
Rather than make all these predictions automatically, beford we are even sure that there is in fact a constituent at the current level, the possible configurations which could make predictions on other levels are saved to be activated later if the predictions from the current set of active configurations are not sufficient.
Page 25 The predictions which are made (not saved) are not acted upon at this time, but ard kept internally by SPARSER until all the islands of the theory have been prooessed.
We shall see bdew what then becomes of the predictions.
Island Once proceasing has proceeded this far, we can go back and consider the set of configurations which represent states the parser could be in just after processing the first word of the island.
In our example, these are configwatiqns WP/ADJ:22 and NP/N:22.
Configurations such as those whlch are waiting to be extended to the right are called active configurations.
SPARSER selects a subset of the set of active co'nfi~urations (how this subset is selected will be discussed in the next section) and for each configuration tries to extend it by tryin8 to parse the rest of the island beginning in that confipuration.
When the parser is considerin~ a configuration at some position, the input pointer is set to the word match of the island, if any, which begins ah the same position in the input.
The grammar associates with the state of the configuration a list of arcs which may be tested (using the arc type, the context free test on the arc, and the Current input) to determine whether a transition can be made to extend the path.
We will consider each type of arc in turn, since the effects of taking various types of arca are different, and explain for each case what happens if the arc is taken.
Whether just one transition, or several, or all possible transitions are made from an active Page 26 conf,iguration is a matter to be discussed in Section Five.
Soma JUMP arcs do not look at the current item, so they may be taken whether the input pointer is set to a word match or to NIL.
The transition which results from taking an arc of this type has a null item associated with it, even if there is a word match in the theory at this point.
The positions of the confipurations at each end of the transition are the same; this corresponds to the fact that an ATN parser would not move the input pointer as a,consequence of taking this arc.
Rarely, a JUMP arc may test the current iten in some way, for example, to make a feature check.
If there is no word match for input, an arc of this type cannot be taken.
If there is a word match, it is noted on the trahsition wh ch is created, but the configurations at each end of the transition have the same posit ion.
(It is than the case that thainext input-using or input-consuming transition on the path including this transition must use the same word match).
These are TST, CAT, and WRD arcs which end in a (TO nextstate) action.
The operation is exactly tho same as that above except that the configuration on which the transition terminates has the position of the right boundary of the current word match.
Taking a POP arc results in the creation of a transition which has a null final configuration and a null item, because POP arcs are not permited to look at input.
Page 27 When a PUSH arc is encountered, a monitor is placed in the Well-Formed Substring Table (WFST) at the current Dosition to await the occurrence of a constituent of the required type.
If one or nore such constituents are already in the table, then for each one there are three possibilities: it may be composed of word matches which are in the current theory, it may be composed of word matches some of which are not in the current theory but which could be added without violating the non-overlapping constraint, or it may be composed of word matches some of which are incompatibld with the current theory.
In the first caae a transition is set up using the constibuent as the current word.
The transition terminates on a confleuration whose state is determined from the termination of --.
the PUSH arc and whose position is that of the right boundary of the rightmost word match in the constituent.
In the second case, a notice is created and sent to the control component.
A notice is a request that SPARSER be called to enlarge a theory by addinp some new information, in this case, some additional word matches which form a constituent that the theory can use.
SPARSER does not try to determine when (or even whether) the theory should be so enlarged.
That is an issue for the main cqntroller to decide (see Rovner, et.al.
C231). We will discuss below how SPARSER enlarges a theory if called upon to do so.
In the final case, if there are no usable cohstituents in the WFST, a new configuration is set up to start looking for one and is added to the list of active configurations.
Its state is Page 28 the state specified by the PUSH arc and its position is the same as the current configuration.
There is a considerable amount of processinc that can happen any time one of the transitions lust discussed is rnndr.
Whenever% an initial configuration is constructed, this fact is r*t)c@rded in the configuration.
Whenever a transition is nade from such a confipuration, the information that there is a path I sene initial conf igur-at ion is recorded on the subsoquenf configuration.
Similarly, whenever a POP tr-ansition is made, the c~n~figuration it emanates from and all previotls configuratims on any path which can terminate with the POF transition are marked to indicate that they can reach a POP transition.
Whenever a transition is made which completes a path from an initial configuration to a POP transition, the path is executed, one transition at a time, and the rqister setting actions and context sensitive tests are executed.
If a test fails or an arc aborts, the transitions and configurations f the path are removed from the list of configurati~ns and transitions which are in the syntactic part of the currant thaory (unless they are used by another path in the theory) but not removed from the nap.
If the execution is successful, a deep structure tree is produced.
That structure together with its features is given a score, whioh may include evaluations by other cornpodants such as semantics and prosodies, and is entered in the WFST.
It is quite important that sources of knowledge other than syntax be called upon to verify and to rank syntactic constituents.
This is because there are likely to be many Page 29 ombinations of plausible words from the word lattice which form ayntact%rally reasonable constituents but which may be ruled out om @%her grounds.
To allow immediate use of this information which syntax cannot provide alone, SPARSER has an interface to he semantic component so that constituents can be vrrif ied dinctlg without going through the control component.
It will be %trivial modification to insert verification aalls to pragmatics aad prosodies when they become available.
In the meantime, even semantic knowledge can be turned off; if the parser gets no IngosnratIon from the call to semantics, it proceeds without it.
Plaoement of a constituent in the WFST causes a number of Lbiags to happen.
First, any monitors which have been set by the mmnt theory at that position aFe activated.
That is, for each ebnfigaration which was waiting for this constituent, a PUSH tmaaitioa 3s made which uses the constituent as its input item.
If ao rmonltors have been set which can use this constituent, it as treated exactly as if it were the first word of an island: a11 the PUSH arcs which can use it are found in the grammar index md appropriate configurations and transitions (including lead-in tnositions, if appropriate) are set up.
Next, if there are any monitors for other theories which can use the constituent, patfees are created and output to Control as was described above is the section on PUSH transitiohs.
Figure 4.6 shows SPARSER s map after our example island has ken completely processed.
The parsing results-in the creation oi a CAT II transitio~ to configuration NP/N:30 using the word *~~alyses~ The PUSH PP/ arc at state NP/N would oause configuration PP/:30 to be created.
Similarly, PP/:22 would be created when tho configuration NP/N:22 is picked up to be extended.
The POP arc transitions from each of the configurations for state NP/N result in the formation of complete paths, resulting in the creation of two noun phrases ("chemical analysesw and ttchamicalft).
Since there were no monitors for them, they result in the creation of configuration PP/PREP:14 and $8 subsequent paths.
Figure 4.6 Map after processing island 7 w NPIADJ CATN NP/N POP 22 ANALYSES.
30 NP/ JUMP 14 NIL N P/AD J 30 NP/ART 14 JUMP MIL NPIQUANT 14 JUMP NP/ADJ NIL 14 Page 31 Endinq an Island It may be the case that no path can be found from one end of an island to the other, (This would occur when all active configurations block).
In this case, there is no possible way that the island could form part of a grammatical string, so SPARSER can inform the control component that the theory is wrong, When an active configuration is picked up to be extended and there is no word match at that point, the end of the island has been reached.
That does not mean that no more transitions can be made, since arcs which do not test the input word can be taken as usual.
Arcs which do use input cannot be taken, but they can be used to predict what sort of input would be acceptable at that position.
For example, a GAT V arc which has a test requiring the verb to be untansed would allow SPARSER to predict an untensed verb beginning at the position of the ourrent configuration.
CAT and WRD arcs cause the prediction of syntactic categories and specific words, respectively, modified by the Context-free test on the arc.
TST arcs provide only the test which must be satisfied, and PUSH arcs cause a monitor to be set in the WFST as well as a TST monitor for the the look-ahead test (if any) on the arc.
Bndinq 3 Theorv When k11 the islands of a theory have been processed in the manner just described, it is time to deal with the gaps between the islands.
As we have seen, arcs in the grammar which can Page 32 enter configurations at the left end of an island or which can leave configurations at the right and of an island can be used to make predictions about warda that may be adjacent to the island.
The prediction is a list of the arc, the confi~uration it would connect to, and an indication of whether the transition caused by the arc will enter the configuration from the left or leave it to the right.
If a gap between two island8 is small enou~h that it may contain just one word, than it is likely that tho arc which would process that word may have caused a prediction from both tho left and right sides of the gap.
If this is the case, and if the predictions intersect in a single possibility, it is highly probable that the word (or syntactic class) so predicted is correct.
If the predictions do not intersdct, parsing is continued from the active cbnfigurations which were not triad earlier because of their scores and from the configurations which could begin constituents at the right and of an island.
This continued parsing is an attempt to find a path which results in a common prediction acg#ss the gap.
If that too fails, then the configurations which were saved because they could lead up a chain of PUSHes or POPS to new configurations are triad.
If no possibilities are left to try and there is still no prediction to fill the gap, this information is noted, but it does not definitely mean that the islands are incompatible, since in some cases the gap could actually be filled by two words instead of one.
SPARSER has two kind of predictions those bhioh seem highly likely and those which seemaless likely.
A highly likely prediction, such as one which is made from both side3 of a small gap, is output in the form of a prooosal, which is a request to the rest of the system to find a word meeting the requirements of the proposal.
A proposal contains: 1) the item being proposed, which is either a particular word or list of words (from a WAD arc), or a syntactic olass (from a CAT arc), @r NIL, meaning any word (from a TSI a~c) 2) tho loft and/or right boundary pointb) of the item 3) a test which the item must satisfy (the context free test from bhe arc) 4) the context of the proposal, i.e. the word match(es) on the left apd/or right side of the item baing proposed.
(This is to help the lexical retrieval component take into acceunt phonological phenomena which may occur across word boundaries).
All predictions whether or not they are confident enough to become proppsals are output oas monitoys.
A monitor is a notification to the control component that if a word meeting the requirements of the monitor is somehow found (perhaps by the action of a proposal), it may be added to the theory.
Thus a monitor acts like a demon which sits at a particular point in the word lattice and watches for the appearance of a word match which it can use.
A monitor contains: 1) the item being monitored for (generally a syntactic categcry, but may be a word or a test) 2) the left or right boundary position of the item baing monitored far 3) a.
test which the item must satisfy (same as for proposals) 4) the thaory which generated the sonitor 5) the arc in the grammar which will process the item if found 6) the configuration from which the prediction was made 7) a score, indicating roughly how important the monitor is, i.e. how much information is likely to be gained by processing an event for that monitor.
(Notice that monitors which are sent to the control component are very much like monitors which are set in the WFST by the occurrence of PUSH arcs).
Once the proposals have been made and the monitors have been set., SPARSER bundles up the information it knows about the current theory, such as the configurations and transitions in the theory, any configurations which a still candidates for expansion, the constituents in the theory, the notices, proposals, and monitors which have been created, etc.
and associates the bundle with the thaory number.
This insures that SPARSER will be able to pickup where it left off if it is later given the thmry to process further.
Processinq Mult i~le Theories Thus far we have seen only the opSrations which SPARSER performs on a single theory, but we made the assumption that SPARSER would be given a number of theories to process in sequence.
Let us now examine what will happen when the second (or nth) tkeo~y is processed.
Page 35 SPARSER will no longer have a blank map and WFST; instead it will have all the configurations, transitions, and constituents which have been constructed by all previous theories.
For concreteness, let us imagine that the theory (1 CHEMICAL 14 22) (2 ANALYSES 22 30) has been processed, result in^ in the map shown in Figure 4.6.
Now we are going to process a theory containin8 the island (4 NICKEL 16 22) (2 ANALYSES 22 30), which results in the map of Figure 4.7 where the configurations and transitions added by this theory are shown in dotted lines.
The process bagina as usual with the creation of conf,iguration #P/ADJ:16 and three possible lead-in tranbitions.
Tho transitions for the two CAT N arcs, however terminate on configurations which already existed in the map, so the complete paths from configuration NP/:16 to configurations NP/N:30 and NP/N:22 will be discovered and processed, resulting in the construction of two new noun phrases.
Those new constituents would then result in the creation of configuration PP/PREP:16 and two new transitions.
Thus we have constructed only five new configurations and seven new transitions and have been able to take advantage of six old configurations and six old transitions.
In this fash-ion any information which has once been discovered about a possible parse path is made available to any other path which can use it.
No reparsinq is ever done SPARSER merely realizes the existewe of relevant configurations and transitions and incorporates them into the current theory.
NP/ADJ CATN NP/Npgp 22 ANALYSES? 30 NPI ~UMP* NPART JUMP.
NPIQUANT JUMP NPIADJ #' NIL ' 0 14 14 NIL 14 > NP/ADJ 30 ..
I---1 ' NP/N ' POPI NWADJ t I CAJJ?*--16 'N\C(ICKEL +!J L--,,J Figure 4.7 Map after processing islaad for "nickel analysesif If the new word (or wokds) in a theory are at the and (or in the middle) of an i$land, when SPARSER begins to parse the island it will discover the existing configurations and transitions from the previous theory.
Whenever a transition which can be used in the current theory is discovered in the map, it and its 'terminating configuration are added to the syntactic part of the current theory.
This is callad tracinq the transition.
In addition, all paths beginning with that transition which do not require the next word of input are also included in the syntactic part of the theory.
This is accomplished by tracing from the terminating configuration all transitions which use either tho same word of input as the previous transition or no input word at all.
(A similar process 19 used to trace backwards, i.e. right to left, when neessary).
Uhen a configuration is reached which has no traceable transitions emanating from it, the tracing process, stops.
Since both transitions and configurations are stored in such a way as to facilitate tracin~ (for example, each transition has a code attached to indicate whether or not it consumes or tests input), this process is considerably faster than creating that portion of the map in the first place.
(To illustrate this, a theory was processed twice, onca with an empty map and onca start in^ with the map previously created; the time required for processing the theory fall from 47.5 seconds td 16.5).
Configurations which can end trvaced paths are put on the active conCigurations list.
If, when one oP them is picked up for extension, it is discovered that the next word of input was used on a transition already in the map, the tracin~ process is repeated.
If the next word of in~ut is new (or at least has not caused any transitions from thi? con fipurat ion beinp considered ) then para in^ continues in' the normal manner.
Processih~ Events As-was mentioned earlier, SPARSER can be called upon to add some new word matches to a theory it has previously processed.
In this case, SPARSER is said to process an event.
An avant may be thought of rather abstractly as the discovery of a piece of information that has been syntactically proposed, monitored for, Page 38 or noticed.
Concretely, an event is a piece of data consistina of: 1) the old theory that proposed or set a monitor far the event 2) something to be added to the theory (a new ward natch or constituent) 4) the arc in the Rramnar which will process the new information 4) the corifipuration ih the old tbrory which will be at one end of the transition created by the above arc When SPARSER is ~iven an event, it retrieves from its tables the bundle of configurations, transit ion, etc.
in the old theory.
Then using the arc and the new word or constituent in the event, it creates the appropriate transitioncs).
Then processing continues as usual, that is, any complete paths are noticed and processed, and any new active confi~urations are exbended, if possible.
New predictions may be made as a result of this increased information.
(A record is kept of previous predictions so none are remade unless with a more liberal score).
Finally SPARSER returns the nGw, larger theory.
This new thsory may be processed as part of another event at some later time, thus gradually reducing the number and size of the caps in the theory.
If an event results in filling the final gap in a theory, and if the resultinp complete sequence of words can be parsed, SPARSER notifies the control component of this fact, since the entire utterance may have been discovered.
Of course, this may not be the correct solution -it is up to the control component to look atm the acoustic ~oodness, semantic meaningfulness, pragmatic likelihood, etc.
of the result as well as the syntactic structure before daclarlng the utterance to have been understood.
If for reasons other tham syntactic, the utterance appears to be bad, the control component of the system could ~o on to try to find anothar, more suitable, possibility.
Section 5 More Details of the Parsinfi Process 5.1 DEPTH vs BREADTH The parsing strategy just outlined works bottom up when beg inn in^ to parse an island and when a constituent is created which was not monitored for by the current theory.
It works top dawn after an island has been started and to make syntactic predictions at the ends of islands.
Both top down and bottom up techniques can be either depth or breadth first.
Depth first processing takes at every step the ffl-st piece of information available and pursues its consequences.
Breadth first processing considers at every step every possible next step of every alternative and pursues all paths in parallel.
Breadth first processing generally takes much more space than the depth first many paths would have to be remembered at once Rags 40 instead of having just one stack which could be popped and reused when necessary.
The breadth first process mi~ht save some computation steps and might produce several ambiguous parsin~s simultaneously while tba depth first process would find one before the others (the latter is a'small difference, since both processes would have to be run to exhaustion to insure that all possible parsings had been found).
In parsing speech, some mixture of breadth first and depth first proc~ssing can be extremely useful.
To illustrate an advanta~e of breadth first processing in the speech environment, consider what might happen if, durinp the processin8 of an island the parser picks up a confirtiration to extend which has several possible arcs emanating from it.
If one arc is chosen and all the others are held as alternativss (imem depth first), but the chosen arc is wronp, all subsequent paths beg inn in^ with that arc would have to block before the alternatives would be tried.
However, if the end of the island were reached before.
the success or failure of the first choice were confirmsd, the only way that backup would ever take place would be to have one or more events add words to the thsory so that the path could be extended until it failed.
Since the pap wou.ld be likely to be filled by (incorrect) words predicted by the erroneous path, or by no words at all if the (incorrect) predictions were not satisfied, it is not at all clear how the process would aver know to back up.
This problem cannot be eliminated completely without pursuing all alternatives to thair Fulleat extent (a combinatorially unacceptable solution) but it can be modified to a praat extant by a judioious combination of depth and breadth first processing to find the best path, not just the first one, through the island.
This "bast pathw is not ~uaranteed to be the correct one, so it is possible to continue process in^ by extending paths with were suspended earlier.
SPARSER handles the problem by assigning a score &o every configuration which reflects the likelihood of the path which terminates on that confi~uration to be correct.
The score can also be thought of as a measure of how good that eonfiguration looks in relation to others as a candidate for extension.
One question which was previously left unanswered, how a subset of the active confisurations is chosen for extension, can now be answered : the subset of maximally scoring configurations is chosen at each step until the maximal score of act ivt! configurations begins to fall.
(The score on a configuration and the score of a path terminatinr on that configuration are the same thing -we will use which ever terminology seems most natural at the time).
The result of this process is a sort of modified breadth first approach, where at one step all the alternatives are tried but at the next step only th& best ones are chosen for further extension.
This is similar to the best-first parser describad by Paxton in [I81 but it can be applied to the sort of partial paths which SPARSER generates rather then requiring the perfect Page 42 information resulting from a strictly left to ripht approach.
The auccesg of this method is directly dapandent on the ralativz accuracy of the scores which are assigned to the paths.
5.2 SCORING
PATHS Sevdral attempts have b~en made to d*velop rigorous systems for parsing arrorful or spedch-like input baaed on probabilities [I, 14, 271.
These attempts have all simplified the problem to such an axtent that it is no lonper realistic or extendible, e.p. by assuming the input is a sequence (rather than a lattice) of probability distributions, by assuming that all the neclsbary information is present in the searhh space to begin with so the only problem is to find an optimal path throuph ths spacz, by requirine a small vocabula~y, and/or by limiting the Rranmar to be context free.
The ideal scoring mechanism for SPARSER would be one which accurately reflected at every step the probability that the path 8 correct.
Bayas rule could be Used, but it would ba necessary to know, at any point in ths parsing process, what the probability is that th~ next arc under consideration is correct, given that the entire path up to the current step is correct.
In order to use this application of Bayas rule it would be necessary to pra-calculate the probabilitiss for evary possible path and partial path which could be generated -a clearly impossible task sincs there are an inftnite number of such paths.
Givzn that we cannot calculate the probabilities we need exactly, what is the next best option?
If we ignore the effect of tila path traversed up to the current point, but can say for ray ~iven state how likely each arc em an at in^ from that state is to ba correct, we would have a model which uses only local Paforaation rather than one which takes into account accurately all1 tbe Left context which is available.
Since it was not practical to run large amounts of data tbm~h a parser in order to obtaid accurate measurements even Sor the limited model, the author re'lied on considerable experfeace with ATN grammars to assign a weight to each arc of tba grqmmar representing tho intutive likelihood that the arc fii it can be taken) is the correct cane to choose from that state.
These weights are small integers (0 throu~h 5) -the jarger the weight the more likely the arc.
The question might arise as to why the score oG the word sate4 used by an arc should not be used to influence the score of tbu path using it.
SPARSER tries to treat each theory as ladrependently as possible and tq assign scores based only on the syntactk information which is available.
The one exception to this rule is the semantic information which is used to score constituents.
If lexical Qord match scores were used, the cbmtrol component would not be able to separate the lexical goudwss from the syntactic goodness of the theory and make Judgments aa to their relative importancz.
In a syntax-driven apeecb understanding system, however, it would probably bz usaful to combine lexical scores with syntactic information.
Page 44 As was described in the previous section, when SPARSER begins to parse an island each possible partial path is begun by creating a configuration at the head of a transition for an arc which can use th@ current word.
Rather arbitrarily, it was decided to giva this confi~uratioq a score of one.
This starts all partial paths out equally, a technique which is not quite accuratb, since some contexts are more likely than othms.
For example, the words tttom and "forw are more likely to occur In prepositional phrases than in sentantial complements.
If this simplification appaarv to harm the overall performance of SPARSER, it coula be remadikcf by giving eaah state an a priori score similar to the weights on arcs.
Configurations on lead-id paths are also given a score of one.
After the initial step, whenaver a transition (othar than a PUSH or POP) is made, the score of the subsequent configuration is influenced by the score of the conf'i~uration being extended and the weight on the arc beins us$d.
If the scores were actual probabilities; they would be multiplied; since they are not, it was arbitrarily decided to add them.
When attempting to create a configuration which already exists (a situation encountered whenever two or more parse paths for the same theory merge), ths configuration is given the maximum of the sxisting score and the score which would haw been assigned had the configuration been created anew.
Whsn a PUSH arc is encountered and a configuration created to begin the search for the required constituznt, the score of that configuration is set to be the sum of the scora of the configuration causing the PUSH, and the value (If any) of the look-ahead test on the PUSH arc.
For example, upon encountarinr an arc such as (PUSH NP/ ((NPSTART) T T) ...
) the look-ahead function NPSTART returns a high integer valua ir the next word is a noun and a lowen valua if it is a verb (e.g.
ltaccounting coststt).
Of course, if tha look-ahead funcrion fails altogether, the c~nfipuration is not set up, althou~h hhe monitor in the WFST remains.
When a constituent is completed (or found in the WFST) end a PUSH tranaitiqn is about to be made, the score of the confi~uration on Ghich the transition terminates is a function ~f the score of the confi~uration heinc extended the weipht on the arc, an@ the score of the constituent itself.
The score of tht* constituent is currently very ad hoc, bdng a function of the number of words in th* constituent (lass a function of the number of sub-constituents subsumed by this constituent, boosted if the constituent is a mador one) and the score which is determined by semantic verification.
Thus semantically ''roodIf constituents will st the scores of the paths which use them more than semantically "badw ones.
Due to the level of effort required to gather accurate statistics on the relative frequencies of arcs, the current scores are admittedly ad hoc.
It is not clear whether different scoring mechanisms would be better, however it is clear that the current scoring strategy is better than no scoring at all, as praiiminary measurements indcate that the number of transitions created (as well as the number of confi~urations and predicions) is reduced about 25% by thz current strategy.
(It is rzasonable to ask why semantic scores are used to influance parse paths, sincd it was tust argued that lexical Ycores should not be ushd in this way Semantic scores may be more reliable than lexical ones because we are assuming that the utteran& is semantically maanibaful.
Under this ausumption, a constitudnt like "range remainder" as a noun-noun modifier analogous to ltaurplus moneyff should be ruled out as early as possible.
Since such con8tituents cannot be ruled out on syntactic rounds alone, since prosodic information (which might help to rule them out) is not available (see discussion in Section 7.2), and since they would seriouslv overrun the parser with a plethora of false paths if they wwe not reJected, it seems reasonable to permit semantics to influence the parser).
5.3 SCORING
PREDICTIONS The previous section discussed three ways in which SPARSER can make predictions about what could fill in gaps between islands.
Monitors wait for the occurrence of a word in the word lattice (or a constituent in the WFST), proposals request a search for a particular set of words, and notices indicate the presence of a usable word in the word lattice (or a constituent in the WFST).
Since the processing of a typical theory is likely to result in a number of predictions it is necessary t'o be able to order them so that predictions most likely to bz correct or most likely to yield important information will be acted upon first.
For example, it is more important to fill a Rap between two islands than to extend a sin~le islahd, since by filling the Rap one can chzck the consistency of information which was locally good in aach island individually but may not be consistent when they are joined.
Since two words can occur to~ethar in (usually) many contexts but lon~er szquances arl generally more restrictiv~, addine a word to a one word island is likely to be leas profitable in terms of the number a,f possible paths which are sliminatod by the addition than add in^ a word to a multi-word island.
It is up to the syntactic component to indicate to the control component the relative importance attachsd to each notica and monitor; the hipher the score, the stron~er the prediction.
Several factors influence the score attached to predictions.
One is the length of the island to which the prediction is attached.
One word islands, if they are processed at all, yield very little information anc many pradictions, bznca the predictions are not scored high.
Proposals are lass important if there is already a noticsable word in the word lattice (since that word is acoustically better than the word to be proposed, else it would have bean found earlier.
Howevsr, if a proposal fills a gap between two islands, it is given a higher score.
Notices are boosted in importance if an entire constituent may be added and penalized if they will add onto a one word island.
Scores range from 0 to 95 for proposals, 0 to 40 for notices, and 0 to 15 for monitors.
Page 48 Theas scores appear to work fairly well with the rest of the BEN SPEECHLIS system, but have been dcvalopod by a process of interaction with tha other components (in order to make the scores of syntactic prediction8 commensurate with those of semantic predictions) and may be chanazd considerably au the atire sy~tem evolves.
Small syntactic classes em.
detePml'naru and prepositions) are proposed in their entirety (that is, their elurnants are to be dnumrratad and ~ivdn to the lexical matchinc componlnt for verification) if the island which monitored for them is more than one word lon~.
If a gap batwsen two island3 is small enouch for iust one word and if a syntactic class has been monitored for L from both sides of the gap, it is propossd ir its entirety also.
Sect ion 6 Examples and Results SPAR3ER is written in INTERLISP and runs on a PDP-10 und~r the TENEX operating system . The program and initial data structures occupy approximately 90000 words of virtual memory.
(The other componants of the BBN speech undzrstandicp system occupy separate forks from the syntactic component).
Page 49 At the time the 'examples in this section were run, the al~orithm controlling tha dacision-making process in the control component was under~oing reviaion and was not solidified into a function which could operate automatically.
Rathar, there ware a number of primitlvd operations such as scanning an utterance (or some specified partion of it), creating thoorieu, call in^ SPARSER with a theory or event, calling f"or the processing of proposals, etc., which could be invoked by a human simulator The follow in^ examples were produced in this mode, with the user act in^ as the control component in a way which could be modelled by later imple,mentation.
Several convention8 have been used in tracin~ the operation of SPARSER.
Conf ieurations are rspresented as NUMBER : STATE : POSITION (SCORE).
For example, the configuration written as 30:NP/HEAD:23(39) is the confi~uration for state NP/HEAD at position 23 which has been given the (unique) numbe~ 30 and which currently has a score of 39.
The creation of a transition is indicated by naming the type of arc causing the transition, the (unique) number of the transition, and the configurations at each and of the transition.
For example, CAT N TRANS #9 FROM 14:~PhET:6(1) TO 15:NP/DET:19(4).
Annotations havc been inserted within brackets { 1; typeout in upper case was produced by the program.
Page 50 EXAMPLE 1 This example parallels that given in Section Four.
A word lattice was artificially created which contained only the following three word matches: (1 SUMMER 12 16 100) (2 WINTER 12 16 100) (3 TRIP 16 21 100 -S) (In this version or the system, ra~ular inflectional endings are included in word matches after the element representing the score, hence the somewhat peculiar word match for the word "trips").
Two theories were constructed, one for word matches 2 and 3, the other for 1 and 3.
What follows is an annotated (but otharwisc! unedit,ed except for considerations of spacing) transcript of SPARSER processing these two theories in sequence, using the MINIGRAMMAR of Figura 3.3 and Appendix I.
SPARSER PROCESSING THEORY'I: 0 12 WINTER 16 TRIP -S 21 30 (This is a linear representation of the thepry baing processed.
The endpoints are 0 and 30, but the words occupy-only the middle part of the utterance).
STARTING AN ISLAND "WINTER" TRYING CAT N ARC FROM NP/ADJ TO NP/ADJ {This is the first of two arcs retrieved from ths index tables ) CAT N TRANS 81 FROM 1:NP/ADJ:12(1) TO 2:NP/ADJ:16(3) (The first transition is created, and since th~re is,a CAT N arc which enters state NP/ADJ, a monitor is set up to monitor for nouns which end at position 12).
ENDING AT 12: MONITORING [ N 1 JUMP TRANS #2 FROM 3:NP/QUANT:12(1) TO 1:NP/ADJ:12(1) (Now the lead-in transitions are being created, along with the monitors far syntactic categories which may precede the newly constructed configurations.
Configurations along the lead-in path are all assigned a score of 1.3 MONITORING [ ADJ 1 JUMP TRANS #3 FROM 4:NP/ART: 12( 1 ) TO 3:NP/QUANT: 12( )?
ENDING AT 12: MONITORING [ ART 1 JUMP TRANS #4 FROM 5:NP/:12(1) TO 4:NP/ART:12(1) {The laad-in transitions are all made.
Now the second arc which can uae the noun ia about to be processed).
"WINTER" TRYING CAT N ARC FROM NP/ADJ TO NP/N CAT N TRANS #5 FROM 1:NP/APJ:12(1) TO 6:NP/N:16(6) {This is tha second of thd two arcs obtained from the index table for "winterw.
Th* lead-ln transitions to configuration 1 havd already been Constructed, so they are not remade.
Now we are ready to choose configurations to extend.
The pool of candidates for extension contains confi~urations 2 and 6).
SELECTED CONFIGS (6) FOR EXTENSION [Only this one is chosen because it has a hi~har score than configQration 2, since the use of a noun as a head noun of a noun phraac! is more likely than its use au a modifier).
PICKING UP CONFIG 6:NP/N':16(6) WITH WORD TRIP TRYING PUSH PP/ ARC {No.action is taken about starting a configuration for state PP/ because the look-ahead test which checks that the next word can begin a prepositional phrase fails on the word trip).
TRYING POP ARC POP TRANS 86 FROM 6:NP/N:16(6) {Creating ths POP transition completes a gath from configuration 5.
Thz path is expressed as a list of transition numbers.
We are about to-execute the path, that is, check the context-'sensitlvs tests and db tha re~ister build in^ actions along it).
EXECUTING PATH (4 3 2 5 6) BEGINNING AT TRANS 4, CONFIC 5 {We must ba~ln axecutinp the path at the first transition, because no part of it has been executed before.
Later we will see that it is possible to begin execution of a path in the middle, since th* repister contents are stored at each step.] DOING JUMP ARC FROM 5:NP/ TO 4:NP/ART DOING JUMP ARC FWM 4:NP/ART TO 3:NP/QUANT DOING JUMP ARC FROM 3:NP/QUANT TO I:NP/ADJ DOING CAT ARC WITH WINTER FROM 1 :NP/ADJ TO 6: NP/N DOING POP ARC FROM 6:NP/N TEST FAILED {Tha test failed because thzre is no determiner, and MINIGRAMMAR requires that singular, undetermined nouns can be complete noun phrases only if they are mass nouns.
"Winterv is not marked as a mass noun in our dictionary, hence it will not parse as a complete noun phrase.
) Page 52 SELECTED CONFIGS (2) FOR EXTENSION {Since oxtanding confi~uration 6 did not do much for us, we go back to try tha lower scoring confi~uration 2).
PICKING UP CONFIG 2:NP/ADJ:16(3) WITH WORD TRIP TRYING CAT N ARC CAT N TRANS 17 FROM 2:NP/ADJ:16(3) TO 7:NP/N:21(8) TRYING CAT N ARC CAT N TRANS 18 FROM 2:NP/ADJ:16(3) TO q:NP/hDJ:21(5) SELECTED CONFIGS (7) FOR EXTENSION {Again, the hi~her coring of the two active configurations, 7 and 8, is chosen).
PICKING UP CONFIG 7:NP/N: 21 (8) STARTING AT 21: MONITORING [ PP/ 1 SETTING UP CONFIG ?:PP/r21(8) MONITORLNG [ PREP ] (Since there is no next word to test, a confipuration is set up to begin processing a prepositional phrase, and the? syntactic catogories'which @an be~in such a phrase -in this case, only one -are monitorzd for).
TRYING POP ARC POP TRANS #g FROM 7:NP/N:21(8) EXECUTING PATH (4 3 2 1 7 9) B'EGINNING AT TRANS 1, CONFIG 1 {Creation of the POP trans completed a path, the first part of which has already -bean exicutid.
We can therefore pick up in the midads.
of' the path and execute only the last three transitions).
DiIItlG CAT AliC I ! E'HL'E! 1 : flP/A?J TO 2: NP/ADJ DOING CAT ARC WIT[; '.'RIP FROt-1 2:?!E3/ADJ TO 7:NP/N DOING POP ARC FROM 7:NP/N **#* MADE #1 FROIi 12 TO 21: NP ADJ NP N WINTER NU SG N TRIP NU PL +**+ {Thz path succeeds -no determiner is needed since the head noun is plural -and a constituent is constructed.
The semantic camponent has been turned eff for this example, so it adds nothin~ to the acore r:hich SPAzSZS wssiun$ -5 poifits for each word in the constit~lent.~ SYIJ WEIGHT + SEM WT = 10 + 0 = 10 {No monitor exists in the WFST for a NP/ at this placz, so the arcs (in NINIGRAMMAR there is only one) which could push for a NP are processed bottom up in exactly the same manner as the two arcs which couqd use a noun at the beginninp of the island).
NP/ WAS NEVER PUSHED FOR PUSH NP/ TRANS #I0 FROM 10:PP/PREP:12(1) Page 53 MONITORING [ PREP ] 1 SELECTED CONFIGS (11) FOR EXTENSION PICKING UP CONFIG 11:PP/NP:21(7) TRYING POP ARC POP TRANS #11 FROM 11:PP/NP:21(7) SELECTED CONPIGS (8) FOR EXTENSION PICKING UP .CONFIG 8:NP/ADJ:21(5) STARTING AT 21: MONITORING [ N ] MONITORING [ N 1 ALL ARCS TRIED AT THIS CONFIG (Now the thewy has been procdssed.
There followa a summary of the proposals, monitors, and notlces constructed.
The syntactic wore assigned to the theory is ~ivon -here just the acore of the conutituent constructed.
Then there is a summary of ~tatistics).
PREDICTIONS: MONITORENG [ PREP 1 STARTING AT 21, SCORE 10 MONITORING [ N ] STARTING AT 21, SCORE 10 MONITORING [ N ] ENDING AT 12, SCORE 10 MONITORING [ QUANT 1 ENDING AT 12, SCORE 10 MONITORI~G [ ADJ 1 ENDING AT 12, SCORE 10 MONITORING [ ART j ENDING AT 12, SCORE 10 MC?~ITORING [ PREP ] ENDING AT 12, SCORE 10 PROPOSING (QUANT ART PREP) ENDING AT 12 FINISHED THEORY 1 WITH SYN SCORE 10 {Exclusive of tracin~ and fork interactions, this processing took 5.5 seconds).
{Now we are ready to process the second theory syntactically).
SPARSER PROCESSING THEORY 2: 0 12 SUMMER 16 TRIP -S 21 30 STARTING AN ISLAND nSUMMER" TRYING CAT N ARC FROM NP/ADJ TO NP/ADJ CAT N TRANS 112 FROM 1:NP/ADJ:12(1) TO 2:NP/ADJ:16(3) {This transition completes a path which includes transitions and configurations constructed auring the pravious theory).
EXECUTING PATH (4 3 2 12 7 9) BEGINNING AT TRANS 12, CONFIG 1 DOING CAT ARC WITH SUMMER FROM 1:NP/APJ TO 2:NP/ADJ DOING CAT ARC WITH TRIP FROM 2:NP/ADJ TO 7:NP/N +*+u -DOING POP ARC FROM 7:NP/N MADE #2 FROM 12 TO 21 : NP ADJ NP N SUMMER NU) SG N TRIP NU PL **** SYN WEIGHT + SEM WT = 10 + 0 = 10 NP/ WAS PUSHED FOR AT CONFIG 10 {This time there are monitory in the WFST, one which is looking for a NP start in^ at position 12 and one which is looking for a NP ending at position 21.
One transition is sufficient to satisfy both of these, ant! the preposition needed to complete a PP/ is monitored for).
PUSH NP/ TRANS61 3 FROM TO: PP/PREP: 12( 1 ) TO tl:PP/NP:21(8) NPI MAY LEAD TO CONFIC 11 {This is caused by the fact that there was a monitor Tor a, noun phrase ending at confipurat ion 11 -the one craated when constituent 1 was made.
The transit ion which would bd 8at up is the transition Just created, so it is not remade.
All of the processinp which resulted from the completion of a constituent is finished; however there are honitors still to be set for configurations alone the path).
ENDING AT 12: MONITORING [ PREP ] ENDING AT 12: MONITORING [ N ] ENDING AT 12: MONITORING [ QUANT ] MONITORING [ ADJ 1 ENDING AT 12: MONITORING [ ART ] (Since each monitor consist8 of thz item beinc monitored for, its associated test (if any) th~ theory which is to be notified when the monitor is satisfied, and the configuration and arc causing the monitor, monitors must be mad* anew each time one of the elements changes, although some of the list structure can be shared, hence thz seeming proliferation ofmonitors).
{N80w SPARSER procassss the othsr arc which could uss the word "summerw.
) "SUMMER" TRYING CAT N ARC FROM NP/ADJ TO NP/N CAT N TRANS #14 FROM l:NP/ADJ:12(1) TO 6:NP/N:16(6) EXECUTING PATH (4 3 2 14 6) BEGINNING AT TRANS 14, CONFIG 1 DOING CAT ARC WITH SUMMER FROM 1:NP/ADJ TO 6:NP/N DOING FOP ARC FROM 6:NP/N TES?
FAILED i&cauae, "sum apn cannot be a complete noun phrase in s grammar.
Y Page 55 SELECTED CONFIGS (11) FOR EXTENSION PICKING UP CONFIG 11:~~/NP:21(8) TRACING POP TRANS 11 FROM 11:PP/NP:21(8) [This transition was created before, but is now mads part of the currant theory.
It doas not compllte a pqth or cause any further action.
If it had a trrminatinff configuration, i.e. if a transition other than a POP tranai tion hao been traced, the terminatinp configuratdon would have been placed on the list of possible confipurations to extend.} SELECTED CONFIGS (6) FOR EXTENSION PICKING UP CONFIO 6:NP/N:e16(6) WITH WORD TRIP TRACING POP TRANS 6 FPOM 6:~~/~:16(6) SELECTED CONFIGS (2) FOR EXTENSION PICKING UP CONFIG 2:Np/A~J:16(3) WITH WORD TRIP TRACING CAT N TRANS 8 USING "TRIP" FROM 2:NP/ADJ:16(3) TO 8:NP/ADJ:21(5) STARTING AT 21 : MONITORING [ N ] MONITORING [ N ] {Tharz are two noun arcs Leaving state NP/ADJ, hence two monitors).
SELECTED CONFIGS (8) FOR EXTENSION PICKING UP CONFIG 8:NP/ADJ:21(5) ALL ARCS TRIED AT THIS CONFIG PREDICTIONS: MONITORING [ N ] STARTING AT 21, SCORE 10 MONITORING r PREP ] ENDING AT 12, SCORE 10 MONITORING [ N ] ENDING AT 12, SCORE 10 MONITORING [ QUANT ] ENDING AT 12, SCORE 10 MONITORING [ ADJ ] ENDING AT 12, SCORE 10 MONITORING [ ART 1 ENDING AT 12,' SCORE 10 PROPOSING (PREP QUANT ART) ENDING AT 12 FINISHED THEORY 2 WITH SYN SCORE 10 {The processing of this theory tbok approximately 4.5 seconds.
) This example has shown the trace produced by runninp SPARSER on input which is analogous to the example presented with illustrations of the map in Section Four.
The Interested reader is urged to draw his own maps while reading the follow in^ Pact! 56 examples in order to best understand the dynanic operation of SPARSER.
EXAMPLE 2 I-This example is more realist ic t ban the previous cntb -it shows the operation of SPARSER in the context of an pt terance which has been 2utornatically segmented and labelee, ~ith the lexical rr;trieval and match component in operat ion.
It demonstrates how SPARSER can help to select the best set vf words from a or complex word lattict.
This example uses the SPEECHGRAMMAR described in [4].
The utterance "What is the registration fee"? was spoken by an adult male speaker in a quite room and was record on tape.
Tha ut teranca was automatically diqit ized rind passed through the warnentat ion and labelinr routines of the BBN speech understandins ~ystem.
The initial scan of the utterance, usinp the lexical retrieval component, produced a w~rd lattice of fifteen entries, includin~ several for inflectional endings.
(In this version of thl system, they were not combined with the root form into a single word hatch, and hsncr? could match evan without a root word).
Tha format for a word match is: (NUMBER WORD LEFT-END RIGHT-END LEXICAL-SCORE).
(2 WHAT 0 3 191) (3 ONE 0 3 189) (11 WHEN 0 3 102) (9 THE 4 6) ( 1 REGISTRATION 6 19 237) (10 REGISTRATION 7 19 103) (5 HAS 9 12 121) Page 57 The two best matches, for "what1! and ftrzgistrationll, appear to be good candidates for a theory, so we begin by build in^ and procasring that theory.
ARSER PROCESSING THEORY 1: %PWHL~ 3 ' 6 REGISTRATION 19 23 STARTING AN ISLAND STARTIHG AT LEFT END OF SENTENCE (Itno~ing that it is not necsssary to go through the usual startup procedure for islands when beginning an island at position 0, SPARSER starts with a configuration for state 3/ at position 0).
SELECTED CONFICS (1) FOR EXTENSION PICKING UP CONFIG 1:S/:0(1) WITH WORD WHAT TRYIIG JUMP S/Q ARC JUMP TRANS #I FROM 1:S/:0(1) TO 2:S/Q:0(6) SELECTED CONFIGS (2) FOR EXTENSION PICKXICG UP CONFIG 2:S/O:0(6) WITH WORD WHAT TRYIMG PUSH NP/ ARC HONrTORING 1 NP/ ] SETTING UP CONFIG 3:NP/:0(11) TBYIHG CAT QWORD ARC CAT QWORD TRANS #2 FROM 2:S/Q:0(6) TO 4:S/NP:3(11) SELECTED COHFIGS (3 4) FOR EXTENSION {This time two active configurations have the same maximal score, so they are both processed).
PIafffi UP COIMG 3:NP/:0(11) WITH WORD WHAT TPTIYC GAT QDET ARC a CAT QOET TRANS #3 FROM 3:NP/:0(11) TO 5:NP/ORD:3(16) ma= UP CONFIG ~:s/NP:~(II) STAPIIMG At 3: HMITO~lrG [ MODAL 1 WI~~~ING I V 1 Page 58 TRYING POP ARC POP TRANS #4 FROM 4:S/NP:3(11) EXECUTING PATH (1 2 4) BEGINNING AT TRANS 1, CONFIG 1 DOING JUMP ARC WITH WHAT FRdlY l:S/ TO 2:S/Q DOING CAT ARC WITH WHAT FROM 21S/Q TO 4:SINP DOING POP ARC FROM Q:S/WP TEST FAILED {This test failed because the crammar does not allow 'lwhatw to be a complete sante.nca).
SELECTED CONFIGS (5) FOR EXTENSION PICKING UP CONFIG 5:NP/ORU:3(16) STARTING AT 3: MONITORING [ QUANT/ ] SETTING UP CONFIG 6: QUANT/ : 3 ( 16 ) (Hare all the words which can start quantifiers, like "a hundredw or "point fivam, ard proposed.
The grammar does not preclude a quantifier following a que$tion-determihzr, e-g.
"What three men traveled to Spain'llI.
) {MONITORING [ INTEGER ZERO NO POINT A] For considerations of space, long listings of monitors and proposals in this example will be compacted as shown here.
Such alterations to the actual trace produced will be surrounded by brackets).
TRYING JUMP NP/QUANT ARC JUMP TRANS #5 FROM 5:NP/ORD:3(16) TO 7:NP/QUANT:3(21) SELECTED CONFIGS (7) FOR EXTENSION PICKING UP CONFIG 7 : NP/QUANT :,3 (2 1 ) TRYING JUMP NP/DET ARC JUMP TRANS 86 FROM 7:NP/QUANT:3(21) TO 8:NPIDET:3(26) SELECTED CONFIGS (8) FOR EXTENSION PICKING UP CONFIG 8:NP/DET:3(26) STARTING AT 3: MONITORING [ NPR/ NPP/ 1 {There are two PUSH NPR/ arcs from this state so two monitors are craated, but only one configuration is set up1 SETTING UP CONFIG g:NPR/:3(26) {MONITORING [NPR NPR N ADJ N V ADV]) TRYING JUMP NP/HEAD ARC JUMP TRANS #7 FROM 8:NP/DET:3(26) TO 10:NP/HEAD:3(29) SELECTED CONFIGS (10) FOR EXTENSION PICKING UP CONFIG 10:NP/HEAD:3(29) {This is an ex~mple of he fallibility of using only context free tests on partial paths, The parser thinks it has succassfully reached state NP/HEAD, while in fact tbis cannot be the case because no head nouh has bean dlscovarad for the noun phrase.
Thus it is incorrect to predict relativz clauses at this point.
This issue will be discussed in more detail be1ow.J STARTING AT 3: MONITORING R/ PP/ R/NIL 1 SETTING UP CONFIG 11:R/:3(29) {MONITORING lPREP WHOSE WHO WHICH THAT WHOM]} (PROPOSING LYHOSE1l lfWHOw 'WHICH" "THAT" "WHOM"] SETTING UP CONFIG 12:PP/:3(29) MONITORING [ PREP 1 SETTING UP CONFIG 13: R/NIL: 3(29) MONITORING [ THERE ] PROPOSING "THERE" TRYING POP ARC POP TRANS 88 FROM 10:NP/HEAD:3(29) EXECUTING PATH (3 5 6 7 8) BEGINNING AT TRANS 3, CONFIG 3 DOING CAT ARC WITH WHQ FROM 3:NP/ TO 5:NP/ORD DOING JUMP ARC FROM 5:NP/ORD TO 7:NP/QUANT DOING JUMP ARC FROM 7:NP/QUANT TO 8:NP/DET DOING JUMP ARC FROM 8:NP/DET TO 10:NP/HEAD TEST FAILED {A question-determiner alone cannot bs a complete noun phrase; although this is permitted by considering "whatw as a QWORD as in tranaition #2).
STARTING AN ISLAND "REGISTRATION" TRYING CAT N ARC FROM NP/DET TO NP/DET CAT N TRANS #g FROM 14:NP/DET:6(1) TO 15:N~/DET:19(4) {This arc is using wrsgiatsationv as a noun modifier for Qome future head noun).
ENDING AT 6: {MONITORING [ NPR/ ADJ N V ]} JUMP TRANS 110 FROM 16:NP/QUANT:6(1) TO 14:NP/DET:6(1) ENDING AT 6: MONITORING [ QUANT/ ] JUMP TRANS 811 FROM 17:NP/ORD:6(1) TO 16:NP/QUANT:6(1) ENDING AT 6: (MONITORING [ ORD QDET ONLY I) PROPOSING "ONLY" JUMP TRANS #I2 FROM 18:NP/ART:6( 1 J TO 17:NP/ORD: 6 1 ) ENDING AT 6: (MONITORING [ ART QUANT POSS k1HOSE I} NOTICING IITHE" PROPOSIllG "WHOSE1' JUMP TRANS W13 FROM 19:MP/ONLY:6(1) TO 18:NP/AHT:6(1) El.IDIIJG AT 6: llOMITORING [ ONLY 1 PROPOSING 'lfObJLY" JUMP TRP!IS #14 FROM 20:NP/:6(1) TO 19:NP/ONLY:6(1) HEGISTRATION" TRYING CAT N ARC FROM NP/DET TO NP/HEAD CAT N TRANS #J5 FROM 14:NP/DET:6(1) TO 21:NP/HEAD:19(6) {This arc is using ltregistrationtl as the head noun of a noun phrase.
) SELECTED CONFIGS (21) FOR EXTENSION PICKING UP CONFIG 21 : NP/HEAD: 19( 6) STAHTING AT 10: FIONIT nxwc [ R/ PP/ W/NIL I SETT 9 NG UP CONFIG 22:R/:lQ(6) {MONITORING [PREP WHOSE \,!I10 WHICH THAT WHOM] ) SETTING UP CONFIG 23:PP/:19(6) MONITORING [ PREP ] SETTING UP CONFIG 24:R/NIL:19(6) MONITORING [ THERE ] NOTICING "FEEtt {This notice is in response to the look-ahead test on the push arc to atate R/NIL.
Since 'vfee'l can start a reduced re&ative clause, it is noticed, but there is not a specific monitor set up becauue the arc within the relativd clause network which will actually process the word Itfed" is not known).
TRYING POP ARC POP TRANS #16 FROM 21:NP/HEAD:19(6) EXECUTING PATH (14 13 12 11 10 15 16) BEGINNING AT TRANS 14, CONFIG 20 TEST FAILED {The path failed because thare is no determiner for tlregistrat ion.
"1 PREDICTIONS: NOTICING (4 FEE 19 23 155 Q), SCORE -5 NOTICING (9 THE 4 6 103 01, SCORE 0 PROPOSING (ONLY WHOSE) ENDING AT 6 PROPOSING (ZERO NO POINT A WHOSE WHO WHICH THAT WHOM THERE) STARTING AT 3 {MONITORING [ PREP ] STARTING AT 19, SCORE 0 MONITORING [ WHOSE WHO WHICH THAT WHOM THERE ] STARTING AT 19, SCORE 5 MONITORING [ ADJ N V ORD QDET ART QUANT POSS ] ENDING AT 6, SCORE 0 MONITORINC [ WHOSE ONLY ] ENDING AT 6, SCORE 5 MONITORINC [ MODAL V INTEGER NPR N ADJ V ADV PREP ] STARTING AT 3, SCORE 0 MONITORING [ WHOSE WHO WHICH THAT WHO11 THERE ZERO NO POINT A ] STARTING AT 3, SCORE 5) PROPOSING (V N ADJ) FROM 3 TO 6 {Proposals were made to fill the gap because there were monitors from both sides of a pap small enoush to contain one word).
FINISHED THEORY 1 WITH SYN SCORE 0 {It took 11.9 seconds to process this theory).
{Processing the proposal-s just made results, notably, in the detection of the word lvotherv between vv~hatvl and wre~istrationlv, but the word match score is very low.
Word matches for *isw and Itare" from position 3 (next to "whatvv) to position 4 are also found, but since they do Page 61 not fill the pap, the event scores are low.
The bzst event is that for the word "faen.
Procassin~ it is fairly uninteresting, since it completes no constituent, so we will omit the trace of that event.
After it has bean processed, however, the best event is that for the word "thevf and the theory just created.
] SYNTAX PROCESSING EVENT FOR THEORY#2 WITH NEW WORD (4 THE 6) TO GET NEW THEORY#3.
0 WHAT
3 4 THE 6 R,EGISTRATION 19 FEE 23 "THE" TRYING (CAT ART --) FROM STATE NP/ONLY TO CONFIG 18 CAT ART TRANS #26 FROM ~~:NP/ONLY:~(~) TO 18:NP/ART:6(6) ENDING AT 4: MONITORING [ ONLY ] PROPOSING "ONLY" JUMP TRANS #27 FROM 33:NP/:4(1) TO 32:NP/ONLY:4(3) EXECUTING PATH (27 26 12 11 10 9 22 25) BEGINNING AT TRANS 22, CONFIG 15 +*++ MADE #l FROM 4 TO 23: NP DET ART THE ADJ NP N RECISTRATION NU SC N FEE FEATS NU.
SG **++ ha format of this noun p5rase is slightly different from that in the previous example because tpe ~tructure building action for noun phrases in SPEECHGRAMMAR is different from that in MINIGRAMMAR.
There ara many places in the SPEECHGRAFIMAR which push for noun phrases, and since there were no monitors in the WFST which can us& thiv constitusnt, all of them must be tried, resulting in a numbdr of predictions and notices.
) SYN WEIGHT + SEM WT = 15 + 0 = 15 NP/ WAS NEVER PUSHED FOR PUSH NP/ TRANS #28 FROM 34:FOR/FOR:4(1) TO 35:T0/:23(10) ENDING AT 4: MONITORING [ FOR ] PROPOSING "FO' NP/ WAS NEVER PUSHED FOR PUSH NP/ TRANS 129 FROM 36:PP/PREP:4(1) TO 37:PP/NP:23(10) ENDING AT 4: MONITORING [ PREP ] NP/ WAS NEVER PUSHED FOR PUSH NP/ TRANS 130 FROM 38:R/NIL:4(1) TO 39:S/NP:23(9) NP/ WAS NEVER PUSHED FOR PUSH NP/ TRANS 131 FROM 40:R/WH:4(1) TO 39:S/NP:23(9) ENDING AT 4: MONITORING [ R/WHOSE 1 MONITORING [ WHICH THAT WHO WHOM WHICH WHOM ] {PROPOSING "THATf1 llWHO1l llWHOMu "WHICH" "WHOM") {There arc two arcs entwine state R/WH which use the words l'whichn and llwhomv.
There is a check made to see that duplicate proposal8 alre not actually communicated to the control component, although they appear to be duplicated in the trace.
} NP/ WAS NEVER PUSHED FOR PUSH NP/ TRANS #32 FROM 41:S/DCL:4(1) TO 39:S/NP:23(9) JUMP TRANS #33 FROM 42:S/:4(1) TO 41:S/DCL:4(1) ENDING AT 4: MONITORING [ PP/ I NP/ WAS NEVER PUSHED FOR PUSH NP/ TRANS #34 FROM 43:S/NO=SUBJ:4(1) TO 44:VP/V:23(9) JUMP TRANS #35 FROM 45:S/AUX:4(1) TO 43:S/NO-SUBJ:4(1) ENDING AT 4: {MONITORING [ MODAL NEG V 1) (NOTICING I~IS~' tl~~~v} NP/ WAS NEVER PUSHED FOR PUSH NP/ TRANS #36 FROM 46:S/Q:4(1) TO 39:S/NP:23(9) ENDING AT 4: MONITORING [ QADV 1 NP/ WAS NEVER PUSHED FOR PUSH NP/ TRANS #37 FROM 47:VP/MEAD:4(1) TO 48:VP/NP:23()?
ENDING AT 4: MONITORING [ PARTICLE 1 MONITORING [ V ] JUMP TRANS #38 FROM 49:VP/V:4(1) TO 47:VP/HEAD:4(1) ENDING AT 4: {MONITORING [ NP/ NP/ V V ADV V ]} {NOTICING "IS" "ARE" "IS" "AREr1} {The words "isV1 and failed the? context fres test on the arc causing tha last monitor, hsnce they are not noticed.
} JUMP TRANS 839 FROM 45:S/AUX:4(1) TO 49:VP/V:4(1) JUMP TRANS /I40 FROM 43:S/NO-SUBJ:4(1) TO 49:VP/V:4(1) JUMP TRANS #41 FROM 43:S/NO=SUBJ:4(1) TQ 49:VP/V:4(1) JUMP TRANS 842 FROM 50:S/THERE:4(1) TO 49:VP/V:4(1) ENDING AT 4: !lONITORING THERE J PROPOSING "THEREvf NP/ WAS NEVER PUSHED FOR PUSH NP/ TRANS #43 FROM 51:VP/NP:J+(1) TO ~~:VP/VP:Z~()?
ENDING AT 4: MONITORING [ NP/ 1 JUMP TRANS #44 FROM 47:VP/HEAD:4(1) TO ~I:VP/NP:~(I) NP/ WAS NEVER PUSHED FOR PUSH NP/ TRANS #45 FROM 49:V~/V:4(1) TO 44:VP/V:23(9) {The creation of transition 1/27 completed 3 paths.
The first two have bean executed, resulting respaatively jn failure and the completion of a constituent with all the processing that entails.
Now the third path is still pending and is about to be executed.] EXECUTING PATH (27 26 12 11 10 15 16) BEGINNING A?
TRANS 15, CONFIG 14 DOING CAT ARC WITH REGISTRATION FROM 14:NP/DET TO 2l:NWHEAD DOING POP ARC FROM 21tNP/HEAD **+* MADE #2 FROM 4 TO 13: NP DET ART THE N REGISTRATION FEATS NU SG *#** {This constituent can now satisfy the monitors set by the discovery of the largar one, result in^ in the creation of many new transitions but no new predictions).
SYN WEIGHT + SEM WT = 10 + 0 = 10 NP/ WAS PUSHED FOR AT CONFIG 34 PUSH NP/ TRANS C46 FROM 34:FOR/FOR:4(7) TO 53:T0/:19(8) {Similar tIP/ transitions are ast up at configurations 36,38,40,41,43,46,47,49, and 51 because of the monitors set when the first constituent was found).
SELECTED CONFIGS (55 54 39 37) FOR EXTENSION {Because these ara the maximally scoring configurations from ths large pool of possibilities).
PICKING UP CONFIG 55:S/NP:19(8) WITH WORD FEE TRYING POP ARC POP TRANS 856 FROM 55:S/NP:19(8) EXECUTING PATH (48 56) BEGINNING AT TRANS 40, CONFIG 38 TEST FAILED EXECUTING PATH (33 50 56) BEGINNING AT TRANS 33, CONFIG 42 ++** MADE #3 FROM 4 TO 19: S NPU NP DET ART THE ADJ NP N REGISTRATION NU SG N FEE FEATS NU.
SG WITH FEATURES (NPU) **** {Here is an example of a constituent which has features attached to it.
The feature NPU can be tested by tne semantic component to determine that the constituent is a noun phrasa utterance.
If nscessary, it could also be tasted on a PUSH S/ arc in the prammar; since there are aome times, e.~.
durin~ the construction of a sentantial complement, when an embedded sentence must contain a verb).
SYN WEIGHT + SEM WT = 10 + 0 = 10 {No arcs in this ramm mar push for noun phrase utterances, so this constituent is not used furthar).
PICKING UP CONFLG 54:PP/NP:19(8) WITH WORD FEE TRYING POP ARC POP TRANS #57 FROM 54:PP/N~:1?(8) PICKING UP CONFIG 39:S/NP:23(9) TRYING POP ARC POP TRANS #58 FROM ~~:s/NP:?~(!J) EXECUTING PATH (30 58) BEGINNING AT TRANS 30, CONFIG 38 TEST FAILED EXECUTING PATH (33 32 58) REGINNING AT TRANS 32, CONFIG 41 **** MADE #4 FROM 4 TO 23: S NPU NP ADJ NP N REGISTRATION NU SG DET ART THE N FEE FEATS NU SG WITH FEATURES (NPU) **** SYN WEIGHT + SEM WT = 15 + 0 = 15 PICKING UP CONFIG 37:PP/NP:23(10) TRYING POP ARC POP TRANS 659 FROM 37:~~/~P:23(10) PREDICTIONS: NOTICING (19 IS 3 4 -79 0), SCORE 10 NOTICING (20 ARE 3 4 -128 O), SCORE 10 PROPOSING (ONLY FOR WHICH THAT WHO WHOM THERE) ENDING AT 4 {MONITORING [ ONLY FOR WHICH THAT WHO WHOM THERE 1 ENDING AT 4, SCORE 15 MONITORING [ MODAL NEC V QADV PARTICLE V V V ADV PREP V 1 ENDING AT 4, SCORE 10 MONITORING [ MODAL V INTEGER NPR N ADJ V ADV PREP 1 STARTING AT 3, SCORE 0 MONITORING [ ZERO NO POINT A WHOSE WKO WHICH THAT WHOM THERE] STARTING At 3, SCORE 5) PROPOSIHG (MODAL) FROM 3 TO 4 PROPOSING (MODAL PREP) STARTING AT 3 PROPOSING (PREP MODAL NEG QADV) ENDING AT 4 CREATING THEORY 3: 0 WHAT 3 4 THE 6 REGISTRATION 19 FEE 23 WITH SYN SCORE 15 {Tkis $vent took 34.5 saconds, largely because of the ex enslvz bottom up procdssinq necessitated by the Page 65 discovery of the noun phrases which were not monitored for.
1 (Processing the proposals from this theory results in the bavt event being the one for "isw in the last gap.
The word "arevt also fills the gap, but the lower lexical acore prevents ths event for it from surfacin~.
If it were syntactically procasued, however, no new theory would be created since tha completed string would be ungrammatical).
SYNTAX PROCESSING EVENT FOR THEORY#3 WITH NEW WORD (3 IS 4) TO GET NEW THEORY#4: 0 WHAT 3 IS 4 THk 6 REGISTRATION 19 FEE 23 "IS" TRYING (CAT V --) FROM CONFIG 4 CAT V TRANS #60 FROM 4:S/NP:3(11) TO 45:S/AUX:4(16) {This transition does not immediately complete any paths, so the best scoring configurations of the theory are triad).
SELECTED CONFIGS (31) FOR EXTENSION PICKING UP CONFIG 31:NP/DET:23(36) TRYING JUMP NPIHEAD ARC JUMP TRANS t61 FROM 31:NP/DET:23(36) TO 30:NP/HEAD:23(39) SELECTED CONFIGS (52 48 44) FOR EXTENSION PICKING UP CONFIG 52:VP/VP:23(9) TRYING JUMP S/VP ARC JUMP TRANS #62 FROM 52:VP/VP:23(9) TO 59:SIVP:23(12) PICKING UP CONFIG 48:VP/NP:23(9) TRYING JUMP VP/VP ARC JUMP TRANS #63 FROM 48:VP/NP:23(9) TO 52:VP/VP:23(11) PICKING UP CONFIG 44:VP/V:23(9) TRYIRG JUMP VPIHEAD ARC JUMP TRANS P64 FROM 44:VP/V:23(9) TO 60:VP/HEAD:23(13) SELECTED CONFIGS (60) FOR EXTENSION PICKING UP CONFIG 60:VP/HEAD:23(13) TRYING JUMP VP/NP ABC JUMP TRANS 865 FROM 60:VP/HEAD:23(13) TO 48:VP/NP:23(16) SELECTED CONFIGS (59) FOR EXTENSION PICKING UP CONFIG 59:S/VP:23(12) TRYING JUMP S/S ARC JUMP TRANS C66 FROM 59:S/VP:23(12) TO 61:S/S:23(14) D CONFIGS (61) FOR EXTENSION ~T~UP CONFIG 61:S/S:23(14) TRYING POP ARC POP TRANS 867 FROM 61:S/S:23(14) EXECUTING PATH (1 2 60 35 34 64 65 63 62 66 67) BEGINNING AT TRANS 34, CONFIG 43 **** MADE 85 FROM 0 TO 23:.
S Q SUBJ NP DET ART THE ADJ NP N REGISTRATION NU SG N FEE FEATS NU SG AUX TNS PRESENT VOICE ACTIVE VP V BE OBJ NP N WHAT FEATS NU SQ/PL #YO* {This iu the complete parve of the utterance, but SPARSER continues the operations it has pending bePore raturninc to Control).
NO SEMANTICS FOR HEAD {This is a comment from the sarnantic component indicating that it cannot currently interpret the construction).
SYK WEIGHT + SEM WT = 25 + O = 25 S/ WAS NEVER PUSHED FOR PUSH S/ TRANS 868 FROM 62:COMPL/NTYPE:O(1) TO 63:COMPL/S:23(15) S/ WAS NEVER PUSHED FOR PUSH S/ TRANS #69 FROM 64:S/THEN:O(1) TO 65:S/IFTHEN:23(:5) S/ WAS NEVER PUSHED FOR PUSH S/ TRANS #70 FROM 66:VP/HEAD:O(1) TO 52:VP/VP:23(13) JUMP TRANS #71 FROM 67:VP/V:0(1) TO 66:VP/HEAD:O(1) JUMP TRANS #72 FROH 68:S/AUX:0(1) TO 67:VP/V:0(1) JUMP TRANS #73 FROM 69:S/NO-SUBJ:O(1) TO 67:VP/V:0(1) JUMP TRANS #74 FROM 68:S/AUX:0(1) TO 69:S/NO=SUBJ:O(l) JUMP TRANS 875 FROM 69:S/NO=SUBJ:O(l) TO 67:VP/V:0(1) JUMP TRANS #76 FROM 70:S/THERE:O(l) TO 6?:VP/V:0(1) {One of the pending operations is to check thz othar arcs which caused monitors for the verb "isf1 1 "IS" TRYING (CAT V --) FROM STATE S/NP TO CONFIG 45 "IS" TRYING (CAT V --$ FROI-I STATE FOR/TO TO CONFIG 49 CAT V TRANS 1/77 FROM 71:FOR/TO:3(5) TO 49:VP/V:4(20) "IS1' TRYING (CAT V --) FROM STATE VP/V TO CONFIG 49 CAT V TRANS t78 FROM 72:VP/V:3(5) TO 49:V~/~:4(20) JUMP TRANS #79 FROM 73:S/AUX:3(1) TO 72:VP/V:3(5) JUMP TRANS #80 FROM 74:S/NO-SUBJ:3(1) TO 72:VP/V:3(5$ JUMP TRANS #81 FROM 73:S/AU~:3(1) TO 74:S/NO-SUBJ:3(1) JUMP TRANS 882 FROM ~~sS/NO-SUBJ:~(I) TO 72:VP/V:3(5) JUMP TRANS 883 FROM 75:S/THERE:3(1) TO 72:VP./V:3(5) CREATING THEORY 4: 0 WHAT 3 IS 4 THE 6 REGISTRATION 19 FEE 23 WITH SYN SCORE 15 {This processing took 34.45 seconds).
This example was run with a vary simple, mechanical control structure.
After the processing of the initial theory, the proposals which had bean made by SPARSER were processed by th+ lexical retrieval component and the results added to the word lattice -a process which can sat off monitors and result in the creation of event notices.
The ~v*nts are scored by a combination of the monitor score assigned by SPARSER and the lexical score asslpnad by the word match component.
In this sentence, syntax and lexical score alone ware suffhient to make ths besb scorinp event at each step be one which resulted in a correct extension of the theory.
Vz now ahow how the same utterance use@ in the prdvious example cah be recognized when dkfferznt theories qre crsated and when avants and theories are processed in a diffarent orda? from that in Example 2.
Suppose that after the initial scan of the Ptterance the semantic component created two thzorias, one for the words llwhatw and "feeff and the other fori the wobds I'vhat1' and nrzristrationtl Let us see what happen9 in SPARSER when we hepin by procassipg these two tbsories in sequence.
Page 68 SPARSER PROCESSING THEORY 1: 0 WHAT 3 19 FEE 23 {The processing of this thzory iq very similar to that of the first theory in the previous ~xample, and will not be commented upon here.
The purpoae in show in^ it is to provide a map, part of which the next call to SPARSER will trace.
) STARTING AN ISLAND STARTING AT LEFT END OF SENTENCE SELECTED CONFIGS (1) FOR EXTENSION PICKING UP CONFIG 1:S/:0(1) WITH WOHD WHAT TRYING PUSH PP/ ARC TRYING JUMP S/Q ARC JUMP TRANS #I FHOM 1:S/:0(1) TO 2:S/Q:0f6) TRYING WRD IF AHC TRYING JUMP S/IblP ARC TRYING JUMP S/DCL ARC SELECTED CONFIGS (2) FOR EXTENSION PICKING UP CONFIG 2:S/Q:0(6) WITH WORD WHAT TRYING PUSH NPI ARC MONITORING [ NP/ ] SETTING UP CONFIG 3:NP/:0(11) TRYING WRD HOW ARC TRYING CAT QWORD ARC CAT QWORD TRANS #2 FRO11 2:S/Q:0(6) TO Q:S/NP:3(11) TRYING CAT QADV ARC TRYShJG JUMP S/NP ARC SELECTED CONFIGS (3 4) FOR EXTENSION PICKING UP CONFIG 3:NP/:0(11) WITH WORD WHAT TRYING WRD ONLY ARC TRYING CAT QDET ARC CAT QDET TRANS #3 FHOM 7:NP/:O(ll) TO 5:NPiORD:3(16) TRYING PUSH DATE/ ARC TRYING TST ARC TRYING JUMP NPIONLY ARC PICKING UP CONFIG 4:S/NP:3(11) STARTING AT 3: MONITORING [ MODAL ] MONITORING [ V ] TRYING POP ARC POP TRANS #4 FROM 4:S/NP:3(11) EXECUTING PATH (1 2 4) BEGINNING AT TRANS 1, CONFrG 1 DOING JUMP ARC WITH WHAT FROM l:S/ TO 2:S/Q DOING CAT ARC WITH WHAT FROM 2:S/Q TO 4:S/NP DOING POP ARC FROM 4:S/NP TEST FAILED SELECTED CONFIGS (5) FOR EXTENSION PICKING UP CONFIG 5:NP/ORD:3(16) STARTING AT 3: PJONITORING [ QUANT/ 1 SETTING UP CONFIG 6:QUANT/:3(16) MONITORING [ INTEGER 1 TRYING JUMP NP/QUANT ARC JUMP TRANS #5 FROM 5:NP/ORD:3(16) TO 7:NP/QUANT:3(21) SELECTED CONFIGS (7) FOR EXTENSION PICKING UP CONFIG ~:NP/QUANT:~(~~) TRYING JUMP NP/DET ARC JUMP TRANS U6 FROM 7:N~/QUAblT:3(21) TO 8:NP/DET:3(26) SELECTED CONFIGS (8) FOR EXTENSION PICKING UP CONFIG 8:NP/DET:3(26) STARTING AT 3: {MONITORING [ NPR/ NPR/ NPR NPH N ADJ N V ADV I) SETTING UP CONFIG g:NPR/:3(26) TRYING JUMP NP/HEAD ARC JUMP TRANS #7 FROM 8:NP/DET:3(26) TO 10:NP/MEAD:3(29) SELECTED CONFIGS (10) FOR EXTENSION PICKING UP CONFIG 10:NP/HEAD:3(2)?
STARTING AT 3: IMONITORZNG c R/ PP/ RINIL PREP WHOSE WHO WHICH THAT WHOM 11 SETTING UP CONFIG 11 : R/ : 3 ( 29) {PROPOSING IIWHOSE~~ lf~~~w ll~~~~~n 11~~~~11 tl~~~~n~ SETTING UP CONFIG 12:PP/:3(29) MONITORING PREP 1 SETTING UP CONFIG 13:R/NIL:3(2Q) MONITORING [ THEaE ] PROPOSING llTHERE'l TRYING POP ARC POP TRANS #8 FROM 10:NP/HEAD:3(29) EXECUTING PATH (3 5 6 7 8) BEGINNING AT TRANS 3, CONFIG 3 DOING CAT ARC WITH WHQ FROM 3:NP/ TO 5:NP/OHD DOING JUMP ARC FROM S:NP/ORD TO 7:NP/QUANT DOING JUMP ARC FROM 7:NP/QUANT TO 8:NPfDET DOING JUMP ARC FROM 8:NP/DET TO 10:NP/HEAD TEST FAILED STARTING AN ISLAND "FEEf1 TRYING CAT N ARC FROM NP/DET TO NP/DET CAT N TRANS #9 FROM 14:NP/DET:19(1) TO 15:NP/DET:23(4) ENDING AT 19: MONITORING [ NPR/ ] MONITORING [ ADJ ] MONITORING [ N ] NOTICING "REGISTRATIONv NOTICING "REGISTRATION" {There are two instances of the word llragistrationn in the word lattice, hence two notices are created).
MONITORING [ V 1 JVMP TRANS #I0 FROM 16:NP/QUANT:19(1) TO 14:NP/DET:19(1) ENDING AT 19: t N TOR NG WANT/ ] JU~ IRAN^ 1 FROM 17:NP/ORD: 19( 1) TO 16:RP/QUANT: 19( 1) ENDING AT 19: {MONITORING [ ORD QDET ONLY 1) PROPOSING "ONLY" JUMP TRANS R12 FROM 18:NP/ART:19(1) TO 17:NP/OHD:l?(l) ENDING AT 19: {MONITORING [ ART QUANT POSS WHOSE 1) PROPOSING "WHOSE" JUMP TRANS 113 FROM 10:NP/ONLY:1?(1) TO 18:NP/ART:lo(l) ENDING AT 19: MONITORING [ ONLY 1 PROPOSING I'ONLY" JUMP TRANS #I4 FROM 20:NP/:19(1) TO 1Q:NY/ONLY:19(1) tlFEE1' TRYING CAT N ARC FROM NP/DET TO NP/I!EAD CAT N TRANS #I5 FROI-i 14:NP/DBT: 1Q( 1) TO 21 :NP/E!EtZD:2?(6) SELECTED CONFIGS (21) POH EXTENSION PICKING UP CONFIC 21:NP/I1EAD:23(6) TRYING POP ARC POP TRANS #16 FROM 21:NP/HEAD:2?(6) EXECUTING PATH (14 13 12 11 10 15 16) BEGINNING AT TRANS 14, CONFIG 20 TEST FAILED PREDICTIONS: NOTICING (1 REGISTRATION 6 19 277 O), SCORE -5 NOTICING (10 REGISTRATIOIJ 7 19 103 O), SCORE -5 PROPOSING (ONLY WHOSE) ENDING AT 19 PHOPOSING (WHOSE WHO WHICH THAT WHO14 THERE) STARTING AT 3 {EIONITORING [ ADJ N V ORD QDET ART QUANT POSS 1 ENDING AT 10, SCORE O MONITORING [ WHOSE ONLY ] ENDING AT 19, SCORE 5 MONITORING MODAL V INTEGER NPH N ADJ V.ADV P3EP ] STARTING AT 3, SCORE 0 MONITORING [ WHOSE WHO WHICH THAT WHOM TWEHE ] STARTING AT 3, SCORE 5) FINISHED THEORY 1 WITH SYN SCORE 0 {This processing took 12.5 seconds).
{Now wz will process the second theory).
SPARSER PROCESSING THEORY 2: 0 WHAT 3 6 REGISTRATION 19 23 STARTING AN ISLAND STARTING AT LEFT END OF SENTENCE SELECTED CONFIGS (1) FOR EXTENSION {Upon picking up this configuration to extend it, SPARSER finds thd transitions which were created durin~ the processing of the word I1whatfr by tha previous theory . It tttracesll them all, that is, it does not recreate them but simply puts the transition numbers on a list which will form part of the syntactic infornation associated with the current theory.
The tracing process also involves the creation of monitors (and notices, uherr applicable) for constituants along the path.
These monit~rs and notices must be remade, since the previous monitors will activate only th6 previous t heary . Due to the recursive nature of the tracin~ process, the transitions a not necessarily followad in t.he same order that they were originally created, nor are the monitors made in exactly the same order.
Notice that the many arcs which we tried but which did not result in the creation of transitions in the previous theory are not retried here.} PICEING UP CONFIG 1:S/:0(1) WITH WORD WHAT TRAGING JUMP S/Q TRANS 1 FROM I:s/:o(I) TO 2:~/~:0(6) NONITORING [ NP/ 1 TRACING CAT QWORD TRANS 2 USING "WHATu FROM 2:S/Q:0(6) TO 4:S/NP:3(11) STARTING AT 3: MONITORING [ MOBAL 1 MONITORING [ V 1 TRACING POP TRANS 4 FROM 4:S/NP:3(11) TRACING CAT QDET TRANS 3 USING "WHQ1I FROM 3:NP/:0(11) TO 5:MP/ORD:3(16) STARTING AT 3: MONITORING [ QUANT/ ] SETTING UP CONFIG 6:QUANT/:3(16) [This does not mean that confiauration 6 was just created.
Since it already existed in the map, having been craated during the p~ocessinc of the previous theory, the configuration number is merely put on the list of configurations in the current theory).
MONITORING [ INTEGER 1 TRACING JUMP NP/QUANT TRANG 5 FROM 5:~p/ORD:3(16) TO 7:NP/QUANT:3(21) TRACING JUMP NP/DET TRANS 6 FROM 7:NP/QUANT:3(21) TO 8:NPIDET:3(26) STARTING AT 3: {MONITORING [ NPR/ NPR/ NPH NPR N ADJ N V ADV 1) SETTING UP CONFIG 9:NPR/:3(26) TRACING JUMP NP/HEAD TRANS 1 FROM 8:NP/DET:3(26) TO 10:NP/HEAD:3(29) STARTING AT 3: {!¶OtJITORING [ R/ PP/ R/NIL,PREP WHOSE WHO WHICH THAT WHOM PREP THERE 1) SETTING UP CONFIG 11:R/:3(29) {NOTICING 'WHOSE" "WHOw ) SETTING UP CONFIG 12:PP/:3(29) SETTING UP CONFIG 13:R/NIL:3(29) {No proposals were made here because proposals are not theory dependent; that is, the word proposals which were made during the processing of the previous theory resulted in some words baing placed in the word lattice whLch were noticed here.
Remaking the proposals would not lead to the discovery of any new information).
TRACfNG POP TRANS 8 FROM 10:NP/HEAD:3(29) {The processinp of thz island for llregistrationw is idBntical to that in the last example, so the remainder of tha trace will be omitted.
Thz total processing took 12.2 seconds.] {Let us now process the event which adds the word "theu to ths thleory just processed.
This will result in the creation of a conutituent event).
SYNTA PROCESSING EVENT FOR THEORY#2 WIT tr NEW WQRD (4 THE 6) TO GET NEW THEORYf3: 0 WHAT 3 4 THE 6 REGISTRATION 19 23 "THEJ' TRYING (CAT ART --) FRO14 STATE NP/ONLY TO CONFIG 25 CAT ART TRANS 825 FROM 32:NP/ONLY:4()?
TO 25:NP/ART:6(6) ENDING AT 4: MONITORING [ ONLY ] PROPOSING "ONLY~~ JUMP TRANS C26 FROM 33:NP/:4(1) TO 32:NP/QNLY:4(3) EXECUTING PATH (26 25 20 19 18 17 15 16) BEGINNING AT TRANS 26, CONFIG 33 **** MADE #I FROM 4 TO 23: NP DET ART THE ADJ NP N REGISTRATION NU SG N FEE FEATS NU SG *+** NOTIFYING THEORY 3 ABOUT CONSTITUENT #l his constituent cannot ba used immediately by this theory because it contains a word ("feevf) which is not in the thaory.
Therefore a noticd is sdnt to Control which may be turned into an event at some later time.
Nothing further is done with this constituznt at this time, i.e., no transitions using it are created.
It is, howevsr, placed in the WFST for later use).
EXECUTING PATH (26 25 20 19 18 23 24) BEGINNING AT TRANS 23, CONFIG 22 (?he creation of transition #26 complctas another path).
+**+ MADE 92 FROM 4 TO 19: NP DET ART THE N REGISTRATION FEATS NU SG **** SYN WEIGHT + SEM WT = 10 + 0 = 10 {Tljis constituent is completely consistent with the current theory, that is, it is composed only of word matches already in the theory, and there are no nonitors in the WFST for it, so it is proce~sed bottom up as we have swp before.
) NP/ WAS NEVER PUSHED FOR PUSH NP/ TRANS #27 FROM 34:FOR/FOR:4(1) TO 35:T0/:19(7) ENDING AT 4: k¶ONITORING [ FOR ) PROPOSING "FORu NP/ WAS NEVER PUSHED FOR PUSH NP/ TRANS #28 FROM 36:PP/pREP:4(1) TO 37:PP/NP:19(7) ENDING AT 4: MONITORING [ PREP 1 NP/ WAS NEVER PUSHED FOR PUSH NPI TRANS #29 FROM 38:R/N1L:4(1) TO 39:S/NP:19(6) NP/ WAS NEVER PUSHED FOR PUSH NP/ TRANS #30 FROM 4O:R/WH:4(1) TO 39:S/NP:19(6) ENDING AT 4: MONITORING [ R/WHOSE 1 MONITORING [ WHICH THAT WHO M~OH WHICH WHOM ] (NOTICING "WHOlt llWHOM1l "WHICHw llWHOM'l 1 NP/ WAS NEVER PUSHED FOR PUSH NP/ TRANS 831 FROM 41:S/DCL:4(1) TO 39:S/NP:19(6) JUMP TRANS #32 FROM 42:S/:4(1) TO 41:S/DCL:4(1) ENDING AT 4: MONITORING [ PP/ 1 NP/ WAS NEVER PUSHED FOR PUSH NP/ TRANS 833 FROM 43:S/NOwSUBJ:4(1) TO 44:VP/V:19(6) JUMP TRANS #34 FROM 45:S/AUX:4(1) TO 43:S/NO-SUBJ:4(1) ENDING AT 4: {MONITORING [ MODAL NEG V 1) {NOTICING "IS" IIARE" l'PAYW) NP/ WAS NEVER PUSHED FOR PUSH NP/ TRANS 135 FROM 46:S/Q:4(1) TO 39:S/NP:19(7) ENDING AT 4: MONITORING [ QADV ] NP/ WAS NEVER PUSHED FOR PUSH NP/ TRANS 136 FROM 47:VP/HEAD:4(1) TO 48:VP/NP:19(7) ENDING AT 4: MONITORING [ PARTICLE ] MONITORING [ V ] NOTICING "PAYn JUMP TRANS 837 FROM 49:VP/V:4(1) TO 47:VP/HEAD:4(1) ENDING AT 4: {MONITORING [ NP/ NP/ V ADV V ]] (NOTICING "IS1' "ARE" "PAY" l'IS" "AREt1 "PAY") JUMP TRANS 138 FROM 45:S/AUX:4(1) TO 49:VP/V:4(1) JUMP TRANS #39 F~~OM 43:S/NO-SUBJ:4(1) TO 49:VP/V:4(1) JUMP TRANS #40 FROM 43:S/NO=SUBJ:4(1) TO 49:VP/V:4(1) JUMP TRANS 141 FROM 50:S/THERE:4(1) TO 49:VP/V:4(1) ENDING AT 4: MONITORING [ THERE .J PROPOSING "THEREw NP/ WAS NEVER PUSHED FOR PUSH NP/ TRANS 842 FROM 51:VP/NP:4(1) TO 52:VP/VP:19(6) ENDING AT 4: MONITORING [ NP/ ] JUMP TRANS #43 FROM 47:VP/HEAD:4(1) TO 51:VP/NP:4(1) NP/ WAS NEVER PUSHED FOR PUSH NP/ TRANS #44 FROM 49:VP/V:4(1) TO 44:VP/V:1?(7) SELECTED CONFIGS (35 37 39 44 48) FOR EXTENSION PICKING UP CONFIG 35:TO/:19(7) STARTING AT 19: MONITORING [ NEG 1 MONITORING [ TO 3 PROPOSING If TOtf ALL ARCS TRIED AT THIS CONFIG PICKING UP CONFIG 37:PP/NP:19(7) TRYING POP ARC POP TRANS #45 FROM 37:PP/NP:19(7) PICKING UP CONFIG 39:S/NP:19(7) STARTING AT 19: MONITORING [ MODAL 1 MONITORING [ V 1 TRYING POP ARC POP TRANS #46 FROM 39:S/NP:19(7) EXECUTING PATH (32 31 46) BEGINNING AT TRANS 32, CONFIG 42 **** MADE $3 FROM 4 TO 19: S NPU NP DET ART THE N REGISTRATION FEATS NU SG WITH FEATURES (NPU) +*** SYN WEIGHT + SEM WT = 10 + 0 = 10 PICKING UP CONFIG 44:VP/V:19(7) STARTING AT 19: {MONITORING NP/ N-QDET ADJ INTEGER ARE QUANT PRO NPR POSS V V ADV TEST(N0T (CAT V)) I} SETTING UP CONFIG 20:NP/:19(7) NOTICING "FEE" ALL ARCS TRIED AT THIS CONFIG PICKING UP CONFIG 48:VP/NP:19()?
STARTING AT 19: {MONITORING [ COMPL/ TO/ COMPL/ NP/ FOR THAT TO FOR THAT N QDET AD3 INTEGER ARE QUANT PRO NPR POSS V PARTICLE 1) SETTING UP CONFIG 53:COMPL/:19(7) SETTING UP CONFIG 35:T0/:19(7) SETTING UP CONFIG 53:COMPL/:19(7) Page 75 SETTING UP MNFIG 20:NP/:19(7) NOTICING "FEE" TRYING JUMP VP/VP ARC JUMP TRANS #47 FROM 48:VP/NP:19(7) TO 52:VP/VP:19(9) SELECTED CONFIGS (52) FOR EXTENSION PICKING UP CONFIG 52:~~/~P:19(9) STARTING AT 19: MONITORING [ PP/ 1 SETTING UP CONFIG 30 : PP/ : 19 ( 9) MONITORJNG [ PREP 1 MONITORING [ PREP ] TRYING JUMP S/VP ARC JUMP TRANS 648 FROM 52:VP/VP:lQ(g) TO 54:S/VP:19(12) SELECTED CONFIGS (54) FOR EXTENSION PICKING UP CONFIG 54t S/VP: 19 ( 12) TRYING JUMP $IS AHC JU!lP TRAMS #49 FROll 54:S/VP:lQ(12) TO 55:S/S:la(lll) Sl2LECTED CONFIGS ( 55 ) FOR KXTl<i!SION PICKfNG UP CONFIC 55;S/S:19(14) TRYING POP ARC POP TRANS #50 FROM 55:S/3:10(14) PREDICTIONS: NOTICING (4 FEE 19 23 155 O), SCORE 5 NOTICING (19 WHO 3 4 -180 O), SCORE 10 NOTICING (21 IS 3 4 -39 O), SCORE 10 NOTICING (23 ARE 3 4 -128 O), SCORE 10 NOTICING (25 PAY 3 4 -146 O), SCORE 5 PROPOSING (TO) STARTING AT 19 PROPOSING (ONLY FOR WHOM WHICH THERE) EIJDI~~K AT 4 PROPOSING (V IIODAIa) FHOi1 3 TO 4 PROPOSING (MODAL PREP) STARTING AT 3 PROPOSING (PREP MODAL NEG QADV) ENDING AT 4 [The lengthy summary of monitor,a set by this event is omit tad.
) CREATING THEORY 3: 0 WHAT 3 4 THE 6 REGISTRATION 19 23 WITH SYN SCORE 15 {This took 30.8 seconds).
{Now we will process the constituent event for the theory just created.
Because of the constituent for "the registrationm there art3 now monitors in the WFST for a noun phrase beginning at position 4, so ths appropriate transitions are made).
SYNTAX PROCESSING EVENT FOR THEORYiI3 WITH CONSTITUENT #1 TO GET NEW THEORYf4 0 WHAT 3 4 THE 6 REGISTRATION 19 FEE 23 {Processing begins exactly where it left off when the constituent was made -thz constituent is semantically evaluated with rwspact to this theory so that the constituent weight may be altered.
In this case, howevar, Semantics has been turned off, so them is no increment in the score).
SYN WEIGHT + SEM WT = 15 + 0 = 15 NP/ WAS PUSHED FOR AT CONPIG 34 PUSH NP/ TRANS #51 FHOll 34:FOR/FOH:I+(l) TO 96:T0/:27('11) {Similar transitions are set up for all 9 other confipurationu where an NP/ was used in the previous theory.
The monitors set by these path8 are copied from the previous theory, 80 there is no indication h+rz of a new monitor beinp created).
SELECTED CONFIGS (58 57) FOR EXTENSION PICKING UP CONFIG 5R:S/NP:23(10) TRYING POP ARC POP TRANS 961 FRO11 58:S/NP:23(10) EXECUTING PATH (32 55 61) BEGINNING AT TRANS 55, CONFIO 41 DOING PUSH ARC WITH #1 FROM 41:S/DCL TO 58:S/NP DOING POP ARC FROt1 58:S/NP **** MADE #4 FROM 4 TO 23: S NPU NP DET ART THE ADJ NP N REGISTRATION NU SC N FEE FEATS NU SG WITH FEATURES (NPU) **** SYN WEIGHT + SEM WT = 15 4 0 = 15 PICKING UP CONFIG 57:PP/NP:23(11) TRYING POP ARC POP TRANS #62 FROM 57:PP/NP:23(11) PRQDICTIONS : NOTICING (19 WHO 3 4 -180 Q), SCORE 10 NOTICING (21 IS 3 4 -39 O), SCORE 10 NOTICING (23 ARE 3 4 -128 O), SCORE 10 NQTICING (25 PAY 3 4 -146 O), SCORE 5 PROPOSING (V MODAL) FROM 3 TO 4 PROPOSING (MODAL PREP) STARTING AT 3 PROPOSI~~G (PREP MODAL NEG QADV) ENDING AT 4 {Again, the monitor list is omitted for considerations of space.
) CREATING THEORY 4: 0 WHAT 3 4 THE 6 REGISTRATION 19 FEE 23 WITH SYN SCORE 15 {This avant took only 9.7 seconds).
The procrssinp of the final event, that which adds the word "isw to the theory Just created, will not bz shown.
ThesB examples have shown that SPARSER is a u~oful tool in th* automatic reco~nition df speech.
The t iminp measurements indicate that considerable procesain~ iu done when the parser is forced to work in bottom ~p mode, especially with a large Eranmar Of course there is some implementaion ovdrhead involvdd in doins the timin~s themselves.
If the paruinc al~orithm were to be carefully recoded in assembly langua~e a speed up of at least a factqr of 20 (and perhaps much nore) could be achieved.
Another way to cut down the time-con sum in^ processing mipht be to at-tempt to obtain gar6 semantic ~uidance.
For sample, if the semantic hypothesis asuociated with a theory indicates that a particular noun is likely to be used in a noun phrase modifier (ern& lttomorrowtt), than SPARSER should be able to take advanta~r of this information by scorinp the PUSH NP/ transition from a confi~uration for atata PP/ (i.e.
to pet something like 'Iby tomarroww) hi~har than those PUSH NP/ transitions for other syntactic slots.
In fact, th* others may not need' to be constructed at all.
The Erammar could also be further tuned to eliminate soma spurious predictions and reduce the time spent following erroneous paths.
Section 7 Conclusions and Further Research */.I STHENGTHS AND WEAKNESSES OF SPARSER One of the weak points of the current system is the fact that some context Information is not uued until a path is complete, result in^ in the creation of false paths and predictions which should not have been nade.
This is partly miti~ated by the fact that this avoids a too rea at dependence on left context and allows the creation of partial paths which may be followed if an earlier word is changed.
It is important, however, to rininize the number of predi'ctions which are made and to ~ake the predictions as accurate as possible.
In this r~~ard, it is unfoqtunatc that the currqt system makes predictions on the left of an island solely on the bavis of the first word in the island and a makes predictions 6n the right end from confi~urations which, if context sensitive tests had besn done alone the path, would never nave been created.
One way to help tighten the predictions would be to take each context free path threu~h an island and walk it in a special mode after the island has been processed but before predictions are communicated to the cohtrol component, This mode would set and check registers, assuming that any tests which require unknown left context are true.
Only if the path did not fail under this mode of operation would the pr~dictions at aither end pf it be made.
If a really efficient way of handlinp unknown left context and of storinp this informafion ware developed, it could be uued in place of the context free pass in the first place, thus a1 lminatinp a 11 inconsistent paths.
The problem with st or in^ all possible contexts is that they must be ~ecomputed each time a new atep is added to the path.
Nia ie relatively easy if th* next step is taken to the ripht of an 2xistin~ path, since ATN s are mora auitdd to left to ripht processing, but it becomes extremely complex when a transition is added to the left end of a path (or set of paths) or when a transition Joins two sets of paths togethzr.
To be absolutely sure that no contexts haw been misued, all the paths would have to be walked and their contexts reprocessed and copied in whole or in part (since the new step may be wronR, the old context rnuvt be preserved).
Of eoursa this is not the only approach which could be .used -a merninp technique like that of Earley's algorithm might be feasible, if the structure of the crammar were also chanped to make it less left to ri~ht oriented.
One ~reat ytren~th of the system is its ability to store and merge information in such a way that it does not hava to be redone when tha context is changed.
For example, once an arc has bean tried with a particular word match, a transition will be created if the arc may be taken and the arc will ba removed from further consideration if it may not be taken.
Then, if the configuration should ever be reached with the same word match apai'n (perhaps in a later theory) not only will any relevant transitions be reco~nized without havinp to po throuyh the work of r*-creatin~ them, but also ne arc8 whic_h had prev,Lou3+ly failed, Y~ ec hk ~&~kd* Another feature of SPARSER.
in the fact that it \~i4~ ~!trsiyned and implemented with many unsolved problar~ :jnd unsviiiluble bats in mPnd, and therefore many wl~oleall have been lef't on which to lfhooku further developments.
For exnmle, a1 t h~urh pro::~l!ic verification of constituents is not yet available, the scorinr rntrchani:~~ for ~wnst ituenty is structured in such a way that it would be easy to include the results of verification by prcsodics (or any other component).
Th* oripinsl implenen'tation of SPAHSEH used a depth first search but Qas implemented in such a way that the chanre to dodified breadth first was quite simple.
This foresight has paid off in a flexible systen rrhich has shown that it can be readily experimented with in o~der to explore many still unsolved problems ccrncoerning the nature and use of syntactic information in understandinp A tremendous amount of information in speech is conveyed by proscdic features: stress, intonation duration, loudness, pauses, pitch.
For example, if John mumbles to Bill, ??The mailman left something for you," Bill may reply aithar IrWhat"? with much energy and a sharply risih'p intanation or "What?I1 with a flat o'f.
falling intonation.
In th~ first case John is vzry likely to shout 111 said, Ths mailman left something for you interpreting lvWhat?lv to mean "What did you say"? whereas in the second case he is likely to say something like *A package from youp rnothw," interpreting ltWhat?vv to mean "What is it"?
To i~nora prosudics iu to ipnore a sourca of information which has bssn shown repeatedly to be an extremely important factor in human underatandinp.
Consider the following examples of sentences and sentence fhpmehts which illustrate some of the ways prosodie8 are used: 1.
I ~trppad on the man with black yhoes.
(Who wau wearing the ~h~as)? 2a.
The new gnu knew news.
2b. Ths Rnu knew new Rnus.
3 I
m ~oing to move on Thursday.
(stress on "move1n implies moving to a new house; stress on fronw imp1 ies traveling to a new place).
4a. Can you swim to Daddy? 4b.
Can you swim too, Daddy? 5a.
. ..
two-fifty for . . . 5b . . . two-fifty-four ., . Prosodic vmification could help a lot in reJect ing semantically correct, syntactically consistent phrases which are nonethsless wrong.
If the constituent "speech understandingtf were identified and relied upon, it might be very difficult to produce a correct analysis of the utterancs: wBd~a~~d of peculiarities in his speech, understanding Joa is not easymN Besides indicatinp syntactic boundaries and/or providinp intonation contours for certain constituents, prosodic features can be used to mark emphasis, introduce new topics, cpnvey information about the speaker s internal mental and t:notion:il state (e.p.
whether hl is teasinp or ~erioua), nnd probably more.
It is particularly interestin~ to note that some well known phenomena ~uch us l'pranouns are almost never atrt*suedU :in3 "ip discourse wh~n new topic word 1:: gention~bd it is alcost always stressedm hhve very naturtil explainat ions in ll~ht of what we know about acoustic processing.
Stressed war-da are ~anerdlly easier to identify because there is less acoustic aztbicuity, but unstressed words may differ creatdy f rorn their ideal pronunciation and hence are harder to reliabl y identify Pronouns refer to antecedents which are presu~ably known to the listener, so he can anticipate them or at leaut verify them easily, hence they need not have mod acoustic characteristics.
A new topic may not have been anticipated, so the listener will have to depend heavily on identify in^ the word from acoustic information alone and the speaker can provide this extra reliable information by stressine the word.
Unfortunately, not a przat deal is known about either the acoustic correlat~s of prosodic features or the ways in which they are used.
Manyof the rules which have bezn developed thus far are speaker dependent and are snfficient for convey in^ information but are not necessary.
This makes them difficult to use in thz analysis mode.
Althou~h a good start has been made in exploring pr-osodies (see, for example, Lea [52, 131 and Eates and Wolf [8]), much more work remains to be done before prosodic Page 83 information can be reliably used by speech understandinc systems.
SPARSER could use prosodic information in several ways.
Verification of constituents would be a prest help, but local proaicid information could be used ev*n earlier in the parsing p~oceas.
For example, if maJor constituent boundaries could be accurately determined, then inatead of both POPing a constituent and continuing it in parallel, as is done now, one alternative could be chosen inatdad of the other on the badis of prosodic informal ion.
If, as is more likely, yome major boundariey could be reliably detected, then it would be easy to revise SPARSER to begin procsssinp at such places even #within an island at states which can begin constituents.
This would a~ain reduce the number of partial paths created when pars in^ an island.
7.3 EXTENSIONS
AND FURTHER RESEARCH One of the obvious extension8 to a basic speech underatandinp . . system is to relax the restrictions on the input to the system.
Syntactically, this can mean removing the requirement that the initial utterance be prammatical.
Since people frequently speak unprammatically in informal discourse, this is a natural step to want to take.
In order to extend SPARSER to handle such input, sevaral approaches are possible.
Certain types of errors may be called errors of style (and may not be called errors at all by some people) such as the use of tlain'tlt and the occurrence of a prepositjon at thd and of a sentence.
These resularit ies r?:ly simply be declared ~ramrnatical by ~0difyin.r the pr&+nnur to accept them.
Hany speech errors have hern shown to fQcllc-\: rt~\:l~~:* piitterns and hence gay ht? :~vn:tble to t!lis :ippl*~;~ck.
t her PC~V~!OII 1 speci 1"ic t tbst s \~?:ic!? :jp;-kS:+!p .! the arcs at' the r, f 1 t ?r*~hikit i+t~:~-l~net is n t 0 check for nu~ber a~~trtwven t bet wet'11 s1:5.-~vt :iz:q verb or between determiner itnd noun (c.~.
*I is SPY Y=~~*V severe restrictions on this rule.").
In this case, rather t!?ap renovinp the tests from the yramar it w~uld ~CJ rcre suita?le tc & modify the^ SP th;it if t 1 t -arc 1 still hr L~.<C*Y, thourh it h a :nvch reeuce~ w~~ir!~t or \:it h ;in icbicat icn in sere re~ister that an error has occurred.
Cne way to in2lzxent this would be to have all tests return a nu~bttr as their value indicating how well they succeeded on some sca& fr~a "perf"ect1y" to ltnot at all".
Not all arc tests are of this reIaxablc! nature, however, since certain types of errors are so rare, if they occur at all, that they may be Judged ~nacceptati~.
Examples of such tests are the case checks for pronouns (2.~.
*"I pavz it to he1'] and the requirement that a verb modifyinp.
a noun must be in either the present or past participle form (2.g.
Itthe sincinc brookn vs.
*"ths sinp brook1').
These methods would not allow all tvpes of qran~atical errors to be handled (in particular it irnores the problen of constituent ordering errors such as ItThrow !lama from the train a kissw), but uould handle many of the most common syntactic errors.
& experiment hoepinr in mind that SPARSER is not intended to be a mo'dd ~f human svntactic analysis, it is nonetheless reasonable to ask whether there are anv ~imilarities which may be seen.
The followinp experiment is suppest ed with the hypothesis tkit it will indicate that people do considerable processing at the end of svntactic constituenks in a way similar to-some repister set tin^ and testing actions and szmantic (or othw) verification The experiment is this: a subject is seated in front of a switch which hi is asked td press whenever ha is surd that he is hear in^ an anomalous sentence, He is then presented with a pulaber of recorded utterances, some of which are incorrect, e.~.
{'The cat and dog which live3 next door are friendly.
I sawma red big barn on f he farm.
1 hypothesize that the subject will indicate the presence of an ewer at a point shortry after the end of the constituent in uhich the error occurred more often thand shortly after the earliest possible place where the error coul be detected.
In conclusion, it is obvibds t~gf there is much work yet to be done in the problem.of speech undzrgtanding, but it is hoped Dha( the system presented behas no.t, only advadczd our current Page 87 APPENDIX I MINIGRAMMAR This appendlx contains a listing (slightly edited for clarity) of theqrammar called MINIGRAMMAR which was discussed in Section Three (irlustrated in Fi~ure 3.3) and which was used in Seatdon Six.
(NP/ (CAT ART (T T) 5 (SETR#ART (BUILDQ ((ART *)))) (TO' NP/ART) ) (JUMP NP/ART (T T) 4)) (N'P/ADJ (CAT N (T T) 5 (SETR #'*I (SETR NU (GETFEATURE NUMBER)) (TO NP/N)) (CAT N (T T) 2 (ADDL ADJS (BUILDQ tADJ (NP (N *) ( N.U (GETF;EATURE NUMBER))) (TO NPIADJ))) ( NPIART (CAT QUINT (T, T) 4 (SETR QUANT (BUILDQ ((QUANT *))I) (TO NPIQUANT) 1 (JUMP NPIQUANT (T T) 5)) (NP/QUANT (CAT ADJ (T T) 4 (ADDR ADJS (BUILDQ (@ (ADJ) (*I FEATURES)) (TQ NP/QUANT)) (JUMP NP/ADJ (T T) '4 1) (NP/N (PUSH PP/ ((PPSTART) 7'.
TI 4 (ADDL NMODS *) (TO NP/N)) (POP (BUILDQ (Q (NP) + + + ((N +)I ((NU +)I + ART QUANT ADJS N NU NMODS) IT (DETAGREE)) 5) (PP/ (CAT PREP (T T) 5 (SETR PREP *) (.TO PP/PREP) ) ) (PP/PREP (PUSH NP/ ( (NPSTART) T T) 5 (SETR NP *) (TO PP/NP))) -C (PP/NP (POP (BUILDQ (PP (PREP +) +) PREP NP) (T TI 5)) APPENDIX I1 Vocabulary and Syntax CIaaaes This appendix lista the 351 words which wwa fn the dictionary of the BBN speech understanding ayatem wtkn the examples in Chapter Six ware run (July 19n).
(A 569, word dictionary and one with 1000 entries arc now available.
After the listing of the wards in the dictionary, Obey ara.broken into syntactic classes, with the number of words in.
each alas3 indicated beside the class name.
Finally, the ~yntactic features are given together with a list of the word8 which carry each feature.
Features may be of the form FEATURE, (FEATURE), or (FEATURE VALUE).
This is not a listing of tha dictionary as dt appears to the systerrt, but rather a derived crosv reference which indicates the various parts of speech and ~yntactic features for each word Tha words: (A ABOUT ABOVE ACL ACOUSTICAL ACOUSTICS ADDITIONAL AFFORD AFTER A1 AIR AIRPLANE ALL ALREADY ALSO AM AMHERST AMOUNT AN AND ANTICIPATE ANY ANYONE ANYWHERE APRJL ARE ARPA ARRANGE ASA ASK &SSOOIATION ASSUME ASSUMPTION AT ATTEND AUGUST AVAILABLE BATES 3E BECAUSE BEEN BEFORE BEGINNING BEING BIG BILL BpNNfE BOSTON BDTH BREAKDOWN BUDGET BUS BY CALIFORNIA CAN CANCEL CAR CARMEGIE CENT CHANGE CITY COLARUSSO COMPUTATIONAL CONFERENCE CONTXWUE COSELL COST COSTING COSTS COUNTRY CRAIG CURRENT DATE DAY-E IAY DECEMBER DENNIS DID DIVISION DO DOES DOLLAR DONE DUE@TO DURING EACH EIGHT EIGHTEEN EIGHTEENTH EIGHTH EIOHTY EITHER ELEVEN ELEVENTH END ENGLAND E~OUGH ESTIMATE-N ESTIMATE-V EVERY EVERYONE EXPECT EXPENSE EXPENSIYE FALL FARE FEBRUARY FEE FIFTEEN FIFTEENTH FIFTH FIFTY FIGURE FINAL FIRST FISCAL FIVE FOR FORTY FOUR FOVRTEEN FOURTEENTH FOURTH GET GETS GETTING GIVE GIVEN GIVES GIVItJG GO GOES CO3'NG GONE GOT GOTTEN CROUP WAD HALF HALVES HAS HAVE HAVING HE HER HIM HIS HPw HOWMANY HOWMUCH HUNDRED 1 IF IFIP IJCAI IN INTERNATIONAL IS IT JANUARY JERRY JOHN JULY JUNE KNOW L.A.
LAST LATE LEFT LINDA L,INGUISTICS LIST LONDON LONG LOSeANGELES LYN LYNN.
MADE MAKE .MAKES MAKMOUL MA~INC MANY MARCH MASSACtIUSETTS MAY ME HEETING MEMBER MISCELLANEOUS MONEY MONTH MORE MOST MUCH MY NEED NEW@YORK NEXT NINE NINETEEN NINETEENTH NINETY NINTH NQ NOT NOTE NOVEMBER NOW.
OCTOBER OF ON ONE ONLY OH OTIIER OUT OVERHEAT, PAJARRDPDUNES PARTICIPAIJT PAUL PAY PENNSYLVANIA PEOPLE PER PEHSON PHONOLOGY PITTSBURGH PLACE PLEASE PLUS PRINT PROJECT-N PROJECT-V PURPOSE QUARTER REGISTRATION REMAIN REST REVISE RICH RICHARD ROUNDeTRhIP SANTAeBARBARA SCHEDULE SECOND SEND SENDING SENDS SENT SEPTEMBER SkVEN SEVENTEEN SEVENTEENTlI SEVENTH SEVENTY SHE SINCE SETE SIX SIXTEEN SIXTEENTH SIXTH SIXTY SO SOCIETY SOME SOMEONE SPEECH SPEND SPENDING SPENDS SPENT SPRING ST.LOUIS START STATUS STOCKHOLH SUMMER SUPPOSE SUPPOSED SUPPOSITION SUN SWEDEN TAKE TAKEN TAKES TAKING TEN TENTH THAN THANKQYOU THAT THE THEIH THEM THERE THESE THEY THIRD THIRTEEN THIRTEENTH THIRTIETH THIRTY THIS THOSE THOUSAND THREE TIME TO TOO TOOK TOTAL TRAVEL TRIP TWELFTH TWELVE TWENTIETH TWENTY TWO UNANTICIPATED UNDUDCETED UNSPEilT UNTAXEN NUS VARIOUS VISIT WANT WAS WASHINGTON WE WEEK WENT WERE WHAT WHEN WHERE WHICH iJHO WHOM WHOSE WILL WINTER WISCONSIN WITH WITHIN WORKSHOP YEAR YES YOU) The syntactic categories: (ADJ 23 (ACOUSTICAL ADDITIONAL AVAILABLE BIG COI-IPUTATIONAL CURRENT EACII ENOUGH EXPZNSIVE FINAL FISCAL INTERNATIONAL LATE LEFT LONG MANY MISCELLANEOUS OTHER UNANTICIPATE~ UNBUDGETED UNSPENT UNTAKEN VARIOUS)) (ADV 18 (ALREADY ALSO ANYWHERE EITHER ENOUGH WOW [,ATE LONG NORE MOST MUCH NOW ONLY PLEASE SO THERE TOO YES)) (ART 8 (A AN NO THAT THE THESE THIS THOSE)) (CONJ 8 (AND BECAUSE BOTH IF OR PLUS SINCE SO)) (IN.TEQER 27 (EIGHT EIGHTEEN EIGHTY ELEVEN FIFTEEN FIFTY FIVE FOR*TY FOUR FOURTEEN NINE NINETEEN NINETY ONE SEVEN SEVENTEEN SEVENTY SIX SIXTEEN SIXTY TEN THIRTEEN THIRTY THREE TWELVE TWENTY TWO)) (MODAL 5 (CAN DID DO DOES WILL)) (N 70 (ACOUSTICS AIR AIRPLANE AMOUNT ASSOCIATION ASSUMPTION BEGINN-ING BREAKDOWN BUDGET BUS CAR CENT CHANGE CITY CONFERENCE COST COUNTRY DATE DAY DIVISION END ESTIMATE-N EXPENSE FALL FARE FEE FIGURE GROUP HALF HALVES LINGUISTICS LIST MEETING MEMBER MONEY MONTH MUCH NEED NOTE OVERHEAD PARTICIPANT PEOPLE PERSON PHONOLOGY PLACE PROJECT-N PURPOSE QUARTER REGISTRATION REST ROUNDQTRIP SCHEDULE SITE SOCIETY SOME SPEECH SPRING STATUS SUMMER SUPPOSITION TH4NKeYOU TIME TOTAL TRAVEL TRIP VISIT WEEK WINTER WORKSHOP YEAR)) (NEG 1 (NOT)) (NPR 53 (ACL A1 AMHERST APRIL ARPA SA AUGUST BATES BILL BONNIE BOSTON CALIFORNIA CARNEGIE C:ILARUSSO COSELL CRAIG DECEMBER DENNIS ENGLAND FEBRUARY IFIP IJCAI JANUARY JERRY JOHN JULY JUNE L.A.
LINDA LONDON LOS@ NGELES LYN LYNN MAKHOUL MARCH MASSACHUSETTS MAY NEWCYORK NOVEMBER OCTOBER PAJARROeDUNE-S.
PENNSYLVANIA PITTSBURGH RICH RICHARP SANTA~BARBAHA SEPTEMBER ST.LOU1S STOCKHOLM SUR SWEDEN WASHINGTON WISCONSIN)) (ORD 23 (EIGHTEENTH EIGHTH ELEVENTH FIFTEENTH FIFTH FIHST F~URTEBNTH FOURTH LAST NEXT NINETEENTH NINTH SECOND SgVENTEENT11 SEVENT.H SIXTEENTH SIXTH TENTH' THIRD THIIjTEENTH Tflf RTIETH TWELFTH TWENTIETH) ) (PARTICLE 3 (IN ON OUT)) (POSS 5 (H~R HIS MY THEIR WHOSE)) (PRECONJ 2 (BOTH EITHER)) (PREP 18 (ABOUT ABOVE AFTER AT BEFORE BY DUEeTO DURING FOR IN OF ON OUT PER SINCE TO WITH WITHIN)) (PRO 23 (ANYONE EVERYONE HE HER HIM I IT ME ONE SHE SOMEONE THAT THEM THESE THEY THIS THOSE US WE WHAT WHO WHOM YOU)) (QADV 2 (WHEN WHERE)) (QDET 5 (HOWNANY HOWMUCH WHAT WHICH WHOSE)) (QUANT 14 (ALL ANY BOTH EACH EITHER ENOUGH EVERY HOWMANY HOWMUCH MANY MORE MUCH OTHER SOME)) (SPECIAL 8 (DOLLAR HUNDRED K NO THAN THANKdlOU THOUSAND YES) ) (.V 85 (AFFORD AM ANTICIPATE ARE ARRANGE ASK ASSUME ATTEND BE BEEN BEGINNING BEING BUDGET CAN CANCEL CHANGE CONTINUE COST COSTING COSTS DID DO DOES DONE END ESTIMATE-V EXPECT FIGURE GET GETS GETTING GIVE GIVEN GIVES GIVING GO GOES GOING GONE GOT GOTTEN HAD HAS HAVE HAVING IS KNOW LAST LEFT LIST MADE MAKE MAKES MAKING NEED NOTE PAY PRINT PROJECT-V REMAIN REVISE SCHEDULE SEND SENDING SENDS SENT SPEND SPENDING SPENDS SPENT START SUPPOSE TAKE TAKEN TAKES TAKING TOOK TOTAL TRAVEL VISIT WANT WAS WENT WERE WILL)) Ths syntactic features: (INGCOMP (CANCEL CONTINUE GIVE START START)) (INTRANS (CONTINUE GO GO GO START)) (PASSIVE (CANCEL CONTINUE FIGURE GET GIVE MAKE MAKE SEND SEND START START TAKE) ) (QCOMP (CANCEL CONTINUE FIGURE GIVE SEND SEND TAKE)) (THATCOMP (END FTGURE)) (TRANS (CANCEL CONTINUE END FIGURE GET GIVE MAKE MAKE SEND SEND START START TAKE)) ((ANAPHORIC) (WHICH)) ((DETERMINED) (ANYONE I IT ME THAT THESE THIS THOSE US WE WHO WHOM YOU)) ((INDOBJ FOR) (MAKE HAKE)) ((NUMBER PL) (HALVES PEOPLE THEM THESE THEY THOSE US WE)) ((NUMBER SO) (ANYONE ME HER HIM I IT ME ONE SHE SOMEONE THAT THIS WHO WHOM WHOM)) ((NUMBER SG/PL) (WHAT Wi3AT WHICH WHO YOU)) ((PARTICLEOF (LEAVE PUT ADD)) (IN)) ((PARTICLEOF (ADD CONTINUE)) (ON)) ((PARTICLEOF (LEAVE PRINT SEND MAKE CANCEL FIGURE FIND GO)) (OUT) ) ((PASTPART) (BEEN COST DONE GIVm GONE GOTTEN HAD LEFT MADE SENT SPENT TAKEN ) ) ((PNCODE 13SG) (WAS)) ((PNCODE 1SG) (AM)) ((PNCODE 3SG) (COST COST COST COST COSTS DOES DOES GETS GIVES GOES HAS IS MAKES SENDS SPENDS TAKES)) ((PNCODE ~IJ (CAN CAN DID)) ((PNCO'DE X13SG) (ARE WERE)) ((PNCODE X3SG) (COST DO WILL)) ((PRESPART) (BEGINNING BEING COSTING GETTING GIVING GOING HAVING MAKING SENDING SPENDING TAKING) ) ((ROLE OBJ) (HER HIM ME THEM US WHOM WHOM)) ((ROLE SUBJ) (HE I SHE WE)) ((ROLE SUBJ/OBJ) (WHAT WHICH WHO WHO)) ((TNS FUTURE) (WILL WILL)) ( (TNS PAST) (COST DID DID GOT HAP MADE SENT SPENT TOOK WAS WENT WERE) ) ((TNS PRESENT) (AM ARE CAN CAN LJST COST COST COST COST COSTS DO DOES DOES GETS GIVES GOES HAS IS MAKES SENDS SPENDS TAKES WILL) ) ((UNTENSED) (BE WILL)))) ) BIBLIOGRAPHY C1] Baker, James K.
(1975b) St-ochastic Modeling as a Means of Automatic Speech Rewgai tAw PhD thesis, Speech and Computer Sciepca, Carne~ia-Mallon Univ., AprW 1975 r2] Barnztt, Jeffrey (1973) A Vocal Data Management System IEEE Trans.
on Audio and Elactroacoustics, AU-21:3 (June 1973) p.
185-188 [31 Bates, M.
(1974) The UY~ of Syntax in a Speech under st end in^ System in Erman IEEE Symposlum on 'Speech Rclcogni t ion, p.
226-233 [4] Pates, M.
(1975) Syntactic Analysis in a Speech Understandine System Ph.D. thesis, Harvard Univeruity,l975 also BBN Report No.
3116, Bolt Beranek and Nswman Ind., Cambridge, Mass.
Aucust 1975 [5] Bates, N..and 901f, J.
(1975) Prosodies technical report in BBN Speech Understanding system QPR noi 2, BBN Report no 3080 (A1 Report no.
30), BoltBargnek and Newman Inc, Cambridge, Mass.
(MEW, 1975) 161 Bruce, Bertram (1976) Pragnoat ics.
in Speaeh Understanding Procaedinps of the IJCAI, 1976 C71 Erman, Lee (ad).
(1934) IEEE Symposium on Speech hzcsgnition IEEE, Inc.
(1974) 18) Forgria, James W.
(1974b) Speech Understanding Systems;-Semiannual Technical Summary Lincoln Laboratory, Lexington, Mass., 31 May 1974 C91 Fu; K.C.
(1976) SYNTACTIC PATTERN RECOGNITION Springer-VerJan* N.Y.0 [lo] Hyde, S.R.
(4368) Automatic Speech Recognition: Literatbre Surveye and Discussion Report No.
45, Post Office Research Dept.
Dollis Hill, London, 1968 [I11 Klovstad, John W.
(1975) personal communication 12.
W. Lea, M.
Medress, and T.
Skinner. 1972.
Prosodic aids to speech recognition: I.
Basic algorithms and stress studies, Univac report no.
PX7940, Oct.
1972. 13.
W. Lea, M.
Medress, and T.
Skinner. 1973.
Prosodic Aids to speech recognition: II.
Syntactic segmentation and stressed syllable location, Univac report no.
PX10232, 1973.
14. Richard J.
Lipton and Lawrence Snyder.
1974. On the optimal parsing of speech, research report 37, Oct.
1974, Yale.
15. Bonnie Nash-Webber.
1974. Semantic Support for a speech understanding system, IEEE trans.
on ASSP, 1975.
16. R.
B. Neely.
1973. On the use of syntax and semantics in a speech understanding system.
PhD. thesis, Stanford, 1973.
17. A.
Newell, et.
al., 1973.
Speech understanding systems: final report of a study group, North-Holland, Amsterdam.
18. William H.
Paxton, 1974.
A best-first parser, in Erman, IEEE Symp.
on Speech Recognition.
19. William H.
Paxton and Ann E.
Robinson, 1975.
System integration and control in a speech understanding system.
Tech. note 111, Artifical intelligence center, Stanford, 1975.
20. D.
R. Reddy, L.
Erman, and R.
B. Neely.
1970. The CMU speech recognition project, IEEE System Sciences and Cybernetics Conf., 1970.
21. D.
R. Reddy, et.
al., 1973.
The HEARSAY speech understanding system: an example of the recognition process.
IJCAI, Stanford, 1973.
22. D.
R. Reddy, ed.
1975. Speech Recognition.
IEEE Symposium, Academic Press, NY.
Rovnzr, P., Nash-Wabber, B., and Woods, W.
(1974) Control Concepts in a Speech Undervtanding System IEEE Trans.
on ASSP,, 32:1 (Fab: 1975), p.
136-140 Aovn~r, P., at .al.
(1974) Where the Words are: Lexical Rrtriaval*in a Spaech Undervtanding Sy~tem in Erman, IEEE Symposium on Speech Recopnition, p.
160-164 Rustin, Randall (ad).
(1973) NATURAL LANGUAGE PROCESSING Alcorithmica Pray, NwY.
1973 Schwartz, Rw and Makhoul, Jm (1074) Where the Phoneme8 are: Dealinn with Arnbirulty in Acouutic-Phonetic Heco~nition IEEE Trans.
on ASSP, 231 (Feb.
11)75), p.
50-53 Shirai, K.
and Fujiuawa, H.
(197)? An Al~orithm for Spoken Sentence Recognition and its Application to the Speech Input-Output Sy~tern IEEE Trans.
on Systemu, Man, and Cybrenetics, Sept 1924, p.
475-479 Walker, Donald Ew (1972) Speech Understanding Research SRI Annual Technical Report,'~ct 1071 0ct 1972 Walker-, Donald E.
(1973aJ Speech Understanding Throuph Syntactic and Semantic Analysis Prac.
3rd IJCAI, Stanford, Aup.
1973, p.
208-215 Walker, Donald E.
(1973b) Automated Lan~uape Proce~sinfl in Cuadra (ed.), Annual Review of Information Science and Trchnolohy, vol 8 [31] Wolf, Jared (1976) Spaach Recognition and Understanding in Fu, Syntactic Pattern Recognition [32] Woods, W.A-1 (-193Q) Transition Network Grammars for Natural Language Analysis CACM, 13:10 (Oct 1970) p.
591'9606 1331 Woods, WeAw (1974) Motivatiod and Overview of BBN'SPEECHLIS.: An Experimental Prototype far Speach Undarstandihg Rassarch IEEE Trans.
on ASSP, 23:1 (Fzb.
1975), p.
2-10 [34] Woods, W.A.
(1975) Syntax, Semantics, and Speach in qeddy, Spaech Recognition: Invited Papers Presented at the IEEE Symposium, Acadsmic Press [35] Woods, W.A. et.
el. (1974) Natural Languajia Communication with Computeru, Final Report, Volume I, Spadch Undsrstanding Research at BBN BBN Report 2976, Bolt Beranek and Newman Inc., Cambridpa, Mauu., Dec., 1974 [36] Woods, WoA,, et .ale (1?72) The Lunar Sciencds Natural Lan~uare Information System: Final Report Report No.
2378, Bolt Reranek and Newman Inc., Cambridge, Mass .

