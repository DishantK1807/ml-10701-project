<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>P Boersma</author>
<author>D Weenink</author>
</authors>
<title>Praat. A system for doing phonetics by computer</title>
<date>1999</date>
<pages>http://www.fon.hum.uva.nl/praat/.</pages>
<contexts>
<context>pshot of our multimodal annotation software ViMar during hand gesture labelling. Speech Labelling Speech productions have been separately labelled with the software package for speech analysis PRAAT (Boersma &amp; Weenink, 1999) on two different levels: one marking words boundaries, and the other one marking syllables boundaries, specifying whether each syllable was characterised or not by a pitch prominence. A number of no</context>
<context>ion (waveform, fundamental frequency), the latter imported by existing speech analysis software tools (a snapshot is shown in Figure 2). It can import annotation files produced by systems like PRAAT (Boersma &amp; Weenink, 1999), SegWin (Refice et al., 2000), Entropic XWAVES. A special conversion procedure has been implemented for PRAAT annotation files, whose file format is the less similar to SAM-like format. Figure 2: Sn</context>
</contexts>
<marker>Boersma, Weenink, 1999</marker>
<rawString>Boersma, P. &amp; D. Weenink (1999).  Praat. A system for doing phonetics by computer, http://www.fon.hum.uva.nl/praat/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cassell</author>
<author>M Stone</author>
</authors>
<title>Living Hand to Mouth: Psychological Theories about Speech and Gesture in Interactive Dialogue Systems</title>
<date>1999</date>
<booktitle>In Proceedings of the AAAI 1999 Fall Symposium on Psychological Models of Communication in Collaborative Systems</booktitle>
<pages>5--7</pages>
<location>North Falmouth, MA</location>
<contexts>
<context>e. Modelling these typical human abilities represents a main objective in human communication research, including technological applications like the development of human-machine interaction systems (Cassell and Stone, 1999). A specific question we would like to arise is the following: when listeners judge two communicative behaviours as different, how can we quantitatively estimate such differences in parametric terms?</context>
</contexts>
<marker>Cassell, Stone, 1999</marker>
<rawString>Cassell, J. and Stone, M. (1999). Living Hand to Mouth: Psychological Theories about Speech and Gesture in Interactive Dialogue Systems. In Proceedings of the AAAI 1999 Fall Symposium on Psychological Models of Communication in Collaborative Systems, North Falmouth, MA, Nov. 5-7, pp.34--42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fourcin</author>
</authors>
<title>The SAM project</title>
<date>1993</date>
<publisher>Ellis Horwood</publisher>
<location>Chichester</location>
<contexts>
<context>es. It is characterised by the common basic features shared by other video labelling tools, as it can be seen from the system snapshot shown in Figure 1. ViMar’s output data format is the simple SAM (Fourcin, 1993)-like format, i.e. &lt;starting time&gt; &lt;ending time&gt; &lt;label&gt; This makes the system very easy to be controlled by human transcribers with different backgrounds, and therefore even by those without particu</context>
</contexts>
<marker>Fourcin, 1993</marker>
<rawString>Fourcin, A. (1993). The SAM project. Chichester: Ellis Horwood.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Jannedy</author>
<author>N Mendoza-Denton</author>
</authors>
<title>Structuring Information through Gesture and Intonation. In</title>
<date>2005</date>
<booktitle>Interdisciplinary Studies on Information Structure 03</booktitle>
<pages>199--244</pages>
<contexts>
<context>n the literature as speech-accompanying gestures, i.e. gestures which are rhythmically synchronised with speech (Kendon, 1980; McNeill, 1992; Valbonesi et al, 2002; Loehr, 2004; Yannisik et al, 2004; Jannedy &amp; Mendoza-Denton, 2005). Synchrony between speech and gesture represents a strong evidence of the co-expressive, intrinsically multimodal nature of human communication (see also McNeill, 2005). Therefore, we looked not onl</context>
<context> where we plan to include also further aspects of audio-visual parameters. It would be interesting, for example, to look at the correlation between gestural peaks and pitch accent types (Loehr, 2004, Jannedy &amp; Mendoza-Denton, 2005), and also between “gesticular phrasing” (Kendon, 1980) and prosodic phrasing. In this paper we also introduced a software package for multimodal annotation, ViSuite, we developed and used for the wo</context>
</contexts>
<marker>Jannedy, Mendoza-Denton, 2005</marker>
<rawString>Jannedy, S. &amp; N. Mendoza-Denton (2005). Structuring Information through Gesture and Intonation. In S. Ishihara, M. Shmitz and A. Schwartz (eds), Interdisciplinary Studies on Information Structure 03, pp. 199--244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kendon</author>
</authors>
<title>Some relationships between body motion and speech</title>
<date>1972</date>
<booktitle>Studies in dyadic communication</booktitle>
<pages>17--210</pages>
<editor>In A. Siegmand and B. Pope (eds</editor>
<publisher>Pergamon Press</publisher>
<location>New York</location>
<contexts>
<context>he work described in this paper. 1. Introduction In current research on human communication, it is widely acknowledged the co-expressive nature of verbal and non-verbal channels (see seminal works of Kendon, 1972; 1980, McNeill 1992). Therefore, in human face-to-face interaction, participants can rely on a number of audio-visual information for interpreting interlocutors’ communicative intentions, such inform</context>
</contexts>
<marker>Kendon, 1972</marker>
<rawString>Kendon, A. (1972). Some relationships between body motion and speech. In A. Siegmand and B. Pope (eds.), Studies in dyadic communication, New York: Pergamon Press, pp.17--210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kendon</author>
</authors>
<title>Gesticulation and Speech: Two Aspects of the Process of Utterance</title>
<date>1980</date>
<booktitle>The relation between verbal and nonverbal communication, The Hague: Mouton</booktitle>
<pages>207--227</pages>
<editor>In M.R. Key (ed</editor>
<contexts>
<context>es, and on the visual level on the occurrence of gestures which have been referred to in the literature as speech-accompanying gestures, i.e. gestures which are rhythmically synchronised with speech (Kendon, 1980; McNeill, 1992; Valbonesi et al, 2002; Loehr, 2004; Yannisik et al, 2004; Jannedy &amp; Mendoza-Denton, 2005). Synchrony between speech and gesture represents a strong evidence of the co-expressive, intr</context>
<context>. It would be interesting, for example, to look at the correlation between gestural peaks and pitch accent types (Loehr, 2004, Jannedy &amp; Mendoza-Denton, 2005), and also between “gesticular phrasing” (Kendon, 1980) and prosodic phrasing. In this paper we also introduced a software package for multimodal annotation, ViSuite, we developed and used for the work described in this paper. 7. Acknowledgements We woul</context>
</contexts>
<marker>Kendon, 1980</marker>
<rawString>Kendon, A. (1980). Gesticulation and Speech: Two Aspects of the Process of Utterance. In M.R. Key (ed.) The relation between verbal and nonverbal communication, The Hague: Mouton, pp. 207--227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kipp</author>
</authors>
<title>ANVIL – A generic annotation tool for multimodal dialogue</title>
<date>2001</date>
<booktitle>In Proceedings of the 7th European Conference on Speech Communication and Technology (Eurospeech</booktitle>
<note>Alborg, pp.136--1370</note>
<contexts>
<context>ts Currently a number of software tools are already available for multimodal annotation (for a detailed description and evaluation of the most popular tools see Rohlfing et al., 2006), notably ANVIL (Kipp, 2001) among the others. All these tools are XML-based: even though we are fully aware of all the potentialities of XML scheme, we agree with the view that working with XML annotation schemes and file form</context>
</contexts>
<marker>Kipp, 2001</marker>
<rawString>Kipp, M. (2001). ANVIL – A generic annotation tool for multimodal dialogue. In Proceedings of the 7th European Conference on Speech Communication and Technology (Eurospeech 2001), Alborg, pp.136--1370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Loehr</author>
</authors>
<title>Gesture and Intonation, Doctoral Dissertation</title>
<date>2004</date>
<location>Georgetown University, Washington, DC</location>
<contexts>
<context>tures which have been referred to in the literature as speech-accompanying gestures, i.e. gestures which are rhythmically synchronised with speech (Kendon, 1980; McNeill, 1992; Valbonesi et al, 2002; Loehr, 2004; Yannisik et al, 2004; Jannedy &amp; Mendoza-Denton, 2005). Synchrony between speech and gesture represents a strong evidence of the co-expressive, intrinsically multimodal nature of human communication </context>
<context> for Italian, where we plan to include also further aspects of audio-visual parameters. It would be interesting, for example, to look at the correlation between gestural peaks and pitch accent types (Loehr, 2004, Jannedy &amp; Mendoza-Denton, 2005), and also between “gesticular phrasing” (Kendon, 1980) and prosodic phrasing. In this paper we also introduced a software package for multimodal annotation, ViSuite, </context>
</contexts>
<marker>Loehr, 2004</marker>
<rawString>Loehr, D. (2004). Gesture and Intonation, Doctoral Dissertation, Georgetown University, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McNeill</author>
</authors>
<title>Hand and Mind, Chigago</title>
<date>1992</date>
<publisher>Chicago University Press</publisher>
<contexts>
<context> this paper. 1. Introduction In current research on human communication, it is widely acknowledged the co-expressive nature of verbal and non-verbal channels (see seminal works of Kendon, 1972; 1980, McNeill 1992). Therefore, in human face-to-face interaction, participants can rely on a number of audio-visual information for interpreting interlocutors’ communicative intentions, such information strongly contr</context>
<context> visual level on the occurrence of gestures which have been referred to in the literature as speech-accompanying gestures, i.e. gestures which are rhythmically synchronised with speech (Kendon, 1980; McNeill, 1992; Valbonesi et al, 2002; Loehr, 2004; Yannisik et al, 2004; Jannedy &amp; Mendoza-Denton, 2005). Synchrony between speech and gesture represents a strong evidence of the co-expressive, intrinsically multi</context>
<context>ses, according to the following criteria: 1. the preparation phase, when the subject is moving his hand/arm with respect to his start position 2. the stroke, i.e. “the peak of effort in the gesture” (McNeill, 1992:83) 3. (post-stroke) hold, which is “the final position reached by the hand at the end of the stroke” (McNeill, 1992: ibidem) and whose duration is variable (pre-stroke holds are also possible, but w</context>
</contexts>
<marker>McNeill, 1992</marker>
<rawString>McNeill, D. (1992), Hand and Mind, Chigago: Chicago University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McNeill</author>
</authors>
<title>Gesture and Thought, Chigago</title>
<date>2005</date>
<publisher>Chicago University Press</publisher>
<contexts>
<context> et al, 2004; Jannedy &amp; Mendoza-Denton, 2005). Synchrony between speech and gesture represents a strong evidence of the co-expressive, intrinsically multimodal nature of human communication (see also McNeill, 2005). Therefore, we looked not only the occurrence of specific events on each level separately, but also at the occurrence of the overlappings between some types of speech and gesture events, namely betw</context>
</contexts>
<marker>McNeill, 2005</marker>
<rawString>McNeill, D. (2005), Gesture and Thought, Chigago: Chicago University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Refice</author>
<author>M Savino</author>
<author>Altieri M Altieri R</author>
</authors>
<date>2000</date>
<contexts>
<context>cy), the latter imported by existing speech analysis software tools (a snapshot is shown in Figure 2). It can import annotation files produced by systems like PRAAT (Boersma &amp; Weenink, 1999), SegWin (Refice et al., 2000), Entropic XWAVES. A special conversion procedure has been implemented for PRAAT annotation files, whose file format is the less similar to SAM-like format. Figure 2: Snapshot of our multimodal visua</context>
</contexts>
<marker>Refice, Savino, R, 2000</marker>
<rawString>Refice M., Savino M, Altieri M. Altieri R. (2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>SegWin</author>
</authors>
<title>A Tool for Segmenting, Annotating and Controlling the Creation of a Database of Spoken Italian Varieties</title>
<booktitle>In Proceedings of LREC 2000</booktitle>
<volume>3</volume>
<pages>1531--1536</pages>
<location>Athens</location>
<marker>SegWin, </marker>
<rawString>SegWin: A Tool for Segmenting, Annotating and Controlling the Creation of a Database of Spoken Italian Varieties. In Proceedings of LREC 2000, Athens, vol. 3, pp. 1531—1536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Rohlfing</author>
<author>D Loehr</author>
<author>S Duncan</author>
<author>A Brown</author>
<author>A Franklin</author>
<author>I Kimbara</author>
<author>J-T Milde</author>
<author>F Parrill</author>
<author>T Rose</author>
<author>T Schmidt</author>
<author>H Sloetjes</author>
<author>A Thies</author>
<author>S Wellinghoff</author>
</authors>
<title>Comparison of multimodal annotation tools – workshop report</title>
<date>2006</date>
<journal>In Gespraechsforschung</journal>
<volume>7</volume>
<pages>99--123</pages>
<contexts>
<context>of user interface and output file formats Currently a number of software tools are already available for multimodal annotation (for a detailed description and evaluation of the most popular tools see Rohlfing et al., 2006), notably ANVIL (Kipp, 2001) among the others. All these tools are XML-based: even though we are fully aware of all the potentialities of XML scheme, we agree with the view that working with XML anno</context>
</contexts>
<marker>Rohlfing, Loehr, Duncan, Brown, Franklin, Kimbara, Milde, Parrill, Rose, Schmidt, Sloetjes, Thies, Wellinghoff, 2006</marker>
<rawString>Rohlfing K., Loehr, D., Duncan, S., Brown, A., Franklin, A., Kimbara, I., Milde J-T., Parrill F., Rose, T., Schmidt, T., Sloetjes, H., Thies, A., Wellinghoff, S. (2006). Comparison of multimodal annotation tools – workshop report. In Gespraechsforschung 7 (2006), pp.99--123 (www.gespraechsforschung-ozs.de).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Valbonesi</author>
<author>R Ansari</author>
<author>D McNeill</author>
<author>F Quek</author>
<author>S Duncan</author>
<author>K E McCullough</author>
<author>R Bryll</author>
</authors>
<title>Multimodal signal analysis of prosody and hand motion: temporal correlation of speech and gesture</title>
<date>2002</date>
<booktitle>In Proceedings of EUSIPCO 2002</booktitle>
<location>Toulouse, FR</location>
<contexts>
<context>n the occurrence of gestures which have been referred to in the literature as speech-accompanying gestures, i.e. gestures which are rhythmically synchronised with speech (Kendon, 1980; McNeill, 1992; Valbonesi et al, 2002; Loehr, 2004; Yannisik et al, 2004; Jannedy &amp; Mendoza-Denton, 2005). Synchrony between speech and gesture represents a strong evidence of the co-expressive, intrinsically multimodal nature of human c</context>
</contexts>
<marker>Valbonesi, Ansari, McNeill, Quek, Duncan, McCullough, Bryll, 2002</marker>
<rawString>Valbonesi, L., R. Ansari, D. McNeill, F. Quek, S. Duncan, K.E. McCullough, R. Bryll (2002). Multimodal signal analysis of prosody and hand motion: temporal correlation of speech and gesture. In Proceedings of EUSIPCO 2002, Toulouse, FR.</rawString>
</citation>
</citationList>
</algorithm>

