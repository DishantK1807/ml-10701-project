<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Emily M Bender</author>
<author>Dan Flickinger</author>
<author>Ara Kim</author>
<author>Stephan Oepen</author>
</authors>
<title>Road-testing the english resource grammar over the british national corpus</title>
<date>2004</date>
<contexts>
<context>es that try to extend the lexicon to avoid gaps while processing. This section looks only at those approaches that are geared towards the specific type of grammar/lexicon in which we are interested. (Baldwin et al., 2004) describes an approach to extending the lexicon in the ERG manually, after ”road-testing” it on the BNC. This approach is very much in the second category above. It involves much more manual effort, </context>
</contexts>
<marker>Baldwin, Bender, Flickinger, Kim, Oepen, 2004</marker>
<rawString>Timothy Baldwin, Emily M. Bender, Dan Flickinger, Ara Kim, and Stephan Oepen. 2004. Road-testing the english resource grammar over the british national corpus.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of the fourth International Conference on Language Resources and Evaluation</booktitle>
<pages>2047--50</pages>
<location>Lisbon, Portugal</location>
<marker></marker>
<rawString>In Proceedings of the fourth International Conference on Language Resources and Evaluation, pages 2047–50, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patra Barg</author>
<author>Markus Walther</author>
</authors>
<title>Processing unknown words in hpsg</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th International Conference of the ACL and the 17th International Conference on Computational Linguistics</booktitle>
<location>Montreal, Quebec, Canada</location>
<contexts>
<context> be a very significant difference, but our approach allows us to generate entries based on a single entry rather than a whole set. Other approaches that don’t quite fit into either category include, (Barg and Walther, 1998) and (Fouvry, 2003), who use the notion of underspecified lexical entries to allow for partial matching of unknown words. These approaches tend to involve dynamically generating lexical entries which</context>
</contexts>
<marker>Barg, Walther, 1998</marker>
<rawString>Patra Barg and Markus Walther. 1998. Processing unknown words in hpsg. In Proceedings of the 36th International Conference of the ACL and the 17th International Conference on Computational Linguistics, Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
</authors>
<title>An open source grammar development environment and broad coverage english grammar using hpsg</title>
<date>2000</date>
<booktitle>In Proceedings of the Second International Conference on Language Resources and Evaluation</booktitle>
<contexts>
<context>exical entries to allow for partial matching of unknown words. These approaches tend to involve dynamically generating lexical entries which are then added permanently to the lexicon. 3. LinGO LinGO (Copestake and Flickinger, 2000) is a combined parser/generator which comes with a range of resources, including the English Resource Grammar (ERG)1. This grammar consists of HPSG grammar rules for English and a lexicon of around 2</context>
</contexts>
<marker>Copestake, Flickinger, 2000</marker>
<rawString>Ann Copestake and Dan Flickinger. 2000. An open source grammar development environment and broad coverage english grammar using hpsg. In Proceedings of the Second International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
</authors>
<title>Minimal recursion semantics: An introduction</title>
<date>2005</date>
<booktitle>Research on Language and Computation</booktitle>
<pages>3--281</pages>
<contexts>
<context>ns. The LinGO system can be used to either parse or generate. In parsing mode, it takes sentences of English and produces a representation of the semantics in the form of Minimal Recursion Semantics (Copestake, 2005). In generation mode, it takes an MRS and produces an English sentence. The system comes with a wide coverage grammar (known as the English Resource Grammar, or ERG) consisting of HPSG grammar rules </context>
<context>wishing to use LinGO will typically need to extend the lexicon to include the words required2. The LinGO parser takes sentences of English and produces representations in Minimal Recursion Semantics (Copestake, 2005). These representations can be given to the LinGO generator to produce sentences of English. If a word in a sentence is not present in the LinGO lexicon, the system will fail to produce a parse. Simi</context>
</contexts>
<marker>Copestake, 2005</marker>
<rawString>Ann Copestake. 2005. Minimal recursion semantics: An introduction. Research on Language and Computation, 3:281–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Felbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database</title>
<date>1998</date>
<publisher>MIT Press</publisher>
<location>Cambridge</location>
<contexts>
<context>syntactic as well as semantic information is required for the words to be included. This information can be found in a number of different types of resource, such as electronic dictionaries, WordNet (Felbaum, 1998) etc.. The problem with many of these resources is that they do not provide the precise information needed, or do not provide it in a form that is easily accessible. WordNet, for example, gives us se</context>
</contexts>
<marker>Felbaum, 1998</marker>
<rawString>Christiane Felbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederik Fouvry</author>
</authors>
<title>Lexicon acquisition with a largecoverage unification-based grammar</title>
<date>2003</date>
<booktitle>In Companion to the 10th EACL</booktitle>
<pages>87--90</pages>
<location>Budapest, Hungary</location>
<contexts>
<context>ence, but our approach allows us to generate entries based on a single entry rather than a whole set. Other approaches that don’t quite fit into either category include, (Barg and Walther, 1998) and (Fouvry, 2003), who use the notion of underspecified lexical entries to allow for partial matching of unknown words. These approaches tend to involve dynamically generating lexical entries which are then added per</context>
</contexts>
<marker>Fouvry, 2003</marker>
<rawString>Frederik Fouvry. 2003. Lexicon acquisition with a largecoverage unification-based grammar. In Companion to the 10th EACL, pages 87–90, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
</authors>
<title>Co-occurrence retrieval: A flexible framework for lexical distributional similarity</title>
<date>2005</date>
<journal>Computational Linguistics</journal>
<pages>31--4</pages>
<contexts>
<context>easily accessible. WordNet, for example, gives us semantic information, but lacks the syntactic information needed. What we do have available to us is software, developed at the University of Sussex (Weeds and Weir, 2005), which can be used to extract, from any corpus, information about the distributional similarity of the words that occur in the corpus. If we can harness this resource to generate lexical entries bas</context>
<context>lexical types that need to be hypothesized for non-verbs.”3. This is interesting from our point of view as it suggests exactly the kind of experiment we are undertaking. 4. Distributional similarity (Weeds and Weir, 2005) provides a flexible framework for extracting measures of distributional similarity from corpora. The framework allows parametrised analysis of distributional similarities using a variety of differen</context>
</contexts>
<marker>Weeds, Weir, 2005</marker>
<rawString>Julie Weeds and David Weir. 2005. Co-occurrence retrieval: A flexible framework for lexical distributional similarity. Computational Linguistics, 31:4:439–476.</rawString>
</citation>
</citationList>
</algorithm>

