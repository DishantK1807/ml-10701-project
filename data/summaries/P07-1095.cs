Extended Feature Set Sha and Pereira (2003) applied a conditional random field to the NP chunking task, achieving excellent results.
N03-1028
Using the original HMM feature set and the extended feature set, we trained four models that can use arbitrary features: conditional random fields (a near-replication of Sha and Pereira, 2003), maximum entropy Markov models (MEMMs; McCallum et al., 2000), pseudolikelihood (Besag, 1975; see Toutanova et al., 2003, for a tagging application), and our M-estimator with the HMM as q0.
N03-1028 N03-1033
If that is the case, theniA = 1 for allA∈N, and the system becomes linear (see also Corazza and Satta, 2006).5 If tightness is not guaranteed, iterative propagation of weights, following Stolcke (1995), works well in our experience for solving the quadratic system, and converges quickly.
J95-2002 N06-1043
Generative and discriminative models have been comparedanddiscussedagreatdeal(NgandJordan, 2002), including for NLP models (Johnson, 2001; Klein and Manning, 2002).
P01-1042 W02-1002
The runtime overhead incurred by using Dyna is estimated as a slow-down factor of 3–5 against a handtuned implementation (Eisner et al., 2005), though the slow-down factor is almost certainly less for the MEMM and pseudolikelihood.
H05-1036
Then∀A∈N: oA = summationdisplay B∈N summationdisplay C∈N oBiC[rB(A C) + rB(C A)] + braceleftbigg 1 if A = S 0 otherwise iA = summationdisplay B∈N summationdisplay C∈N rA(B C)iBiC + summationdisplay x rA(x)ix ox = summationdisplay A∈N oArA(x),∀x∈Σ ix = 1,∀x∈Σ In most practical applications, the PCFG will be “tight” (Booth and Thompson, 1973; Chi and Geman, 1998).
J98-2005
The dataset comes from the Conference on Natural Language Learning (CoNLL) 2000 shallow parsing shared task (Tjong Kim Sang and Buchholz, 2000); we apply the model to NP chunking only.
W00-0726
