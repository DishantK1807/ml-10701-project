<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Philip Chan</author>
<author>Salvatore Stolfo</author>
</authors>
<title>Toward Scalable Learning with Non-uniform Class and Cost Distributions: A Case Study in Credit Card Fraud Detection</title>
<date>1998</date>
<booktitle>In Knowledge Discovery and Data Mining</booktitle>
<pages>164--168</pages>
<publisher>AAAI</publisher>
<location>Menlo Park, CA, USA</location>
<contexts>
<context>rees. (McCarthy et al., 2005) come to the opposite conclusion. The fact that down-sampling actually ignores some labeled data is particularly controversial when it comes to very small training sets. (Chan and Stolfo, 1998) propose partitioning the majority classes to n samples so that each partition is approximately of the size of the minority class. For each of the new resulting n training sets an independent classif</context>
</contexts>
<marker>Chan, Stolfo, 1998</marker>
<rawString>Philip Chan and Salvatore Stolfo. 1998. Toward Scalable Learning with Non-uniform Class and Cost Distributions: A Case Study in Credit Card Fraud Detection. In Knowledge Discovery and Data Mining, pages 164–168, Menlo Park, CA, USA. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Drummond</author>
<author>Robert Holte</author>
</authors>
<title>C4.5, Class Imbalance, and Cost Sensitivity: Why Under-Sampling Beats Over-Sampling</title>
<date>2003</date>
<booktitle>In Workshop on Learning from Imbalanced Datasets</booktitle>
<location>Washington, DC, USA</location>
<marker>Drummond, Holte, 2003</marker>
<rawString>Chris Drummond and Robert Holte. 2003. C4.5, Class Imbalance, and Cost Sensitivity: Why Under-Sampling Beats Over-Sampling. In Workshop on Learning from Imbalanced Datasets, Washington, DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Drummond</author>
<author>Robert Holte</author>
</authors>
<title>Severe Class Imbalance: Why Better Algorithms Aren’t the Answer</title>
<date>2005</date>
<booktitle>In Proceedings of the 16th European Conference of Machine Learning</booktitle>
<location>Porto, Portugal. NRC</location>
<contexts>
<context>ng software. 1. Introduction One problem of data-driven answer extraction in opendomain factoid question answering (QA) is that the class distribution of labeled training data are fairly imbalanced. (Drummond and Holte, 2005) show that, in general, this imbalance has a deteriorating effect on the performance of resulting classifiers. This effect can be very drastic in answer extraction. Our initial answer extraction algo</context>
<context>ately, most research on learning with imbalanced class distribution deals with this classification type. All standard learning methods suffer from the effects caused by imbalanced class distribution (Drummond and Holte, 2005). A popular solution to this problem is sampling. The two most common types are down-sampling, where some of the training instances of the majority class are discarded so that the class distribution </context>
</contexts>
<marker>Drummond, Holte, 2005</marker>
<rawString>Chris Drummond and Robert Holte. 2005. Severe Class Imbalance: Why Better Algorithms Aren’t the Answer. In Proceedings of the 16th European Conference of Machine Learning, Porto, Portugal. NRC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Elkan</author>
</authors>
<title>The Foundations of Cost-Sensitive Learning</title>
<date>2001</date>
<booktitle>In 17th International Joint Conference on Artificial Intelligence</booktitle>
<pages>973--978</pages>
<location>Seattle, WA, USA</location>
<contexts>
<context>idual classifiers by some sort of metalearning. Though this method overcomes some of the problems encountered with simple sampling methods, it is fairly processing-intensive. Cost-sensitive learning (Elkan, 2001) supersedes sampling methods in that it does not alter the original distribution of classes. Since some state-of-the-art toolkits already support this meta-learning, it should be fairly easy to imple</context>
</contexts>
<marker>Elkan, 2001</marker>
<rawString>Charles Elkan. 2001. The Foundations of Cost-Sensitive Learning. In 17th International Joint Conference on Artificial Intelligence, pages 973–978, Seattle, WA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kate McCarthy</author>
<author>Bibi Zabar</author>
<author>Gary Weiss</author>
</authors>
<title>Does Cost-Sensitive Learning Beat Sampling for Classifying Rare Classes</title>
<date>2005</date>
<booktitle>In Proceedings of the 1st International Workshop on Utility-based Data Mining</booktitle>
<pages>69--77</pages>
<publisher>ACM Press</publisher>
<location>Bronx, NY, USA</location>
<contexts>
<context>his research was carried out while the second author was a research associate at Spoken Language Systems, Saarland University. Holte, 2003) report better results for downsampling for decision trees. (McCarthy et al., 2005) come to the opposite conclusion. The fact that down-sampling actually ignores some labeled data is particularly controversial when it comes to very small training sets. (Chan and Stolfo, 1998) propo</context>
<context>er the original distribution of classes. Since some state-of-the-art toolkits already support this meta-learning, it should be fairly easy to implement a classifier using this approach. According to (McCarthy et al., 2005), the results of cost-sensitive learning are comparable with or even outperform sampling methods. To the best of our knowledge, there has not been any application of cost-sensitive learning to answer</context>
</contexts>
<marker>McCarthy, Zabar, Weiss, 2005</marker>
<rawString>Kate McCarthy, Bibi Zabar, and Gary Weiss. 2005. Does Cost-Sensitive Learning Beat Sampling for Classifying Rare Classes? In Proceedings of the 1st International Workshop on Utility-based Data Mining, pages 69–77, Bronx, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Voorhees</author>
<author>Donna Harman</author>
</authors>
<title>TREC: Experiment and Evaluation in Information Retrieval</title>
<date>2005</date>
<publisher>The MIT Press</publisher>
<location>Cambridge, Massachusetts, USA</location>
<contexts>
<context>thod of cost-sensitive learning, we will briefly discuss the answer extraction algorithm we use. 3.1. Answer Extraction in Context The task of open-domain question answering (QA) as proposed by TREC (Voorhees and Harman, 2005) is to retrieve answers from a text collection given a natural language question, such as When was Mozart born? The text collection is usually a large corpus, for example, a collection of newspaper a</context>
</contexts>
<marker>Voorhees, Harman, 2005</marker>
<rawString>Ellen Voorhees and Donna Harman. 2005. TREC: Experiment and Evaluation in Information Retrieval. The MIT Press, Cambridge, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Voorhees</author>
</authors>
<title>Overview of the TREC-14 Question-Answering Track</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th Text Retrieval Conference (TREC</booktitle>
<location>Gaithersburg, MD</location>
<contexts>
<context>earner to wrap around standard learning methods, the implementation of this method is very easy and efficient. 4. Evaluation Our answer extraction classifier is built using the TREC 14 QA Collection (Voorhees, 2005). All results we state below are based on averaged 10-fold cross-validation. The cost-sensitive meta-learner embeds a base learner, we first look at logistic regression here. Two classifiers are buil</context>
</contexts>
<marker>Voorhees, 2005</marker>
<rawString>Ellen Voorhees. 2005. Overview of the TREC-14 Question-Answering Track. In Proceedings of the 14th Text Retrieval Conference (TREC 2005), Gaithersburg, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Wiegand</author>
</authors>
<title>Event-Based Modelling in Quesion Answering. Master’s thesis</title>
<date>2007</date>
<institution>Saarland University</institution>
<location>Saarbr¨ucken, Germany</location>
<note>http://www.coli.uni-saarland.de/˜miwieg/ pub/dipl-thesis.pdf</note>
<contexts>
<context> levels. A list of some important orthographic, syntactic and semantic features we use is shown in Table 2. A more detailed discussion of the algorithm and its corresponding features can be found in (Wiegand, 2007). 3.2. Cost-Sensitive Learning Applied to Answer Extraction The question-answer pair from the previous section illustrated by Table 1 exemplifies the inherent imbalance of the 1In our experiments, we</context>
</contexts>
<marker>Wiegand, 2007</marker>
<rawString>Michael Wiegand. 2007. Event-Based Modelling in Quesion Answering. Master’s thesis, Saarland University, Saarbr¨ucken, Germany, March. http://www.coli.uni-saarland.de/˜miwieg/ pub/dipl-thesis.pdf.</rawString>
</citation>
</citationList>
</algorithm>

