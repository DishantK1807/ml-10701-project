Earlier versions of SNoW (Roth, 1998; Golding and Roth, 1999; Roth and Zelenko, 1998; Munoz et al., 1999) have been applied successfully to several natural language related tasks.
P98-2186 W99-0621
A sub j-verb 3This information can be produced by a functional dependency grammar (FDG), which assigns each word a specific function, and then structures the sentence hierarchically based on it, as we do here (Tapanainen and Jrvinen, 1997), but can also be generated by an external rule-based parser or a learned one.
A97-1011
Machine learning based classifiers and maximum entropy models which, in principle, are not restricted to features of these forms have used them nevertheless, perhaps under the influence of probabilistic methods (Brill, 1995; Yarowsky, 1994; Ratnaparkhi et al., 1994).
J95-4004 P94-1013
Learning Approach Our experimental investigation is done using the SNo W learning system (Roth, 1998).
P98-2186
Studies have shown that both machine learning and probabilistic learning methods used in NLP make decisions using a linear decision surface over the feature space (Roth, 1998; Roth, 1999).
P98-2186
Most efficient learning methods known today and, in particular, those used in NLP, make use of a linear decision surface over their feature space (Roth, 1998; Roth, 1999).
P98-2186
Table 2 compares our method to methods that use similarity measures (Dagan et al., 1999; Lee, 1999).
P99-1004 P99-1005
The training and the test data were processed by the FDG parser (Tapanainen and Jrvinen, 1997).
A97-1011
Nevertheless, the efforts in this direction so far have shown very insignificant improvements, if any (Chelba and Jelinek, 1998; Rosenfeld, 1996).
P98-1035
We chose the verb prediction task which is similar to other word prediction tasks (e.g.,(Golding and Roth, 1999)) and, in particular, follows the paradigm in (Lee and Pereira, 1999; Dagan et al., 1999; Lee, 1999).
P99-1004 P99-1005
Efforts in this directions consists of (1) directly adding syntactic information, as in (Chelba and Jelinek, 1998; Rosenfeld, 1996), and (2) indirectly adding syntactic and semantic information, via similarity models; in this case n-gram type features are used whenever possible, and when they cannot be used (due to data sparsity), additional information compiled into a similarity measure is used (Dagan et al., 1999).
P98-1035
