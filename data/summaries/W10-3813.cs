DeNero, John and Dan Klein. 2008. The complexity of
P08-2007
MOSES (Koehn et al., 2007) for decoding. A
P07-2045
based POS tagger MXPOST (Ratnaparkhi, 1996)
W96-0213
mum Error-Rate Training (MERT) (Och, 2003),
J03-1002 P03-1021
word alignment models (Vogel et al., 1996) have
C96-2141
Cherry, Colin and Dekang Lin. 2006. Soft syntactic con-
P06-2014
ous findings (Fraser and Marcu, 2007b) that the
D07-1006
al., 1996; Och and Ney, 2003), since this model
J03-1002 P03-1021
Fox, Heidi. 2002. Phrasal cohesion and statistical machine
W02-1039
IBM models (Brown et al., 1993) and HMM
J93-2003
(Cherry and Lin, 2006)). However, most re-
P06-2014
tion of IBM Model 4 (Brown et al., 1993) is used
J93-2003
imum Error-Rate Training (MERT) (Och, 2003)
J03-1002 P03-1021
Och, Franz. 2003. Minimum Error Rate Training in Statis-
J03-1002 P03-1021
Marcu, Daniel and William Wong. 2002. A Phrase-Based,
W02-1018
Koehn, Philipp, Franz Och, and Daniel Marcu. 2003.
J03-1002 N03-1017 P03-1021
2002) as explored by Cherry and Lin (2006),
P06-2014
Fraser, Alexander and Daniel Marcu. 2007b. Measuring
D07-1006
heuristics described in (Koehn et al., 2003), Min-
N03-1017
alignment models (Marcu and Wong, 2002).
W02-1018
Deng, Yonggang and William Byrne. 2006. MTTK: An
N06-4004
Och, Franz and Hermann Ney. 2003. A systematic com-
J03-1002 P03-1021
of the BLEU scores (Papineni et al., 2002) on
P02-1040
Ratnaparkhi, Adwait. 1996. A maximum entropy model
W96-0213
Fraser, Alexander and Daniel Marcu. 2007a. Getting the
D07-1006
The GIZA++ (Och and Ney, 2003) implementa-
J03-1002 P03-1021
Hiero-style decoder Joshua (Li et al., 2009) is
W09-0424
