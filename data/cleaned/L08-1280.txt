<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Andrew P Bradley</author>
</authors>
<title>The use of the area under the ROC curve in the evaluation of machine learning algorithms</title>
<date>1997</date>
<journal>Pattern Recognition</journal>
<volume>30</volume>
<contexts>
<context>f classification errors and the F measure is biased towards maximizing the TPs. In the same vein we have in (Reynaert, 2005) looked in depth at the area under the ROC curve or AUC first advocated by (Bradley, 1997). The AUC is a single scalar value between 0.0 and 1.0 representing a system’s performance. The AUC is a reduction of a Receiver Operating Characteristic or ROC curve depicting performance. A ROC cur</context>
</contexts>
<marker>Bradley, 1997</marker>
<rawString>Andrew P. Bradley. 1997. The use of the area under the ROC curve in the evaluation of machine learning algorithms. Pattern Recognition, 30(7):1145–1159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Robert C Moore</author>
</authors>
<title>An improved error model for noisy channel spelling correction</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the ACL</booktitle>
<pages>286--293</pages>
<contexts>
<context>esearch where in fact the system is often evaluated only on its correction capabilities, by presenting it with a list of errors only. Highly influential and interesting examples of this practice are (Brill and Moore, 2000), (Toutanova and Moore, 2002). We regard detection as Level 2 in the evaluation of SCCs, because this is where two components, i.e. the correction mechanism and the detection mechanism work in concer</context>
</contexts>
<marker>Brill, Moore, 2000</marker>
<rawString>Eric Brill and Robert C. Moore. 2000. An improved error model for noisy channel spelling correction. In Proceedings of the 38th Annual Meeting of the ACL, pages 286–293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>EAGLES-I</author>
</authors>
<title>Final Report</title>
<date>1996</date>
<booktitle>In Evaluation of Natural Language Processing Systems, volume EAGLES DOCUMENT EAG-EWG-PR.2</booktitle>
<contexts>
<context>lass of word forms as it is in fact an open class. Further, the authors refer to an EAGLES specification that a metric should ‘constantly provide the same results when applied to the same phenomena’ (EAGLES-I, 1996). They argue that huge differences in results in two of their scores obtained by a single spelling checker on three different texts, differing both in length and percentages of errors present, ‘motiv</context>
</contexts>
<marker>EAGLES-I, 1996</marker>
<rawString>EAGLES-I. 1996. Final Report. In Evaluation of Natural Language Processing Systems, volume EAGLES DOCUMENT EAG-EWG-PR.2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Fawcett</author>
</authors>
<title>ROC graphs: Notes and practical considerations for data mining researchers. Technical report, HP Laboratories</title>
<date>2003</date>
<location>Palo Alto, USA</location>
<contexts>
<context> a Receiver Operating Characteristic or ROC curve depicting performance. A ROC curve is obtained by plotting the systems’ False Positive Rates on the X-axis and the True Positive Rates on the Y-axis (Fawcett, 2003). ROC curves are insensitive to changes in class distribution. In that the AUC is derived from these, it too should be insensitive. To calculate the AUC we need to know the True and False Positive Ra</context>
</contexts>
<marker>Fawcett, 2003</marker>
<rawString>Tom Fawcett. 2003. ROC graphs: Notes and practical considerations for data mining researchers. Technical report, HP Laboratories, Palo Alto, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions, and reversals</title>
<date>1966</date>
<booktitle>Cybernetics and Control Theory, 10(8):707–710. Original in Doklady Akademii Nauk SSSR</booktitle>
<volume>163</volume>
<pages>845--848</pages>
<contexts>
<context>n the next and performance failures at the lower levels naturally percolating through to the higher. If the corecorrection mechanism cannot handle errors at e.g. Levenshtein distance 4 (further: LD, (Levenshtein, 1966)), errors of that type will never be corrected by the system that is based on it. One may well think that an error needs to be detected before it can be corrected and argue that detection should be d</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>V. I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions, and reversals. Cybernetics and Control Theory, 10(8):707–710. Original in Doklady Akademii Nauk SSSR 163(4): 845–848 (1965).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Sch¨utze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing</title>
<date>1999</date>
<publisher>MIT Press</publisher>
<location>Cambridge, Massachusetts; London, England</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>C.D. Manning and H. Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. MIT Press, Cambridge, Massachusetts; London, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph J Pollock</author>
<author>Antonio Zamora</author>
</authors>
<title>Automatic spelling correction in scientific and scholarly text</title>
<date>1984</date>
<journal>Commun. ACM</journal>
<volume>27</volume>
<contexts>
<context>the system deals with words it does not have in its dictionary and to measure this. Precision is strongly determinative of a system’s ‘fitness’ for automatic correction. We have known at least since (Pollock and Zamora, 1984) (p. 104) that ‘Automatic correction requires a much more precise detection phase than manual correction and, surprisingly, it seems easier to achieve high accuracy in correction than in detection.’ </context>
</contexts>
<marker>Pollock, Zamora, 1984</marker>
<rawString>Joseph J. Pollock and Antonio Zamora. 1984. Automatic spelling correction in scientific and scholarly text. Commun. ACM, 27(4):358–368.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Reynaert</author>
</authors>
<title>Text-Induced Spelling Correction</title>
<date>2005</date>
<tech>Ph.D. thesis</tech>
<institution>Tilburg University</institution>
<contexts>
<context>d, whether they represent real-world data or fabricated data, how large the sets are and what they are composed of. We leave these topics for a future paper, but the interested reader is referred to (Reynaert, 2005) where these aspects are treated to greater or lesser extent. In Section 2 we take an introductory look at current practice in the field. In Section 3 we introduce the task, the measures and the metr</context>
<context>s and FNs should correspond exactly to the target, i.e. the known number of errors in the particular test set. We will call this the ‘sum’ test. They are not alone in erring here, we have done so in (Reynaert, 2005). On page 107 we stated: ‘ The score for False Positives, i.e. Precision errors, is incremented in the same manner by the type’s token frequency for those types for which the system returns correctio</context>
<context>ach token’s context and the particular (n-best) CC(s) proposed by the system. 5. Current practice revisited We posit that no lexicon can ever be complete. A fuller discussion of this can be found in (Reynaert, 2005). A system encountering a word absent from its dictionary will try to correct it and will suggest correction candidates. This needs measuring. It is not really sufficient to only state how many items</context>
<context>uncorrected true typos, the False Negatives. We would in fact very much like to learn how the variable threshold in practice performs, as it is very similar to the ‘Zipf Filters’ we have proposed in (Reynaert, 2005). In terms of our proposal, a full evaluation of this kind of work would entail first a Level 1 evaluation in order to assess how many of the real-life errors in the test text are in fact covered by </context>
<context>nslate into increasing F measure values, because Accuracy is sensitive only to the number of classification errors and the F measure is biased towards maximizing the TPs. In the same vein we have in (Reynaert, 2005) looked in depth at the area under the ROC curve or AUC first advocated by (Bradley, 1997). The AUC is a single scalar value between 0.0 and 1.0 representing a system’s performance. The AUC is a redu</context>
</contexts>
<marker>Reynaert, 2005</marker>
<rawString>Martin Reynaert. 2005. Text-Induced Spelling Correction. Ph.D. thesis, Tilburg University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Reynaert</author>
</authors>
<title>Non-interactive OCR postcorrection for giga-scale digitization projects</title>
<date>2008</date>
<booktitle>Proceedings of the Computational Linguistics and Intelligent Text Processing 9th International Conference, CICLing 2008. Lecture Notes in Computer Science</booktitle>
<volume>4919</volume>
<pages>617--630</pages>
<editor>In A. Gelbukh, editor</editor>
<publisher>Heidelberg. Springer</publisher>
<location>Berlin</location>
<marker>Reynaert, 2008</marker>
<rawString>Martin Reynaert. 2008. Non-interactive OCR postcorrection for giga-scale digitization projects. In A. Gelbukh, editor, Proceedings of the Computational Linguistics and Intelligent Text Processing 9th International Conference, CICLing 2008. Lecture Notes in Computer Science Vol. 4919/2008, pages 617–630, Berlin / Heidelberg. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Ringlstetter</author>
<author>Klaus U Schulz</author>
<author>Stoyan Mihov</author>
</authors>
<title>Orthographic errors in web pages: Toward cleaner web corpora</title>
<date>2006</date>
<journal>Computational Linguistics</journal>
<volume>32</volume>
<contexts>
<context>aper. 2. Current Practice We take it that current practice in the evaluation of SCCs is best exemplified by the latest major publication on spelling correction in a leading journal. In Section 10 of (Ringlstetter et al., 2006), the authors report on experiments geared at the fully automated correction of English web-documents using different web-crawled domain dictionaries. The full test text contains 17,697 tokens, of wh</context>
<context>ion.’ Furthermore, if one measures Precision, one thereby also measures the lexical coverage of a system, although indirectly. In the concisely statable terms of the framework we have just proposed, (Ringlstetter et al., 2006) perform a True, First-Best, Level 5 evaluation on Tokens. Given the information in the article, we cannot know exactly what their scores in terms of the metrics we propose are. We are given informat</context>
</contexts>
<marker>Ringlstetter, Schulz, Mihov, 2006</marker>
<rawString>Christoph Ringlstetter, Klaus U. Schulz, and Stoyan Mihov. 2006. Orthographic errors in web pages: Toward cleaner web corpora. Computational Linguistics, 32(3):295–340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Schaback</author>
<author>Fang Li</author>
</authors>
<title>Multi-level feature extraction for spelling correction</title>
<date>2007</date>
<booktitle>In IJCAI-2007 Workshop on Analytics for Noisy Unstructured Text Data</booktitle>
<pages>79--86</pages>
<location>Hyderabad, India</location>
<contexts>
<context>ix represents the full ‘universe’ of an evaluation. As such, it can be used to help guide the evaluation process. This is perhaps best illustrated by an example where the evaluators were led astray. (Schaback and Li, 2007) start their piece on evaluation by writing they ‘base their evaluation on precision and recall measures as if spell checking were a retrieval task’. This creates false expectations because their def</context>
</contexts>
<marker>Schaback, Li, 2007</marker>
<rawString>Johannes Schaback and Fang Li. 2007. Multi-level feature extraction for spelling correction. In IJCAI-2007 Workshop on Analytics for Noisy Unstructured Text Data, pages 79–86, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marianne Starlander</author>
<author>Andrei Popescu-Belis</author>
</authors>
<date>2002</date>
<contexts>
<context>n 5 we revisit the topic of current practice and outline how and why we think evaluations based on our framework are more informative. In Section 6 we study related work on the evaluation of SCCs in (Starlander and Popescu-Belis, 2002) and (van Huyssteen et al., 2004) and contrast their main proposals with our own. Section 7 briefly contrasts the metrics we propose to be used to Accuracy, the metric employed most often in the eval</context>
</contexts>
<marker>Starlander, Popescu-Belis, 2002</marker>
<rawString>Marianne Starlander and Andrei Popescu-Belis. 2002.</rawString>
</citation>
<citation valid="true">
<title>Corpus-based evaluation of a French spelling and grammar checker</title>
<booktitle>In LREC 2002 : Third International Conference on language resources and evaluation</booktitle>
<volume>1</volume>
<pages>268--274</pages>
<location>Spain. Paris</location>
<marker></marker>
<rawString>Corpus-based evaluation of a French spelling and grammar checker. In LREC 2002 : Third International Conference on language resources and evaluation, volume 1, pages 268–274, Las Palmas de Gran Canaria, Spain. Paris : ELRA, European Language Resources.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Robert C Moore</author>
</authors>
<title>Pronunciation modeling for improved spelling correction</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL</booktitle>
<pages>144--151</pages>
<contexts>
<context> system is often evaluated only on its correction capabilities, by presenting it with a list of errors only. Highly influential and interesting examples of this practice are (Brill and Moore, 2000), (Toutanova and Moore, 2002). We regard detection as Level 2 in the evaluation of SCCs, because this is where two components, i.e. the correction mechanism and the detection mechanism work in concert and should be evaluated in </context>
</contexts>
<marker>Toutanova, Moore, 2002</marker>
<rawString>Kristina Toutanova and Robert C. Moore. 2002. Pronunciation modeling for improved spelling correction. In Proceedings of the 40th Annual Meeting of the ACL, pages 144–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G B van Huyssteen</author>
<author>E R Eiselen</author>
<author>M J Puttkammer</author>
</authors>
<title>Re-evaluating evaluation metrics for spelling checker evaluations</title>
<date>2004</date>
<booktitle>In Proceedings of First Workshop on International Proofing Tools and Language Technologies</booktitle>
<pages>91--99</pages>
<institution>Patras: University of Patras</institution>
<location>Greece</location>
<marker>van Huyssteen, Eiselen, Puttkammer, 2004</marker>
<rawString>G.B. van Huyssteen, E.R. Eiselen, and M.J. Puttkammer. 2004. Re-evaluating evaluation metrics for spelling checker evaluations. In Proceedings of First Workshop on International Proofing Tools and Language Technologies, pages 91–99, Patras: University of Patras, Greece.</rawString>
</citation>
</citationList>
</algorithm>

