Och, Franz Josef and Hermann Ney. 2003. A sys-
J03-1002 P03-1021
IBM-models (Brown et al., 1993) and Hidden
J93-2003
Grammars (Saers et al., 2010). To be able to in-
N10-1050
(LITGs) were introduced in Saers et al. (2010),
N10-1050
Saers, Markus, Joakim Nivre, and Dekai Wu. 2010.
N10-1050
ing with MERT (Och, 2003). To carry out the ac-
J03-1002 P03-1021
GIZA++ (Brown et al., 1993; Vogel et al., 1996;
C96-2141 J93-2003
Och and Ney, 2003). A heuristic for converting
J03-1002 P03-1021
sistent with Saers et al. (2009), SBITG has a sharp
W09-2304 W09-3804
tried (Zhang et al., 2008; Saers and Wu, 2009;
P08-1012 W09-2304 W09-3804
Saers et al. (2009) present a scheme for prun-
W09-2304 W09-3804
tual translations, Moses (Koehn et al., 2007) was
P07-2045
(Wu, 1997) are transduction grammars that have
J97-3002
Wu, Dekai. 1997. Stochastic inversion transduction
J97-3002
Och, Franz Josef. 2003. Minimum error rate training
J03-1002 P03-1021
Markov Models (Vogel et al., 1996). These
C96-2141
(Wu, 1997; Huang et al., 2009). This is an
J09-4009 J97-3002
Saers, Markus, Joakim Nivre, and Dekai Wu. 2009.
W09-2304 W09-3804
Haghighi et al., 2009; Saers et al., 2009; Saers
P09-1104 W09-2304 W09-3804
The LITG presented in Saers et al. (2010) is
N10-1050
Saers, Markus and Dekai Wu. 2009. Improving
W09-2304 W09-3804
