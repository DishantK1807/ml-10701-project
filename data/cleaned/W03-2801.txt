1:128	Reuse and Challenges in Evaluating Language Generation Systems: Position Paper Kalina Bontcheva University of Sheffield Regent Court, 211 P ortobello Street Sheffield S1 4DP, UK kalina@dcs.shef.ac.uk Abstract Although there is an increasing shift towards evaluating Natural Language Generation (NLG) systems, there are still many NLG-specific open issues that hinder effective comparative and quantitative evaluation in this field.
2:128	The paper starts off by describing a task-based, i.e., black-box evaluation of a hypertext NLG system.
3:128	Then we examine the problem of glass-box, i.e., module specific, evaluation in language generation, with focus on evaluating machine learning methods for text planning.
4:128	1 Introduction Although there is an increasing shift towards evaluating Natural Language Generation (NLG) systems, there are still many NLG-specific open issues that hinder effective comparative and quantitative evaluation in this field.
5:128	As discussed in (Dale and Mellish, 1998), because of the differences between language understanding and generation, most NLU evaluation techniques1 cannot be applied to generation.
6:128	The main problems come from the lack of well-defined input and output for NLG systems (see also (Wilks, 1992)).
7:128	Different systems assume different kinds of input, depending on their domains, tasks and target media, which makes comparative evaluation particularly 1For a comprehensive review see (Sparck Jones and Galliers, 1996).
8:128	difficult.2 It is also very hard to obtain a quantitative, objective, measure of the quality of output texts, especially across different domains and genres.
9:128	Therefore, NLG systems are normally evaluated with respect to their usefulness for a particular (set of) task(s), which is established by measuring user performance on these tasks, i.e., extrinsic evaluation.
10:128	This is often also referred to as black-box evaluation, because it does not focus on any specific module, but evaluates the systems performance as a whole.
11:128	This paper presents one such evaluation experiment with focus on the issue of reusing resources such as questionnaires, and task and experiment designs.
12:128	It then examines the problem of glass-box, i.e., module specific, evaluation in language generation, with focus on the problem of evaluating machine learning methods for text planning.
13:128	2 The System in Brief HYLITE+ (Bontcheva and Wilks, 2001; Bontcheva, 2001b) is a dynamic hypertext system3 that generates encyclopaedia-style explanations of terms in two specialised domains: chemistry and computers.
14:128	The user interacts with the system in a Web browser by specifying a term she wants to look up.
15:128	The system generates a 2The same is not true for understanding tasks since they all operate on the same input, i.e., existing texts.
16:128	So for example, two part-of-speech taggers or information extraction systems can be compared by running them on the same test corpus and measuring their relative performance.
17:128	3In dynamic hypertext page content and links are created on demand and are often adapted to the user and the previous interaction.
18:128	hypertext explanation of the term; further information can be obtained by following hypertext links or specifying another query.
19:128	The system is based on applied NLG techniques, a re-usable user modelling component (VIEWGEN), and a flexible architecture with module feedback.
20:128	The adaptivity is implemented on the basis of a user and a discourse models which are used to determine, for example, which concepts are unknown, so clarifying information can be included for them.
21:128	The user model is updated dynamically, based on the users interaction with the system.
22:128	When a user registers with the system for the first time, her model is initialised from a set of stereotypes.
23:128	The system determines which stereotypes apply on the basis of information provided by the user herself.
24:128	If no such information is provided, the system assumes a novice user.
25:128	3 Extrinsic Evaluation of HYLITE+ Due to the fact that HYLITE+ generates hypertext which content and links are adapted to the user, it can be evaluated following strategies from two fields: NLG and adaptive hypertext.
26:128	After reviewing the approaches, used for evaluation of the NLG and adaptive hypertext systems most similar to ours,e.g., (Cox et al., 1999), (Reiter et al., 1995), (Hook, 1998), we discovered that they were all evaluated extrinsically by measuring human performance on a set of tasks, given different versions of the system.
27:128	The experiments were typically followed by an informal interview and/or questionnaire, used to gather some qualitative data, e.g., on the quality of the generated text.
28:128	Setting up and conducting such task-based experiments is costly and time-consuming, therefore we looked at opportunities for reusing materials and methodologies from previous evaluation experiments of similar systems from the two fields.
29:128	This resulted in a substantial reduction of the time and effort needed to prepare the experiments.
30:128	We also used the findings of some of these experiments in order to improve the design of our own evaluation.
31:128	For example, (Cox et al., 1999) used pre-generated static pages as a baseline and the study reported that the difference in the two systems response times might have influenced some of the results.
32:128	Therefore, we chose instead to have both the baseline non-adaptive and the adaptive systems to generate the pages in real time, which eliminated the possible influence of the different response times.
33:128	3.1 Choosing the Main Goals of the Evaluation The first issue that needs to be addressed when designing the extrinsic, or black-box, evaluation is to determine what are the goals of the experiment.
34:128	Hypermedia applications are evaluated along three aspects: interface look and feel, representation of the information structure, and application-specific information (Wills et al., 1999).
35:128	The information structure is concerned with the hypertext network (nodes and links) and navigation aids (e.g., site maps, links to related material, index).
36:128	The application-specific information concerns the hypermedia content  text, images, audio and video.
37:128	For our system there is no need to evaluate the interface, since HYLITE+ uses simple HTML and existing Web browsers (e.g. Netscape, Internet Explorer) as rendering tools.
38:128	Therefore, the evaluation efforts were concentrated on the information content and navigational structure of the generated hypertext.
39:128	Information content was measured on the basis of: a0 average time to complete each task; a0 average number of pages visited per task; a0 average number of distinct pages visited per task; a0 percent of correctly answered questions per task; a0 questionnaire results about content and comprehension of the generated pages; a0 user preference for any of the systems.
40:128	The navigational structure was measured by the following metrics: a0 average time per page visited; a0 average number of pages visited; a0 total number of pages visited; a0 number of links followed; a0 usage of the browser Back button; a0 usage of the systems topic list to find information; a0 observation and subjective opinion on orientation; a0 subjective opinion on navigation and ease of finding information.
41:128	3.2 Choosing the Methodology The experiment has a repeated measures, taskbased design (also called within-subjects design), i.e., the same users interacted with the two versions of the system, in order to complete a given set of tasks.
42:128	Prior to the experiment, the participants were asked to provide some background information (e.g., computing experience, familiarity with Web browsers, and electronic encyclopaedia) and fill in a multiple choice pre-test, that diagnosed their domain knowledge.
43:128	The design of the tasks follows the design used in the evaluation of two other adaptive hypermedia applications  PUSH (Hook, 1998) and (Wills et al., 1999).
44:128	Each of the participants was first given a set of three tasks  each set contained one browsing, one problem-solving, and one information location task.
45:128	The order was not randomised, because the browsing task was also intended as a task that would allow users to familiarise themselves with the system and the available information; it was not used for deriving the quantitative measures discussed above.
46:128	The participants performed the first set of tasks with the non-adaptive/adaptive system and then swapped systems for the second set of three tasks.
47:128	The types of tasks  browsing, problem-solving, and information location  were chosen to reflect the different uses of hypermedia information.
48:128	Qualitative data and feedback were obtained using a questionnaire and semi-structured interviews, where the subjects could discuss their experience with the two systems.
49:128	There were two main types of questions and statements: those related to the usability of the adaptive and baseline systems, e.g., statements like I found the adaptive system difficult to use; and those related to hypertext and navigation, e.g., links, text length, structure.
50:128	3.3 Results Due to the small number of participants and the differences in their prior domain knowledge and browsing styles, the results obtained could not be used to derive a statistically reliable comparison between the measures obtained for the adaptive and the non-adaptive versions, but the quantitative results and user feedback are sufficiently encouraging to suggest that HYLITE+ adaptivity is of benefit to the user.
51:128	The most important outcome of this small-scale evaluation was that it showed the need to control not just for users prior knowledge (e.g., novice, advanced), but also for hypertext reading style.
52:128	Although previous studies of people browsing hypertext (e.g., (Nielsen, 2000)) have distinguished two types: skimmers and readers, in this experiment we did not control for that, because the tasks from which we derived the quantitative measures were concerned with locating information and problem solving, not browsing.
53:128	Still, our results showed the need to control for this variable, regardless of the task type, because reading style influences some of the quantitative measures (e.g., task performance, mean time per task, number of visited pages, use of browser navigation buttons).
54:128	Due to space limitations no further details can be provided in this paper, but see (Bontcheva, 2001a) for a detailed discussion.
55:128	3.4 Discussion The methodology used for HYLITEs black-box evaluation was based on experience not only in the field of language generation, but also in the field of hypermedia, which motivated us to evaluate also the usability of the system and elicit the users attitudes towards the intelligent behaviour of our generation system.
56:128	This emphasis on usability, which comes from human-computer interaction, allowed us to obtain results which ultimately had implications for the architecture of our generation system (see (Bontcheva and Wilks, 2001) for further details) and which we would have not obtained otherwise.
57:128	This leads us to believe that reuse of evaluation resources and methodologies from different, but related fields, can be beneficial for NLP systems in general.
58:128	On the other hand, even though evaluating the NLG system in a task-based fashion has had positive impact, there is still a need for glass-box evaluation on a module by module basis, especially using quantitative evaluation metrics, in order to be able to detect specific problems in the generation modules.
59:128	This is the evaluation challenge that we discuss in the rest of the paper.
60:128	4 The Challenge: Automatic Quantitative Evaluation of Content Planners Content planning, also called deep language generation, is the stage where the system needs to decide what to say, i.e., select some predicates encoding the semantics of the text to be generated, and then decide when to say them, i.e., choose an ordering of these predicates that will result in the generation of coherent discourse.
61:128	Typically content plans are created manually by NLG experts in collaboration with domain specialists, using a corpus of target texts.
62:128	However, this is a time consuming process, so recently researchers have started experimenting with using machine learning for content planning.
63:128	This is the research area which we will investigate as part of building an NLG system for the e-science Grid project MIAKT4.
64:128	The surface realisation module will be reused from HYLITE+, while the HYLITE+ content planner will be used as a baseline.
65:128	An integral part of the development of machine learning approaches to NLP tasks is the ability to perform automatic quantitative evaluation in order to measure differences between different configurations of the module and also allow comparative evaluation with other approaches.
66:128	For example, the MUC corpora and the associated scoring tool are frequently used by researchers working on machine learning for Information Extraction both as part of the development process and also as means for comparison of the performance of dif4The MIAKT project is sponsored by the UK Engineering and Physical Sciences Research Council (grant GR/R85150/01) and involves the University of Southampton, University of Sheffield, the Open University, University of Oxford, and Kings College London.
67:128	ferent systems (see e.g., (Marsh and Perzanowski, 1998)).
68:128	Similarly, automatic quantitative evaluation of content planners needs: a0 an annotated corpus; a0 an evaluation metric and a scoring tool, implementing this metric.
69:128	Below we will discuss each of these components and highlight the outstanding problems and challenges.
70:128	4.1 Evaluation Corpora for Content Planning Research on content planning comes from two fields: document summarisation which uses some NLG techniques to generate the summaries; and natural language generation where the systems generate from some semantic representation, e.g., a domain knowledge base or numeric weather data.
71:128	Here we review some work from these fields that has addressed the issue of evaluation corpora.
72:128	4.1.1 Previous Work (Kan and Mckeown, 2002) have developed a corpus-trained summarisation system for indicative summaries.
73:128	As part of this work they annotated manually 100 bibliography entries with indicative summaries and then used a decision tree learner to annotate automatically another 1900 entries with 24 predicates like Audience, Topic, and Content.
74:128	For example, some annotations for the Audience predicate are: For adult readers; This books is intended for adult readers.
75:128	The annotated texts are then used to learn the kinds of predicates present in the summaries, their ordering using bigram statistics, and surface realisation patterns.
76:128	(Barzilay et al., 2002) have taken the problem of learning sentence ordering for summarisation one step further by considering multi-document summarisation of news articles.
77:128	Their experiments show that ordering is significant for text comprehension and there is no one ideal ordering, rather there is a set of acceptable orderings.
78:128	Therefore, an annotated corpus which provides only one of the acceptable orderings is not sufficient to enable the system to differentiate between the many good orderings and the bad ones.
79:128	To solve this problem they developed a corpus of multiple versions of the same content, each version providing an acceptable ordering.
80:128	This corpus5 consists of ten sets of news articles, two to three articles per event.
81:128	Sentences were extracted manually from these sets and human subjects were asked to order them so that they form a readable text.
82:128	In this way 100 orderings were acquired, 10 orderings per set.
83:128	However, since this procedure involved a lot of human input, the construction of such a corpus on a larger scale is quite expensive.
84:128	The difference between the techniques used for summarisation and those used for generation is that the summarisation ones typically do not use very detailed semantic representations, unlike the full NLG systems.
85:128	Consequently this means that a corpus annotated for summarisation purposes is likely to contain isufficient information for a full NLG application, while corpus with detailed semantic NLG annotation will most likely be useful for a summarisation content planner.
86:128	Since the experience from building annotated corpora for learning ordering for summarisation has shown that they are expensive to build, then the creation of semantically annotated corpora for NLG is going to be even more expensive.
87:128	Therefore, reuse and some automation are paramount.
88:128	So far, only very small semantically annotated corpora for NLG have been created.
89:128	For example, (Duboue and McKeown, 2001) have collected an annotated corpus of 24 transcripts of medical briefings.
90:128	They use 29 categories to classify the 200 tags used in their tagset.
91:128	Each transcript had an average of 33 tags with some tags being much more frequent than others.
92:128	Since the tags need to convey the semantics of the text units, they are highly domain specific, which means that any other NLG system or learning approach that would want to use this corpus for evaluation will have to be retargetted to this domain.
93:128	4.1.2 The Proposed Approach for MIAKT As evident from this discussion, there are still a number of problems that need to be solved so that a semantically annotated corpus of a useful size 5Available at http://www.cs.columbia.edu/ noemie/ordering/.
94:128	can be created, thus enabling the comparative evaluation of different learning strategies and content planning components.
95:128	Previous work has typically started from already existing texts/transcripts and then used humans to annotate them with semantic predicates, which is an expensive operation.
96:128	In addition, the experience from the Information Extraction evaluations in MUC and ACE has shown that even humans find it difficult to annotate texts with deeper semantic information.
97:128	For example, the interannotator variability on the scenario template task in MUC-7 was between 85.15 and 96.64 on the f-measures (Marsh and Perzanowski, 1998).
98:128	In the MIAKT project we will experiment with a different approach to creating an annotated corpus of orderings, which is similar to the approach taken by (Barzilay et al., 2002), where humans were given sentences and asked to order them in an acceptable way.
99:128	Since MIAKT is a full NLG system we cannot use already existing sentences, as it was possible in their summarisation systems.
100:128	Instead, we will use the HYLITE+ surface realiser to generate sentences for each of the semantic predicates and then provide users with a graphical editor, where they can re-arrange the ordering of these sentences by using drag and drop.
101:128	In this way, there will be no need for the users to annotate with semantic information, because the system will have the corresponding predicates from which the sentences were generated.
102:128	This idea is similar to the way in which language generation is used to support users with entering knowledge base content (Power et al., 1998).
103:128	The proposed technique is called What You See Is What You Meant (WYSIWYM) and allows a domain expert to edit a NLG knowledge base reliably by interacting with a text, generated by the system, which presents both the knowledge already defined and the options for extending it.
104:128	In MIAKT we will use instead the generator to produce the sentences, so the user only needs to enter their order.
105:128	We will not need to use WYSIWYM editing for knowledge entry, because the knowledge base will already exist.
106:128	The difference between using generated sentences and sentences from human-written texts is that the human-written ones tend to be more complex and aggregate the content of similar predicates.
107:128	This co-occurence information may be important, because, in a sense, it conveys stronger restrictions on ordering than those between two sentences.
108:128	Therefore we would like to experiment with taking an already annotated corpus of humanauthored texts, e.g., MUC-7 and compare the results achieved by using this corpus and a corpus of multiple orderings created by humans from the automatically generated sentences.
109:128	In general, the question here is whether or not it is possible to reuse a corpus annotated for information extraction for the training of a content planning NLG component.
110:128	4.2 Evaluation Metrics Previous work on learning order constraints has used human subjects for evaluation.
111:128	For example, (Barzilay et al., 2002) asked humans to grade the summaries, while (Duboue and McKeown, 2001) manually analysed the derived constraints by comparing them to an existing text planner.
112:128	However, this is not sufficient if different planners or versions of the same planner are to be compared in a quantitative fashion.
113:128	In contrast, quantitative metrics for automatic evaluation of surface realisers have been developed (Bangalore et al., 2000) and they have been shown to correlate well with human judgement for quality and understandability.
114:128	These metrics are two kinds: using string edit distance and using tree-based metrics.
115:128	The string edit distance ones measure the insertion, deletion, and substitution errors between the reference sentences in the corpus and the generated ones.
116:128	Two different measures were evaluated and the one that treats deletions in one place and insertion in the other as a single movement error was found to be more appropriate.
117:128	In the context of content planning we intend use the string edit distance metrics by comparing the proposition sequence generated by the planner against the ideal proposition sequence from the corpus.
118:128	The tree-based metrics were developed to reflect the intuition that not all moves are equally bad in surface realisation.
119:128	Therefore these metrics use the dependency tree as a basis of calculating the string edit distances.
120:128	However, it is not very clear whether this type of metrics will be applicable to the content planning problem given that we do not intend to use a planner that produces a tree-like structure of the text (as do for example RST-based planners, e.g., (Moore, 1995)).
121:128	If the reuse experiments in MIAKT are successful, we will make our evaluation tool publically available, together with the annotated corpus and the knowledge base of predicates, which we hope will encourage other researchers to use them for development and/or comparative evaluation of content planners.
122:128	5 Conclusion In this paper we discussed the reuse of existing resouces and methodologies for extrinsic evaluation of language generation systems.
123:128	We also showed that a number of challenges still exist in evaluation of NLG systems and, more specifically, evaluation of content planners.
124:128	While other fields like machine translation and text summarisation already have some evaluation metrics and resources available for reuse, language generation has so far lagged behind and no comparative system evaluation has ever been done on a larger scale, e.g., text summarisation systems are compared in the DUC evaluation exercise.
125:128	As a step towards comparative evaluation for NLG, we intend to make available the annotated corpus, evaluation metric(s) and tools to be developed as part of the recently started MIAKT project.
126:128	6 Acknowledgments The work on MIAKT described here is being supported by the UK Engineering and Physical Sciences Research Council (grant GR/R85150/01).
127:128	The work on HYLITE+ was supported by a PhD fellowship by the University of Sheffield and an Overseas Research Students Award.
128:128	I also wish to thank Yorick Wilks and Hamish Cunningham for their comments on this work, the anonymous reviewers who helped me improve the paper, and the human evaluators who participated in the HYLITE+ experiments.

