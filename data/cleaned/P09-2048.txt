Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 189–192,
Suntec, Singapore, 4 August 2009. c 2009 ACL and AFNLP
Learning Semantic Categories from Clickthrough Logs
Mamoru Komachi
Nara Institute of Science and Technology (NAIST)
8916-5 Takayama, Ikoma, Nara 630-0192, Japan
mamoru-k@is.naist.jp
Shimpei Makimoto and Kei Uchiumi and Manabu Sassano
Yahoo Japan Corporation
Midtown Tower, 9-7-1 Akasaka, Minato-ku, Tokyo 107-6211, Japan
{smakimot,kuchiumi,msassano}@yahoo-corp.jp
Abstract
As the web grows larger, knowledge ac-
quisition from the web has gained in-
creasing attention. In this paper, we pro-
pose using web search clickthrough logs
to learn semantic categories. Experimen-
tal results show that the proposed method
greatly outperforms previous work using
only web search query logs.
1 Introduction
Compared to other text resources, search queries
more directly reflect search users’ interests (Sil-
verstein et al., 1998). Web search logs are get-
ting a lot more attention lately as a source of in-
formation for applications such as targeted adver-
tisement and query suggestion.
However, it may not be appropriate to use
queries themselves because query strings are often
too heterogeneous or inspecific to characterize the
interests of the user population. Although it is not
clear that query logs are the best source of learning
semantic categories, all the previous studies using
web search logs rely on web search query logs.
Therefore, we propose to use web search
clickthrough logs to learn semantic categories.
Joachims (2002) developed a method that utilizes
clickthrough logs for training ranking of search
engines. A search clickthrough is a link which
search users click when they see the result of
their search. The intentions of two distinct search
queries are likely to be similar, if not identical,
when they have the same clickthrough. Search
clickthrough logs are thus potentially useful for
learnin semantic categories. Clickthrough logs
have the additional advantage that they are avail-
able in abundance and can be stored at very low
cost.1 Our proposed method employs search click-
1As for data availability, MSN Search query logs
(RFP 2006 dataset) were provided to WSCD09: Work-
through logs to improve semantic category acqui-
sition in both precision and recall.
We cast semantic category acquisition from
search logs as the task of learning labeled in-
stances from few labeled seeds. To our knowledge
this is the first study that exploits search click-
through logs for semantic category learning.2
2 Related
Work
There are many techniques that have been devel-
oped to help elicit knowledge from query logs.
These algorithms use contextual patterns to extract
a category or a relation in order to learn a target in-
stance which belongs to the category (e.g. cat in
animal class) or a pair of words in specific relation
(e.g. headquarter to a company). In this work,
we focus on extracting named entities of the same
class to learn semantic categories.
Pas¸ca and Durme (2007) were the first to dis-
cover the importance of search query logs in nat-
ural language processing applications. They fo-
cused on learning attributes of named entities, and
thus their objective is different from ours. An-
other line of new research is to combine various re-
sources such as web documents with search query
logs (Pas¸ca and Durme, 2008; Talukdar et al.,
2008). We differ from this work in that we use
search clickthrough logs rather than search query
logs.
Komachi and Suzuki (2008) proposed a boot-
strapping algorithm called Tchai, dedicated to the
task of semantic category acquisition from search
query logs. It achieves state-of-the-art perfor-
mance for this task, but it only uses web search
query logs.
shop on Web Search Click Data 2009 participants. http://
research.microsoft.com/en-US/um/people/nickcr/WSCD09/
2After the submission of this paper, we found that (Xu et
al., 2009) also applies search clickthrough logs to this task.
This work independently confirms the effectiveness of click-
through logs to this task using different sources.
189
Figure 1: Labels of seeds are propagated to unla-
beled nodes.
3 Quetchup3
Algorithm
In this section, we describe an algorithm for
learning semantic categories from search logs us-
ing label propagation. We name the algorithm
Quetchup.
3.1 Semi-supervised Learning by Laplacian
Label Propagation
Graph-based semi-supervised methods such as la-
bel propagation are known to achieve high perfor-
mance with only a few seeds and have the advan-
tage of scalability.
Figure 1 illustrates the process of label propa-
gation using a seed term “singapore” to learn the
Travel domain.
This is a bipartite graph whose left-hand side
nodes are terms and right-hand side nodes are
patterns. The strength of lines indicates related-
ness between each node. The darker a node, the
more likely it belongs to the Travel domain. Start-
ing from “singapore,” the pattern “♯ airlines” 4 is
strongly related to “singapore,” and thus the label
of “singapore” will be propagated to the pattern.
On the other hand, the pattern “♯ map” is a neu-
tral pattern which co-occurs with terms other than
the Travel domain such as “google” and “yahoo.”
Since the term “china” shares two patterns, “♯ air-
lines” and “♯ map,” with “singapore,” the label of
the seed term “singapore” propagates to “china.”
“China” will then be classified in the Travel do-
main. In this way, label propagation gradually
propagates the label of seed instances to neigh-
bouring nodes, and optimal labels are given as the
3Query Term Chunk Processor
4♯ is the place into which a query fits.
Input:
Seed instance vector F(0)
Instance similarity matrix A
Output:
Instance score vector F(t)
1: Construct the normalized Laplacian matrix L = I −
D−1/2AD−1/2
2: Iterate F(t + 1) = α(−L)F(t) + (1 − α)F(0) until
convergence
Figure 2: Laplacian label propagation algorithm
labels at which the label propagation process has
converged.
Figure 2 describes label propagation based on
the regularized Laplacian. Let a sample xi be xi ∈
X, F(0) be a score vector of x comprised of a
label set yi ∈ Y, and F(t) be a score vector of
x after step t. Instance-instance similarity matrix
A is defined as A = WTW where Wis a row-
normalized instance-pattern matrix. The (i,j)-th
element of Wij contains the normalized frequency
of co-occurrence of instance xi and pattern pj. D
is a diagonal degree matrix of N where the (i,i)th
element of D is given as Dii =summationtextj Nij.
This algorithm in Figure 2 is similar to (Zhou
et al., 2004) except for the method of construct-
ing A and the use of graph Laplacian. Zhou et al.proposed a heuristic to set Aii = 0 to avoid self-
reinforcement5 because Gaussian kernel was used
to create A. The Laplacian label propagation does
not need such a heuristic because the graph Lapla-
cian automatically reduces self-reinforcement by
assigning negative weights to self-loops.
In the task of learning one category, scores of la-
beled (seed) instances are set to 1 whereas scores
of unlabeled instances are set to 0. The output is
a score vector which holds relatedness to seed in-
stances in descending order. In the task of learning
two categories, scores of seed instances are set to
either 1 or −1, respectively, and the final label of
instance xi will be determined by the sign of out-
put score vector yi.
Label propagation has a parameter α ∈ (0,1]
that controls how much the labels of seeds are em-
phasized. As α approaches 0 it puts more weight
on labeled instances, while as α increases it em-
ploys both labeled and unlabeled data.
There exists a closed-form solution for Lapla-
cian label propagation:
5Avoiding self-reinforcement is important because it
causes semantic drift, a phenomenon where frequent in-
stances and patterns unrelated to seed instances infect seman-
tic category acquisition as iteration proceeds.
190

