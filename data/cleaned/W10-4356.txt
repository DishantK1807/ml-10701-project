Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 297–305,
The University of Tokyo, September 24-25, 2010. c©2010 Association for Computational Linguistics
Dialogue Act Modeling in a Complex Task-Oriented Domain 
 
 
Kristy 
Elizabeth 
Boyer 
Eun 
Young Ha 
Robert 
Phillips
*
 
Michael D. 
Wallis
*
 
Mladen A. 
Vouk 
James C. 
Lester 
 
Department of Computer Science, North Carolina State University 
Raleigh, North Carolina, USA 
 
*
Dual afiliation with Aplied Research Asociates, Inc. 
Raleigh, North Carolina, USA 
 
{keboyer, eha, rphilli, mdwallis, vouk, lester}@ncsu.edu 
 
Abstract 
Clasifying the dialogue act of a user uterance 
is a key functionality of a dialogue 
management system. This paper presents a 
data-driven dialogue act clasifier that is 
learned from a corpus of human textual 
dialogue. The task-oriented domain involves 
tutoring in computer programing exercises. 
While engaging in the task, students generate a 
task event stream that is separate from and in 
paralel with the dialogue. To deal with this 
complex task-oriented dialogue, we propose a 
vector-based representation that encodes 
features from both the dialogue and the 
hierarchicaly structured task for training a 
maximum likelihod clasifier. This clasifier 
also leverages knowledge of the hiden 
dialogue state as learned separately by an 
HM, which in previous work has increased 
the acuracy of models for predicting tutorial 
moves and is hypothesized to improve the 
acuracy for clasifying student uterances. 
This work constitutes a step toward learning a 
fuly data-driven dialogue management model 
that leverages knowledge of the user-generated 
task event stream. 
1 Introduction

Two central chalenges for dialogue systems are 
interpreting user uterances and selecting system 
dialogue moves. Recent years have sen an 
increased focus on data-driven techniques for 
adresing these chalenging tasks (Bangalore et 
al., 208; Frampton & Lemon, 209; Hardy et 
al., 206; Sridar et al., 209; Young et al., 209). 
Much of this work utilizes dialogue acts, built on 
the notion of spech acts (Austin, 1962), which 
provide a valuable intermediate representation 
that can be used for dialogue management. 
Data-driven aproaches to dialogue act 
interpretation have included models that take into 
acount a variety of lexical, syntactic, acoustic, 
and prosodic features for dialogue act taging 
(Sridhar et al., 209; Stolcke et al., 200). In 
task-oriented domains, recent work has 
approached dialogue act clasification by 
learning dialogue management models entirely 
from human-human corpora (Bangalore et al., 
208; Chotimongkol, 208; Hardy et al., 206). 
Our work adopts this aproach for a corpus of 
human-human dialogue in a task-oriented 
tutoring domain. Unlike the majority of task-
oriented domains that have ben studied to date, 
our domain involves the separate creation of a 
persistent artifact, in our case a computer 
program, by the user during the course of the 
dialogue. Our corpus consists of human-human 
textual dialogue uterances and a separate, 
paralel stream of user-generated task actions. 
We utilize structural features including 
task/subtask, speaker, and hiden dialogue state 
along with lexical and syntactic features to 
interpret user (student) uterances. 
This paper makes thre contributions. First, it 
adreses representational isues in creating a 
dialogue model that integrates task actions with 
hierarchical task/subtask structure. The task is 
captured within a separate synchronous event 
stream that exists in paralel with the dialogue. 
Second, this paper explores the performance of 
dialogue act clasifiers using diferent 
lexical/syntactic and structural feature sets. This 
comparison includes one model trained entirely 
on lexical/syntactic features, an important step 
toward robust unsupervised dialogue act taging 
297
(Sridhar et al., 209). Finaly, it investigates 
whether the adition of HM and task/subtask 
features improves the performance of the 
dialogue act clasifiers. The findings suport this 
hypothesis for thre student dialogue moves, 
each with important implications for tutorial 
dialogue. 
2 Related
Work 
A variety of modeling aproaches have ben 
investigated for statistical dialogue act 
clasification, including sequential aproaches 
and vector-based clasifiers. Sequential 
aproaches typicaly formulate dialogue as a 
Markov chain in which an observation depends 
on a finite number of preceding observations. 
HM-based aproaches make use of the Markov 
asumption in a doubly stochastic framework 
that alows fiting optimal dialogue act sequences 
using the Viterbi algorithm (Rabiner, 1989; 
Stolcke et al., 200). Like this work, the 
aproach reported here adopts a first-order 
Markov formulation to train an HM on 
sequences of dialogue acts, but the prediction of 
this HM is subsequently encoded in a feature 
vector for training a vector-based clasifier. 
Vector-based aproaches, such as maximum 
entropy modeling, also frequently take into 
acount both lexical/syntactic and structural 
features. Lexical and syntactic cues are extracted 
from local uterance context, while structural 
features involve longer dialogue act sequences 
and, in task-oriented domains, task/subtask 
history. Work by Bangalore et al. (208) on 
learning the structure of human-human dialogue 
in a catalogue-ordering domain (also extended to 
the Maptask and Switchboard corpora) utilizes 
features including words, part of spech tags, 
supertags, and named entities, and structural 
features including dialogue acts and task/subtask 
labels. In order to perform incremental decoding 
of dialogue acts and task/subtask structure, they 
take a gredy aproach that does not require the 
search of complete dialogue sequences. Our 
work also acomplishes left-to-right incremental 
interpretation with a gredy aproach. Our 
feature vectors difer from the aforementioned 
work slightly with respect to lexical/syntactic 
features and notably in the adition of a set of 
structural features generated by a separately 
trained HM, as described in Section 4.2. 
Recent work has explored the use of lexical, 
syntactic, and prosodic features for online 
dialogue act taging (Sridhar et al., 209); that 
work explores the notion that structural (history) 
features could be omited altogether from 
incremental left-to-right decoding, resulting in 
computationaly inexpensive and robust dialogue 
act clasification. Although our textual dialogue 
does not feature prosodic cues, we report on the 
use of lexical/syntactic features alone to perform 
dialogue act clasification, a step toward a fuly 
unsupervised aproach.   
Like Bangalore et al. (208), we treat task 
structure as an integral part of the dialogue 
model. Other work that has taken this aproach 
includes the Amitiés project, in which a dialogue 
manager for a financial domain was derived 
entirely from a human-human corpus (Hardy et 
al., 206). The TRIPS dialogue system also 
closely integrated task and dialogue models, for 
example, by utilizing the task model to facilitate 
indirect spech act interpretation (Alen et al., 
201). Work on the Maptask corpus has modeled 
task structure in the form of conversational 
games (Wright Hastie et al., 202). Recent work 
in task-oriented domains has focused on learning 
task structure with unsupervised aproaches 
(Chotimongkol, 208). Emerging unsupervised 
methods, such as for detecting actions in multi-
party discourse, also implicitly capture a task 
structure (Purver et al., 206).  
Our domain difers from the task-oriented 
domains described above in that our dialogues 
center on the user creating a persistent artifact of 
intrinsic value through a separate, synchronous 
stream of task actions. To ilustrate, consider a 
catalogue-ordering task in which one subtask is 
to obtain the customer’s name. The fulfilment of 
this subtask ocurs entirely through the dialogue, 
and the resulting artifact (a completed order) is 
produced by the system. In contrast, our task 
involves the user constructing a solution to a 
computer programing problem. The fulfilment 
of this task ocurs partialy in the dialogue 
through tutoring, and partialy in a separate 
synchronous stream of user-driven task actions 
about which the tutor must reason. The stream of 
user-driven task actions produces an artifact of 
value in itself (a functioning computer program), 
and that artifact is the subject of much of the 
dialogue. We propose a representation that 
integrates task actions and dialogue acts from 
these streams into a shared vector-based 
representation, and we investigate the use of the 
resulting structural, lexical, and syntactic 
features for dialogue act clasification. 
298
3 Corpus
and Annotation 
The corpus was colected during a controled 
human-human tutoring study in which tutors and 
students worked through textual dialogue to 
solve an introductory computer programing 
problem. The dialogues were efective: on 
average, students exhibited significant learning 
and self-confidence gains (Boyer et al., 209).  
The corpus contains 48 dialogues each with a 
separate, synchronous task event stream as 
depicted in Excerpt 1 of the apendix. There is 
exactly one dialogue (tutoring sesion) per 
student. The corpus captures aproximately 48 
hours of dialogue and contains 1,468 student 
uterances and 3,38 tutor uterances. Because 
the dialogue was textual, uterance segmentation 
consisted of spliting at existing sentence 
boundaries when more than one dialogue act was 
present in the uterance. This segmentation was 
conducted manualy by the principal dialogue act 
anotator.
1
 
The corpus was manualy anotated with 
dialogue act labels and task/subtask features. 
Lexical and syntactic features were extracted 
automaticaly. The remainder of this section 
describes the manual anotation. 
3.1 Dialogue
Act Anotation 
The dialogue act anotation scheme was inspired 
by schemes for conversational spech (Stolcke et 
al., 200) and task-oriented dialogue (Core & 
Alen, 197). It was also influenced by tutoring-
specific tagsets (Litman & Forbes-Riley, 206). 
Inter-rater reliability for the dialogue act taging 
on 10% of the corpus selected via stratified (by 
tutor) random sampling was ĸ=0.80. The 
dialogue act tags, their relative frequencies, and 
their individual kapa scores from manual 
anotation are displayed in Table 1.  
3.2 Task
Anotation 
Al task actions were generated by the student 
while implementing the solution to an 
introductory computer programing problem in 
Java. These task actions were recorded as a 
separate event stream in paralel with the 
dialogue corpus. This stream included 97,509 
keystroke-level user task events, which were 
manualy agregated into task/subtask event 
clusters and anotated for subtask structure and 
then for corectnes. A total of 3,793 agregated 
                                                
1
 Automatic segmentation is a chalenging problem in itself 
and is left to future work. 
student subtask actions were identified through 
manual anotation. The task anotation scheme 
is hierarchical, reflecting the nested nature of the 
subtasks. A subset of this task anotation scheme 
is depicted in Figure 1. In the models reported in 
this paper, the 6 leaves of the task/subtask 
hierarchy were encoded in the input feature 
vectors.  
 
Table 1. Student dialogue acts 
Student Dialogue Act 
Rel. 
Freq. 
Human 
κ 
ACKNOWLEDGMENT (ACK) 
.17  .90 
REQUEST FOR FEDBACK (RF) 
.20  .91 
EXTRA‐DOMAIN (EX) 
.08  .79 
GRETING (GR) 
.04  .92 
UNCERTAIN FEDBACK WITH ELABORATION (UE) 
.01  .53 
UNCERTAIN FEDBACK (U) 
.02  .49 
NEGATIVE FEDBACK WITH ELABORATION (NE) 
.01  .61 
NEGATIVE FEDBACK (N) 
.05  .76 
POSITIVE FEDBACK WITH ELABORATION (PE) 
.02  .43 
POSITIVE FEDBACK (P) 
.09  .81 
QUESTION (Q) 
.09  .85 
STATEMENT (S) 
.16  .82 
THANKS (T) 
.05  1 
 
Each group of task events that ocured betwen 
dialogue uterances was taged, posibly with 
many subtask labels, by a human judge. The 
judge agregated the raw task keystrokes and 
taged the task/subtask hierarchy for each 
cluster. (Please se Excerpt 1 in the apendix.) A 
second judge taged 20% of the corpus in a 
reliability study for which one-to-one subtask 
identification was not enforced, an aproach that 
was intended to give judges maximum flexibility 
to cluster task actions and subsequently aply the 
tags. Al unmatched subtask tags were treated as 
disagrements. The resulting kapa statistic at 
the leaves was ĸ= 0.58. However, we also 
observe that the sequential nature of the subtasks 
within the larger task produces an ordinal 
relationship betwen subtasks. For example, in 
Figure 1, the “distance” betwen subtasks 1-a 
and 1-b can be thought of as “les than” the 
distance betwen subtasks 1-a vs. 3-d because 
those subtasks are farther from each other within 
the larger task. The weighted Kapa statistic 
(Artstein & Poesio, 208) takes into acount 
such an ordinal relationship and its implicit 
distance function. The weighted Kapa is 
299
ĸ
weighted
=0.86, which indicates aceptable inter-
rater reliability on the task/subtask anotation. 
 
Figure 1. Portion of task anotation scheme 
 
Along with its tag for hierarchical subtask 
structure, each task event was also judged for 
corectnes acording to the requirements of the 
task as depicted in Table 2. The agrement 
statistic for corectnes was calculated for task 
events on which the two judges agred on 
subtask tag. The resulting unweighted agrement 
statistic for corectnes was ĸ=0.80. 
 
Table 2. Task corectnes labels 
 
Label  Description 
CORECT  Fuly satisfying the requirements of 
the learning task. Does not require 
tutorial remediation. 
BUGGY  Violating the requirements of the 
learning task. Often requires tutorial 
remediation. 
INCOMPLETE  Not violating, but not yet fuly 
satisfying, the requirements of the 
learning task. May require tutorial 
remediation. 
DISPREFERED  Technicaly  satisfying  the 
requirements of the learning task, 
but not adhering to its pedagogical 
intentions. Usualy requires tutorial 
remediation. 
4 Features

The vector-based representation for training the 
dialogue act clasifiers integrates several sources 
of features: lexical and syntactic features, and 
structural features that include dialogue act 
labels, task/subtask labels, and set of hiden 
dialogue state prediction features.  
4.1 Lexical
and Syntactic Features 
Lexical and syntactic features were automaticaly 
extracted from the uterances using the Stanford 
Parser default tokenizer and part of spech (pos) 
tager (De Marnefe et al., 206). The parser 
created both phrase structure tres and typed 
dependencies for individual sentences. From the 
phrase structure tres, we extracted the top-most 
syntactic node and its first two children. In the 
case where an uterance consisted of more than 
one sentence, only the phrase structure tre of the 
first sentence was considered. Typed 
dependencies betwen pairs of words were 
extracted from each sentence. Individual word 
tokens in the uterances were further procesed 
with the Porter Stemer (Porter, 1980) in the 
NLTK package (Loper & Bird, 204). The pos 
features were extracted in a similar way. 
Unigram and bigram word and pos tags were 
included for feature selection in the clasifiers.   
4.2 Structural
Features 
Structural features include the anotated 
dialogue acts, the anotated task/subtask labels, 
and atributes that represent the hiden dialogue 
state. Our previous work has found that a set of 
hiden dialogue states, which corespond to 
widely acepted notions of dialogue modes in 
tutoring, can be identified in an unsupervised 
fashion (without hand labeling of the modes) by 
HMs trained on manualy labeled dialogue acts 
and task/subtask features (Boyer et al., 209). 
These HMs performed significantly beter than 
bigram odels for predicting human tutor moves 
(Boyer et al., 2010), which indicates that the 
hiden dialogue state leveraged by the HMs 
has predictive value even in the presence of 
“true” (manualy anotated) dialogue act labels. 
Therefore, we hypothesized that an HM could 
also improve the performance of models to 
clasify student dialogue acts. To explore this 
hypothesis, we trained an HM utilizing the 
methodology described in (Boyer et al., 209) 
and used it to generate hiden dialogue state 
predictions in the form of a probability 
distribution over posible user uterances at each 
step in the dialogue. This set of stochastic 
features was subsequently pased to the clasifier 
as part of the input vector (Figure 2). 
4.3 Input
Vectors 
The features were combined into a shared vector-
based representation for training the clasifier. 
As depicted in Table 3, the components of the 
300
feature vector include binary existence vectors 
for lexical and syntactic features for the curent 
(target) uterance as wel as for thre uterances 
of left context (this left context may include both 
tutor and student uterances, which are 
distinguished by a separate indicator for the 
speaker). The task/subtask and corectnes 
history features encode the separate stream of 
task events. There is no one-to-one 
corespondence betwen these history features 
and the left-hand dialogue context, because 
several task events could have ocured betwen 
a pair of dialogue events (or vice versa). This 
distinction is indicated in the table by the 
representation of dialogue time steps as [t, t-1,…] 
and task history steps as [task(t), task(t-1),…]. In 
total, the feature vectors included 1,432 
atributes that were made available for feature 
selection. 
 
Figure 2. Generation of hiden dialogue state 
prediction features 
5 Experiments

This section describes the learning of maximum 
likelihod vector-based models for clasification 
of user dialogue acts. In adition to investigating 
the acuracy of the overal model, we also 
performed experiments regarding the utility of 
feature types for discriminating betwen 
particular dialogue acts of interest.  
The clasifiers are based on logistic 
regresion, which learns a discriminant for each 
pair of dialogue acts by asigning weights in a 
maximum likelihod fashion.
2
 The logistic 
regresion models were learned using the Weka 
machine learning tolkit (Hal et al., 209). For 
                                                
2
 In general, the model that maximizes likelihod also 
maximizes entropy under the same constraints 
(Berger et al., 196).  
feature selection, we performed atribute subset 
evaluation with a best-first aproach that 
gredily searched the space of posible features 
using a hil climbing aproach with 
backtracking. The prediction acuracy of the 
clasifiers was determined through ten-fold 
cros-validation on the corpus, and the results 
below are presented in terms of prediction 
acuracy (number of corect clasifications 
divided by total number of clasifications) as 
wel as by the kapa statistic, which adjusts for 
expected agrement by chance.  
 
Table 3. Feature vectors 
Feature vector f  Description 
[w
t,1,…w
t,|w|, 
p
t,1,,p
t,|p|, 
d
t,1,…,d
t,|d|, 
s
t,1,,s
t,|s|
] 
Binary existence vector for word 
unigrams & bigrams, pos unigrams & 
bigrams, dependency types, and syntactic 
nodes for curent target uterance t  
[w
t-k,1,…w
t-k,|w|, 
p
t-k,1,,p
t-k,|p|, 
d
t-k,1,…,d
t-k,|d|, 
s
t-k,1,,s
t-k,|s|
] 
where k=1,…,3 
Binary existence vector for word 
unigrams & bigrams, pos unigrams & 
bigrams, dependency types, and syntactic 
nodes for thre uterances of left context 
[p(o
1
),…,p(o
|S|
)] 
Probability distribution for emision 
symbols in predicted next hiden state as 
generated by HM 
[da
t-1,
 
da
t-2, da
t-3
] 
Dialogue act left context 
[sp
t-1,sp
t-2, sp
t-3
] 
Speaker label left context 
[tk
task(t-1),
 
tk
task(t-2), 
tk
task(t-3)
] 
Thre steps of subtask history (each level 
of hierarchy represented as a separate 
feature) 
[c
task(t-1), c
task(t-2), 
c
task(t-3)
] 
 
Thre steps of task corectnes history 
pt 
Indicator for whether the target 
uterance was imediately preceded by 
a task event 
 
5.1 Overal
Clasification Task 
The overal dialogue act clasification model was 
trained to clasify each uterance with respect to 
the thirten dialogue acts (Table 1). For this task, 
the feature selection algorithm selected 63 
atributes including some syntax, dependency, 
pos, and word atributes as wel as dialogue act, 
speaker, and task/subtask features. No hiden 
dialogue state features or task corectnes 
atributes were selected. The overal 
clasification acuracy was 62.8%. This acuracy 
constitutes a 369% improvement over baseline 
chance of 17% (the relative frequency of the 
most frequently ocuring dialogue act, ACK). 
An alternate nontrivial baseline is a bigram 
model on true dialogue acts (including speaker 
tags); this model’s acuracy was 36.8%. The 
301
overal kapa for the ful clasifier was ĸ=.57. 
The confusion matrix for this model is depicted 
in Figure 3. 
   In adition to the clasifier described above, 
we experimented with clasifiers that used only 
the lexical and syntactic features of each 
uterance. This aproach is of interest in part 
because it avoids the eror propagation that can 
hapen when a model relies on a series of its 
own previous clasifications as features. The 
clasifier that used only the set of lexical and 
syntactic features achieved a prediction acuracy 
of 60.2% and ĸ=.53 using 85 atributes. 
 
 
5.2 Binary
Dialogue Act Clasification 
In tutoring, some student dialogue acts are 
particularly important to identify because of their 
implications for the tutor’s response or for the 
student model. For example, a student’s 
REQUEST FOR FEDBACK requires the tutor to 
ases the condition of the task, rather than to 
query the in-domain factual knowledge base. 
UNCERTAIN FEDBACK is another dialogue act of 
high importance because identifying it alows the 
tutor to respond in an afectively advantageous 
way (Forbes-Riley & Litman, 209). 
To explore which features are useful for 
clasifying particular dialogue acts, we 
constructed binary dialogue act clasifiers, one 
for each dialogue act, by preprocesing the 
dialogue act labels from the set of thirten down 
to TRUE or FALSE depending on whether the label 
of the uterance matched the target dialogue act 
for that specialized clasifier. Table 4 displays 
the features that were selected for each binary 
clasifier, along with the percent acuracy and 
kapa for each model. Note that for some 
dialogue acts the chance baseline is very high, 
and therefore even a model with high prediction 
acuracy achieves a low kapa.  
   As depicted in Table 4, for several dialogue 
act models, the feature selection algorithm 
retained subtask and HM features. 
 
Table 4. Binary DA clasifiers 
 
DA  # Features Selected 
% 
Corect 
Model 
κ 
ACK 51 
Lexical/syntax, HM, DA history 
(preceding=S), speaker history 
(preceding=Tutor) 
.93  .75 
RF  42 
Lexical/syntax, DA history, 
preceded by subtask 
.905  .72 
EX  57 
Dependency, pos, word, HM, 
DA history (preceding=EX), 
subtask 
.939  .45 
GR 11 
Syntax, pos, word, DA 
(previous=EMPTY), speaker, 
subtask 
.98  .97 
UE 21 
Dependency, pos, word, subtask  .91  .3 
U  63 
Syntax, dependency, pos, word, 
HM, subtask 
.979  .21 
NE 44 
Dependency, pos, word, HM, 
DA history (2 ago=UNCERTAIN), 
subtask 
.987  0 
N  83 
Lexical/syntax, DA history, 
subtask 
.966  .76 
PE 90 
Dependency, pos, word, HM, 
subtask 
.976  .10 
P 110 
Dependency, pos, word, HM, 
DA history (previous=REQUEST 
FEDBACK) 
.945  .58 
Q  43 
Syntax, dep, pos, word, HM, 
subtask 
.940  .60 
S  92 
Syntax, pos, word, HM, DA 
history (previous=EMPTY or Q) 
.901  .57 
T  29 
Syntax, pos, word, DA history 
(previous=POSITIVE) (3 
ago=POSITIVE) 
.92  .92 
 
  In an experiment to quantify the utility of these 
features, it was found that for many dialogue 
acts, a binary dialogue act clasifier that was 
trained using only lexical and syntactic features 
achieved the same or beter clasification 
acuracy than the model that was given al 
features (Figure 4). For comparison, the modified 
baseline model used the last thre true dialogue 
acts (with speaker tags); this model achieved 
beter than chance for four dialogue acts and 
achieved nearly as wel as the ful model for 
GRETING (GR). The models that were given al 
posible features for selection outperformed the 
lexical/syntax-only model for seven of the 
thirten dialogue acts (GRETING (GR), REQUEST 
FOR FEDBACK (RF), POSITIVE FEDBACK (P), 
POSITIVE ELABORATED FEDBACK (PE), 
UNCERTAIN ELABORATED FEDBACK (UE), 
NEGATIVE FEDBACK (N), and EXTRA-DOMAIN 
(EX); however, it should be noted that none of 
these diferences in performance is statisticaly 
reliable at the p=0.05 level. 
 
Figure 3. Confusion matrix 
302
 
Figure 4. Kapa for binary DA clasifiers by 
features available for selection 
6 Discussion

We have presented a maximum likelihod 
clasifier that asigns dialogue act labels to user 
uterances from a corpus of human-human 
tutorial dialogue given a set of lexical, syntactic, 
and structural features. Overal, this clasifier 
achieved 62.8% acuracy in ten-fold cros-
validation on the corpus. This performance is on 
par with other automatic dialogue act taging 
models, both sequential and vector-based, in 
task-oriented domains that do not feature 
complex, user-driven paralel tasks. 
In a catalogue ordering domain with an 
integrated task and dialogue model, Bangalore et 
al. (209) report 75% clasification acuracy for 
user uterances using a maximum entropy 
clasifier, a 275% improvement over baseline. 
Poesio & Mikhev (198) report 54% 
clasification acuracy by utilizing 
conversational game structure and speaker 
changes in the Maptask corpus, an improvement 
of 170% over baseline. Recent work on Maptask 
reports a clasification acuracy of 65.7% using 
local uterance (such as lexical/syntactic) 
features alone, with prosodic cues yielding 
further slight improvement (Sridhar et al., 209). 
This clasifier is analogous to our 
lexical/syntactic feature model, which achieved 
60.2% acuracy. 
The results of these models demonstrate that, 
consistent with the findings in other task-oriented 
domains, lexical/syntactic features are highly 
useful for clasifying student dialogue moves in 
this complex task-oriented domain. Models 
trained using those lexical/syntactic features 
performed almost universaly beter (with the 
exception of the binary clasifier for GRETING) 
than models that were given the same left context 
of true dialogue act tags. 
It was hypothesized that leveraging both the 
hiden dialogue state and hierarchical subtask 
features would improve the performance of the 
clasifiers. There is some evidence that the 
subtask structure was helpful for the overal 
clasifier; however, no HM features were kept 
during feature selection for the overal model. Of 
the binary models, aproximately half performed 
beter than the overal model in terms of true 
positive rate; of those, thre did so by including 
HM or task/subtask features among the 
selected atributes to diferentiate diferent tones 
of student fedback. However, this diference in 
performance was not statisticaly reliable. This 
finding sugests that, given lexical and syntactic 
features which are strong predictors of dialogue 
acts, the hiden dialogue state as captured by an 
an HM may not contribute significantly to the 
dialogue act clasification task. 
7 Conclusion
and Future Work 
Dialogue modeling for complex task-oriented 
domains poses significant chalenges. An 
efective dialogue model alows systems to 
detect user dialogue acts so that they can respond 
in a maner that maximizes the chance of 
suces. Experiments with the data-driven 
clasifiers presented in this paper demonstrate 
that lexical/syntactic features can efectively 
clasify student dialogue acts in the task-oriented 
tutoring domain. For POSITIVE, NEGATIVE, and 
UNCERTAIN ELABORATED student fedback acts, 
which play a key role in tutorial dialogue system, 
the adition of hiden dialogue state features (as 
learned by an HM) and task/subtask features 
(anotated manualy) improve clasification 
acuracy, but not statisticaly reliably. 
 The overarching goal of this work is to create 
a data-driven tutorial dialogue system that learns 
its behavior from corpora of efective human 
tutoring. The dialogue act clasification models 
reported here constitute an important step toward 
that goal, by integrating the dialogue stream with 
a paralel user-driven task event stream. The next 
generation of data-driven systems should 
leverage models that capture the rich interplay 
betwen dialogue and task. Future work wil 
focus on data-driven aproaches to task 
recognition and tutorial planing. Aditionaly, 
as dialogue system research adreses 
303
increasingly complex task-oriented domains, it 
becomes increasingly important to investigate 
unsupervised aproaches for dialogue act 
clasification and task recognition. 
 
Acknowledgements.  This work is suported in 
part by the North Carolina State University 
Department of Computer Science and the 
National Science Foundation through a Graduate 
Research Felowship and Grants CNS-0540523, 
REC-0632450 and IS-081291. Any opinions, 
findings, conclusions, or recomendations 
expresed in this report are those of the 
participants, and do not necesarily represent the 
oficial views, opinions, or policy of the National 
Science Foundation. 
References  
Alen, J., Ferguson, G., & Stent, A. (201). An 
architecture for more realistic conversational 
systems. Procedings of the IUI, 1-8. 
Artstein, R., & Poesio, M. (208). Inter-coder 
agrement for computational linguistics. 
Computational Linguistics, 34(4), 55-596. 
Austin, J. L. (1962). How to do things with words. 
Oxford: Oxford University Pres. 
Bangalore, S., Di Fabrizio, G., & Stent, A. (208). 
Learning the structure of task-driven human-human 
dialogs. IEE Transactions on Audio, Spech, and 
Language Procesing, 16(7), 1249-1259. 
Berger, A. L., Pietra, V. J. D., & Pietra, S. A. D. 
(196). A maximum entropy aproach to natural 
language procesing. Comp. Ling., 2(1), 71. 
Boyer, K. E., Philips, R., Ha, E. Y., Walis, M. D., 
Vouk, M. A., & Lester, J. C. (209). odeling 
dialogue structure with adjacency pair analysis and 
hiden markov models. Procedings of NACL-
HLT, Short Papers, 49-52. 
Boyer, K. E., Philips, R., Ha, E. Y., Walis, M. D., 
Vouk, M. A., & Lester, J. C. (2010). Leveraging 
hiden dialogue state to select tutorial moves. 
Procedings of the 5th NACL HLT Workshop on 
Inovative use of NLP for Building Educational 
Aplications, Los Angeles, California. 
Chotimongkol, A. (208). Learning the structure of 
task-oriented conversations from the corpus of in-
domain dialogs. (Unpublished Ph.D. Disertation). 
Carnegie Melon University Schol of Computer 
Science. 
Core, M., & Alen, J. (197). Coding dialogs with the 
DASL anotation scheme. AAI Fal Symposium 
on Comunicative Action in Humans and 
Machines, 28–35. 
De Marnefe, M. C., MacCartney, B., & Maning, C. 
D. (206). Generating typed dependency parses 
from phrase structure parses. Procedings of LREC, 
Genoa, Italy.  
Forbes-Riley, K., & Litman, D. (209). Adapting to 
student uncertainty improves tutoring dialogues. 
Procedings of AIED, 33-40. 
Frampton, M., & Lemon, O. (209). Recent research 
advances in reinforcement learning in spoken 
dialogue systems. The Knowledge Enginering 
Review, 24(4), 375-408. 
Hal, M., Frank, E., Holmes, G., Pfahringer, B., 
Reuteman, P., & Witen, I. (209). The WEKA 
data mining software: An update. SIGKD 
Explorations, 1(1) 
Hardy, H., Bierman, A., Inouye, R. B., McKenzie, 
A., Strzalkowski, T., Ursu, C., Web, N., & Wu, M. 
(206). The Amitiés system: Data-driven techniques 
for automated dialogue. Spech Com., 48(3-4), 
354-373. 
Litman, D., & Forbes-Riley, K. (206). Corelations 
betwen dialogue acts and learning in spoken 
tutoring dialogues. Natural Language Enginering, 
12(2), 161-176. 
Loper, E., & Bird, S. (204). NLTK: The natural 
language tolkit. Procedings of the ACL 
Demonstration Sesion, Barcelona, Spain. 214-217. 
Porter, M. F. (1980). An algorithm for sufix 
striping. Program, 14(3), 130-137. 
Purver, M., Kording, K. P., Grifiths, T. L., & 
Tenenbaum, J. B. (206). Unsupervised topic 
modeling for multi-party spoken discourse. 
Procedings of the ACL, Sydney, Australia. , 4(1) 
17. 
Rabiner, L. R. (1989). A tutorial on hiden markov 
models and selected aplications in spech 
recognition. Procedings of the IEE, 7(2), 257-
286. 
Sridhar, V. K. R., Bangalore, S., & Narayanan, S. 
(209). Combining lexical, syntactic and prosodic 
cues for improved online dialog act taging. 
Computer Spech & Language, 23(4), 407-42. 
Stolcke, A., Ries, K., Cocaro, N., Shriberg, E., Bates, 
R., Jurafsky, D., Taylor, P., Martin, R., Van Es-
Dykema, C., & Meter, M. (200). Dialogue act 
modeling for automatic taging and recognition of 
conversational spech. Comp. Ling., 26(3), 39-373. 
Wright Hastie, H., Poesio, M., & Isard, S. (202). 
Automaticaly predicting dialogue structure using 
prosodic features. Spech Comunication, 36(1-2), 
63-79. 
Young, S., Gasic, M., Keizer, S., Mairese, F., 
Schatzman, J., Thomson, B., & Yu, K. (209). The 
hiden information state model: A practical 
framework for POMDP-based spoken dialogue 
management. Computer Spech and Language, 
24(2), 150-174. 
 
304
 
 
 
 
Time Stamp Dialogue Stream 
Task 
Stream  
2008-04-1 18:23:45 Student: so do i have to manipulate the 
aray this time? [Q]   
2008-04-1 18:23:53 Tutor: this time, we ned to do two things 
[S]   
2008-04-1 18:24:02 Tutor: first, we ned to create a new 
aray to hold the changed values 
[S]   
2008-04-1 18:24:28   i 
2008-04-1 18:24:28   n 
2008-04-1 18:24:28   t 
2008-04-1 18:24:28   \sp 
1-a-i 
BUGGY 
2008-04-1 18:24:35   \del  
2008-04-1 18:24:36   \sp  
2008-04-1 18:24:36   d 
2008-04-1 18:24:36   o 
2008-04-1 18:24:36   u 
2008-04-1 18:24:36   b 
2008-04-1 18:24:37   l 
2008-04-1 18:24:37   e 
2008-04-1 18:24:37   \sp 
2008-04-1 18:24:39   [] 
1-a-i 
CORECT 
2008-04-1 18:24:40   \sp  
2008-04-1 18:24:42   n 
2008-04-1 18:24:42   e 
2008-04-1 18:24:42   w 
2008-04-1 18:24:43   \sp 
2008-04-1 18:24:4   \del 
2008-04-1 18:24:45   T 
2008-04-1 18:24:46   \del 
2008-04-1 18:24:54   T 
2008-04-1 18:24:54   i 
2008-04-1 18:24:54   m 
2008-04-1 18:24:54   e 
2008-04-1 18:24:54   s 
2008-04-1 18:24:5   3 
2008-04-1 18:24:57   ; 
1-a-ii 
CORECT 
2008-04-1 18:25:1 Student: god? [RF]   
2008-04-1 18:25:14 Tutor: god so far, yes [PF]   
2008-04-1 18:25:29 Student: so now i have to change parts of 
the times aray right? [Q]   
2008-04-1 18:25:34 Tutor: not quite [LF]   
2008-04-1 18:25:57 Tutor: So, when you create a new object, 
like a String for example, you'd say 
something like  String s = new 
String() [S]   
2008-04-1 18:25:59 Tutor: right? [AQ]   
2008-04-1 18:26:06 Student: right [P]   
2008-04-1 18:26:14 Tutor: arays are similar [S]   
 
 
 
 
 
 
 
 
 
Appendix
 
Excerpt 1. Parallel synchronous dialogue and task event streams 
with annotations. (Note tutor dialogue acts: AQ=ASESING 
QUESTION, LF=LUKEWARM FEDBACK, PF=POSITIVE FEDBACK) 
305

