Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 11–17, Vancouver, October 2005.
c©2005 Association for Computational Linguistics Parsing Linear Context-Free Rewriting Systems Håkan Burden Dept.
of Linguistics Göteborg University cl1hburd@cling.gu.se Peter Ljunglöf Dept.
of Computing Science Göteborg University peb@cs.chalmers.se Abstract We describe four different parsing algorithms for Linear Context-Free Rewriting Systems (Vijay-Shanker et al., 1987).
The algorithms are described as deduction systems, and possible optimizations are discussed.
The only parsing algorithms presented for linear contextfree rewriting systems (LCFRS; Vijay-Shanker et al., 1987) and the equivalent formalism multiple context-free grammar (MCFG; Seki et al., 1991) are extensions of the CKY algorithm (Younger, 1967), more designed for their theoretical interest, and not for practical purposes.
The reason for this could be that there are not many implementations of these grammar formalisms.
However, since a very important subclass of the Grammatical Framework (Ranta, 2004) is equivalent to LCFRS/MCFG (Ljunglöf, 2004a; Ljunglöf, 2004b), there is a need for practical parsing algorithms.
In this paper we describe four different parsing algorithms for Linear Context-Free Rewriting Systems.
The algorithms are described as deduction systems, and possible optimizations are discussed.
1 Introductory
definitions A record is a structure Γ = {r1 = a1;...rn = an}, where all ri are distinct.
That this can be seen as a set of feature-value pairs.
This means that we can define a simple version of record unification Γ1 unionsqΓ2 as the union Γ1∪Γ2, provided that there is no r such that Γ1.r negationslash= Γ2.r.
We sometimes denote a sequence X1,...,Xn by the more compact vectorX.
To update the ith record in a list of records, we write vectorΓ[i := Γ].
To substitute a variable Bk for a record Γk in any data structure Γ, we write Γ[Bk/Γk].
1.1 Decorated
Context-Free Grammars The context-free approximation described in section 4 uses a form of CFG with decorated rules of the form f : A → α, where f is the name of the rule, and α is a sequence of terminals and categories subscripted with information needed for post-processing of the context-free parse result.
In all other respects a decorated CFG can be seen as a straight-forward CFG.
1.2 Linear
Context-Free Rewriting Systems A linear context-free rewriting system (LCFRS; VijayShanker et al., 1987) is a linear, non-erasing multiple context-free grammar (MCFG; Seki et al., 1991).
An MCFG rule is written1 A → f[B1...
Bδ] := {r1 = α1; ...
rn = αn } where A and Bi are categories, f is the name of the rule, ri are record labels and αi are sequences of terminals and argument projections of the form Bi.r.
The language L(A) of a category A is a set of string records, and is defined recursively as L(A) = { Φ[B1/Γ1,...,Bδ/Γδ] | A → f[B1 ...Bδ] := Φ, Γ1 ∈ L(B1), ..., Γδ ∈ L(Bδ) } It is the possibility of discontinuous constituents that makes LCFRS/MCFG more expressive than context-free grammars.
If the grammar only consists of single-label records, it generates a context-free language.
Example A small example grammar is shown in figure 1, and generates the language L(S) = { sshm | s ∈ (a ∪ b)∗ } where shm is the homomorphic mapping such that each a in s is translated to c, and each b is translated to d.
Examples of generated strings are ac, abcd and bbaddc.
However, neither abc nor abcdabcd will be 1We borrow the idea of equating argument categories and variables from Nakanishi et al.(1997), but instead of tuples we use the equivalent notion of records for the linearizations.
11 Figure
1: An example grammar describing the language {sshm |s ∈ (a ∪ b)∗ } S → f[A] := {s = A.p A.q} A → g[A1 A2] := {p = A1.p A2.p; q = A1.q A2.q} A → ac[] := {p = a; q = c} A → bd[] := {p = b; q = d} generated.
The language is not context-free since it contains a combination of multiple and crossed agreement with duplication.
If there is at most one occurrence of each possible projection Ai.r in a linearization record, the MCFG rule is linear.
If all rules are linear the grammar is linear.
A rule is erasing if there are argument projections that have no realization in the linearization.
A grammar is erasing if it contains an erasing rule.
It is possible to transform an erasing grammar to non-erasing form (Seki et al., 1991).
Example The example grammar is both linear and nonerasing.
However, given that grammar, the rule E → e[A] := {r1 = A.p; r2 = A.p} is both non-linear (since A.p occurs more than once) and erasing (since it does not mention A.q). 1.3 Ranges Given an input string w, a range ρ is a pair of indices, (i,j) where 0 ≤ i ≤ j ≤ |w| (Boullier, 2000).
The entire string w = w1 ...wn spans the range (0,n).
The word wi spans the range (i − 1,i) and the substring wi+1, ...,wj spans the range (i,j).
A range with identical indices, (i,i), is called an empty range and spans the empty string.
A record containing label-range pairs, Γ = {r1 = ρ1,...,rn = ρn } is called a range record.
Given a range ρ = (i,j), the ceiling of ρ returns an empty range for the right index, ceilingleftρceilingright = (j,j); and the floor of ρ does the same for the left index floorleftρfloorright = (i,i).
Concatenation of two ranges is non-deterministic, (i,j) · (jprime,k) = {(i,k)|j = jprime } . 1.3.1 Range restriction In order to retrieve the ranges of any substring s in a sentence w = w1 ...wn we define range restriction of s with respect to w as 〈s〉w = {(i,j) | s = wi+1 ...
wj }, i.e. the set of all occurrences of s in w.
If w is understood from the context we simply write 〈s〉.
Range restriction of a linearization record Φ is written 〈Φ〉, which is a set of records, where every terminal token s is replaced by a range from 〈s〉.
The range restriction of two terminals next to each other fails if range concatenation fails for the resulting ranges.
Any unbound variables in Φ are unaffected by range restriction.
Example Given the string w = abba, range restricting the terminal a yields 〈a〉w = {(0,1), (3,4)} Furthermore, 〈aA.rabB.q〉w = {(0,1)A.r(0,2)B.q, (3,4)A.r(0,2)B.q} The other possible solutions fail since they cannot be range concatenated.
2 Parsing
as deduction The idea with parsing as deduction (Shieber et al., 1995) is to deduce parse items by inference rules.
A parse item is a representation of a piece of information that the parsing algorithm has acquired.
An inference rule is written γ1 ...γn C γ where γ is the consequence of the antecedents γ1 ...γn, given that the side conditions in C hold.
2.1 Parsing
decorated CFG Decorated CFG can be parsed in a similar way as standard CFG.
For our purposes it suffices to say that the algorithm returns items of the form, [f : A/ρ → B1/ρ1 ...
Bn/ρn • ] saying that A spans the range ρ, and each daughter Bi spans ρi.
The standard inference rule combine might look like this for decorated CFG: Combine [f : A/ρ → α • Bx β] [g : B/ρprime → ...
• ] ρprimeprime ∈ ρ · ρprime [f : A/ρ → α Bx/ρprimeprime • β] Note that the subscript x in Bx is the decoration that will only be used in post-processing.
12 3 The Naïve algorithm Seki et al.(1991) give an algorithm for MCFG, which can be seen as an extension of the CKY algorithm (Younger, 1967).
The problem with that algorithm is that it has to find items for all daughters at the same time.
We modify this basic algorithm to be able to find one daughter at the time.
There are two kinds of items.
A passive item [A; Γ] has the meaning that the category A has been found spanning the range record Γ.
An active item for the rule A → f[vectorB vectorBprime] := Ψ has the form [A → f[vectorB • vectorBprime]; Φ; vectorΓ] in which the categories to the left of the dot, vectorB, have been found with the linearizations in the list of range records vectorΓ.
Φ is the result of substituting the projections in Ψ with ranges for the categories found in vectorB.
3.1 Inference
rules There are three inference rules, Predict, Combine and Convert.
Predict A → f[vectorB] := Ψ Φ ∈ 〈Ψ〉 [A → f[ • vectorB]; Φ; ] Prediction gives an item for every rule in the grammar, where the range restriction Φ is what has been found from the beginning.
The list of daughters is empty since none of the daughters in vectorB have been found yet.
Combine [A → f[vectorB • Bk vectorBprime]; Φ; vectorΓ] [Bk; Γk] Φprime ∈ Φ[Bk/Γk] [A → f[vectorB Bk • vectorBprime]; Φprime; vectorΓ, Γk] An active item looking for Bk and a passive item that has found Bk can be combined into a new active item.
In the new item we substitute Bk for Γk in the linearization record.
We also add Γk to the new item’s list of daughters.
Convert [A → f[vectorB • ]; Φ; vectorΓ] Γ ≡ Φ [A; Γ] Every fully instantiated active item is converted into a passive item.
Since the linearization record Φ is fully instantiated, it is equivalent to the range record Γ.
Figure 2: The example grammar converted to a decorated CFG f : (S.s) → (A.p) (A.q) g : (A.p) → (A.p)1 (A.p)2 g : (A.q) → (A.q)1 (A.q)2 ac : (A.p) → a ac : (A.q) → b bd : (A.p) → c bd : (A.q) → d The subscripted numbers are for distinguishing the two categories from each other, since they are equivalent.
Here A.q is a context-free category of its own, not a record projection.
4 The
Approximative algorithm Parsing is performed in two steps in the approximative algorithm.
First we parse the sentence using a contextfree approximation.
Then the resulting context-free chart is recovered to a LCFRS chart.
The LCFRS is converted by creating a decorated context-free rule for every row in a linearization record.
Thus, the rule A → f[vectorB] := {r1 = α1; ...
rn = αn } will give n context-free rules f : A.ri → αi.
The example grammar from figure 1 is converted to a decorated CFG in figure 2.
Parsing is now initiated by a context-free parsing algorithm returning decorated items as in section 2.1.
Since the categories of the decorated grammar are projections of LCFRS categories, the final items will be of the form [f : (A.r)/ρ → ...(B.rprime)x/ρprime ...
• ] Since the decorated CFG is over-generating, the returned parse chart is unsound.
We therefore need to retrieve the items from the decorated CFG parse chart and check them against the LCFRS to get the discontinuous constituents and mark them for validity.
The initial parse items are of the form, [A → f[vectorB]; r = ρ; vectorΓ] where vectorΓ is extracted from a corresponding decorated item [f : (A.r)/ρ → β], by partitioning the daughters in β such that Γi = {r = ρ|(B.r)i/ρ ∈ β }.
In other words, Γi will consist of all r = ρ such that B.r is subscripted by i in the decorated item.
Example Given β = (A.p)2/ρprime (B.q)1/ρprimeprime (A.q)2/ρprimeprimeprime, we get the two range records Γ1 = {q = ρprimeprime} and Γ2 = {p = ρprime; q = ρprimeprimeprime}.
13 Apart
from the initial items, we use three kinds of parse items.
From the initial parse items we first build LCFRS items, of the form [A → f[vectorB]; Γ • ri ...rn; vectorΓ] where ri ...rn is a list of labels, vectorΓ is a list of |vectorB| range records, and Γ is a range record for the labels r1 ...ri−1.
In order to recover the chart we use mark items [A → f[vectorB • vectorBprime]; Γ; vectorΓ • vectorΓprime] The idea is that vectorΓ has been verified as range records spanning the daughters vectorB.
When all daughters have been verified, a mark item is converted to a passive item [A; Γ].
4.1 Inference
rules There are five inference rules, Pre-Predict, Pre-Combine, Mark-Predict, Mark-Combine and Convert.
Pre-Predict A → f[vectorB] := {r1 = α1;...rn = αn} vectorΓδ = {}, ..., {} [A → f[vectorB]; • r1 ...rn; vectorΓδ] Every rule A → f[vectorB] is predicted as an LCFRS item.
Since the context-free items contain information about α1 ...αn, we only need to use the labels r1,..., rn.
vectorΓδ is a list of |vectorB| empty range records.
Pre-Combine [R; Γ • rri ...rn; vectorΓ] [R; r = ρ; vectorΓprime] vectorΓprimeprime ∈ vectorΓ unionsqvectorΓprime [R; {Γ;r = ρ}• ri ...rn; vectorΓprimeprime] If there is an initial parse item for the rule R with label r, we can combine it with an LCFRS item looking for r, provided the daughters’ range records can be unified.
Mark-Predict [A → [vectorB]; Γ • ; vectorΓ] [A → [ • vectorB]; Γ; •vectorΓ] When all record labels have been found, we can start to check if the items have been derived in a valid way by marking the daughters’ range records for correctness.
Mark-Combine [A → f[vectorB • Bi vectorBprime]; Γ; vectorΓ • Γi vectorΓprime] [Bi; Γi] [A → f[vectorB Bi • vectorBprime]; Γ; vectorΓΓi •vectorΓprime] Record Γi is correct if there is a correct passive item for category Bi that has found Γi.
Convert [A → f[vectorB • ]; Γ; vectorΓ • ] [A; Γ] An item that has marked all daughters as correct is converted to a passive item.
5 The
Active algorithm The active algorithm parses without using any contextfree approximation.
Compared to the Naïve algorithm the dot is used to traverse the linearization record of a rule instead of the categories in the right-hand side.
For this algorithm we use a special kind of range, ρepsilon1, which denotes simultaneously all empty ranges (i,i).
Range restricting the empty string gives 〈epsilon1〉 = ρepsilon1.
Concatenation is defined as ρ·ρepsilon1 = ρepsilon1·ρ = ρ.
Both the ceiling and the floor of ρepsilon1 are identities, ceilingleftρepsilon1ceilingright = floorleftρepsilon1floorright = ρepsilon1.
There are two kinds of items.
Passive items [A; Γ] say that we have found category A inside the range record Γ.
An active item for the rule A → f[vectorB] := {Φ; r = αβ; Ψ} is of the form [A → f[vectorB]; Γ, r = ρ • β, Ψ; vectorΓ] where Γ is a range record corresponding to the linearization rows in Φ and α has been recognized spanning ρ.
We are still looking for the rest of the row, β, and the remaining linearization rows Ψ.
vectorΓ is a list of range records containing information about the daughters vectorB.
5.1 Inference
rules There are five inference rules, Predict, Complete, Scan, Combine and Convert.
Predict A → f[vectorB] := {r = α;Φ} vectorΓδ = {}, ..., {} [A → f[vectorB]; {}, r = ρepsilon1 • α, Φ; vectorΓδ] For every rule in the grammar, predict a corresponding item that has found the empty range.
vectorΓδ is a list of |vectorB| empty range records since nothing has been found yet.
Complete [R; Γ, r = ρ • epsilon1, {rprime = α;Φ}; vectorΓ] [R; {Γ;r = ρ}, rprime = ρepsilon1 • α,Φ; vectorΓ] When an item has found an entire linearization row we continue with the next row by starting it off with the empty range.
14 Scan
[R; Γ, r = ρ • sα, Φ; vectorΓ] ρprime ∈ ρ ·〈s〉 [R; Γ, r = ρprime • α, Φ; vectorΓ] When the next symbol to read is a terminal, its range restriction is concatenated with the range for what the row has found so far.
Combine [A → f[vectorB]; Γ, r = ρ • Bi.rprime α, Φ; vectorΓ] [Bi; Γprime] ρprime ∈ ρ · Γprime.rprime Γi ⊆ Γprime [A → f[vectorB]; Γ, r = ρprime • α, Φ; vectorΓ[i := Γprime]] If the next thing to find is a projection on Bi, and there is a passive item where Bi is the category, where Γprime is consistent with Γi, we can move the dot past the projection.
Γi is updated with Γprime, since it might contain more information about the ith daughter.
Convert [A → f[vectorB]; Γ, r = ρ • epsilon1, {}; vectorΓ] [A; {Γ;r = ρ}] An active item that has fully recognized all its linearization rows is converted to a passive item.
6 The
Incremental algorithm An incremental algorithm reads one token at the time and calculates all possible consequences of the token before the next token is read2.
The Active algorithm as described above is not incremental, since we do not know in which order the linearization rows of a rule are recognized.
To be able to parse incrementally, we have to treat the linearization records as sets of feature-value pairs, instead of a sequence.
The items for a rule A → f[vectorB] := Φ have the same form as in the Active algorithm: [A → f[vectorB]; Γ, r = ρ • β, Ψ; vectorΓ] However, the order between the linearization rows does not have to be the same as in Φ.
Note that in this algorithm we do not use passive items.
Also note that since we always know where in the input we are, we cannot make use of a distinguished epsilon1-range.
Another consequence of knowing the current input position is that there are fewer possible matches for the Combine rule.
2See e.g. the ACL 2004 workshop “Incremental Parsing: Bringing Engineering and Cognition Together”.
6.1 Inference
rules There are four inference rules, Predict, Complete, Scan and Combine.
Predict A → f[vectorB] := {Φ;r = α;Ψ} 0 ≤ k ≤ |w| [A → f[vectorB]; {}, r = (k,k) • α, {Φ;Ψ}; vectorΓδ] An item is predicted for every linearization row r and every input position k.
vectorΓδ is a list of |vectorB| empty range records.
Complete [R; Γ, r = ρ • epsilon1, {Φ;rprime = α;Ψ}; vectorΓ] ceilingleftρceilingright ≤ k ≤ |w| [R; {Γ;r = ρ}, rprime = (k,k) • α, {Φ;Ψ}; vectorΓ] Whenever a linearization row r is fully traversed, we predict an item for every remaining linearization row rprime and every remaining input position k.
Scan [R; Γ, r = ρ • sα, Φ; vectorΓ] ρprime ∈ ρ ·〈s〉 [R; Γ, r = ρprime • α, Φ; vectorΓ] If the next symbol in the linearization row is a terminal, its range restriction is concatenated to the range for the partially recognized row.
Combine [R; Γ, r = ρ • Bi.rprime α, Φ; vectorΓ] [Bi → ...
Γprime, rprime = ρprime • epsilon1, ...
...] ρprimeprime ∈ ρ · ρprime Γi ⊆ {Γprime;rprime = ρprime} [R; Γ, r = ρprimeprime • α, Φ; vectorΓ[i := {Γprime;rprime = ρprime}]] If the next item is a record projection Bi.rprime, and there is an item for Bi which has found rprime, then move the dot forward.
The information in Γi must be consistent with the information found for the Bi item, {Γprime;rprime = ρprime}.
7 Discussion
We have presented four different parsing algorithms for LCFRS/MCFG.
The algorithms are described as deduction systems, and in this final section we discuss some possible optimizations, and complexity issues.
15 7.1 Different prediction strategies The Predict rule in the above described algorithms is very crude, predicting an item for each rule in the grammar (for the Incremental algorithm even for each input position).
A similar context-free prediction rule is called bottom-up Earley by Sikkel and Nijholt (1997).
Such crude predictions are only intended for educational purposes, since they lead to lots of uninteresting items, and waste of computing power.
For practical purposes there are two standard context-free prediction strategies, topdown and bottom-up (see e.g.
Wirén (1992)) and they can be adapted to the algorithms presented in this paper.
The main idea is that an item for the rule A → f[vectorB] with the linearization row r = α is only predicted if.
. . (Top-down prediction) . . . there is another item looking for A.r.
(Bottom-up prediction) . . . there is an passive item that has found the first symbol in α.
For a more detailed description of these prediction strategies, see Ljunglöf (2004a).
7.2 Efficiency
and complexity of the algorithms The theoretical time complexity for these algorithms is not better than what has been presented earlier.3 The complexity arguments are similar and the reader is referred to Seki et al.(1991). However, theoretical time complexity does not say much about practical performance, as is already clear from context-free parsing, where the theoretical time complexity has remained the same ever since the first publications (Kasami, 1965; Younger, 1967).
There are two main ways of improving the efficiency of existing algorithms, which can be called refinement and filtering (Sikkel and Nijholt, 1997).
First, one wants to be able to locate existing parse items efficiently, e.g. by indexing some properties in a hash table.
This is often done by refining the parse items or inference rules, increasing the number of items or deduction steps.
Second, it is desirable to reduce the number of parse items, which can be done by filtering out redundant parts of an algorithm.
The algorithms presented in this paper can all be seen as refinements and filterings of the basic algorithm of Seki et al.(1991): The naïve algorithm is a refinement of the basic algorithm, since single items and deduction steps are decomposed into several different items and smaller deduction steps.
3Nakanishi et al.(1997) reduce the parsing problem to boolean matrix multiplication, but this can be considered a purely theoretical result.
The approximative algorithm is both a refinement and a filtering of the naïve algorithm; a refinement since the inference rules Pre-Predict and Pre-Combine are added, and a filtering since there will hopefully be less items for Mark-Predict and Mark-Combine to take care of.
The active algorithm is a refinement of the naïve algorithm, since the Combine rule is divided into the rules Complete, Scan and Combine.
The incremental algorithm is finally a refinement of the active algorithm, since Predict and Complete can select from any possible remaining linearization row, and not just the following.
Furthermore, the different prediction strategies (topdown and bottom-up), become filterings of the algorithms, since they reduce the number of parse items.
7.3 Implementing
and testing the algorithms The algorithms presented in this paper have been implemented in the programming language Haskell, for inclusion in the Grammatical Framework system (Ranta, 2004).
These implementations are described by Burden (2005).
We have also started to implement a selection of the algorithms in the programming language Prolog.
Preliminary results suggest that the Active algorithm with bottom-up prediction is a good candidate for parsing grammars written in the Grammatical Framework.
For a normal sentence in the English resource grammar the speedup is about 20 times when compared to context-free parsing and filtering of the parse trees.
In the future we plan to test the different algorithms more extensively.
Acknowledgments The authors are supported by the EU project TALK (Talk and Look, Tools for Ambient Linguistic Knowledge), IST-507802.
References Pierre Boullier.
2000. Range concatenation grammars.
In 6th International Workshop on Parsing Technologies, pages 53–64, Trento, Italy.
Håkan Burden.
2005. Implementations of parsing algorithms for linear multiple context-free grammars.
Master’s thesis, Göteborg University, Gothenburg, Sweden.
Tadao Kasami.
1965. An efficient recognition and syntax algorithm for context-free languages.
Technical Report AFCLR-65-758, Air Force Cambridge Research Laboratory, Bedford, MA.
Peter Ljunglöf.
2004a. Expressivity and Complexity of the Grammatical Framework.
Ph.D. thesis, Göteborg University and Chalmers University of Technology, Gothenburg, Sweden.
Peter Ljunglöf.
2004b. Grammatical Framework and multiple context-free grammars.
In 9th Conference on Formal Grammar, Nancy, France.
Ryuichi Nakanishi, Keita Takada, and Hiroyuki Seki.
1997. An efficient recognition algorithm for multiple context-free languages.
In MOL5: 5th Meeting on the Mathematics of Language, pages 119–123, Saarbrücken, Germany.
Aarne Ranta.
2004. Grammatical Framework, a typetheoretical grammar formalism.
Journal of Functional Programming, 14(2):145–189.
Hiroyuki Seki, Takashi Matsumara, Mamoru Fujii, and Tadao Kasami.
1991. On multiple context-free grammars.
Theoretical Computer Science, 88:191–229.
Stuart Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive parsing.
Journal of Logic Programming, 24(1–2):3– 36.
Klaas Sikkel and Anton Nijholt.
1997. Parsing of context-free languages.
In G.
Rozenberg and A.
Salomaa, editors, The Handbook of Formal Languages, volume II, pages 61–100.
Springer-Verlag, Berlin.
K. Vijay-Shanker, David Weir, and Aravind Joshi.
1987. Characterizing structural descriptions produced by various grammatical formalisms.
In 25th Meeting of the Association for Computational Linguistics.
Mats Wirén.
1992. Studies in Incremental NaturalLanguage Analysis.
Ph.D. thesis, Linköping University, Linköping, Sweden.
Daniel H Younger.
1967. Recognition of contextfree languages in time n3.
Information and Control, 10(2):189–208 .

