(Knight and Graehl, 1997; Kumar and Byrne, 2003)).
N03-1019 P97-1017
The language model is trained using the CMU-Cambridge toolkit and the translation model using the GIZA++ toolkit (Och and Ney, 2000).
P00-1056
For extrinsic evaluation of machine translation, we use the BLEU metric (Papineni et al., 2002).
P02-1040
The second is a slightly modified version of the spelling correction model of Brill and Moore (2000).3 This model allows many-to-many edit operations, which makes P(liter|litre) ≈ P(lmapsto→l)P(imapsto→i)P(tremapsto→ter) possible.
P00-1037
We used the ReWrite decoder (Germann, 2003) for translation.
N03-1010
This is true both in resource acquisition, such as automated bilingual lexicon generation (Kolak et al., 2003), and for end-user applications such as rapid machine translation (MT) in the battlefield for document filtering (Voss and Ess-Dykema, 2000).
N03-1018 W00-0501
Voss and Ess-Dykema (2000) evaluated the effects of OCR errors on MT in the context of the FALCon project, which combines off-the-shelf OCR and MT components to provide crude translations for filtering.
W00-0501
2 Post-Processing System We use the noisy channel framework to formulate the correction problem, revising our previous model (Kolak et al., 2003).
N03-1018
Clearly the BLEU scores are quite low; we are planning to perform experiments on Arabic using a more advanced translation system, such as Hiero (Chiang, 2005).
P05-1033
