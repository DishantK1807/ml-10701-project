<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>N Bel</author>
<author>C H A Koster</author>
<author>M Villegas</author>
</authors>
<title>Crosslingual text categorization</title>
<date>2003</date>
<booktitle>In ECDL’03</booktitle>
<pages>126--139</pages>
<contexts>
<context>l text. Nonetheless there are notable exceptions in the field of cross-lingual information retrieval (CLIR) and text categorisation (CLTC) in which bilingual sources are employed (Grefenstette, 1998; Bel et al., 2003). In this paper, we present an application that differs from that of CLIR and CLTC, since we want to classify bilingual pairs of documents that are translations of each other. We believe that by doin</context>
</contexts>
<marker>Bel, Koster, Villegas, 2003</marker>
<rawString>N. Bel, C.H.A. Koster, and M. Villegas. 2003. Crosslingual text categorization. In ECDL’03, pages 126–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<marker>Brown, 1993</marker>
<rawString>P. F. Brown et al. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Civera</author>
<author>A Juan</author>
</authors>
<title>Bilingual Machine-Aided Indexing</title>
<date>2006</date>
<booktitle>In Proc. of LREC’06</booktitle>
<pages>1302--1305</pages>
<contexts>
<context> translation model in a more natural way than CLIR and CLTC do using external translation resources. Here we introduce an evolution of the relatively simple bilingual multinomial models presented in (Civera and Juan, 2006a; Civera and Juan, 2006b) in order to exploit the structural information in word correlation in bilingual texts. To this purpose a novel model inspired in the combination of a unigram (multinomial) m</context>
<context> components per class (T = 1,2,5,10,20,50,100). The results are shown in Figure 1, together with those of best monolingual (English-based) and the best unigram-based bilingual global classifier from (Civera and Juan, 2006b). Each plotted point is an average over values from 30 random training-test splits, as defined in (Civera and Juan, 2006b); 50%-50% (training-test) in Traveller and 80%-20% in BAF. From the results </context>
</contexts>
<marker>Civera, Juan, 2006</marker>
<rawString>J. Civera and A. Juan. 2006a. Bilingual Machine-Aided Indexing. In Proc. of LREC’06, pages 1302–1305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Civera</author>
<author>A Juan</author>
</authors>
<title>Multinomial Mixture Modelling for Bilingual Text Classification</title>
<date>2006</date>
<booktitle>In Proc. of PRIS’06</booktitle>
<pages>93--103</pages>
<contexts>
<context> translation model in a more natural way than CLIR and CLTC do using external translation resources. Here we introduce an evolution of the relatively simple bilingual multinomial models presented in (Civera and Juan, 2006a; Civera and Juan, 2006b) in order to exploit the structural information in word correlation in bilingual texts. To this purpose a novel model inspired in the combination of a unigram (multinomial) m</context>
<context> components per class (T = 1,2,5,10,20,50,100). The results are shown in Figure 1, together with those of best monolingual (English-based) and the best unigram-based bilingual global classifier from (Civera and Juan, 2006b). Each plotted point is an average over values from 30 random training-test splits, as defined in (Civera and Juan, 2006b); 50%-50% (training-test) in Traveller and 80%-20% in BAF. From the results </context>
</contexts>
<marker>Civera, Juan, 2006</marker>
<rawString>J. Civera and A. Juan. 2006b. Multinomial Mixture Modelling for Bilingual Text Classification. In Proc. of PRIS’06, pages 93–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>Y Singer</author>
</authors>
<title>On the algorithmic implementation of multiclass kernel-based vector machines</title>
<date>2002</date>
<journal>Jour. Mach. Learn. Research</journal>
<pages>2--265</pages>
<contexts>
<context> including support vector machines (SVM) and boosting techniques. On the one hand, SVM were originally thought as binary classifiers, although there have been a generalisation of the 2-class problem (Crammer and Singer, 2002). In practise binary classifiers based on the one-against-one approach, among others, seem to be the most adequate (Hsu and Lin, 2002). This simple yet effective approach consists in defining as many</context>
</contexts>
<marker>Crammer, Singer, 2002</marker>
<rawString>K. Crammer and Y. Singer. 2002. On the algorithmic implementation of multiclass kernel-based vector machines. Jour. Mach. Learn. Research, 2:265–292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Cross-Language Information Retrieval</title>
<date>1998</date>
<publisher>Kluwer Academic Publishers, USA</publisher>
<contexts>
<context>sation of monolingual text. Nonetheless there are notable exceptions in the field of cross-lingual information retrieval (CLIR) and text categorisation (CLTC) in which bilingual sources are employed (Grefenstette, 1998; Bel et al., 2003). In this paper, we present an application that differs from that of CLIR and CLTC, since we want to classify bilingual pairs of documents that are translations of each other. We be</context>
</contexts>
<marker>Grefenstette, 1998</marker>
<rawString>G. Grefenstette. 1998. Cross-Language Information Retrieval. Kluwer Academic Publishers, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hodge</author>
</authors>
<title>CENDI agency indexing system descriptors: A Baseline Report</title>
<date>1998</date>
<tech>Technical report</tech>
<publisher>IIA, Inc</publisher>
<contexts>
<context>on of documents has entailed a time-consuming and arduous task that has been significantly alleviated with the deployment of automatic and machine-aided text categorisation systems (Sebastiani, 2002; Hodge, 1998). However, nowadays the proliferation of multilingual documentation has become a common phenomenon in many international organisations, while most of the current systems has focused on the categorisa</context>
</contexts>
<marker>Hodge, 1998</marker>
<rawString>G. Hodge. 1998. CENDI agency indexing system descriptors: A Baseline Report. Technical report, IIA, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ch-W Hsu</author>
<author>Ch-J Lin</author>
</authors>
<title>A comparison of methods for multiclass support vector machines</title>
<date>2002</date>
<journal>IEEE Transactions on Neural Networks</journal>
<volume>13</volume>
<contexts>
<context>h there have been a generalisation of the 2-class problem (Crammer and Singer, 2002). In practise binary classifiers based on the one-against-one approach, among others, seem to be the most adequate (Hsu and Lin, 2002). This simple yet effective approach consists in defining as many binary classifiers as possible class pairs, then each binary classifier votes for a class and finally, we classify according to the m</context>
</contexts>
<marker>Hsu, Lin, 2002</marker>
<rawString>Ch-W. Hsu and Ch-J. Lin. 2002. A comparison of methods for multiclass support vector machines. IEEE Transactions on Neural Networks, 13(2):415–425.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale support vector machine learning practical. Advances in kernel methods: support vector learning</title>
<date>1999</date>
<pages>169--184</pages>
<contexts>
<context>Traveller task and the BAF corpus. 1gm 1g1gm 1gM1m SV Mlight BoosTexter Traveller 1.5 1.4 1.3 1.5 1.2 BAF 4.1 3.0 2.5 9.0 5.8 per, all the SVM experiments were carried out with the SV Mlight toolkit (Joachims, 1999) adopting the approach to the multi-class problem commented above . On the other hand, the idea behind boosting methods is to find a highly accurate classification rule by combining many weak hypothe</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Making large-scale support vector machine learning practical. Advances in kernel methods: support vector learning, pages 169–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>An efficient method for determining bilingual word classes</title>
<date>1999</date>
<booktitle>In Proc. of EACL’99</booktitle>
<pages>71--76</pages>
<contexts>
<context>1 model. As a future work we plan to explore the combination of smooth n-gram models with IBM model 1 in the powerful framework of mixture modelling. Moreover, the incorporation of bilingual classes (Och, 1999) is an interesting approach to control the model complexity in the presence of data scarcity problems, specifically the number of parameters in modelling topic-dependent statistical dictionaries by a</context>
</contexts>
<marker>Och, 1999</marker>
<rawString>F. J. Och. 1999. An efficient method for determining bilingual word classes. In Proc. of EACL’99, pages 71–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Schapire</author>
<author>Y Singer</author>
</authors>
<title>Boostexter: A boosting-based systemfor text categorization</title>
<date>2000</date>
<booktitle>Machine Learning</booktitle>
<pages>39--2</pages>
<contexts>
<context>ighly accurate classification rule by combining many weak hypotheses, each of which may be only moderately accurate. The implementation of the boosting algorithm employed in this paper is BoosTexter (Schapire and Singer, 2000). As we can observe in Table 2, the unigram-M1 mixture model (1gM1m) supersedes the other two unigram mixture models, monolingual (1gm) and bilingual global (1g1gm), being statistically significant b</context>
</contexts>
<marker>Schapire, Singer, 2000</marker>
<rawString>R. E. Schapire and Y. Singer. 2000. Boostexter: A boosting-based systemfor text categorization. Machine Learning, 39(2-3):135–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sebastiani</author>
</authors>
<title>Machine learning in automated text categorisation</title>
<date>2002</date>
<journal>ACM Computing Surveys</journal>
<volume>34</volume>
<contexts>
<context>anual categorisation of documents has entailed a time-consuming and arduous task that has been significantly alleviated with the deployment of automatic and machine-aided text categorisation systems (Sebastiani, 2002; Hodge, 1998). However, nowadays the proliferation of multilingual documentation has become a common phenomenon in many international organisations, while most of the current systems has focused on t</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>F. Sebastiani. 2002. Machine learning in automated text categorisation. ACM Computing Surveys, 34(1):1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Simard</author>
</authors>
<title>The BAF: A Corpus of EnglishFrench Bitext</title>
<date>1998</date>
<booktitle>In Proc. of LREC’98</booktitle>
<pages>489--496</pages>
<contexts>
<context>, low classification error rates will indicate that our mixture model has been able to capture the multimodal nature of the data. Some statistics of this dataset are shown in Table 1. The BAF corpus (Simard, 1998) is a compilation of bilingual ”institutional” French-English texts ranging from debates of the Canadian parliament (Hansard), court transcripts and UN reports to scientific, technical and literary d</context>
</contexts>
<marker>Simard, 1998</marker>
<rawString>Michel Simard. 1998. The BAF: A Corpus of EnglishFrench Bitext. In Proc. of LREC’98, pages 489–496.</rawString>
</citation>
</citationList>
</algorithm>

