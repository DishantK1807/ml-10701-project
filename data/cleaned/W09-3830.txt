Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 202–205,
Paris, October 2009. c 2009 Association for Computational Linguistics
The chunk as the period of the functions length and frequency       
of words on the syntagmatic axis 
 
 
Jacques Vergne 
GREYC Université de Caen France 
Jacques.Vergne@info.unicaen.fr 
 
Abstract 
Chunking is segmenting a text into chunks, 
sub-sentential segments, that Abney ap-
proximately defined as stres groups. Chunk-
ing usualy uses monolingual resources, most 
often exhaustive, sometimes partial : function 
words and punctuations, which often mark 
beginings and ends of chunks. But, to ex-
tend this method to other languages, mono-
lingual resources have to be multiplied. We 
present a new method : endogenous chunk-
ing, which uses no other resource than the 
text to be segmented itself. The idea of this 
method comes from Zipf : to make the least 
comunication efort, speakers are driven to 
shorten frequent words. A chunk then can be 
characterized as the period of the periodic 
corelated functions length and frequency of 
words on the syntagmatic axis. This original 
method takes its advantage to be aplied to a 
great number of languages of alphabetic 
script, with the same algorithm, without any 
resource. 
Introduction 
Chunking is a frequent segmentation step in 
many processing types : robust parsers, parsers 
of linear complexity (Vergne, 200), computing 
stress groups and linking them in ts systems, to 
compute macro-prosody (Vanier et al., 199), 
in automatic indexing, the chunk as another in-
dexed grain above the word in the grain hierar-
chy, and in sub-sentential alignment, the chunk 
as an aligned grain. 
The method we propose is based on the prop-
erties of the functions length and frequency of 
words on the syntagmatic axis. These two func-
tions are correlated : integer, periodic, synchro-
nous, in phase oposition, and their period al-
lows to define the chunk. On a period, the length 
function is non-decreasing, and the frequency 
function is non-increasing. These concepts con-
tinue in Zipf's direction : minimizing the com-
munication effort drives the speaker to shorten 
frequent words (Zipf, 1949). The length metrics 
defined by Zipf is not the number of letters, but 
the number of sylables or the number of pho-
nemes of the writen form (Zipf, 1935); the met-
rics of our method is also the number of syla-
bles, or more precisely the number of vowel nu-
clei, computable from the writen form; this met-
rics takes its root into the oral origin of the 
chunk. The word frequency is measured in the 
segmented text. 
This method of segmentation into chunks is 
based on digital properties, and is valid on lan-
guages with alphabetic script. It is endogenous, 
as it computes on the text to be segmented and 
does not use any resource external to the parsed 
text. 
1 Structure
model of the chunk accord-
ing to Abney and according to Déjean 
The concept of chunk has been proposed by 
Steve Abney (191). It has been based on prop-
erties of speech : Abney defined the chunk as a 
stress group. As speech is constrained by the vo-
cal system, we can see the chunk as a generic 
concept on natural languages, a concept of lan-
guage. Hervé Déjean (198) has proposed a 
structure model for the chunk : beginings and 
ends of chunk (words or morphemes) around a 
kernel (Déjean, 198, page 17); our method 
uses this model. 
For instance, the writen form "Commision" 
has been found in the folowing chunks in the 
same text : 
     [ Comision européenne ] 
  [ la  Comision ]  
  [ la  Comision européenne ] 
   [ dans  la  Comision ]  
And here is the synthesis : 
   [ dans [ la [ Comision ] européenne ] 
   [ beginings [   kernel   ]     ends   ] 
202
2 Local
deductions and their generali-
zation at text level 
Properties of the chunk are used locally at occur-
rence level : an occurrence of a writen form is 
locally a begining or an end of a chunk. An im-
portant question is to decide how to articulate 
local deductions at occurrence level and their 
global merging at text level. 
We know that occurrences of the same writen 
form may be occurrences of more than one word, 
in different contexts. For instance, "on" in Eng-
lish is the begining of a chunk in "on the con-
trary", but it is the end of a chunk in "it is going 
on". These two occurrences correspond to two 
different words, which have different positions 
and different contexts, and their local deductions 
cannot be merged. So, we can merge local de-
ductions for occurrences of the same word. In 
practice, we merge local deductions for occur-
rences of a writen form if there is no begining 
end contradiction. 
We tried ful merging, as if all occurrences 
were of the same word. This solution remains 
valid for monofocused short texts (some thou-
sands words). But, to be able to chunk longer 
texts, we have chosen now the solution of a par-
tial generalization (see below in 4). 
3 Two
properties of a chunk 
The algorithm exploits two properties of the 
chunk. 
3.1 Property
1 : the chunk is a constituent of 
the virgulot 
Hervé Déjean (198) has defined the "entre-
ponctuations" as a constituent delimited by two 
punctuations. Nadine Lucas (Lucas, 201) has 
proposed the term "virgulot", that we wil use 
now. We define the folowing constituent hierar-
chy : the text is constituted of virgulots, them-
selves constituted of chunks, themselves consti-
tuted of occurrences of writen forms. 
Property exploited by the algorithm : 
a writen form attested at the begining of a 
virgulot is a begining of a chunk, 
a writen form attested at the end of a virgulot 
is an end of a chunk. 
Here are some instances of virgulots : , in denen Aale leben , , bis die Bewirtschaftungspläne vorliegen . 
 
. It also intends  to explore measures , , before migrating upstream  to spend most of 
their lives . 
 , en las aguas centro-ocidentales  del Océano 
Atlántico . , donde transcure  la mayor parte  de su vida . 
 
. Lasciandosi trasportare dala corente  e nuo-
tando , , dove  si riproducono  una sola volta e poi 
muoiono . 
 
First writen forms of virgulots are beginings 
of chunks (prepositions, pronouns, …), and their 
last writen forms are ends of chunks (nouns, 
verbs, adjectives, …). 
3.2 Property
2 : the chunk is the period of the 
correlated functions length and fre-
quency of words on the syntagmatic axis 
We define two integer functions of words on the 
syntagmatic axis (inside a virgulot) : their length, 
defined as their number of sylables, and their 
frequency in the text to be segmented. 
Here is an instance of a virgulot : 
  , would migrate from the rivers on their teritories , 
length: 1    3     1   1   2   1   1    4 
frequ.: 10    3     6  65   2   6   4    1 
On the length function, we have the folowing 
non-decreasing sequences : [1 3] [1 1 2] [1 1 4]. 
On the frequency function, we have the fol-
lowing non-increasing sequences : [10 3] [6] [65 
2] [6 4 1]. 
For these two functions, a period corresponds 
to a sequence; in other words, these sequences 
give a way to segment; these 2 functions are syn-
chronous : sequences of both functions (nearly) 
define the same periods; on a (synchronous) pe-
riod, both functions are in phase oposition : on a 
period (which defines a chunk), the length func-
tion is non-decreasing, and the frequency func-
tion is non-increasing; the common properties of 
these two functions allow us to call them corre-
lated; it is an other way to say that short words 
are frequent and that long words are rare. 
We notice, folowing Zipf (1949) in "Human 
Behavior and the Principle of Least-Effort" that 
writing and speech are an optimal compression; 
it reminds the principles of file compression in 
computer science : frequent data are short coded, 
and rare data are long coded. Let us make an ob-
servation on the Zipf law, as it is known today : 
this law makes a relation between frequency and 
rows of words sorted by decreasing frequency; if 
we knew only this law, we would forget length 
of words; but Zipf proposed to consider length 
and frequency together, in a correlated way, as 
an optimization (the Least-Effort). As we use 
length and frequency together, in a correlated 
way, we go back to the origin of Zipf's concepts. 
203
To compute word length from the writen 
form, length is defined as the number of syla-
bles, i.e. the number of vowel nuclei (a sequence 
of contiguous vowels corresponds to a vowel 
nucleus, and to a length equal to 1). This calcula-
tion needs as input the vowels of the alphabet 
(Latin or Greek). There is a particular case : is 
the y vowel or consonant. The y is vowel in "sys-
tem" (length 2) and consonant in "rayon" (length 
2); y is consonant by default; y is vowel at the 
begining or the end of a word, or alone (usualy, 
by, y); y is vowel between 2 consonants; these 
rules are enough to process all cases for the 20 
natural languages of the corpus. Acronyms (se-
quences of upercases) have a length equal to 
twice their numbers of letters (tendency to be in 
the end of chunk). A number (sequence of fig-
ures) has a length equal to 1, whatever its num-
ber of figures (tendency to be in the begining of 
chunk). 
4 An
algorithm based on these proper-
ties 
The frequency and the length of every writen 
form are computed. 
For the property 1, based on the virgulot, the 
text is processed, and occurrences of writen 
forms at the begining or end of virgulot are 
noted as begining or end of chunk. 
For the property 2, based on monotonous se-
quences, the text is processed, while noting bor-
ders between 2 monotonous sequences, that 
gives for each border an end and a begining of 
chunk. A Boolean function "in the same se-
quence" returns whether 2 contiguous words are 
in the same monotonous sequence (i.e. in the 
same chunk). Four solutions are experimented : 
on length only, on frequency only, on length 
AND frequency (then shorter chunks), or on 
length OR frequency (then longer chunks). Re-
sults are very comparable, because both func-
tions are strongly correlated
1
. For example, this 
function, in "length OR frequency" mode, on 
words i and i+1, to express the fact that these two 
words are in the same sequence, has the folow-
ing form : 
   words i and i+1 are not separators of virgulot 
   AND 
(    length(i+1)  ≥    length(i)   OR  
 frequency(i+1) ≤ frequency(i) ) 
                                                             
1
 Using length alone alows, not using frequency, to 
get a method usable on a very short text, as a 
search engine query. 
The generalization of local deductions is done 
the folowing way : for all occurrences of a writ-
ten form, a synthesis of local deductions is done. 
There are 8 cases : 2 properties, 4 cases for each 
(2 Booleans : begining, end). If all local deduc-
tions are compatible, they are merged, i.e. occur-
rences without any local deduction take the tag 
of occurrences with the same local deduction : 
either begining or end of chunk. 
Here is the trace of the process on our instance 
of virgulot : 
  virgul.  sequ.  general. result 
   b  e   b  e    b  e    b  e  len. freq. 
 [1,0] [1,0] [0,0] [2,0] 1 10 would 
 [0,0] [0,1] [0,1] [0,2] 3 3 migrate 
 
 [0,0] [1,0] [1,0] [2,0] 1 6 from 
 [0,0] [0,0] [1,0] [1,0] 1 65 the 
 [0,0] [0,1] [0,1] [0,2] 2 2 rivers 
 
 [0,0] [1,0] [1,0] [2,0] 1 6 on 
 [0,0] [0,0] [1,0] [1,0] 1 4 their 
 [0,1] [0,1] [0,0] [0,2] 4 1 teritories 
 
From the first property (the first column of 
Booleans), would is the begining, and territo-
ries is the end of the virgulot, therefore begin-
ning and end of a chunk ([ marks a begining of 
chunk, ] marks an end of chunk) : , [ would migrate from the rivers on their teritories ], 
The second property (the second column of 
Booleans) which exploits the monotonous se-
quences, here in "length OR frequency" mode, 
gives the folowing chunking : 
  , [ would migrate ]  [ from the rivers ] 
   [ on their territories ]  , 
The generalization of local deductions (the 
third column of Booleans) adds the fact that the 
and their are beginings of a chunk elsewhere in 
this text. 
Then these three sources of deduction are 
merged, and we obtain the folowing segmenta-
tion (the forth column) : 
   ,  [ would migrate ]  [ from [ the rivers ] 
     [ on [ their territories ]  , 
5 Some
sentences segmented into 
chunks 
The validation corpus of the method is composed 
of 12 press releases (about 100 words each for 
one language), every release is writen into 6 to 
20 languages, and of the part 1 of the "Treaty 
establishing a Constitution for Europe" in 1 
languages (about 10 00 words for one lan-
guage), from the website of the European Union 
(htp:/europa.eu/). 
204
The folowing sentences are extracted from 
the release IP/05/1018 of 205 (and processed in 
"length OR frequency" mode) : 
 
[ Die Laichgründe ]  [ der Aale ] befinden ]  [ sich 
[ im Sargasose ] [ im itleren Westatlantik ] . 
 
[ Eels spawn ] [ in [ the Sargaso Sea [ in [ the west-
ern central Atlantic ] Ocean ] . 
 
[ Las anguilas ] desovan ]  [ en [ el Mar [ de [ los 
Sargazos ] , [ en [ las aguas ] centro-ocidentales ] 
[ del Océano Atlántico ] . 
 
[ La zone [ de frai ] [ de l’anguile ] [ se situe [ en 
mer ]  [ des Sargases ] , [ dans [ la partie centre-
ouest ] [ de l’océan Atlantique ] . 
 
[ Le anguile ]  [ si riproducono ]  [ nel mar [ dei 
Sargasi ] , [ nel’Atlantico centro-ocidentale ] . 
 
The folowing sentences are extracted from 
the part 1 of the "Treaty establishing a Constitu-
tion for Europe" (and processed in "length OR 
frequency" mode) : 
 
[ Die Union ] steht alen europäischen ] Staten of-
fen ] , [ die [ ihre Werte ] achten ] [ und [ sich ver-
pflichten ] , [ sie gemeinsam ] [ zu fördern ] . 
[ The Union ] [ shal be open ] [ to [ al [ European 
States ] [ which respect ] [ its values ] [ and [ are 
comited ] [ to promoting ] them ] together ] . 
[ La Unión ] [ está abierta ] [ a todos ] [ los Esta-
dos ] europeos ] [ que respeten ] [ sus valores ] [ y 
[ se comprometan ] [ a promoverlos ] [ en común ] . 
[ L'Union [ est ouverte ] [ à [ tous [ les États ] euro-
péens ] [ qui respectent ] [ ses valeurs ] [ et [ qui 
s'engagent ] [ à [ les promouvoir ] [ en comun ] . 
[ L'Unione [ è aperta ] [ a tuti ] [ gli Stati europei ] 
[ che rispetano ] [ i suoi valori ] [ e [ si impegna-
no ] [ a promuoverli congiuntamente ] . 
Conclusion 
While characterizing the chunk in a purely digi-
tal way, from properties of length et frequency 
functions of words on the syntagmatic axis, this 
original method consists in calculations on the 
text to segment; it has the advantage to be ap-
plied to a great number of languages, with the 
same algorithm, without any monolingual re-
source : languages with alphabetic script, with a 
writen word which separates function words 
from content words (it is not the case in Finish), 
and compatible with a structure model of the 
chunk where function words generally are before 
content words; the method is promising for the 
2 languages of the European Community
2
. 
                                                             
2
 Se results on : 
htp:/ww.info.unicaen.fr/~jvergne/chunking_mu
ltilingue_endogene/ 
This method can be applied in automatic in-
dexing, for search-engines (as Exalead does, to 
be able to output the most frequent terms associ-
ated to the documents of the answer), and in sub-
sentential alignment, to constraint the statistical 
alignment (as in Similis, the alignment software 
of Lingua et Machina, but this software uses 
monolingual resources for every language). The 
interesting feature of this method is not to need 
any resource for a new language to process
3
. 
As it is independent from specificities of each 
language, this method is not "multilanguage", 
neither "multi-monolanguage", but as it exploits 
generic properties of natural languages, that is 
properties of language, as an abstraction of natu-
ral languages, we could perhaps simply call it a 
"linguistic" method. 
References 
Steven Abney. 191. Parsing By Chunks. in 
Principle-Based Parsing, 257-278, Kluwer 
Academic Publishers. 
Hervé Déjean. 198. Concepts et algorithmes 
pour la découverte des structures formelles 
des langues. Thèse de doctorat de l'université 
de Caen, France. 
Nadine Lucas. 201. Étude et modélisation de 
l'explication dans les textes. Actes du Colo-
que "L'explication: enjeux cognitifs et com-
municationels", Paris. 
Gérald Vannier, Ane Lacheret-Dujour, Jacques 
Vergne. 199. Pauses location and duration 
calculated with syntactic dependencies and 
textual considerations for t.t.s. system. ICPhS 
199, San Francisco, USA, August 199. 
Jacques Vergne. 200. Tutorial : Trends in Ro-
bust Parsing. Coling 200. 
George K. Zipf. 1935. The psychobiology of lan-
guage : An introduction to dynamic philology. 
Boston, Mass., Houghton-Mifflin. 
George K. Zipf. 1949. Human Behavior and the 
Principle of Least-Efort. Adison-Wesley. 
                                                             
3
 But a problem for this large scale multilingual 
method is to evaluate the results on so many lan-
guages : we ned a speaker for every language. 
For the moment, it is done for German, English, 
Spanish, French and Italian. 
205

