A tabular interpretation of a class of 2-Stack Automata 
Eric Villemonte de la Clergerie 
INRIA Rocquencourt B.P. 105 
78153 Le Chesnay Cedex, FRANCE 
Eric. De_La_Clergerie@inr i a. fr 
Miguel Alonso Pardo 
Universidad de La Corufia 
Campus de Elvi/ia s/n 
15071 La Corufia, SPAIN 
al0ns00dc, f i. udc. es 
Abstract 
The paper presents a tabular interpretation for a 
kind of 2-Stack Automata. These automata may be 
used to describe various parsing strategies, ranging 
from purely top-down to purely bottom-up, for LIGs 
and TAGs. The tabnlar interpretation ensures, for 
all strategies, a time complexity in O(n ~) and space 
complexity in O(n ~) where n is the length of the 
input string. 
Introduction 
2-Stack automata \[2SA\] haw~' been identified as pos
sible operational devices to describe parsing strate
gies for Linear Indexed Grammars \[LIG\] or Tree Ad
joining Grammars \[TAG\] (mirroring the traditional 
use of Push-Down Automata \[PDA\] for Context
Free Grammars \[CFG\]). Different variants of 2SA 
(or not so distant EInbedded Push-Down Automata) 
have been proposed, some to describe top-down 
strategies (Vijay-Shanker, 1988; Becket, 1994), some 
to describe bottom-up strategies (Rambow, 1994; 
Nederhof, 1998; Alonso Pardo et al., 1997), but none 
(that we know) that are able to describe both kinds 
of strategies. 
The same dichotomy also exists in the different 
tatmlar algorithms that has been proposed for spe
cific parsing strategies with complexity ranging from 
O(n (;) for bottom-up strategies to O(n 9) for prefix
valid top-down strategies (with the exception of a 
O(n a) tabular interpretation of a prefix-valid hybrid 
strategy (Nederhof, 1997)). It must also be noted 
that the different tabular algorithms may be diffi
cult to understand and it is often unclear to know if 
the algorithms still hold for different strategies. 
This paper overcomes these problems by (a) in
troducing strongly-driven 2SA \[SD-2SA\] that may 
be used to describe parsing strategies for TAGs 
and LIGs, ranging from purely top-down to purely 
bottom-up, and (b) presenting a tabular interpre
tation of these automata in time complexity O(n6) 
and space complexity O(n~). 
The tabular interpretation follows the principles 
of Dynamic Programming: the derivations are bro
ken into elementary sub-derivations that may (a) be 
combined in different contexts to retrieve all possi
ble derivations and (b) be represented in a compact 
way by items, allowing tabulation. 
The strongly-driven 2SA are introduced and moti
vated in Section 1. We illustrate in Sections 2 and 3 
their power by describing several parsing strategies 
for LIGs and TAGs. Items are presented in Sec
tion 4. Section 5 lists the rules to combine items aim 
transitions and establishes correctness theorems. 
1 Strongly-driven 2-Stack Automata 
2SA are natural extensions of Push-Down Automata 
working on a pair of stacks. However, it is known 
that unrestricted 2SA have the power of a Turing 
Machine. The remedy is to consider asymmetric 
stacks, one being the Master Stack MS where most 
of the work is done and the other being tile Auxiliary 
Stack AS mainly used for restricted "bookkeeping". 
The following remarks are intended to give an idea 
of the restrictions we want to enforce. The tirst ones 
are rather standard and may be found nnder differ
ent forms in the literature. The last one justifies the 
qualification of "strongly-driven" for our automata. 
\[Session\] AS should actually be seen as a stack of 
session stacks, each one being associated to a 
session. Only the topmost session stack may 
be consulted or modified. This idea is closely 
related to the notion of Embedded Push-Down 
Automata (Rainbow, 1994, 96-102). 
\[Linearity\] A session starts in mode write w and 
switches at some point in mode erase e. In 
mode w (resp. e), no element can be popped 
from (resp. pushed to) the master stack MS. 
Switching back h'om e to w is not allowed. This 
requirement is related to linearity because it 
means that a same session stack is never used 
twice by "descendants" of an element in MS. 
\[Soft Session Exit\] Exiting a session is only possi
ble when reaching back, with an empty session 
stack and in mode erase, the MS element that 
initiated tile session. 
\[Driving\] Each pushing on MS done in write mode 
leaves some mark in MS about the action that 
1333 
/W \W 
/E "NE 
I I I I 
-*W ~W Write Mode 
-/ 
.,{,_-.-._ 
-*E 
I I I I I I I I I I I I I I I , 
Master stack 
Figure 1: Representation of transitions and derivations 
took place on the session stack. The popping 
of this mark (in erase mode) will guide which 
action should take place on the session stack. 
In other words, we want the erasing actions to 
faithfully retrace the writing actions. 
Formally, a SD-2SA ,4 is specified by a tuple 
(E, A/t, X, $0, $f, O) where E denotes the finite set of 
terminals, .£4 the finite set of master stack elements 
and A' the finite set of auxiliary stack elements. The 
init symbol $0 and final symbol $1 are distinguished 
elements of A4. ® is a finite set of transitions. 
The master stack MS is a word in (:DAd)* where 
T) denotes the set {/2,'.~,-% ~} of action marks 
used to remember which action (w.r.t. the auxiliary 
stack AS) takes place when pushing the next master 
stack element. The empty master stack is noted e 
and a non-empty master stack 51A1 ... 5~An where 
A~ denotes the topmost element. 
The meaning of the action marks is: 
/ Pushing of an element on AS. 
Popping of the topmost element of AS. 
--* No action on AS. 
Creation of a new session (with a new empty 
session stack on AS). 
The auxiliary stack AS is a word of (/CA'*)* where 
/C = {~w,~e} is a set of two elements used to 
delimit session stacks in AS. Delimiter ~w (resp. 
~e) is used to start a new session from a session 
which is in writing (resp. erasing) mode. The empty 
auxiliary stack is noted c. 
Given some input string Xl ... x\] E E*, a configu
ration of .A is a tuple (m, u, E, ~) where m E {w, e} 
denotes a mode (writing or erasing), u a string posi
tion in \[0, f\], E the master stack and ( the auxiliary 
stack. Modes are ordered by w -~ e to capture the 
fact that no switching from e to w is possible. The 
initial configuration of A is (w, 0, ~$0, ~w) and the 
final one (e, f, ~$f, ~w). 
A transition is given as a pair (p, E, () ~ (q, O, 0) 
where p, q are modes (or, with some abuse, variables 
ranging over modes), z in E*, E and @ suffixes of 
master stacks in M(DM)*, and ~, 0 suffixes of aux
iliary stacks in X*(K:X*)* = (XU/C)*. Such a transi
tion applies on any configuration (p, u, ~I/E, ~() such 
that x,,+l ... x, = z and returns (q, v, ~®, ~/~0). 
We restrict the kind of allowed transitions: 
SWAP (p, A, ~) ~ (q, B, ~) with p _ q and either 
E K: ("session bottom check") or ( = e ("no 
AS consultation") . 
/-WRITE (w, A, e) ~ (w, A/B, b) 
/-ERASE (e, A/B, a) ~ (e, D, e) 
-*-WRITE (w, A, e) ~-% (w, A--~B, e) 
--+-ERASE (e, A-*B, e) ~ (e, C, e) 
~-WRITE (m, A, e) ~ (w, A~B, ~'~) 
~-ERASE (e,g~B,~ TM) ~ (m,V,e) 
",,-WRITE (w, A, a) ~ (w, A'NB, e) 
"~-ERASE (e, A'NB, e) ~L~ (e, C, e) 
Figure 1 graphically outlines the different kinds 
of transitions using a 2D representation where tile 
X-axis (Y-axis) is related to the master (resp. aux
iliary) stack. Figure 1 also shows the two forms of 
derivations we encounter (during a same session). 
2 Using
2SA to parse LIGs 
Indexed Grammars (Aho, 1968) are an extension of 
Context-free Grammars in which a stack of indices 
is associated with each non-terminal symbol. Linear 
Indexed Grammars (Gazdar, 1987) are a restricted 
form of Indexed Grammars in which the index stack 
of at most one body non-terminal (the child) is re
lated with the stack of the head non-terminal (the 
father). The other stacks of the production must 
have a bounded stack size. 
Formally, a LIG G is a 5-tuple (VT, VN, S, VI, P) 
where VT is a finite set of terminals, VN is a finite 
set of non-terminals, S E VN is the start symbol, 
IZ~ is a finite set of indices and P is a finite set of 
productions. Following (Gazdar, 1987) we consider 
productions in which at most one element can bc 
pushed on or popped from a stack of indices: 
1334 
\[Terminal\] A~,0\[\] --4 a~ where a~ G VT U {e}, 
\[POP\] A~,0\[oo\] --* A~,, \[\]... A<a\[oowl... A~,,,~ \[\] 
\[PUSH\] A~,0\[ooT\] ~ A~,, \[1... Ak,d\[OO\]... A},,,~ \[1 
\[ItOR\] &,o\[OO l -~ A~,,\[\] ...&.,,\[oo\]...A~,,,~\[\] 
To each production k of type PUStt, POP or 
HOR, we associate a characteristic tuple t(k) = 
(d, 5, (t, ¢/) where d is the position of the child and 
the other arguments given by the following table: 
tL _,v Type 1  --aLe-L  
SH I./ \] e L~ 
L£ °r  LbA_ _IzJ L HoR I-+1 1 
We introduce symbols Vk,i as a shortcut for dotted 
productions \[Ak,o-eAk,1 ... Ak,i • Ak,i+l ... Ak,,~,\]. 
In order to design a broad class of parsing strate
gies ranging from pure top-down to pure bottom-up, 
we parameterize the automaton to be presented by 
a call projection ---+ from V to V ~°~u and a return 
projection +-from l) to l) ~t where 1) = I/N O 171 
and Y ~al and y~¢t are two sets of elements. We re
quire F ~au N l) ''~t = 0 and (--*, *--) to be invertible, 
i.eVX, Ye\]2, (Z, ~) = (Y, Y) ::> x=Y 
The projections extend to sequences by taking 
X1... ~,~ = ~11... ~,~ and -U =e (similarly for ~--). 
Given a LIG G and a choice of projections, we 
define the 2SA A( G, --+, *--) = (Vv , M, X, ~, ~-, O) 
transitions are tmilt using the following rules. 
• Call/Return of a non child 
) CALL : (m,V~,i,e) ~ (w, --+ "~ 
RET: (e, Vk,i~-l,~") I~ (/~,Vk,i+I,E) 
• Call/Return of a child for t(k) = (i + 1, (5, a, fl). 
CALL(S) : (w, V~,~, -&+) ~ (w, Vk,,6Ak,~+,, ~) 
RET(5) : (e, Vk,,5~, ~) ~ (e, Vk,i+l, W) 
• Production Selection 
SEL: (w, Ak,o, e) ~-+ (w, Vk,0, e) 
• Production Publishing 
PUB: (e, Vk,n,,e) ~ (e,~,e) 
• Scanning (for terminal productions) 
SCAN: (w, Ako, "~ ~ ~ ~"~) , ~ )~--~(e, Aa,0, 
The reader may easily check that el(G, "--+, e--) 
recognizes L(G). The choice of the call and return 
elenmnts for the MS (~,/ and ~,i) and the AS 
(--~ and ~') defines a parsing strategy, by controlling 
how information flow between the phases of predic
tion and propagation. The following table lists the 
choices corresponding to the main parsing strategies 
(but others are definable). 
---+ ~ .___+ 
Top-Down A _L h' J 
Bottom-Up ± A' ± 7 
Earley A A' 7 7' 
It is also worth to note that the descrip
tion of A(G,-~, ~--) could be simplified. In
deed, for every configuration (m,u,E,() deriv
able with A(G,--+,+--), we can show that ~Z = 
~Vk,,i,/~l ...Vk,,,i, 5,~X, and that b t only depends 
on Vkz,,~. That means that we conld use a master 
stack without action marks, these marks being im
plicitly given by the elements Vk,~. 
3 Using
2SA to parse TAGs 
Tree Adjoining Gramlnars axe a extension of CFG 
introduced by Joshi ill (Joshi, 1987) that use 
trees instead of productions as primary represent
ing structure. Formally, a TAG is a 5-tuph; G = 
(VN, VT, S,I,A), where VN is a finite set of non
terminal symbols, V T a finite set of terminal sym
bols, S thc axiom of the grammar, I a finite set of 
initial trees and A a finite set of au:riliary trees. IUA 
is the set of elememtary trees. Internal nodes arc la
beled by non-terminals and leaf nodes by terminals 
or e, except for exactly one leaf per mlxiliary tree' 
(the foot) which is labeled by the santo non-terminal 
used as label of its root node. 
New trees are derived by adjoining: let be c~ a 
tree containing a node v labeled by A and let be 
fl an auxiliary trec whose root and foot nodes arc 
also labeled by A. Then, tile adjoining of fl at the 
adjunction node v is obtained by excising the subtree 
ct~ of a with root v, attaching fl to v and attaching 
the excised subtree to the foot of fl (See Fig. 2). 
pine 
Figure 2: Traversal of an adjunction 
An elementary tree a may be represented by a 
set P(c~) of context free productions, each one being 
either of the form 
• 12k, 0 --4 b'k,l.., lJk,r~t. ~ where vk, o denotes sonic 
non-leaf node k of a and v~,~ the ith son of k. 
1335 
• uk,0 -+ ak, where v/¢,0 denotes some leaf node k 
of a with terminal label ak. 
As done for LIGs, we introduce symbols Vk,i 
to denote dotted productions and consider pro
jections ---+ and +to define th__e+e ~_~ameterized 
2SA .A(G, -+, +-) = (VT, .Ad, .Ad, u0,0, uo,0, O) where 
Ad = {Vk,i} U {uk,i} U {uk,i}. The transitions are 
given by the following rules (and illustrated in Fig
ure 2). 
• Call / Return for a node not on a spine. The 
call starts a new session, exited at return. 
CALL: (rn, Vk,i,e)~ (w, Vk,i~,~ m) 
RET: (e, Vk,i~,~") ~ (m, Vk,i+l,e) 
• Call / Return for a node ukd+a on a spine. 
The adjunction stack is propagated un-modified 
along the spine. 
SCALL : (w, Vk,i, e) ~ (w, Vkd--~uk,i+;, e) 
SRET : (e, Vk,~, c) ~ (e, V~,i+l, e) 
• Call / Return for an adjunction on node uk,0. 
The computation is diverted to parse some ac
ceptable auxiliary tree fl (with root node rz), 
and a continuation point is stored on the auxil
iary stack. 
ACALL : (w, uk,---~, e) H (W, ~,0./'Vfi, Vk,0) -----e t----
ARET: (e, uk,o/~-fi,V~m~) ~ (e,u~,0,e) 
• Call / Return for a foot node ffl. The continu
ation stored by the adjunction is used to parse 
the excised subtree. 
FCALL: (w, ~,A) ~ (w, f~z'NA, e) 
_c-:-FRET: (e, fa".~A, ~) ~ (e, fZ, A) 
Note: These two transitions use a variable A 
over Ad. This is a slight extension of 2SA that 
preserves correctness and complexity. 
• Production Selection 
SEL: (w, ~ e) ~ (w,V~,o e) l\]k,O, 
• Production Publishing 
PUB: (m,V~,~,e) H (e,v~,0,e) 
• Scanning 
SCAN: (w,u~,0, "~ a~ ) 
Different parsing strategies can be obtained by 
choosing the call (u~--~,,) and return (u~,~--i) elements: 
Strategy --~ ~
prefix-valid Top-Down u 3_ 
Bottom-Up _L u' 
prefix-valid Earley u u' 
Non prefix-vMid variants of the top-down and 
Earley-like strategies can also be defined, by tak
ing ~ = 3_ and Ffi = r~ for every root node rp of 
an auxiliary tree fl (the projections being unmodi
fied on the other elements). In other words, we get 
a full prediction on the context-fi'ee backbone of G 
but no prediction on the adjunctions. 
4 Items

We identify two kinds of elementary deriva
tions, namely Context-Free \[CF\] and escaped 
Context-Free \[xCF\] derivations, respectively rep
resented by CF and xCF items. An item keeps the 
pertinent information relative to a derivation, which 
allows to apply the sequence of transitions associ
ated with the derivation in different contexts. 
Before presenting these items, we introdm:e the 
following classification about derivations. 
A derivation (p,u, EA,~)I-fE (q,v, O, O) is said 
rightward if no element of E is accessed (even for 
consultation) during the derivation and if A is only 
consulted. Then EA is a prefix of O. 
Similarly, a derivation (p, u, E, ~)1-~ (q, v, @, 0) is 
said upward if no element of ~ is accessed (even for 
consultation). Then ( is a prefix of 0. 
We also note w\[q/p\] the prefix substitution of p by 
q for all words w,p, q on some vocabulary such that 
p is prefix of w. 
4.1 Context-Free Derivations 
A Context-Free \[CF\] derivation only depends on 
the topmost element A of the initial stack MS. That 
means that no element of the initial AS and no ele
ment of MS below element A is needed: 
:k 
(o, u, EA, (w, OB, 0) I;f w, 
where 
• dl and did2 are both rightward and upward. 
• d2 is rightward. 
• either (6 ~: ~, o = w, and c e X) or 
(6 = and c = 
For such a derivation, we have: 
Proposition 4.1 For all prefix stacks E', ~, 
(°,u,'='A,~ ') I*~ (w,v,(9'B,O') dl 
w, d2 
where ®' = o\[E'/~\] and O' = 0\[~'/~\]. 
The proposition suggests representing the CF 
derivation by a CF item of the form 
ABS(Tm 
where A = (u, A) and B = (v, B) are micro config
urations and C = (w, C, c) a mini configuration. 
1336 
B 'C 
CF(-+) Item 
xCF(--+) Item r~ , 
I I I I I I I I 
:C 
CF(/) or CF(~) Item 
A~Xx xCF(/) Item e; "" 
I I I I I I 
B 
CF(",,~) Item B 
A xC F(',~) Item 
I I' I I I I I , 
Figure 3: Items Shapes 
4.2 Escaped
Context-Free Derivations 
An escaped Context-Free \[xCF\] derivation is al
most a CF derivation, except for an escape sub
derivation that accesses deep elements of AS. 
(w, u, EA, ~)l ~-~-~ (w,v,6)B,O) 
I~ (w,,, cD, (d) 
I~-~ (e, t, ,~D"...~E, ¢) 
I~ (e, w, oB6c, Ce) 
where 
• dl and did2 are both rightward and upward. 
• d 2 and dx are rightward. 
• da is Ul)ward. 
• 6)k ~ and d, cE A'. 
Proposition 4.2 For all prefix stacks -~ and ~', 
stack Ct, and rightward derivation 
• 'D, ('d)\[~x , (e, t, )'D'~E, ¢') (w, 
where '~' = 45\[E'/E\], we have 
(w,~,E'A,() I-dl 
d~ 
I ~->-, 
I-*-da 
(w, v, o \[~'/F.\]B, 0\[(/(\]) 
(w, s, e\[E'/E\]D, (d) 
( e, t, g'\[E' /EID'NE, ¢') 
(e, ~,, O\[-='/=\]B~C, ¢'e) 
The proposition suggests representing the xCF 
derivation by an xCF item of the form 
AB~5119 EIOe 
where A = {u,A>, B = (v,B>, D = (s,D,d}, E = 
<t,E) and O = (w,C,c>. 
In order to homogenize notations, we also use 
the alternate notation AB6\[oo\]Cm to represent CF 
item A/38(~m, introducing a dummy symbol o. 
The specific forms taken by CF and xCF items for 
the different actions (5 arc outlined in Figure 3. 
5 Combining
items and transitions 
We provide the rules to combine items and transi
tions in order to retrieve all possible 2SA derivations. 
These rnles do not explicit the scanning con
straints and suppose that the string z may be read 
between positions w and k of the input string. They 
use holes * to denote slots that not need be con
suited. For any mini configuration A = {u, A, a), we 
note A° = (u, A) its micro projection. 
\[--+--WRITE\] r = (w, C, e) ~ (w, C-+F, e) 
A~\[ooIOw =~ AG'°-+\[oo\]Fw 
where G' = <w, C, c), and /~ = {}, F, c>. 
\[/--WRITE\] ~ = (w, C, ~) ~ (w, C/F, I) 
(1) 
A**\[oolg:w =2=> 6 ,0 ~,o /\[oo1Fw 
whore O = <~,, C, 4, and *> = (~, F, f>. 
\[~--WRITE\] r = (m,C,e)~2+ (w,C~F, >"~) 
(2) 
a**\[~\]Om ~ O°O°~\[oo\]Fw 
where G' = (w, C, c), and ~' = (k, F, ~'~). 
\[x,,--WRITE\] r = (w, C, c) ~ (w, C'NF, e) 
(3) 
2F~,\[oo\]OWM**\[oo\]Aw }:=~ MOO\\[oo\]Fw (4) 
where C = {w,C,c>, h = (u,A, eL>, and F = <<Y,a>. 
\[-+--ERASE\] r = (e, B--+C, e) ~L+ (e, F, e) 
A°/)°-+\[DE\]Oe } Ao MA\[oo\]/) w =2=> gl° MA\[DE\]~'e (5) 
where O = (y., C, c>, B = <v, P, b>, /~ = <k, F, e>, 
and (when D ¢ o) D = (s,D,b). 
1337 
\['N-ERASE\] r = (e, Bx.~C, e) ~ (e, F, f) 
A°B°\\[D*\]Ce } 
A°.~\[oo\]Mw ~ M°O,\[B~°I~'e (6) 
~/°O,\[oo\]Bw 
where 0 = <w,C,c>, B = <~,/3, b>, M = 
(l,M,m), ~' = (k,F,/), and (when D ¢ o) 
D = (.,.,m>. 
\[~-ERASE\] ~ = (e, E~C, ~'~) ~ (m, r, e) 
/~°B~\[°°\]C'e }~ MNA\[~)E\]Fm MNA\[DE\]Bm (7) 
where C = (w~C, ~"~), /) = (v,B,b), and ~' = <k, 
F, ~) 
\[/-ERASE\] r = (e, B/zC, c) ~-* (e, F, e) 
/)°B°/Z\[oo\]Ce }~ MNA\[oo\]Fe (8) 
MNA\[oo\]/)w 
where (7 = (w,C,c), /) = (v,B,b>, and ~' = 
(k, F, b> 
/)°/)°/~\[DE°\]Oe } 
MNA\[oo\]Bw ~ MNA\[OP\]I~e (9) 
MD°".~\[OP\]E,e 
where C = (_w, C, c), \[3 = (v, B, b}, F = (k, F, b), 
and (when O ¢ o) 0 = (1, O,b}. 
\[SWAP\] r = (p, C, () ~-+ (q, F, () 
ABh\[DE\]Cm ~ ABh\[DE\]Fm (10) 
where C' = (w,C,c), ~' = (k,F,c), and either 
c=~=~°or~=e. 
The best way to apprehend these rules is to vi
sualize them graphically as done for the two most 
complex ones (Rules 6 and 9) in Figures 4 and 5. 
O B 
A "-.~L 
Figure 4: Application of Rule 6 
N C D 
Figure 5: Application of Rule 9 
5.1 Reducing
the complexity 
An analysis of the time complexity to apply each rule 
gives us polynomial complexities O(n ~') with u < 6 
except for Rule 9 where u -8. However, by adapt
ing an idea from (Nederhof, 1997), we replace Rule 9 
by the alternate and equivalent Rule 11. 
/)°*/z\[D/~°\]Ce } 
*D°\\[OP\]E,e MNA\[o<>\]/)w ~ MNA\[OP\]Fe (11) 
M*'%\[OP\]*e 
where C' = (w,C,e), /) = (v,B,b>, /~ = (k,F,b), 
and (when 0 ¢ o) O = (1, O,b). 
Rule 11 has same complexity than Rule 9, but may 
actually be split into two rules of lesser complex
ity O(n6), introducing an intermediary pseudo-item 
BB/z\[\[OP\]\]Ce (intuitively assimilable to a "deeply 
escaped" CF derivation). 
Rule 12 collects these pseudo-items (indepeu
dently from any transition) while Rule 13 combines 
them with items (given a 7 ERASE transition r). 
BB ff\[DE°\]Ce } .Do..N\[Op\]E,e ~ BB/\[\[OP\]\]C'e (12) 
/~°/)°'7 \[\[OPI\]C'e } 
MNA\[ool/)w ~ MNA\[OP\]Fe (13) 
M* ".~\[OP\].e 
where C' = (w,C,c), /) = (%/3, b>, P = (k,F,b), 
and (when O # o) O = (l,O,b). 
Theorem 5.1 The worst time complexity of the ap
plication rules (1,2,3,~,5,6,7,8,10,12,13) is O(n c') 
where n is the length of the input string. The worst 
space complexity is O(nh). 
5.2 Correctness
results 
Two main theorems establish the correctness of 
derivable items w.r.t, derivable configurations. 
A derivable item is either the initial item or 
an item resulting from the application of a combi
nation rules on derivable items. The initial item 
(0, e)(0, e)~\[oo\]<0," $0,~W)w stands for the virtual 
derivation step (w, 0, e, e)l(w, 0, ~$0, ~w). 
Theorem 5.2 (Soundness)For every derivable 
item Z = ABh\[DE\]Cm, there exists a derivation 
on configurations 
(o, e>l~~1~ v 
such that NI~12 is a CF or xCF derivation repro> 
sentable by I. 
Proof: By induction on the item derivation length 
and by case analysis. | 
1338 
Theorem 5.3 (Completeness) For all derivable 
configuration (m,, w, F~C, @), there exists a derivable 
item ABS\[DE\]Cm such that C = (w, C, c). 
Pro@ By induction on the configuration deriva
tion length and by case analysis of the different ap
plication rules. We also need the following "Extrac
tion Lemma". | 
Proposition 5.1 From any derivation 
<0, ~>I-~('n, w,-ZC, ~c) 
may be extracted a suJflx CF or xCF sub-derivation 
bll~ (m, w, ~C, @) for some configuration Lt. 
5.3 Illustration

In the context of TAG parsing (Sect. 3), we can 
provide some intuition of the items that are built 
with A(G,--~, ~-), using some characteristic points 
encountered during the traversal of an adjunction 
(Fig. 6). 
after CALL __\] before RET 
on ADJ A,al/z\[e,O\]RlW l A1A1/2\[FIA4\]R.2e on 
SPINE A,S~\[oolF~w j A,S,--*\[F, A4\]F2e 
onFOOT ~~\] Baa'%\[G,B4\]A4e 
: i 
Figure 6: Adjunction and Items 
6 Conclusion

This paper unifies different results about TAGs and 
LIGs in an uniform setting and illustrates the ad
vantages of a clear distinction between the use of 
an operational device and the evaluation of this de
vice. The operational device (here SD-2SA) helps us 
to focus on the description of parsing strategies (for 
LIGs and TAGs), while, independently, we design an 
efficient evaluation mechanism for this device (here 
tabular interpretation with complexity O(n6)). 
Besides illustrating a methodology, we believe our 
approach also opens new axes of research. 
For instance, even if the tabular interpretation 
we have presented has (we believe) the best possi
ble complexity, it is still possible (using techniques 
outside the scope of this paper, (Barthdlenly and 
Villemonte de la Clergerie, 1996)) to improve its ef
ficienc.y by refining what information should be kept 
in each kind of items {hence increasing computation 
sharing and reducing the number of items). 
To handle TAGs or LIGs with attributes, we also 
plan to extend SD-2SA to deal with first-order terms 
(rather than just symbols) using unification to apply 
transitions and subsumption to check items. 

References 

Alfred V. Aho. 1968. Indexed grammars an ex
tension of context-free grammars. Journal of the 
ACM, 15(4):647-671, October. 

Miguel Angel Alonso Pardo, Eric de la Clergerie, 
and Manuel Vilares Ferro. 1997. Automata-based 
parsing in dynamic programming for Linear In
dexed Grammars. In A. S. Narin'yani, editor, 
Proc. of DIALOGUE'97 Computational Linguis
tics and its Applications International Workshop, 
pages 22-27, Moscow, Russia, June. 

F. P. Barth(flemy and E. Villemontc de la Clergeric. 
1996. hfformation flow in tabular interpretations 
for generalized push-down automata. To appear 
in journal of TCS. 

Tilman Becker. 1994. A new automaton model 
for TAGs: 2-SA. Computational Intelligence, 
10(4):422--430. 

Gerald Gazdar. 1987. Applicability of indexed 
grammars to natural languages. In U. Reyle and 

C. Rohrer, editors, Natural Language Parsing and 
Linguistic Theories, pages 69-94. D. Reidel Pub
lishing Company. 

Aravind K. Joshi. 1987. An introduction to tree 
adjoining grammars. In Alexis Manaster-Ranler, 
editor, Mathematics of Language, pages 87 
115. John Benjamins Publishing Co., Amster
dam/Philadelphia. 

Mark-Jan Nederhof. 1997. Solving the correct
prefix property for TAGs. In T. Becket and H.-V. 
Krieger, editors, Proc. of MOL'97, pages 124 130, 
Schloss Dagstuhl, Germany, August. 

Mark-Jan Nederhof. 1998. Linear indexed automata 
and tabulation of TAG parsing. In Proc. of First 
Workshop on Tabulation in Parsing and Deduc
tion (TAPD'98), pages 1-9, Paris, France, April. 

Owen Rainbow. 1994. Formal and Computational 
Aspects of Natural Language Syntax. Ph.I). thesis, 
Univcrsity of Pemlsylvania. 

K. Vijay-Shanker. 1988. A Study of Trec Adjoining 
Grammars. Ph.D. thesis, University of Pennsyl
vania, January. 

