Identifying Concepts Across Languages: A First Step towards a Corpus-based Approach to Automatic Ontology Alignment Grace Ngaia0 Marine Carpuata1 Pascale Funga0a3a2a1 grace@intendi.com eemarine@ust.hk pascale@ee.ust.hk a0 Intendi Inc.
Hong Kong a1 Human Language Technology Center HKUST Clear Water Bay, Hong Kong 1 Introduction The growing importance of multilingual information retrieval and machine translation has made multilingual ontologies an extremely valuable resource.
Since the construction of an ontology from scratch is a very expensive and time consuming undertaking, it is attractive to consider ways of automatically aligning monolingual ontologies, which already exist for many of the world’s major languages.
This paper presents a first step towards the creation of a bilingual ontology through the alignment of two monolingual ontologies: the American English WordNet and the Mandarin Chinese HowNet.
These two ontologies have structures which are very different from each other, as well as being constructed for two very different languages, which makes this an appropriate and challenging task for our algorithm.
2 Alignment
of Ontologies In this paper, we address the problem of automatic multilingual ontology alignment.
Multilingual ontologies are very useful, but are also very timeconsuming and expensive to build.
For example, Euro WordNet (Vossen, 1998), a multilingual ontology for 8 European languages, involved 11 academic and commercial institutions and took 3 years to complete.
Furthermore, for many of the world’s major languages, monolingual ontologies already exist in some shape or form.
Therefore, it is reasonable and attractive to investigate whether a multilingual ontology could be quickly and robustly constructed from monolingual resources.
Given the easy availability of bilingual dictionaries, the task might seem easy at a first blush.
However, given two independently constructed ontologies, there always exists some difference in their structure that makes it difficult to perform a purely structural alignment.
These differences arise from different approaches and philosophies taken during the construction of the ontology; and for ontologies in different languages, differences which stem from dissimilarities between the languages concerned.
In addition, multilingual ontology alignment also has to deal with machine translation issues.
Since an ontology arranges words in a semantic hierarchy, it is possible for a word to appear in several different places in the hierarchy depending on its semantic sense.
However, words and concepts in a given language do not always translate cleanly into a second language; a word often has multiple translations, and they do not always share the same meanings.
In the absence of any ambiguity resolution, synonym sets in one ontology will be erroneously aligned to multiple synonym sets in the second ontology.
This is a serious problem: an investigative experiment with two ontologies, the American English WordNet and the Mandarin Chinese HowNet, found that, in the absence of any word sense disambiguation, each HowNet definition (the equivalent of a synonym set from WordNet) corresponded to an average of 8.1 WordNet synonym sets.
The approach taken in this paper works upon the assumption that even though a word may have different translations that correspond to different semantic senses, it is not likely that its synonyms will have the same exact set of translations.
Given a synonym set, or synset, in one ontology, our approach considers the average similarity between its words and words from all potential alignment candidates: Given two ontologies a4 and a4a6a5, a synonym set (synset) a7a9a8a10a4, and a similarity score a7a12a11a14a13a16a15a18a17a20a19a22a21a3a17a24a23a26a25 between any two words: 1.
For each word a17a27a8a28a7, find the synsets in a4a29a5 that it appears in (in the cross-lingual case, find the synsets ina4a30a5 in which the translations ofa17 appear.) 2.
For each of these candidate synsets a7a30a5: (a) if words a17a31a8a32a7 (or their translations) appear in the direct hyperset or hyposet, add them toa7a5.
(b) ifa7a5 contains a single word (a0a7a5 a0a2a1a4a3 ), expand it by adding words from its direct hyperset.
(c) Calculate a7a11a14a13a16a15a7 a21a7a5a25 : a5a7a6a9a8a11a10a12a5a14a13a15a5a17a16a19a18a21a20 a22a24a23a26a25a14a27a28a22a29a23a31a30a32a25a14a27a33a30 a5a7a6a9a8a34a10a36a35a37a13a38a35 a16 a18 a22 a23a26a25a14a27a40a39a17a41a42a41a44a43a17a39a46a45 a5a46a10a36a35a47a18 a35a49a48 a43a17a45a50a43 a5a7a6a9a8a11a10a51a35a37a13a52a35a53a16a32a18 as defined in Section 3 a39a17a41a42a41a40a43a54a39a46a45 a5a46a10a36a35a47a18a21a20 a55a57a56 a58a53a59 a10a51a35a37a13a15a5a54a18a61a60 a56 for some a5 a62 otherwise.
The candidate synsets froma4a6a5 are then ranked according to their similarity with a7, and the synset with the largest similarity is considered to be the alignment “winner”.
3 Cross-lingual Semantic Similarity Since automatic ontology alignment involves the comparison of sets of words to each other, it is necessary to define some measure for semantic similarity.
Much work has been done on this topic, but most of it has been in monolingual semantic similarity calculation.
Our problem is more complicated, as a cross-lingual ontology alignment will require measuring semantic similarity of words from different languages.
The method used in this paper is an extension of work from Fung and Lo (1998).
The assumption is that there is a correlation between word cooccurrence patterns that persists across languages, and the similarity between word cooccurrence patterns is indicative of the semantic similarity.
To construct a representation of the cooccurrence patterns, a list of seedwords is compiled.
The seedwords in one language is a direct translation of those in the other language.
Given a bilingual corpus, a context vector can then be constructed for each of the words of interest, where each element in the vector is a weight corresponding to a function of the significance of a particular seedword and its cooccurrence frequency with the word of interest.
This method, which was applied to the problem of automatic dictionary induction, has the advantage of being able to utilize non-parallel bilingual corpora, which is by nature much more plentiful than parallel corpora.
The most important extension that our work makes to the work of Fung et al.is the introduction of translation groups of words.
A major issue with translation research is that, given two arbitrary languages, it is common for a word in one language to have multiple translations in the other.
It is also common for a given translation of a particular word to be a translation of one of its synonyms as well.
To address this problem, this work uses seedword groups, a13 -to-a63 translations of sets of words, rather than 1-to-1 translations of single words.
This increases the robustness of the method, since a word need not be consistently translated for its context to be accurately identified.
An additional benefit is that the sparse data problem is alleviated somewhat: the increased number of seedwords increases the coverage of the corpus, which reduces the possibility that a rare word whose translation we are interested in does not occur with any of the seedwords.
Given two languages, a64a52a65 and a64a67a66, the algorithm proceeds as follows: 1.
Define a list a68 a65 a1 a69a70a68 a65a52a71 a21a72a68 a65a72a65 a21a50a73a32a73a32a73a74a68 a65a76a75a78a77, where each member a68 a65 a19 of the list is a set of words in a64 a65. 2.
Create a list a68 a66 a1a79a69a70a68 a66a72a71 a21a72a68 a66a17a65 a21a50a73a32a73a32a73a74a68 a66a80a75a72a77, where a68 a66 a19 is a set of words in a64 a66 which are translations of the words from a68 a65 a19.
3. For each worda17 of interest in a64 a19, create a vector a81a82 a1a79a69 a82 a71 a21 a82 a65 a21a50a73a50a73a50a73a26a21 a82 a75a80a77 such that: a82 a23a83a1 a22a24a84a78a85a14a86a50a87 a17a89a88 a11a33a90a28a91a31a92 a15a7a25 a0 a68 a23a40a0 where a17a89a88 a11a33a90a28a91a31a92 a15a7a25a93a1 a15a9a64 a4a14a90 a15a67a94a96a95 a15a18a17 a21a7a25a3a25a98a97a99a3a12a25a101a100a103a102a40a104a105a95 a15a7a25 a94a96a95 a15a18a17 a21a7a25a106a1 Term frequency (number of occurrences) ofa17 in the context1 ofa7 a102a107a104a11a95 a15a7a30a25a108a1 a109a37a64 a4a14a90 a63 a84 a110 a63 a84 a1 Number of occurrences of a7 in the corpus a110 a1 Maximum number of occurrences of any seedword in the corpus 4.
Given a pair of words a17 a19 and a17 a23, define a7a12a11a14a13a16a15a18a17 a19a21a3a17a24a23a26a25a111a1a113a112a54a114a2a115a12a15a44a81 a82 a19a21a111a81 a82 a23a25a111a1 a116 a117a72a118a33a119 a116 a117 a87 a120 a116 a117a72a118 a120a74a120 a116 a117 a87 a120 1For this work, the context of a word is defined to be the sentence that it appears in.
4 Experiment
Details 4.1 Ontologies The ontologies selected for alignment in this work were the American English WordNet (Miller et al., 1990) version 1.7, and the Mandarin Chinese HowNet (Dong, 1988).2 There are two main reasons why these particular two ontologies were chosen: they represent very different languages, and were constructed with very different approaches.
WordNet was constructed with what is commonly referred to as a differential theory of lexical semantics (Miller et al., 1990), which aims to differentiate word senses by grouping words into synonym sets (synsets), which are constructed as to allow a user to easily distinguish between different senses of a word.
HowNet, in contrast, was constructed following a constructive approach.
At the most atomic level is a set of almost 1500 basic definitions, or sememes, such as “human”, or “aValue” (attributevalue).
Higher-level concepts, or definitions, are composed of subsets of these sememes, sometimes with “pointers” that express certain kinds of relations, such as “agent” or “target”, and words are associated with the definition(s) that describe them.
For example, the word “a0 ” (scar) is associated with the definition “tracea0a2a1,#diseasea0a4a3 a5,#wounded a0a7a6a9a8 ”.
HowNet contains a total of almost 17000 definitions.
On average, each definition contained 6.5 Chinese words, with 45% of them containing only one word, and 10% of them containing more than 10 words.
Since the words within a definition are composed of the same sememe combination, HowNet definitions can be considered to be the equivalent of WordNet synsets.
A detailed structural comparison between HowNet and WordNet can be found in (Wong and Fung, 2002).
4.2 Supplementary
Dictionary To supplement the English translations included in HowNet, translations were included from CEDict, an open-source Chinese-English lexicon which was downloaded from the web.
The two lexicons were merged to create the final dictionary by iteratively grouping together Chinese words that shared English translations to create our a13 -to-a63 seedword 2The entries in HowNet are mainly in Chinese with a few English technical terms such as “ASCII”.
English translations are included for all the words and sememes.
translation groups.
The finalized dictionary is used to create seed word groups for building the contextual vectors.
First, the mappings in which none of the Chinese or English words appear in the corpus are filtered out.
Second, only the mappings in which all of the Chinese words appear in the same HowNet definition are kept.
The remaining 1975 mappings, which consist of an average of 2.0 Chinese words which map to an average of 2.2 English words, are used as seed word groups.
4.3 Corpora
The bilingual corpus from which the context vectors were constructed are extracted from newspaper articles from 1987–1992 of the American English Wall Street Journal and 1988–1996 of the Mandarin Chinese People’s Daily newspaper (a10a12a11a14a13a16a15 ).
The articles were sentence-delimited and a greedy maximum forward match algorithm was used with a lexicon which included all word entries in HowNet to perform word segmentation on the Chinese corpus.
On the English side, the same greedy maximum forward match algorithm is used in conjunction with a lexicon consisting of all word phrases found in WordNet to concatenate individual words into non-compositional compounds.
To ensure that we were working on well-formed, complete sentences, sentences which were shorter than 10 Chinese words or 15 English words were filtered out.
A set of sentences were then randomly picked to be included: the final corpus consisted of 15 million English words (540k sentences) and 11.6 Chinese words (390k sentences).
Finally, the English half of the corpus was part-of-speech tagged with fnTBL (Ngai and Florian, 2001), the fast adaptation of Brill’s transformation-based tagger (Brill, 1995).
It is important to note that the final corpus thus generated is not parallel or even comparable in nature.
To our knowledge, most of the previous work which utilizes bilingual corpora have involved corpora which were at least comparable in origin or content, if not parallel.
The only previous work that we are aware of which uses unrelated corpora is that of Rapp (1995), a study on word co-occurrence statistics in unrelated German and English corpora.
5 Experiments
To get a sense of the efficacy of our method, a test set of 160 HowNet definitions were randomly chosen as candidates for the test set.3 The Chinese words contained within the definitions were extracted, along with the corresponding English translations.
Two sets of context vectors, a0a2a1 and a0a4a3, can then be constructed for the Chinese words in the definition and their English translations.
Once these context vectors have been constructed, the similarities between the HowNet definitions and the WordNet synsets can be calculated according to the formulae in Section 2.
6 Results
To get a sense of the complexity of the problem, it is necessary to construct a reasonable baseline system from which to compare against.
For a baseline, all of the synsets that directly correspond to the English translations were extracted and enumerated.
Ties were broken randomly and the synset with the highest number of corresponding translations was selected as the alignment candidate.
Because there is no annotated data available for the evaluation, two judges who speak the languages involved were asked to hand-evaluate the resulting alignments, based on, firstly, the set of sememes that make up the definition, with the words that are contained in the definition only as a secondary aid.
Table 1 shows the overall performance of our algorithm, and Table 2 show the highest-scoring alignment mappings.
Correct Incorrect Accuracy Similarity 106 54 66.3% Baseline 94 66 58.8% Table 1: Overall Performance Figures In addition to the overall results, it is also interesting to examine the rankings of the alignment candidates for some of the more difficult HowNet definitions.
Table 3 shows an example definition and the candidate rankings.
This definition includes the words “population” and “number of people”, however, “number of people” was filtered out as it does not occur in WordNet as a single collocation, leaving only “population”, a noun with 6 senses in WordNet, to work with.
This example is a good illustration of the strength and power of the cross-lingual 3The original number of definitions chosen for the test set was higher.
However, upon inspection, it was found that a number had no corresponding WordNet synset and thus cannot be aligned.
The 160 are the ones which are left after the nonalignable definitions were filtered out.
word similarity calculation, as the system correctly identifies the first sense of “population” — “the people who inhabit a territory or state” — as the correct semantic sense of this particular definition from the Chinese words “a10a6a5 ” (number of human mouths), “a10a8a7 ” (number of people) and “a10a10a9 ” (number of human heads).
Another very good example of the algorithm’s strength can be found in the rankings for the HowNet definition “TakeAwaya0a12a11 a13,patient=family a0a15a14 ” (Table 4).
Again, the phrasal word translations “move house”, “change one’s residence”, “move to a better place”, etc were filtered out, leaving the single word “move”, which has a total of 16 senses as a verb in WordNet 1.7.
However, as the table shows, the algorithm correctly assigns the “change residence” sense of “move” to the HowNet definition, which is appropriate for the Chinese words it contains, which include “a11a16a14 ” (move house), “a17a19a18 ” (change one’s dwelling), and “a20 a17 ” (tear down one’s house and move).
7 Analysis
Even though the final goal of our work is to construct a full mapping from HowNet to WordNet, there will be quite a number of HowNet definitions which do not have a WordNet synset equivalent.
The reason is that given an arbitrary pair of languages, there will exist some words in one language which do not have a translation in the other language.
In the case of English and Chinese, many of the encountered problems came from Chinese idiomatic expressions, which are common in everyday usage and are considered to be single words, but do not usually translate to a single word in English.
In addition, the inherent difference in sense granularity and structure between any given two ontologies means that a full-scale mapping of synsets from one ontology to another will not usually be possible.
For example, HowNet’s “livestock” definition covers words which are as diverse as “cow”, “cat” and “dog”, while the finest-grained WordNet synset that covers all these definitions is a69 placental, placental mammal, eutherian, eutherian mammala77 . One of the most troublesome problems encountered in this work was in the selection of seedwords, which define set for the automatic lexicon induction.
If the seedwords occur so frequently in the corpus that other words co-occur with them too easily, they will provide little useful discriminatory information to the algorithm; but if they are too rare, they will HowNet definition WordNet Synset Similarity Correct? ceasea0 a1a3a2,content=discussa0a5a4a7a6 adjournment dissolution a8 termination 0.416 Y ending conclusion institutiona0a10a9a12a11,royala0a14a13,pasta0a16a15 government a8 system system of rules 0.401 Y quantitya0a10a17a3a18,amounta0a20a19a3a21, population a8 people 0.388 Y &humana0 a22 placea0a14a23a25a24,#humana0 a22 region part a8 location 0.358 Y institutiona0a10a9a12a11,policea0a27a26 police station police headquarters a8 0.349 Y station station house police office knowledgea0a10a28a25a29,entertainmenta0a5a30 art artistic creation artistic production 0.336 Y a8 creation creative activity knowledgea0a10a28a25a29,#mentala0a32a31a34a33 psychology psychological science a8 0.31 Y science scientific discipline agreementa0 a35a37a36 agreement accord a8 harmony accord 0.304 N concord concordance shoota0a32a38a34a39,sporta0a41a40a25a42 service serve a8 function work operate 0.287 N go run birda0a5a43,generica0a10a44a12a45 bird a8 vertebrate craniate 0.269 Y attributea0 a46a48a47,distancea0a10a49a51a50, distance a8 region part 0.268 Y &physicala0a10a52a51a53 placea0a14a23a25a24,capitala0a20a54a56a55, victoria a8 town 0.267 Y ProperNamea0a5a57,(Seychellesa0a16a58a60a59a3a61 ) suffera0a41a62a64a63,content=CauseAffecta0a32a65a60a66 catch a8 surprise 0.266 N replacea0a32a67a69a68,content=managea0a27a70a72a71 corkscrew spiral a8 turn 0.264 N Table 2: Top HowNet Definition to WordNet Synset alignments quantitya0a12a7a74a73,amounta0a76a75a74a77,&humana0 a10 WordNet synset Similarity population a78 people 0.388 population a78 group grouping 0.336 population a78 colonization colonisation settlement 0.218 Table 3: Population: a group of human inhabitants, or a group of organisms? not co-occur often enough with other words to be able to provide enough information, either.
This problem can be solved, however, by a better selection of seedwords, or, more easily, simply by using a bigger corpus to alleviate the sparse data problem.
A more serious problem was introduced by the comparability of the corpora involved in the experiment.
Even though both English and Chinese halves were extracted from news articles, the newspapers involved are very different in content and style: the People’s Daily is a government publication, written in a very terse and brief style, and does not concern itself much with non-government affairs.
The Wall Street Journal, on the other hand, caters to a much broader audience with a variety of news articles from all sources.
This creates a problem in the co-occurrence patterns of a word and its translations.
The assumption that word co-occurrence patterns tend to hold across language boundaries seems to be less valid with corpora that differ too much from each other.
An observation made during the experiments was some words occurred much more frequently (relative to the half of the corpus they were in) than their translated counterparts.
The result of this is that their context vectors may not be as similar as desired.
The difference in the co-occurrence patterns between the two halves of the corpora are best illustrated by a dotplot (Church, 1993).
The total term frequency (TF) of each seedword group is plotted against that of its translations.
Figure 1 shows the resulting dotplot.
If the two halves of the corpora were exact copies of each other, the frequencies of the seedwords would be equal and the points would therefore be aligned along the a79a79a1a81a80 diagonal.
The further the points diverge from the diagonal, the more different the two halves of the corpus are from each other.
This TakeAwaya0a12a11 a13,patient=familya0a15a14 WordNet synset Similarity move (Sense 4 of move — to change residence) 0.205 travel go move locomote 0.185 affect impress move strike 0.166 Table 4: Move: to change residence, to travel, or to touch? 1 10 100 1000 10000 100000 1e+06 1 10 100 1000 10000 100000 Word Frequencies -Wall Street Journal Word Frequencies -People’s Daily Figure 1: Seedword Group Occurrence Frequencies on People’s Daily and Wall Street Journal Corpora is quite obviously the case for this particular corpus — the overall point pattern is fan-shaped, with the diagonal only faintly discernible.
This suggests that the word usage patterns of the English and Chinese halves of the corpus are quite dissimilar to each other.
It is, of course, reasonable to ask why parallel or comparable corpora had not been used in the experiments.
The reason is, as noted in Section 2, that noncomparable corpora are easier to come by.
In fact, the only Chinese/English corpus of comparable origin that was available to us was the parallel Hong Kong News corpus, which is about half the size.
Furthermore, the word entries in HowNet were extracted from Mandarin Chinese corpora, which differs enough from the style of Chinese used in Hong Kong such that many words from HowNet do not exist in the Hong Kong News corpus.
Feasibility experiments with that corpus showed that many of the seedwords either did not occur, or did not co-occur with the words of interest, which resulted in sparse context vectors with only a few non-zero co-occurrence frequencies.
The result was that the similarity between many of the candidate WordNet synset-HowNet definition pairs was reduced to zero.
Despite all these problems, our method is successful at aligning some of the more difficult, single-word HowNet definitions to appropriate WordNet synsets, thus creating a partial mapping between two ontologies with very different structures from very different languages.
The method is completely unsupervised and therefore cheap on resource requirement — it does not use any annotated data, and the only resource that it requires — beyond the ontologies that are to be aligned — is a bilingual machine-readable dictionary, which can usually be obtained for free or at very low cost.
8 Previous
Work The preceding sections mentioned some previous and related work that targets the same problem, or some of its subproblems.
This section takes a closer look at some other related work.
There has been some interest in aligning ontologies.
Dorr et al.(2000) and Palmer and Wu (1995) focused on HowNet verbs and used thematic-role information to align them to verbs in an existing classification of English verbs called EVCA (Levin, 1993).
Asanoma (2001) used structural link information to align nouns from WordNet to an existing Japanese ontology called Goi-Taikei via the Japanese WordNet, which was constructed by manual translation of a subset of WordNet nouns.
There has also been a lot of work involving bilingual corpora, including the IBM Candide project (Brown et al., 1990), which used statistical data to align words in sentence pairs from parallel corpora in an unsupervised fashion through the EM algorithm; Church (1993) used character frequencies to align words in a parallel corpus; Smadja et al.(1996) used cooccurrence functions to extract phrasal collocations for translation, and Melamed (1997) identified non-compositional compounds by comparing the objective functions of a translation model with and without NCCs.
The calculation of word semantic similarity scores is also a problem that has attracted a lot of interest.
The numerous notable approaches can usually be divided into those which utilize the hierarchical information from an ontology, such as Resnik (1995) and Agirre and Martinez (2002); and those which simply use word distribution information from a large corpus, such as Lin (1998) and Lee (1999).
9 Conclusion
This paper represents a first step towards a corpusbased approach for cross-lingual identification of word concepts and alignment of ontologies.
The method borrows from techniques used in machine translation and information retrieval, and does not make any assumptions about the structure of the ontology, or use any but the most basic structural information.
Therefore it is capable of performing alignments across ontologies of vastly different structure.
In addition, our method does not require the use of parallel or even comparable corpora, making the task of data acquisition far easier.
We demonstrate the effectiveness of our method by performing a partial mapping of HowNet and WordNet, two very different ontologies from very different languages.
Our method is successful at mapping a number of HowNet definitions — including some fairly difficult ones — to the correct WordNet synsets.
10 Acknowledgements
The authors would like to thank researchers at Intendi Inc.
— Ping-Wai Wong for help on HowNet construction and structure, Chi-Shun Cheung and Chi-Yuen Ma for assistance in resource preparation, as well as the three anonymous reviewers for their useful comments and suggestions.

