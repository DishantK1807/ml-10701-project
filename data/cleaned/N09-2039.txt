Proceedings of NAACL HLT 2009: Short Papers, pages 153–156,
Boulder, Colorado, June 2009. c 2009 Association for Computational Linguistics
Name Perplexity 
 
Octavian Popescu 
 
 
 
 
 
 
Abstract 
The acuracy of a Cros Document Corefer-
ence system depends on the amount of context 
available, which is a parameter that varies 
greatly from corpora to corpora. This paper 
presents a statistical model for computing 
name perplexity clases. For each perplexity 
clas, the prior probability of coreference is 
estimated. The amount of context required for 
coreference is controled by the prior corefer-
ence probability. We show that the prior prob-
ability coreference is an important factor for 
maintaining a god balance betwen precision 
and recal for cros document coreference sys-
tems. 
1 Introduction 
The Person Cros Document Coreference (PCDC) 
task which requires that al and only the textual 
mentions of an entity of type Person be individu-
ated in a large colection of text documents, is a 
chalenging tasks for natural language procesing 
systems (Grishman 194). A PCDC system must 
be able to use the information existing in the cor-
pus in order to asign to each person name mention 
(PNM) a piece of context relevant for coreference. 
In many cases, the contextual information relevant 
for coreference is very scarce or embeded in se-
mantic and ontological dep inferences, which are 
dificult to program, anyway.  
Unlike in other disambiguation tasks, like word 
sense disambiguation for instance, where the dis-
tribution of relevant contexts is mainly regulated 
by strong syntactic rules, in PCDC the relevance of 
contexts is a mater of interdependency. To exem-
plify, consider the name “John Smith” and an or-
ganization, say “U.N.”. The context “works for 
U.N.” is a relevant coreference context for “John 
Smith” if there is just one person named John 
Smith working for U.N.; if there are two or more 
John Smiths working for U.N., then  “works for 
U.N.” is no longer a relevant context for corefer-
ence. For the PCDC task, the relevance of the con-
text depends to a great extent on the diversity of 
the corpus itself, rather than on the specific rela-
tionship that exists betwen “John Smith” and 
“works for U.N.”. 
Valid coreference can be realized when a large 
amount of information is available. However, the 
requirement that only contextualy provable 
coreferences be realized is to strong; the required 
relevant context is not actualy explicitly found in 
the text in at least 60% of the times (Popescu 
2007). 
 
This paper presents a statistical technique devel-
oped to give a PCDC system more information 
regarding the probability of a corect coreference, 
without performing dep semantic and ontological 
analyses. If a PCDC system knows that the prior 
probability for two PNMs to corefer is high, then 
the amount of contextual evidence required can be 
lowered and vice-versa. Our goal is to precisely 
define a statistical model in which the prior 
coreference probabilities can be computed and, 
consequently, to design a PCDC system that dy-
namicaly revises the context relevance acord-
ingly. 
We review the PCDC literature relevant for our 
purposes, present the statistical model and show 
the preliminary results. The paper ends with the 
Conclusion and Further Research section. 
2 Related Work 
In a clasical paper (Baga 198), a PCDC system 
based on the vector space model (VSM) is pro-
posed. While there are many advantages in repre-
senting the context as vectors on which a similarity 
function is aplied, it has ben shown that there are 
153
inherent limitations asociated with the vectorial 
model (Popescu 208). These problems, related to 
the density in the vectorial space (superposition) 
and to the discriminative power of the similarity 
power (masking), become visible as more cases are 
considered. (Goi, 204), testing the system on 
many names, empiricaly observes the variance in 
the results obtained by the same PCDC system. 
Inded, considering just the sentence level context, 
which is a strong requirement for establishing 
coreference, a PCDC system obtains a god score 
for “John Smith”. This is because the probability 
of coreference of any two “John Smith” mentions 
is low. But, as the relevant context is often outside 
the sentence containing the mention, for other 
types of names the same system is not acurate. If 
it considers, for instance, “Barack Obama”, the 
same system obtains a very low recal, as the prob-
ability of any two “Barack Obama” mentions to 
corefer is very high. Without further adjustments, a 
vectorial model canot resolve the problem of con-
sidering to much or to litle contextual evidence 
in order to obtain a god precision for “John 
Smith” and simultaneously a god recal for “Ba-
rack Obama”. 
 
The relationship betwen the prior probabilities 
and the acuracy of a system is also empiricaly 
noted in (Pederson 205). In their experiment, the 
authors note that having in the input of the system 
the corect number of persons carying the same 
name is likely to hurt the results of a system based 
on bigrams. This hapens because the amount of 
context is staticaly considered. The variance in the 
results obtained by a PCDC system has ben noted 
also in (Lefever 207, Popescu 207). 
 
In order to improve the performances of PCDC 
systems based on VSM, some authors have fo-
cused on methods that alow a beter analysis of 
the context (Ng 207) combined with a cascade 
clustering technique (Wei 206), or have relied on 
advanced clustering techniques (Chen 206). 
 
The technique we present in the next section is 
complementary to these aproaches. We propose a 
statistical model designed to ofer to the PCDC 
systems information regarding the distribution of 
PNMs in the corpus. This information is used to 
reduce the contextual data variation and to atain a 
god balance betwen precision and recal. 
3 Name
Perplexity Classes 
The amount of contextual information required for 
the coreference of two or more PNMs depends on 
several factors. Our working hypothesis is that we 
can compute a prior probability of coreference for 
each name and use this probability to control the 
amount of contextual evidence required. Let us 
recal the “John Smith” and “Barack Obama” ex-
ample from the previous section. Both “John” and 
“Smith” are American comon first and last 
names. The chance that many diferent persons 
cary this name is high. On the other hand, as both 
“Barack” and “Obama” are rare American first and 
last names respectively, almost surely many men-
tions of this name refer only to one person. The 
argument above does not depend on the context, 
but just on the prior estimation of the usage of 
those names. Computing an estimation of a name’s 
frequency clas, we may decrease or increase the 
amount of contextual evidence neded acordingly.  
 
To each one-token name we asociate the num-
ber of diferent tokens with which it forms a PNM 
in the corpus. For example, for “John” we can have 
the set “Smith”, “F. Kenedy”, “Travolta” etc. We 
cal this number the perplexity of a one-token 
name. The perplexity gives a direct estimation of 
the ambiguity of a name in the corpus. In Table 1 
we present the relationship betwen the number of 
ocurences (in intervals, in the first column) and 
the average perplexity (second column). The fig-
ures reported here, as wel as those in the next Sec-
tion, come from the investigation of the 
Adige50k, an Italian news corpus (Magnini 
2006). 
ocurences (interval) average perplexity 
1-5 4.13 
6-20 8.34 
21-100 17.4 
101-1,00 68.54 
1,00-5,00 683.95 
5,00-31,091 478.23 
Table 1. Average perplexity one-token names 
We divide the clas of one-token names in 5 
categories acording to their perplexity: very low, 
low, medium, high and very high. It is useful to 
kep separate the first and the last names. It has 
ben shown that the average perplexity is three 
times lower for last names than for first names 
154
(Popescu 207). Therefore, the first and last names 
perplexities play diferent roles in establishing the 
prior probability of coreference. The perplexity 
clas of two-token names is computed using the 
folowing heuristics: the perplexity clas of two-
token names is the average clas of the perplexity 
of the one-token names composing it. If the per-
plexity clases of the one-token names are the 
same, then the perplexity of the whole name is one 
clas les (if posible). 
The perplexity clases represent a partition of 
the name population; each name belongs to one 
and only one clas. In establishing the border be-
twen two consecutive perplexity clases, we want 
to maximize the confidence that inside each stra-
tum the prior coreference probability has a low 
variance. 
The relationship betwen the perplexity clases 
and the prior coreference probability is straight-
forward. The lower the perplexity, the greater the 
coreference probability, and, therefore, the lower 
the amount of relevant context required for 
coreference. 
In order to decide the percentage of the name 
population that goes into each of the perplexity 
clases, we use a distributional fre statistics 
method. In this way we can compute the confi-
dence of the prior conference probability estimates. 
 
We introduce two random variables: X, a ran-
dom variable defined over the name population 
and Y, which represents the number of diferent 
persons carying the same name. Let X
1,…,X
n
 be a 
random sample of names from one perplexity 
clas, and let Y
1,…,Y
n
 be the coresponding values 
denoting the number of persons that cary the 
names X
1,…,X
n
. The indices have ben chosen 
such that Y
1
…,Y
n
 is an ordered statistics: 
Y
1
≤Y
2
≤…≤Y
n
. Let F be the distribution function of 
Y. And let p be a given probability. If F(Y
j
)-F(Y
i
) 
≥ p, then at least 10p percent of the probability 
distribution is betwen Y
i
 and Y
j
; it means that 
γ = P[F(Y
j
)-F(Y
i
)] ≥ p                (1) 
is the probability that the interval (Y
i, 
Y
j
) contains 
10p percent of the Y values. 
In our case, γ is the confidence of the estimation 
that 10p percent of names from a certain perplex-
ity clas have the expected prior coreference prob-
ability in a given interval. 
The γ probability is computed with the formula: 
γ = P(F(Y
j
) – F(Y
i
) < p) =   
1-∫
0
p 
Γ(n+1)/( Γ(j-i) Γ(n-j+i+1)x
j-i-1
(1-x)
n-j+i
dx   (2) 
where Γ is the extension of the factorial function, 
Γ(x) = ∫
0
∝  
t
x-1
e
-t
dt. 
 
In practice, we start with an interval that repre-
sents the prior coreference probability desired for 
that perplexity clas. For example we want to be γ 
= 80% sure that p = 90% of the two-token names 
in the “very low” perplexity clas are names car-
ried by a maximum of 2 persons. We chose a ran-
dom sample of two-token names from that 
perplexity clas, the size of the random sample be-
ing determined by γ and p – se equation (2). If the 
random sample satisfies (1) then we have the de-
sired perplexity clas. If not, the one-token names 
that have the highest perplexity and were consid-
ered “very low” are excluded – they are asigned 
to the next perplexity clas and the computation 
is re made. 
 
In a preliminary experiment, using a sample of 
25 two-token names from a part of the Adige50k 
corpus spaning two years, we have obtained the 
perplexity clases listed in Tables 2 and 3. In 
Adige 50k there are 106, 192 diferent one-token 
names, which combine into 429, 251 diferent two-
token names and 36, 73 thre-token names. 
perplexity clas percentage 
very high 5.3% 
High 8.7 
Medium 20.9% 
Low 27.6% 
very low 37.5 
Table 2. First Name perplexity clases 
perplexity clas percentage 
very high 1.8% 
High 3.36% 
Medium 17.51 
Low 20.31% 
very low 57.02% 
Table 3. Last Name perplexity clases 
The perplexity clas of two-token names is 
computed as specified in the first paragraph of this 
page. In aproximately 60% of the cases, a two-
token name has a “low”, or “very low” perplexity 
clas. If a PCDC system computes the context 
155
similarity based on words with special properties 
or on named entities, in general at least four simi-
larities must be detected betwen two contexts in 
order to have a safe coreference. Our preliminary 
results show that corefering on the basis of just 
one special word and one named entity for those 
names in “low” or “very low” does not lose more 
than 1,5% in precision, while it gains up to 40% in 
recal for these cases. On the other hand, for “very 
high” perplexity two-token names we were able to 
increase precision by requiring a stronger similar-
ity betwen contexts.  
The gain of using prior coreference probabilities 
determined by the perplexity clases is important, 
especialy for those names that are situated at the 
extreme: “very low” perplexity with a big number 
of ocurences and “very high” with a smal num-
ber of ocurences. These cases establish the inter-
val for the amount of contextual similarity required 
for coreference. 
However, the problematic cases remain when 
the perplexity clas is “very high” and the number 
of ocurences is very big. 
4 Conclusion and Further Research 
We have presented a distributional fre statistical 
method to design a name perplexity system, such 
that each perplexity clas maximizes the number of 
names for which the prior coreference belongs to 
the same interval. This information helps the 
PCDC systems to lower/increase adequately the 
amount of contextual evidence required for 
coreference. 
In our preliminary experiment we have observed 
that we can adequately reduce the amount of con-
textual evidence required for the coreference of 
“low” and “very low” perplexity clas. For the top 
perplexity clas names the requirement for extra 
contextual evidence has increased the precision. 
The aproach presented here is efective in deal-
ing with the problems raised by using a similarity 
metrics on contextual vectors. It gives a direct way 
of identifying the most problematic cases for 
coreference. Solving these cases represents our 
first objective for the future. 
We plan to increase the number of cases consid-
ered in the sample required to delimit the perplex-
ity clases. The equation (2) may be developed 
further in order to obtain exactly the number of 
required cases for each perplexity clas. 
References 
A. Baga, B. Baldwin.198. Entity-based Cros-
Document Co-referencing using the Vector Space 
Model, In Procedings ACL. 
J. Chen, D. Ji, C. Tan, Z. Niu.206. Unsupervised 
Relation Disambiguation Using Spectral Cluster­
ing, In Procedings of COLING 
C. Goi, J. Alan.204. Cros-Document Corefer-
ence on a Large Scale Corpus, in Proceding ACL. 
R. Grishman.194. Whither Writen Language 
Evaluation? In procedings Human Language 
Technology Workshop, 120-125. San Mateor. 
E. Lefever, V. Hoste, F. Timur.207. AUG: A 
Combined Clasification and Clustering Aproach 
for Web People Disambiguation, In Procedings of 
SemEval 
B. Magnini, M. Speranza, M. Negri, L. Romano, 
R. Sprugnoli. 206.I-CAB – the Italian Content 
Anotation Bank. LREC 206 
V., Ng.207. Shalow Semantics for Coreference 
Resolution, In Procedings of IJCAI 
T. Pedersen, A. Purandare, A. Kulkarni. 205. 
Name Discrimination by Clustering Similar Con-
texts, in Proceding of CICLING 
O. Popescu, C. Girardi, 208, Improving Cros 
Document Coreference, in Procedings of JADT 
O. Popescu, B. Magnini.207, Infering Corefer-
ence among Person Names in a Large Corpus of 
News Colection, in Procedings of AIA 
O. Popescu, B. Magnini.207. Irst-bp: WePS using 
Named Entities, In Procedings of SEMEVAL 
O. Popescu, M. Magnini, L. Serafini, A. Tamilin, 
M. Speranza.206. From Mention to Ontology: a 
Pilot Study, in Procedings of SWAP 
Y. Wei, M. Lin, H. Chen.206. Name Disambigua­
tion in Person Information Mining, In Proced‐
ings of IEE 
 
156

