Using a maximum entropy approach based on a modification of the system described by Low, Ng, and Guo (2005), our system was able to achieve a respectable level of accuracy when evaluated on the corpora of the word segmentation task of the Third International Chinese Language Processing Bakeoff.
I05-3025
Jacobs Department of Linguistics University of Texas at Austin 1 University Station B5100 Austin, TX 78712-0198 USA aaronjacobs@mail.utexas.edu Yuk Wah Wong Department of Computer Sciences University of Texas at Austin 1 University Station C0500 Austin, TX 78712-0233 USA ywwong@cs.utexas.edu Abstract We extended the work of Low, Ng, and Guo (2005) to create a Chinese word segmentation system based upon a maximum entropy statistical model.
I05-3025
Much of this can be attributedtothevalueofusinganexternaldictionary and additional training data, as illustrated by the experiments run by Low et al.(2005) with their model.
I05-3025
Later testing with the gold-standard data revealed that while the additions we made to Low et al.’s system helped our results for the 2005 data with which we experimented during development, a number of them actually hurt our scores for this year’s corpora.
I05-3025
It should be noted that in our testing during development,evenwhenwestrovetocreateasystem which matched as closely as possible the one described by Low et al.(2005), we were unable to achieve scores for the 2005 bakeoff data as high as their system did.
I05-3025
Our Chinese word segmenter is a modification of the system described by Low et al.(2005), which they entered in the 2005 Second International Chinese Word Segmentation Bakeoff.
I05-3025
