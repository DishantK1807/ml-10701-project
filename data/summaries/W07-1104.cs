Joshi, Supertagging: an approach to almost parsing, Computational Linguistics, v.25 n.2, p.237-265, June 1999 19 Manfred Stede, A generative perspective on verb alternations, Computational Linguistics, v.24 n.3, September 1998
P98-2127 P99-1041
Most progress has been made especially in the field of multiword identification (Villada Moiron and Tiedemann, 2006; Katz and Giesbrecht, 2006; Zhang et al., 2006).
W06-1203
1), p.722-727, October 1994, Seattle, Washington, United States 5 Eric Brill, Transformation-based error-driven learning and natural language processing: a case study in part-of-speech tagging, Computational Linguistics, v.21 n.4, p.543-565, December 1995 6 Bueno et al.(1999), F.
P99-1041
We take the semantic category of a noun to be the ancestor of its first sense in the hypernym hierarchy of WordNet 2.1, cut at the level of the children 1Our preliminary experiments on development data from Fazly and Stevenson (2006) revealed that the cosine measure and a window size of 5 words resulted in the best performance.
E06-1043
Resnik, David Yarowsky, Distinguishing systems and distinguishing senses: new evaluation methods for Word Sense Disambiguation, Natural Language Engineering, v.5 n.2, p.113-133, June 1999 11 Hinrich Schtze, Automatic word sense discrimination, Computational Linguistics, v.24 n.1, March 1998
P98-2127
The work most similar to ours is that of Venkatapathy and Joshi (2005).
H05-1113
Work Much recent work on classifying MWEs focuses on determining different levels of compositionality in verb+particle combinations using a measure of distributional similarity (McCarthy et al., 2003; Baldwin et al., 2003; Bannard et al., 2003).
W03-1810 W03-1812
subcategorization, lexical classes) can now be acquired automatically from parsed data (McCarthy and Carroll, 2003; Schulte im Walde, 2006; Preiss et al., 2007).
W03-1810
They are quite comparable to that obtained by Wu (1994) using a manual Chinese-English parallel corpus.
P94-1019
22 Ruslan Mitkov, Robust pronoun resolution with limited knowledge, Proceedings of the 36th annual meeting on Association for Computational Linguistics, p.869-875, August 10-14, 1998, Montreal, Quebec, Canada 23 Ruslan Mitkov, Multilingual Anaphora Resolution, Machine Translation, v.14 n.3-4, p.281-299, December 1999 24 Wee Meng Soon, Hwee Tou Ng., and Chung Yong Lim.
P98-2127
Approaches evaluated so far make use of dictionaries with semantic annotation (Piao et al., 2006), WordNet (Pearce, 2001), automatically generated thesauri (Lin, 1999; McCarthy et al., 2003; Fazly and Stevenson, 2006), vector-based methods that measure semantic distance (Baldwin et al., 2003; Katz and Giesbrecht, 2006), translations extracted from parallel corpora (Villada Moiron and Tiedemann, 2006) or hybrid methods that use machine learning techniques informed by features coded using some of the above methods (Venkatapathy and Joshi, 2005).
E06-1043 H05-1113 P99-1041 W03-1810 W03-1812 W06-1201 W06-1202 W06-1203
The small amount of previous work on the identification of syntactic fixedness (Wermter and Hahn (2004), Fazly and Stevenson (2006)) has either focused on a single variation variety, or has only been evaluated for combinations of a small preselected list of words, presumably due to noise.
E06-1043
Like Lin (1999), we generate lexical variants of the target automatically by replacing either the verb or the noun constituent by a semantically similar word from the automatically-built thesaurus of Lin (1998).
P98-2127 P99-1041
O'Hara, Development and use of a gold-standard data set for subjectivity classifications, Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, p.246-253, June 20-26, 1999, College Park, Maryland 10 Janyce Wiebe, Rebecca Bruce, Matthew Bell, Melanie Martin, Theresa Wilson, A corpus study of evaluative and speculative language, Proceedings of the Second SIGdial Workshop on Discourse and Dialogue, p.1-10, September 01-02, 2001, Aalborg, Denmark 11 J.
P99-1041
Schtze, Automatic word sense discrimination, Computational Linguistics, v.24 n.1, March 1998 14 Mark Stevenson, Yorick Wilks, The interaction of knowledge sources in word sense disambiguation, Computational Linguistics, v.27 n.3, p.321-349, September 2001 15 Michael Sussna, Word sense disambiguation for free-text indexing using a massive semantic network, Proceedings of the second international conference on Information and knowledge management, p.67-74, November 01-05, 1993, Washington, D.C., United States 16 Hans van Halteren, Jakub Zavrel, Walter Daelemans, Improving data driven wordclass tagging by system combination, Proceedings of the 36th annual meeting on Association for Computational Linguistics, August 10-14, 1998, Montreal, Quebec, Canada 17 Ellen M.
P98-2127
Only a few such corpora exist, including the Hansard English-French corpus and the HKUST EnglishChinese corpus (Wu, 1994).
P94-1019
During development, several other clustering algorithms and parameters have been tested, but the settings described above gave us the best EuroWordNet similarity score (using Wu and Palmer (1994)).
P94-1019
Dependency trees are provided by the MINIPAR dependency parser (Lin, 1998).
P98-2127
References 1 Ted Briscoe, Ann Copestake, Lexical rules in constraint-based grammars, Computational Linguistics, v.25 n.4, p.487-526, December 1999 2 Jean Carletta, Assessing agreement on classification tasks: the kappa statistic, Computational Linguistics, v.22 n.2, June 1996 3 M.
P99-1041
The logic of typed feature structures (Carpenter, 1992) has been widely used as a means of formalizing and developing natural language grammars that support computationally efficient parsing, generation and SLD resolution, notably grammars within the Head-driven Phrase Structure Grammar (HPSG) framework, as evidenced by the recent successful development of the LinGO reference grammar for English (LinGO, 1999).
P99-1041
3 WordNet-Based Similarity Measures We also examine the following measures of semantic similarity which are WordNet-based.3 Wu and Palmer (1994) propose a measure of similarity of two concepts c1 and c2 based on the depth of concepts in the WordNet hierarchy.
P94-1019
Allen, Speech repairs, intonational phrases, and discourse markers: modeling speakers' utterances in spoken dialogue, Computational Linguistics, v.25 n.4, p.527-571, December 1999 14 Arne Jnsson, Nils Dahlbck, Distilling dialogues: a method using natural dialogue corpora for dialogue systems development, Proceedings of the sixth conference on Applied natural language processing, p.44-51, April 29-May 04, 2000, Seattle, Washington 15 Michael Kipp.
P99-1041
However, dependency schemes have been suggested to be preferable in IE, as they represent the semantic structure of the sentences more directly (see, e.g., de Marneffe et al.(2006)). Further, Lin (1998) argues for dependency-based evaluation of both dependency and constituency parsers since it allows evaluation metrics that are more relevant to semantic interpretation as well as intuitively more meaningful.
P98-2127
In Proceedings of the 36th Annual Meeting of the ACL and the 17th International Conference on Computational Linguistics (COLING-ACL 1998), pages 14931495, Montreal, Canada.
P98-2127
Harabagiu, Dan Moldovan, Marius Pasca, Rada Mihalcea, Mihai Surdeanu, Razvan Bunescu, Roxana Grju, Vasile Rus, Paul Morarescu, The role of lexico-semantic feedback in open-domain textual question-answering, Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, p.282-289, July 06-11, 2001, Toulouse, France 4 Irene Langkilde, Kevin Knight, Generation that exploits corpus-based statistical knowledge, Proceedings of the 36th annual meeting on Association for Computational Linguistics, August 10-14, 1998, Montreal, Quebec, Canada 5 Dekang Lin, Automatic retrieval and clustering of similar words, Proceedings of the 17th international conference on Computational linguistics, p.768-774, August 10-14, 1998, Montreal, Quebec, Canada 6 Steven Lytinen, Noriko Tomuro, and Tom Repede.
P98-2127
Finally, there are approaches focused on the memory demands imposed by ?distance??between dependent words (Lin, 1996; Gibson, 1998).
P98-2127
Courcier. 13 Lillian Lee, Measures of distributional similarity, Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, p.25-32, June 20-26, 1999, College Park, Maryland 14 Lillian Lee.
P99-1041
This is because identical words occur more often in coordinate head words than in other lexical dependencies (there were 43 pairs with identical words in the coordination set, compared to 3 such pairs in the 150 SimTest ncoord xcoord SDcoord nnonCoord xnonCoord SDnonCoord 95% CI p-value coordDistrib 503 0.11 0.13 485 0.06 0.09 [0.04 0.07] 0.000 (Resnik, 1995) 444 3.19 2.33 396 2.43 2.10 [0.46 1.06] 0.000 (Lin, 1998) 444 0.27 0.26 396 0.19 0.22 [0.04 0.11] 0.000 (Jiang and Conrath, 1997) 444 0.13 0.65 395 0.07 0.08 [-0.01 0.11] 0.083 (Wu and Palmer, 1994) 444 0.63 0.19 396 0.55 0.19 [0.06 0.11] 0.000 (Leacock and Chodorow, 1998) 444 1.72 0.51 396 1.52 0.47 [0.13 0.27] 0.000 (Hirst and St-Onge, 1998) 459 1.599 2.03 447 1.09 1.87 [0.25 0.76] 0.000 (Banerjee and Pedersen, 2003) 451 114.12 317.18 436 82.20 168.21 [-1.08 64.92] 0.058 (Patwardhan and Pedersen, 2006) 459 0.67 0.18 447 0.66 0.2 [-0.02 0.03] 0.545 random 483 0.89 0.17 447 0.88 0.18 [-0.02 0.02] 0.859 Table 1: Summary statistics for 9 different word similarity measures (plus one random measure):ncoord and nnonCoord are the sample sizes for the coordinate and non-coordinate noun pairs samples, respectively; xcoord, SDcoord and xnonCoord, SDnonCoord are the sample means and standard deviations for the two sets.
P94-1019 P98-2127
Moldovan, A method for word sense disambiguation of unrestricted text, Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, p.152-158, June 20-26, 1999, College Park, Maryland 8 George Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine Miller.
P99-1041
Their method uses Lin?s (1998) automatically generated thesaurus to compute a metric of lexical fixedness.
P98-2127
Fazly and Stevenson (2006) use lexical and syntactic fixedness as partial indicators of noncompositionality.
E06-1043
Both Lin (1996) and Gibson (1998) showed that human performance on sentence processing tasks could be predicted with measures of this sort.
P98-2127
Fazly and Stevenson (2006) propose a measure for detecting the syntactic fixedness of English verb phrases of the same variety as us.
E06-1043
Nonetheless, the mentioned characteristics are useful indicators to distinguish literal and idiomatic expressions (Fazly and Stevenson, 2006).
E06-1043
In Proceedings of the 17th International Conference on Computational Linguistics: COLING-98, pages 867--880, 1998.
P98-2127
The potential MWEs that are extracted with the fully unsupervised method described above and with Fazly and Stevenson?s (2006) method (FS from here onwards) are automatically evaluated by comparing the extracted list to handcrafted MWE databases.
E06-1043
Building on Lin (1998), McCarthy et al.(2003) measure the semantic similarity between expressions (verb particles) as a whole and their component words (verb).
P98-2127 W03-1810
For example, it would be helpful to consider strong correspondence between certain English and Chinese words, as in (Wu, 1994).
P94-1019
Baldwin (2006) characterizes the idiosyncratic behavior of MWEs as ?a lack of compositionality manifest at different levels of analysis, namely, lexical, morphological, syntactic, semantic, pragmatic and statistical??
W06-1201
Similar to Lin (1999), McCarthy et al.(2003) and Fazly and Stevenson (2006), our method makes use of automatically generated thesauri; the technique used to compile the thesauri differs from previous work.
E06-1043 P99-1041 W03-1810
This is especially interesting since much previous research has focused solely on the non-compositionality of MWEs to identify them (McCarthy et al., 2003; Baldwin et al., 2003; Bannard et al., 2003).
W03-1810 W03-1812
Prszky, Balzs Kis, A unification-based approach to morpho-syntactic parsing of agglutinative and other (highly) inflectional languages, Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, p.261-268, June 20-26, 1999, College Park, Maryland 13 Bharati, Akshar, Chaitanya, Vineet and Sangal, Rajeev, 1995 Natural Language Processing A Paninian Perspective.
P99-1041
Combining our semantics-based approach with other extraction techniques such as the syntactic fixedness measure proposed by Fazly and Stevenson (2006) might improve the results significantly.
E06-1043
This measure is similar to Lin?s (1999) proposal for finding noncompositional phrases.
P99-1041
25--30. 8 Andrei Mikheev, Marc Moens, Claire Grover, Named Entity recognition without gazetteers, Proceedings of the ninth conference on European chapter of the Association for Computational Linguistics, June 08-12, 1999, Bergen, Norway 9 Raymond J.
P99-1041
& Evaluation 4.1 Quantitative evaluation In this section, we quantitatively evaluate our method, and compare it to the lexical and syntactic fixedness measures proposed by Fazly and Stevenson (2006).
E06-1043
W S J 8 80923 0163 DocumentIDs Un i g r a m  C o o cc u rre n c e  Sc o r e s B1 MaxH T CMPRH1 CMPRH2 CMPRH3 FT Linear(B1) Linear(MaxH) Linear(T) Linear(FT) Figure9.DUC2001singledocinter-human,baseline,andsystemunigramco-occurrencescoreversus compressionratio.DocumentIDsaresortedbyincreasingcompressionratioCMPRH1.  much.Theunigramco-occurrencescoresforB1,T,and MaxHwerenoisybuthad ageneraltrend(Linear B1, Linear T, and Linear MaxH) of drifting into lower performance when compression ratio increased (i.e. when summaries became shorter); while the performance of FT did not exhibit a similar trend. This confirms our earlier hypothesis that humans are less likely to agree at high compression ratio and system performancewillalsosufferathighcompressionratio. TheconstancyofFTacrossdifferentcompressionratios is reasonable since FT scores should only depend on how well the unigram co-occurrence scoring method capturescontentoverlapbetweenafulltextanditsreference summaries and how likely humans use vocabularyoutsidetheoriginaldocument. 6 Conclusions In this paper we presented an empirical study of the potential and limitations of sentence extraction as a method of automatic text summarization. We showed thefollowing: (1) Howtouseoracleextractstoestimatetheperformance upper bound of sentence extraction methodsatdifferentextractlengths.Weunderstandthatsummariesoptimizedusingunigram co-occurrence score do not guarantee good quality in terms of coherence, cohesion, and overallorganization.However,wewouldargue thatagoodsummarydoesrequiregoodcontent andwewillleavehowtomakethecontentcohesive, coherent, and organized to future research. (2) Inter-humanagreementvariedalotandthedifferencebetweenmaximumagreement(MaxH) and minimum agreement (MinH) was about 18%ontheDUC2001data.Tominimizethe gap,weneedtodefinethesummarizationtask better. This has been addressed by providing guided summarization tasks in DUC 2003 (DUC 2002). We guesstimate the gap should besmallerinDUC2003data. (3) State-of-the-artsystemsperformedatthesame levelasthebaselinesystembutwerestillabout 10% away from the average human performance. (4) The potential performance gains (15% from E100 to E150 and 24% to FT) estimated by oracleextractsofdifferentsizesindicatedthat sentence compression or sub-sentence extractionarepromisingfuturedirections. (5) Therelativeperformanceofhumansandoracle extracts at three inter-human agreement intervalsshowedthatitwasonlymeaningfultooptimize sentence extraction algorithms if interhuman agreement was high. Although overall highinter-humanagreementwaslowbutsubsets of high inter-human agreement did exist. For example, about human achieved at least 60% agreement in 59 out of 303 (~19%) documentsof30sentencesorless. (6) We also studied how compression ratio affectedinter-humanagreementandsystemperformance, and the results supported our hypothesis that humans tend to agree less at high compression ratio, and similar between humansandsystems.Howtotakeintoaccount thisfactorinfuturesummarizationevaluations isaninterestingtopictopursuefurther. Usingexhaustivesearchtoidentifyoracleextractionhas beenstudiedbyotherresearchersbutindifferentcontexts.Marcu(1999a)suggestedusingexhaustivesearch tocreatetrainingextractsfromabstracts.Donawayetal. (2000)usedexhaustivesearchtogenerateallthreesentencesextractstoevaluatedifferentevaluationmetrics. Themaindifferencebetweentheirworkandoursisthat we searched for extracts of a fixed number of words whiletheylookedforextractsofafixednumberofsentences. Inthefuture,wewouldliketoapplyasimilarmethodologytodifferenttextunits,forexample,sub-sentence unitssuchaselementarydiscourseunit(Marcu1999b). We wanttostudyhowtoconstrainthesummarization task to achieve higher inter-human agreement, train sentence extraction algorithms using oracle extracts at different compression sizes, and explore compression techniquestogobeyondsimplesentenceextraction. References Donaway, R.L., Drummey, K.W., and Mather, L.A. 2000. A Comparison of Rankings Produced by Summarization Evaluation Measures. In Proceeding oftheWorkshoponAutomaticSummarization,postconferenceworkshopofANLP-NAACL-2000,Seattle,WA,USA,6978. DUC.2002.TheDocumentUnderstandingConference. http://duc.nist.gov. Edmundson, H.P. 1969. New Methods in Automatic Abstracting.JournaloftheAssociationforComputingMachinery.16(2). Goldstein, J., M. Kantrowitz, V. Mittal, and J. Carbonell. 1999. Summarizing Text Documents: Sentence Selection and Evaluation Metrics. In Proceedingsofthe22ndInternationalACMConference on Research and Development in Information Retrieval(SIGIR-99),Berkeley,CA,USA,121128. Hovy,E.andC.-Y.Lin.1999.AutomaticTextSummarization in SUMMARIST. In I. Mani and M. Maybury (eds), Advances in Automatic Text Summarization,8194.MITPress. Kupiec,J.,J.Pederson,andF.Chen.1995.ATrainable Document Summarizer. In Proceedings of the 18th InternationalACMConferenceonResearchandDevelopmentinInformationRetrieval(SIGIR-95),Seattle,WA,USA,6873. Lin,C.-Y.and E.Hovy.2002. Manualand Automatic Evaluations of Summaries. In Proceedings of the Workshop on Automatic Summarization, postconference workshop of ACL-2002, pp. 45-51, Philadelphia,PA,2002. Lin,C.-Y.andE.H.Hovy.2003.AutomaticEvaluation of Summaries Using N-gram Co-occurrence Statistics. In Proceedings of the 2003 Human Language Technology Conference (HLT-NAACL 2003), Edmonton,Canada,May27June1,2003. Luhn,H.P.1969.TheAutomaticCreationofLiterature Abstracts. IBM Journal of Research and Development.2(2),1969. Marcu,D.1999a.Theautomaticconstructionoflargescale corpora for summarization research. Proceedings of the 22nd International ACM Conference on ResearchandDevelopmentinInformationRetrieval (SIGIR-99),Berkeley,CA,USA,137144. Marcu,D.1999b.Discoursetreesaregoodindicatorsof importanceintext.InI.ManiandM.Maybury(eds), Advances in Automatic Text Summarization, 123 136.MITPress. McKeown, K., R. Barzilay, D. Evans, V. Hatzivassiloglou, J. L. Klavans, A. Nenkova, C. Sable, B. Schiffman,S.Sigelman.2002.TrackingandSummarizingNewsonaDailyBasiswithColumbiasNewsblaster. In Proceedings of Human Language Technology Conference 2002 (HLT 2002). San Diego,CA,USA. NIST.2002.AutomaticEvaluationofMachineTranslationQualityusingN-gramCo-OccurrenceStatistics. Over, P. and W. Liggett. 2002. Introduction to DUC2002:anIntrinsicEvaluationofGenericNewsText Summarization Systems. In Proceedings of Workshop on Automatic Summarization (DUC 2002), Philadelphia,PA,USA. http://www-nlpir.nist.gov/projects/duc/pubs/ 2002slides/overview.02.pdf Papineni, K., S. Roukos, T. Ward, W.-J. Zhu. 2001. Bleu:aMethodforAutomaticEvaluationofMachine Translation. IBM Research Report RC22176 (W0109-022). Radev, D.R. and K.R. McKeown. 1998. Generating NaturalLanguageSummariesfromMultipleOn-line Sources.ComputationalLinguistics,24(3):469500. Strzalkowski,T,G.Stein,J.Wang,andB,Wise.ARobustPracticalTextSummarizer.1999.InI.Maniand M. Maybury (eds), Advances in Automatic Text Summarization,137154.MITPress. White, M., T. Korelsky, C. Cardie, V. Ng, D. Pierce, andK.Wagstaff.2001.MultidocumentSummarization via Information Extraction. In Proceedings of Human Language Technology Conference 2001 (HLT2001),SanDiego,CA,USA.
P99-1041
Both Jiang and Conrath (1997) and Lin (1998) propose extentions of Resnik?s measure.
P98-2127
LinGO. 1999.
P99-1041
While recent experience with LinGO (1999) suggests a trend towards modest increases in numbers of features compared to massive increases in numbers of types as grammars grow large, this is nevertheless an important issue to address.
P99-1041
 ThePotentialandLimitationsofAutomatic SentenceExtractionforSummarization Chin-YewLinandEduardHovy UniversityofSouthernCalifornia/InformationSciencesInstitute 4676AdmiraltyWay MarinadelRey,CA90292,USA {cyl,hovy}@isi.edu   Abstract Inthispaperwepresentanempiricalstudyof thepotentialandlimitationofsentenceextraction in text summarization. Our results show that the single document generic summarizationtaskasdefinedinDUC2001needstobe carefullyrefocusedasreflectedinthelowinter-human agreement at 100-word 1 (0.40 score) and high upper bound at full text 2  (0.88) summaries. For 100-word summaries, theperformanceupperbound,0.65,achieved oracleextracts 3.Suchoracleextractsshowthe promise of sentence extraction algorithms; however, we first need to raise inter-human agreementtobeabletoachievethisperformance level. We show that compression is a promisingdirectionandthatthecompression ratioofsummariesaffectsaveragehumanand systemperformance. 1 Introduction Most automatic text summarization systems existing todayareextractionsystemsthatextractpartsoforiginal documents and output the results as summaries. Among them, sentence extraction is by far the most  1 Wecomputeunigramco-occurrencescoreofapairofmanual summaries, one as candidate summary and the other as reference. 2 Wecomputeunigramco-occurrencescoresofafulltextand itsmanualsummariesof100words.Thesescoresarethebest achievable using the unigram co-occurrence scoring metric sinceallpossiblewordsarecontainedinthefulltext.Three manualsummariesareused. 3 Oracle extracts are the best scoring extracts generated by exhaustive search of all possible sentence combinations of 1005words. popular (Edmundson 1969, Luhn 1969, Kupiec et al. 1995,Goldsteinetal.1999,HovyandLin1999).The majorityofsystemsparticipatinginthepastDocument Understanding Conference (DUC 2002), a large scale summarization evaluation effort sponsored by the US government, are extraction based. Although systems basedoninformationextraction(RadevandMcKeown 1998,Whiteetal.2001,McKeownetal.2002)anddiscourseanalysis(Marcu1999b,Strzalkowskietal.1999) alsoexist,wefocusourstudyonthepotentialandlimitationsofsentenceextractionsystemswiththehopethat ourresultswillfurtherprogressinmostoftheautomatic textsummarizationsystemsandevaluationsetup. TheevaluationresultsofthesingledocumentsummarizationtaskinDUC2001and2002(DUC2002,Paul& Liggett2002)indicatethatmostsystemsareasgoodas thebaselinelead-basedsystemandthathumansaresignificantlybetter,thoughnotbymuch.Thisleadstothe beliefthatlead-basedsummariesareasgoodaswecan get for single document summarization in the news genre, implying that the research community should investfutureeffortsinotherareas.Infact,averyshort summary of about 10 words (headline-like) task has replaced the single document 100-word summary task inDUC2003.Thegoalofthisstudyistorenewinterest in sentence extraction-based summarization and its evaluationby estimatingtheperformanceupperbound usingoracleextracts,andtohighlighttheimportanceof taking into account the compression ratio when we evaluateextractsorsummaries. Section 2 gives an overview of DUC relevant to this study.Section3introducesa recall-basedunigram cooccurrenceautomaticevaluationmetric.Section4presentstheexperimentaldesign.Section5showstheempirical results. Section 6 concludes this paper and discussesfuturedirections.  2 DocumentUnderstandingConference Fully automatic single-document summarization was one of two main tasks in the 2001 Document UnderstandingConference.Participantswererequiredtocreate a generic 100-word summary.There were 30 test setsinDUC2001andeachtestsetcontainedabout10 documents.Foreachdocument,onesummarywascreated manually as the ideal model summary at approximately 100 words.We will refer to this manual summary as H1. Two other manual summaries were alsocreatedataboutthatlength.Wewillrefertothese twoadditionalhumansummariesasH2  andH3.Inaddition,baselinesummarieswerecreatedautomaticallyby taking the first n sentences up to 100 words. We will referthisbaselineextractasB1. 3 UnigramCo-OccurrenceMetric Inarecentstudy(LinandHovy2003),weshowedthat therecall-basedunigramco-occurrenceautomaticscoringmetriccorrelatedhighlywithhumanevaluationand has high recall and precision in predicting statistical significanceofresultscomparingwithitshumancounterpart. The idea is to measure the content similarity betweenasystemextractandamanualsummaryusing simple n-gram overlap. A similar idea called IBM BLEU score has proved successful in automatic machinetranslationevaluation(Papinenietal.2001,NIST 2002).Forsummarization,wecanexpressthedegreeof contentoverlapintermsofn-grammatchesasthefollowingequation: )1( )( )( }{ }{       = UnitsModelCCgramn UnitsModelCCgramn match n gramnCount gramnCount C  Modelunitsaresegmentsof manual summaries.They are typically either sentences or elementary discourse unitsasdefinedbyMarcu(1999b).Count match (n-gram) is the maximum number of n-grams co-occurring in a systemextractandamodelunit.Count(n-gram)isthe number of n-grams in the model unit. Notice that the averagen-gramcoveragescore,C n,asshowninequation1,isarecall-basedmetric,sincethedenominatorof equation 1 is the sum total of the number of n-grams occurringinthemodelsummaryinsteadofthesystem summaryandonlyonemodelsummaryisusedforeach evaluation. In summary, the unigram co-occurrence statisticsweuseinthefollowingsectionsarebasedon thefollowingformula: )2(logexp),(         =  = j in nn CwjiNgram  Wherej i,iandjrangefrom1to4,andw n is1/(ji+1).Ngram(1,4)isaweightedvariablelengthn-gram match score similar to the IBM BLEU score; while Ngram(k,k),i.e.i=j=k,issimplytheaveragek-gram co-occurrencescoreC k .Inthisstudy,weseti=j=1, i.e.unigramco-occurrencescore. Withatestcollectionavailableandanautomaticscoring metric defined, we describe the experimental setup in thenextsection. 4 ExperimentalDesigns As stated in the introduction, we aim to find the performanceupperboundofasentenceextractionsystem andtheeffectofcompressionratioonitsperformance. We present our experimental designs to address these questionsinthefollowingsections. 4.1 Performance Upper Bound Estimation UsingOracleExtract Inordertoestimatethepotentialofsentenceextraction systems,itisimportanttoknowtheupperboundthatan ideal sentence extraction method might achieve and howfarthestate-of-the-artsystemsareawayfromthe bound. If the upper bound is close to state-of-the-art systems performance then we need to look for other summarizationmethodstoimproveperformance.Ifthe upper bound is much higher than any current systems canachieve,thenitisreasonabletoinvestmoreeffortin sentence extraction methods. The question is how to estimatetheperformanceupperbound.Oursolutionis tocastthisestimationproblemasanoptimizationproblem. We exhaustively generate all possible sentence combinationsthatsatisfygivenlengthconstraintsfora summary, for example, all the sentence combinations totaling 1005 words. We then compute the unigram co-occurrence score for each sentence combination, against the ideal. The best combinations are the ones withthehighestunigramco-occurrencescore.Wecall this sentence combination the oracle extract. Figure 1 showsanoracleextractfordocumentAP900424-0035. OneofitshumansummariesisshowninFigure2.The oracle extract covers almost all aspects of the human summaryexceptsentences5and6andpartofsentence 4.However,ifweallowtheautomaticextracttocontain morewords,forexample,150wordsshowninFigure3, the longeroracleextractthen covers everythinginthe humansummary.Thisindicatesthatlowercompression can boost system performance. The ultimate effect of compressioncanbecomputedusingthefulltextasthe oracleextract,sincethefulltextshouldcontaineverything included in the human summary. That situation provides the best achievable unigram co-occurrence score.Anearoptimalscorealsoconfirmsthevalidityof usingtheunigramco-occurrencescoringmethodasan automaticevaluationmethod.  4.2 Compression Ratio and Its Effect on System Performance Oneimportantfactorthat affectsthe averageperformance of sentence extraction system is the number of sentences contained in the original documents. This factorisoftenoverlookedandhasneverbeenaddressed systematically. For example, if a document contains onlyonesentencethenthisdocumentwillnotbeuseful indifferentiatingsummarizationsystemperformance there is only one choice. However, for a document of 100sentencesandassumingeachsentenceis20words long, there are C(100,5) = 75,287,520 different 100wordextracts.Thishugesearchspacelowersthechance of agreement between humans on what constitutes a good summary. It also makes system and human performance approach average since it is more likely to includesomegoodsentencesbutnotallofthem.EmpiricalresultsshowninSection5confirmthisandthat leadsustothequestionofhowtoconstructacorpusto evaluatesummarizationsystems.Wediscussthisissue intheconclusionsection. 4.3 Inter-HumanAgreementandItsEffecton SystemPerformance In this section we study how inter-human agreement affects system performance. Lin and Hovy (2002) reportedthat, comparedtoa manually createdideal,humansscoredabout0.40inaveragecoveragescoreand the best system scored about 0.35. According to these numbers,wemightassumethathumanscannotagreeto eachotheronwhatisimportantandthebestsystemis almostasgoodashumans.Ifthisistruethenestimating anupperboundusingoracleextractsismeaningless.No matterhowhightheestimatedupperboundsmaybe,we probablywouldneverbeabletoachievethatperformance due to lack of agreement between humans: the oracle approximating one human would fail miserably withanother.Thereforewesetupexperimentstoinvestigatethefollowing: 1. Whatisthedistributionofinter-humanagreement? Figure3.A150-wordoracleextractfordocumentAP900424-0035. Figure 2. A manual summary for document AP900424-0035. Figure1.A100-wordoracleextractfordocumentAP900424-0035. <DOC> <DOCNO>AP900424-0035</DOCNO> <DATE>04/24/90</DATE> <HEADLINE> <SHSNTNO="1">ElizabethTaylorinIntensiveCareUnit</S> <SHSNTNO="2">ByJEFFWILSON</S> <SHSNTNO="3">AssociatedPressWriter</S> <SHSNTNO="4">SANTAMONICA,Calif.(AP)</S> </HEADLINE> <TEXT> <SSNTNO="1">AseriouslyillElizabethTaylorbattledpneumoniaather hospital,herbreathingassistedbyaventilator,doctorssay.</S> <SSNTNO="2">HospitalofficialsdescribedherconditionlateMonday asstabilizingafteralungbiopsytodeterminethecauseofthepneumonia.</S> <SSNTNO="3">Analysisofthetissuesamplewasexpectedtotakeuntil Thursday,saidherspokeswoman,ChenSam.</S> <SSNTNO="9">Anotherspokewomanfortheactress,LisaDelFavaro, saidMissTaylor'sfamilywasatherbedside.</S> <SSNTNO="13">``Itisserious,buttheyarereallypleasedwithher progress.</S> <SSNTNO="22">Duringanearlyfatalboutwithpneumoniain1961, MissTaylorunderwentatracheotomy,anincisionintoherwindpipeto helpherbreathe.</S> </TEXT> </DOC> <DOC> <TEXT> <SSNTNO="1">ElizabethTaylorbattledpneumoniaatherhospital, assistedbyaventilator,doctorssay.</S> <SSNTNO="2">HospitalofficialsdescribedherconditionlateMonday asstabilizingafteralungbiopsytodeterminethecauseofthepneumonia.</S> <SSNTNO="3">AnalysisofthetissuesamplewasexpectedtobecompletebyThursday.</S> <SSNTNO="4">Ms.Sam,spokeswomansaid"itisserious,buttheyare reallypleasedwithherprogress.</S> <SSNTNO="5">She'snotwell.</S> <SSNTNO="6">She'snotonherdeathbedoranything.</S> <SSNTNO="7">Anotherspokeswoman,LisaDelFavaro,saidMiss Taylor'sfamilywasatherbedside.</S> <SSNTNO="8">Duringanearlyfatalboutwithpneumoniain1961,Miss Taylorunderwentatracheotomytohelpherbreathe.</S> </TEXT> </DOC> <DOC> <DOCNO>AP900424-0035</DOCNO> <DATE>04/24/90</DATE> <HEADLINE> <SHSNTNO="1">ElizabethTaylorinIntensiveCareUnit</S> <SHSNTNO="2">ByJEFFWILSON</S> <SHSNTNO="3">AssociatedPressWriter</S> <SHSNTNO="4">SANTAMONICA,Calif.(AP)</S> </HEADLINE> <TEXT> <SSNTNO="1">AseriouslyillElizabethTaylorbattledpneumoniaather hospital,herbreathingassistedbyaventilator,doctorssay.</S> <SSNTNO="2">HospitalofficialsdescribedherconditionlateMonday asstabilizingafteralungbiopsytodeterminethecauseofthepneumonia.</S> <SSNTNO="3">Analysisofthetissuesamplewasexpectedtotakeuntil Thursday,saidherspokeswoman,ChenSam.</S> <SSNTNO="4">The58-year-oldactress,whowonbest-actressOscars for``Butterfield8''and``Who'sAfraidofVirginiaWoolf,''hasbeen hospitalizedmorethantwoweeks.</S> <SSNTNO="8">Herconditionispresentlystabilizingandherphysicians arepleasedwithherprogress.''</S> <SSNTNO="9">Anotherspokewomanfortheactress,LisaDelFavaro, saidMissTaylor'sfamilywasatherbedside.</S> <SSNTNO="13">``Itisserious,buttheyarereallypleasedwithher progress.</S> <SSNTNO="14">She'snotwell.</S> <SSNTNO="15">She'snotonherdeathbedoranything,''Ms.Samsaid lateMonday.</S> <SSNTNO="22">Duringanearlyfatalboutwithpneumoniain1961, MissTaylorunderwentatracheotomy,anincisionintoherwindpipeto helpherbreathe.</S> </TEXT> </DOC>  2. Howdoesastate-of-the-artsystemdifferfrom averagehumanperformanceatdifferentinterhumanagreementlevels? We present our results in the next section using 303 newspaperarticlesfromtheDUC2001singledocument summarizationtask.Besidestheoriginaldocuments,we also have three human summaries, one lead summary (B1), and one automatic summary from one top performingsystem(T)foreachdocument. 5 Results In order to determine the empirical upper and lower bounds of inter-human agreement, we first ran crosshumanevaluationusingunigramco-occurrencescoring through six human summary pairs, i.e. (H1,H2), (H1,H3),(H2,H1),(H2,H3),(H3,H1),and(H3,H2).For a summary pair (X,Y), we used X as the model summaryandYasthesystemsummary.Figure4showsthe distributionsoffourdifferentscenarios.TheMaxHdistribution picks the best inter-human agreement scores foreachdocument,theMinHdistributiontheminimum one,theMedHdistributionthemedian,andthe AvgH distributiontheaverage.Theaverageofthebestinterhuman agreement and the average of average interhumanagreementdifferbyabout10percentinunigram co-occurrencescoreand18percentbetweenMaxHand MinH. These big differences might come from two sources. The first one is the limitation of the unigram 0 10 20 30 40 50 60 70 80 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 UnigramCo-occurrenceScores #  of  I n st an ce s AvgH MaxH MedH MinH AverageMAX=0.50 AverageAVG=0.40 AverageMED=0.39 AverageMIN=0.32 Figure 4. DUC 2001 single document interhuman unigram co-occurrence score distributions for maximum, minimum, average, and median. 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00 D 4 1.
P99-1041
Fixedness To measure fixedness, we use statistical measures of lexical, syntactic, and overall fixedness that we have developed in a previous study (Fazly and Stevenson, 2006), as well as some new measures we introduce here.
E06-1043
References 1 Petra Barg, Markus Walther, Processing unknown words in HPSG, Proceedings of the 36th annual meeting on Association for Computational Linguistics, August 10-14, 1998, Montreal, Quebec, Canada 2 Lou Burnard.
P98-2127
