1:206	NP Bracketing by Maximum Entropy Tagging and SVM Reranking Hal Daume III and Daniel Marcu University of Southern California Information Sciences Institute 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 fhdaume,marcug@isi.edu Abstract We perform Noun Phrase Bracketing by using a local, maximum entropy-based tagging model, which produces bracketing hypotheses.
2:206	These hypotheses are subsequently fed into a reranking framework based on support vector machines.
3:206	We solve the problem of hierarchical structure in our tagging model by modeling underspecified tags, which are fully determined only at decoding time.
4:206	The tagging model performs comparably to competing approaches and the subsequent reranking increases our systems performance from an f-score of 81:7 to 86:1, surpassing the best reported results to date of 83:8.
5:206	1 Introduction and Prior Work Noun Phrase Bracketing (NP Bracketing) is the task of identifying any and all noun phrases in a sentence.
6:206	It is a strictly more difficult problem than NP Chunking (Ramshaw and Marcus, 1995), in which only non-recursive (or base) noun phrases are identified.
7:206	It is simultaneously strictly more simple than either full parsing (Collins, 2003; Charniak, 2000) or supertagging (Bangalore and Joshi, 1999).
8:206	NP Bracketing is both a useful first step toward full parsing and also a meaningful task in its own right; for instance as an initial step toward co-reference resolution and noun-phrase translation.
9:206	While existing NP Bracketers (including the one described in this paper) tend to achieve worse overall F-measures than a full statistical parser (eg.
10:206	, (Collins, 2003; Charniak, 2000)), they can be significantly more computationally efficient.
11:206	Statistical parsers tend to scale exponentially in sentence length, unless a narrow beam is employed, which leads to globally poorer parses.
12:206	In contrast, the bracketer described in this paper scales linearly in [[Confidence] in [the pound]] is widely expected to take [another sharp dive] if [[[trade figures] for [September]], due for [release] [tomorrow],].
13:206	Figure 1: Sample sentence with NPs bracketed.
14:206	the length of the sentence to find the globally optimal solution.
15:206	This trade-off is depicted graphically in Figure 2.
16:206	This figure shows the amount of time (excluding any startup overhead) spent parsing or bracketing using this system (the two lowest lines) versus the parsers of Collins (2003) and Charniak (2000) run with default settings.
17:206	NP Bracketing was the shared task of the Computational Natural Language Learning workshop in 1999 (CoNLL-99).
18:206	In this competition, NP Bracketing systems were trained on sections 15-18 of the Wall Street Journal corpus, while section 20 was used for testing.
19:206	The bracketing information was extracted directly from the Penn Treebank, essentially disregarding all non-NP brackets.
20:206	An example bracketed sentence is in Figure 1.
21:206	There have been several successful approaches reported in the literature to solve this task.
22:206	Tjong Kim Sang (1999) first used repeated chunking to attain an f-score of 82:98 during the CoNLL competition and subsequently (Sang, 2002) an f-score of 83:79 using a combination of two different systems.
23:206	Krymolowski and Dagan (2000) have obtained similar results using more training data and lexicalization.
24:206	Brandts (1999) has used cascaded HMMs to solve the NP Bracketing problem; however, he evaluated his system only on German NPs, so his results cannot be directly compared.
25:206	Obviously, the difficulty that arises in NP Bracketing that differentiates it from NP Chunking is the issue of embedded NPs, thus requiring output in the 5 10 15 20 25 30 35 40 45 50 550 5 10 15 20 25 30 Sentence Length Seconds to Parse (normalized) Charniak Collins Bracketer+SVM Bracketer Figure 2: Speed of different systems form of a tree structure.
26:206	Most solutions to problems involving building trees from sequences build in to the model a concept of depth (in parsing, this is typically in the form of a chart; in bracketing and shallow parsing, this is typically in the form of embedded finite-state automata).
27:206	We elect to take a completely different approach.
28:206	The model we use is agnostic to any sort of depth: it hypothesizes underspecified tags and allows the matching bracket constraint to select a solution.
29:206	Specifically, we approach the NP Bracketing problem as a tagging and reranking problem.
30:206	We use an efficient maximum entropy-based tagger to hypothesize possible bracketings (see Section 2) and then rerank these hypotheses using a support vector reranking system (see Section 3).
31:206	Using only the tagger (without reranking), we achieve comparable results to those referenced above and, with the addition of the reranking system, achieve, to our knowledge, the best reported results to date.
32:206	2 Bracketing as a Tagging Problem In any tagging problem, the task is to associate each word in the input with a single tag.
33:206	There are many competing approaches to tagging problems including Hidden Markov Models (HMMs), Maximum Entropy Markov Models (MEMMs) and Conditional Random Fields (CRFs).
34:206	We adopt a slight variant of the MEMM framework.
35:206	2.1 Maximum Entropy Tagging Model In the formulation of the maximum entropy tagging model, we assume that the probability distribution of tags takes the form of an exponential distribution, parameterized by a sequence of feature weights, m1, where there are m-many features.
36:206	Thus, we obtain a distribution for Pr m1 (ti ti 1; w) of the form: 1 Zti 1; w exp 2 4 mX j=1 jfj(ti;ti 1; w) 3 5 (1) where Zti 1; w is a normalizing factor.
37:206	Like other maximum entropy approaches, this distribution is unimodal and optimal values for the s can be found through various algorithms; we use GIS.
38:206	A good introduction to maximum entropy models can be found in (Berger et al. , 1996).
39:206	In our approach, we use a tag set of exactly five tags: fopen;close;in;out;singg.
40:206	An open tag is assigned to all words that open a bracketing (regardless of the number of brackets opened) and do not also close a bracketing.
41:206	A close tag is assigned to all words that close a bracketing and do not also open one.
42:206	An in tag is assigned to all words enclosed in an NP, but which neither open nor close one.
43:206	An out tag is assigned to all words which are not enclosed in an NP.
44:206	A sing(leton) tag is assigned to all words that both open and close a bracketing (regardless of whether they open or close more than just their own bracketing).
45:206	Note that such a tagging does not uniquely determine a bracketing.
46:206	For instance, the tag sequence hsing singi could correspond either to [[w1] [w2]] or to [w1] [w2].
47:206	Nevertheless, due to the constraints involved in the tagging process (namely that a close tag cannot appear unless one is already within an NP and that one cannot have two close tags when the corresponding open tags appear at the same location1), we hope that our system will be able to disambiguate sufficiently.
48:206	In other words, although our taggings are under-specified, we hope that the additional constraints that we subsequently associate with these tags will yield high quality bracketings.
49:206	2.2 Feature Functions The probability distribution shown in Equation 1 is based on m-many real-valued feature functions, fj.
50:206	We use two classes of features, closed features and open features (these roughly correspond to whether they look at closed class elements or open class elements).
51:206	The open features for position i are applied at positions i, i 1 and i + 1.
52:206	The closed features are applied at i, i 1, i 2, i 3 and i + 1, i + 2 and i + 3.
53:206	1For instance, the bracketing [[wi : : : wj]] is disallowed; this bracketing must appear simply as [wi : : : wj].
54:206	Closed features include: part of speech tag (according to Brills (1995) tagger); two character suffix of word; first character of part of speech; initial character capitalized; word fully capitalized; last character is period; word position in sentence; and two features for when the word is either the first or last word in the sentence.
55:206	Open features include: the word itself; the word lower-cased; the lower-cased stem (Porter, 1980); the lower-cased stem plus the part of speech; and 3 features that are each true when there is a CC in the next 2 through 5 words.
56:206	In addition, we include a feature for tag ti 1.
57:206	2.3 Maximum Entropy Training We used generalized iterative scaling to train the maximum entropy model2 on 929;921 features and 211;728 training instances from sections 15-18 of the Penn Treebank (20% of which was set aside as a validation set).
58:206	Training was run for ten thousand iterations and, at convergence, achieved a tagging error rate of 2:1% on the training data and 6:9% on the validation data.
59:206	2.4 Decoding Algorithm We use a Viterbi-like dynamic programming decoding algorithm, where transition probabilities are governed by the discriminative tagging model.
60:206	However, the tags generated by our decoder are not the same as those predicted by the maximum entropy model.
61:206	Our decoder does not search in the original space of tags (sing;in;out;::: ) but rather in a new space that yields only well-formed bracketings.
62:206	In the secondary search space, the algorithm is guaranteed to find the most likely well-formed bracketing, even though this might not correspond to the most likely tag sequence.
63:206	While it would be possible to simply tag using the original tag set and allow the reranker (see Section 3) to select a wellformed bracketing, it is unlikely that this will lead to improved performance: the complexity of the decoders will be the same, yet the bracketer would have to wade through significantly more bad taggings to find a good solution.
64:206	Our decoding tags take one of five forms, capitalized to distinguish them from the maximum entropy tags: On, Cn, N, OnC, OCn where n 1 for all but OCn where n 2.
65:206	The meaning of the tags is: On means n simultaneous open brackets: Cn means n simultaneous close brackets.
66:206	N means that no brackets appear at this position.
67:206	OnC corre2Using the YASMET maximum entropy training package: http://www.isi.edu/och/YASMET/.
68:206	0 20 40 60 80 100 120 140 160 180 20082 84 86 88 90 92 94 96 98 Precision Recall FScore Figure 3: Plot of n versus maximal f-score (and associated precision and recall) for test data.
69:206	sponds to n open brackets and one close bracket, while OCn corresponds to one open bracket and n 2 close brackets.
70:206	These tags are enough to decode any well-formed bracketing.
71:206	Our decoder assumes a maximum depth of tags d has been prespecified and then solves a dynamic programming problem on an n d t array A, where n is the sentence length and t denotes an integer corresponding to the highest possible decoding tag in an enumeration.
72:206	The value Ai;d;t stores the probability of being at position i and depth d after applying tag t at that position.
73:206	It is always the case that t 4d.
74:206	The time and space complexity of this decoding problem is thus O(d2n).
75:206	The dynamic programming problem is: A1;d;t = Pr ^t0 (2) Ap;d;t = max t0 Ap 1;d t;t0 Pr ^td t0 (3) where ^td = 8 >>> >< >>>> : out t = N ^ d = 0 in t = N ^ d > 0 sing t 2 fOnC;OCng begin t = On end t = Cn (4) t = 8 >>> >< >>> >: n t = On n 1 t = OnC n t = Cn n + 1 t = OCn 0 t = N (5) The intuition for calculating the value of Ap;d;t for p > 1 (see Equation 3) is that we first choose the optimal previous tag, t0.
76:206	Furthermore, based on t and d, we can calculate the depth (d t, see Equation 5) we must have been at previously.
77:206	Thus, we must take the value of Ap 1;d t;t0 which is the probability of having arrived at position p 1 at depth d t with tag t0.
78:206	We then multiply this by the probability of getting from that position to the current position, which is given by Pr ^td t0 (note that the normalization occurs over the new space of tags).
79:206	The optimal tagging is given by back-tracing through A, beginning at An;0;t for any tag t. Even for long sentences, this algorithm requires very little time and memory.
80:206	2.5 Model Deficiencies While the bracketing model described above already performs comparably to competing approaches (see Section 4), it is still subject to making categorical mistakes.
81:206	Most of its errors are due to the locality of the decisions made.
82:206	Because of the coarseness of the tags used in the maximum entropy tagging framework, the model is unable to discriminate between some bad bracketings and some good ones.
83:206	For instance, it must assign precisely the same probability to both of the following bracketings, since the maximum entropy tags (shown beneath) are identical: [[John,] [president] of [the company],] [[John,] [[president] of [the company]]],] sing sing in open close close This limitation causes the model to make consistent mistakes distinguishing between, for example, lists and appositional phrases.
84:206	To solve these problems in the tagging model would be nearly impossible, without giving up on efficiency.
85:206	However, our decoder is able to produce n-best lists using exact A search that very frequently contain globally superior taggings, even though the simple tagging model cannot recognize them as such.
86:206	In Figure 3, we show the maximal f-score (and corresponding precision and recall) for the best bracketing chosen out of the n-best, as we let n range from 1 to 400 for both the validation data and the test data.
87:206	As we can see from these graphs, we have the possibility of improving our systems fscore performance by about ten points  from 82% to 93%, simply by being able to choose the correct hypothesis from the n-best list; also working with 100-best lists is likely sufficient.
88:206	3 Hypothesis Reranking In the previous section, we described a tagging model for NP Bracketing that can produce n-best lists.
89:206	In this section, we describe a machine learning method for reranking these lists in an attempt to choose a hypothesis which is superior to the firstbest output of the decoder.
90:206	Reranking of n-best lists has recently become popular in several natural language problems, including parsing (Collins, 2003), machine translation (Och and Ney, 2002) and web search (Joachims, 2002).
91:206	Each of these researchers takes a different approach to reranking.
92:206	Collins (2003) uses both Markov Random Fields and boosting, Och and Ney (2002) use a maximum entropy ranking scheme, and Joachims (2002) uses a support vector approach.
93:206	As SVMs tend to exhibit less problems with over-fitting than other competing approaches in noisy scenarios, we also adopt the support vector approach.
94:206	3.1 Support Vector Reranking A support vector classifier is a binary classifier with a linear decision boundary.
95:206	The selected decision boundary is a hyperplane that is chosen in such a way that the distance between it and the nearest data points is maximized.
96:206	Slack variables are commonly introduced when the problem is not linearly separable, leading to soft margins.
97:206	For reranking, we assume that instead of having binary classes for the yis, we have real values which specify the relative ordering (higher values come first).
98:206	For this task, we get the following optimization problem (Joachims, 2002): minimize 12jj wjj2 + C NX i=1 i;j (6) subject to w xi w xj + 1 i;j (7) i;j 0 (8) Where the i;js are drawn from comparable data points and yi yj and C is a regularization parameter that specifies how great the cost of mis-ordering is. As noticed by Joachims, the condition in Equation 7 can be reduced to the standard SVM model by subtracting w xj from both sides.
99:206	3.2 Reranking Feature Functions Since our problem is closely related to that of Collins (2003), we use many of the same feature functions he does, though we do introduce many of our own (those which are copied from Collins are marked with an asterisk).
100:206	We view the hypothesized bracketing as a tree in a context free grammar and include features based on each rule used to generate the given tree.
101:206	For concreteness, we will use the CFG rule NP ! DT JJ NP (where the NP is selected as the head) as an example.
102:206	Rules*: the full CFG rule; in this case, the active rule would be NP ! DT JJ NP.
103:206	Markov 2 Rules: CFG rules where 2-level Markovization has been applied.
104:206	That is, we look at the rule for generating the first two tags, then the next two (given the previous one), then the next two (given the previous one), and so on.
105:206	A start of branch tag ([S]) and end of branch tag ([/S]) are added to the beginning and end of the children lists.
106:206	In this case, the rules that fire are: NP!
107:206	[S] DT, NP![S] ! DT JJ, NP!DT ! JJ NP and NP!JJ ! NP [/S].
108:206	The notation is X!Y ! A B, where X is the true parent, Y was the previous child in the Markovization, and A B are the two children.
109:206	Lex-Rules*: full CFG rules, where terminal POS tags are replaced with lexical items.
110:206	Markov 2 Lex-Rules: Markov 2-style rules, terminal POS tags are replaced with lexical items.
111:206	Bigrams*: pairs of adjacent tags in the CFG rule; in our example, the active pairs are ([S],DT), (DT,JJ), (JJ,NP) and (NP,[/S]).
112:206	Lex-Bigrams*: same as BIGRAMS, but with lexical heads instead of POS tags.
113:206	Head Pairs*: pairs of internal node tags with the head type; in the example, (DT, NP), (JJ, NP) and (NP, NP).
114:206	Sizes: the child count, conditioned on the internal tag; eg.
115:206	, NP ! 3.
116:206	Word Count: pair of the SIZES and total number of words under this constituent.
117:206	Boundary Heads: pairs of the first and last head in the constituent.
118:206	POS-Counts: a scheme of features that count the number of children whose part of speech tag matches a given predicate.
119:206	There are six of these: (1) children whose tag begins with N, (2) children whose tag begins with N but is not NP, (3) children which are DTs, (4) children whose tag begin with V, (5) children which are commas, (6) children whose tag is CC.
120:206	In this case, we get a count of 1 for rules (2) and (3), and 2 for rule (1).
121:206	Lex-Tag/Head Pairs: same as HEAD PAIRS, but where lexical items are used instead of POS tags.
122:206	Special Tag Pairs: count of the lexical heads to the left and right of leaves tagged with each of POS, CC, IN and TO.
123:206	Tag-Counts: another schema of features that replicates some of the features used in the maximum entropy tagger.
124:206	This schema includes all the original maximum entropy tags, as well as a feature for each maximum entropy tag at position i, paired with (a) the part of speech tag at position i, i 1 and i + 1, (b) the word at position i, i 1 and i + 1, (c) the part of speech + word pair at those positions, (d) the maximum entropy tag at that position.
125:206	3.3 SVM Training We develop three reranking systems, differentiated by the amount of training data used.
126:206	The first, RR1, is trained on the validation part of the training set (20% of sections 15-18).
127:206	The second, RR2.
128:206	is trained on the entire training set through crossvalidation (all of sections 15-18).
129:206	The final, RR3 is trained on the entire Penn Treebank corpus, except section 20.
130:206	Training the reranking system only on the validation data (RR1) results in only a marginal gain of overall f-score, due primarily to the fact that most of the features use lexical information to prefer one bracketing over another.
131:206	The validation data from sections 15-18 gives rise to 2;012 training instances and 362;415 features.
132:206	In order to train the reranking system on all of the training data (RR2), we built five decoders, each with a different 20% of the training data held out.
133:206	Each decoder is then used to tag the held-out 20% (this is done so that the tagger does not do too well on its training data).
134:206	This leads to 8;935 sentences for training, with a total of 1:1 million features.
135:206	Training on all the WSJ data except section 20 (RR3) gives rise to 39;953 training instances and a total of just over 2:1 million features.
136:206	These examples give 1;462;568 rank constraints.
137:206	4 Results We compare our system against those reported in the literature.
138:206	In all, the evaluation is over 2;012 sentences of test data.
139:206	In Table 1, we display the results of state-of-the-art systems, and the system described in this paper (both with and without reranking).
140:206	The upper part of the table displays results from systems which are trained only on sections 1518 of the WSJ.
141:206	The lower part displays results based on systems trained on more data.
142:206	System BR BP BF CB TKS99 76.1 91.3 82.8 0.14 TKS02 78.4 90.0 83.8 TAG 81.0 86.0 83.4 0.26 RR1 82.1 88.8 85.3 0.18 RR2 82.7 89.8 86.1 0.14 COL03NP 68.6 68.9 68.7 0.91 COL03Full 88.2 87.7 87.9 0.31 CHUNK 73.0 100.0 84.4 COL03All 88.0 89.8 88.9 0.18 KD00 79.3 88.5 83.7 RR3 84.3 90.8 87.4 0.12 Table 1: Results on test data.
143:206	The systems in the lower half are not directly comparable, since they were either trained or tested on different data.
144:206	In the table, TKS99 and TKS02 are the systems of Tjong Kim Sang (1999; 2002).
145:206	KD00 is the system of (Krymolowski and Dagan, 2000).
146:206	All the COL03 systems are results obtained using the restriction of the output of Collins (2003) parser.
147:206	In particular, the two comparable numbers coming from Collins parser are COL03NP and COL03Full.
148:206	The difference between these two systems is that the NP system is trained on parse trees, with all non-NP nodes removed.
149:206	The FULL system is trained on full parse trees, and then the output is reduced to just include NPs.
150:206	COL03All is trained on sections 2-21 of WSJ and tested on section 23, and is thus an upper bound, since these numbers are testing on training data.3 Our RR3 system had the reranking component (but not the tagging component) trained on all of the WSJ except for section 20.
151:206	The CHUNK row in the results table is the performance of an optimally performing NP chunker.
152:206	That is, this is the performance attainable given a chunker that identifies base NPs perfectly (at 100% precision).
153:206	However, since this hypothetical system only chunks base NPs, it misses all non-base NPs and thus achieves a recall of only 73:0, yielding an overall F-score below our systems performance.
154:206	Note also that no chunker will perform this well.
155:206	Current systems attain approximately 94% precision and recall on the chunking task (Sha and Pereira, 2002; Kudo and Matsumoto, 2001), so the 3Collins independently reports a recall of 91:2 and precision of 90:3 for NPs (Collins, 2003); however, these numbers are based on training on all the data and testing on section 0.
156:206	Moreover, it is possible that his evaluation of NP bracketing is not identical to our own.
157:206	The results in row COL03F ull are therefore perhaps more relevant.
158:206	actual performance for a real system would be substantially lower.
159:206	The four criteria these systems are evaluated on are bracketing recall (BR), bracketing precision (BP), bracketing f-score (BF) and average crossing brackets (CB).
160:206	Some systems do not report their crossing bracket rate.
161:206	All of these metrics are calculated only on NP* and WHNP* brackets.
162:206	5 Comparison of Performance The results depicted in Table 1 show that, when comparing our system directly to Collins parser, his system tends to achieve significantly higher levels of recall, while maintaining a slight advantage in terms of precision.
163:206	This table, however, does not tell the full story.
164:206	As is typically observed in these sort of applications, it is not the case that Collins parser is winning by a little on all the data, but rather that Collins parser wins on some of the data and our bracketer wins on some of the data.
165:206	In this section, we analyze the differences.
166:206	Overall, there are 2;012 sentences in the test data.
167:206	In 558 cases, both the bracketing system and Collins parser achieve perfect precision.
168:206	In 505 cases, both achieve perfect recall.
169:206	For the remainder of the discussion in this section, when discussing precision, we will only consider the cases in which not both achieved perfect scores, and similarly for recall.
170:206	In Figure 4, we depict (excluding the mutually perfect sentences) the percentage of sentences on which each system is better than the other by a distance of at least . Along the X-axes, the value of ranges from 0 to 20.
171:206	At a given value of, the segmentation along the Y-axes depict (a) along the top (in yellow where available), the proportion of sentences for which the bracketers precision (for the left hand image) was at least of that of Collins; (b) in the middle (in red), the proportion of sentences for which Collins was at least better; and (c) along the bottom (in blue), the proportion of sentences where the two systems performed within of each other.
172:206	As should be expected, as increases, the Equal region also increases.
173:206	However, it is worth noticing that even at an of 20 precision points, there are still roughly 11% of the sentences for which one systems performance is noticeably different from the others (and furthermore, that these are about even).
174:206	As can be immediately seen from the right-hand graph, Collins parser consistently outperforms the bracketer in terms of recall.
175:206	HowFigure 4: Proportion of sentences for which one system outperforms the other with difference at least . Precision Recall Tag RR2 COL03 RR2 COL03 NP 21.4 19.8 20.5 21.3 VP 7.49 8.52 8.31 7.57 NN 8.22 7.62 7.43 7.83 IN 6.01 5.89 5.31 6.15 PP 5.90 5.63 5.16 6.03 S 4.96 5.82 5.44 5.15 NNP 6.15 4.79 6.29 5.82 Table 2: Percentage of tags on superior system.
176:206	ever, in contrast to the Precision graph, for the first 10 or so values of, these proportions remain roughly the same (in fact, for a short period, Collins actually looses ground).
177:206	This suggests that there are a relatively large proportion of sentences for which our system is performing abominably (with > 10 recall points difference) in comparison to Collins.
178:206	However, once a critical mass of > 10 is reached, the relative differences become less strong.
179:206	Since neither system is winning in all cases, in an effort to better understand the conditions in which one system will outperform the other, we inspect the sentences for which there was a difference in performance of at least 10 (for precision and recall separately).
180:206	To perform this investigation, we look at the distribution of tags in the true, full parse trees for those sentences.
181:206	These percentages, for the 7 most common tags, are summarized in Table 2 (for example, the relative frequency of the NP tag in sentences where the RR2 system achieved higher precision was 21.4, while for the sentences for which COL03 achieved higher precision was 19.8).
182:206	The first thing worth noticing in this table is that in general, when one system achieves higher precision, the other system achieves higher recall, which is not surprising.
183:206	However, in the last row, corresponding to proper nouns, the RR2 system outperforms the COL03 (this is the Full implementation) in both precision and recall, suggesting that our system is better able to capture the phrasing of proper nouns.
184:206	We attribute this to the fact that our model is specialized to identify noun phrases, of which proper nouns comprise a large part.
185:206	Similarly, the largest gains in recall for COL03 over RR2 are in sentences with many PPs.
186:206	This coincides with our intuition about the syntactic parser being better able to capture long, embedded noun phrases.
187:206	6 Conclusion We have presented a method for performing noun phrase bracketing, which outperforms competing methods both in terms of f-score and recall.
188:206	The system is based on two separate components: a maximum entropy-based tagging system and a support vector machine reranking system.
189:206	The key component of the tagging system is that it produces underspecified tags that are determined only at decoding time by bracketing constraints.
190:206	The tagging system operates very quickly and can tag and rerank at a rate of approximately two sentences per second.
191:206	The tagger alone achieves an f-score of 83:4.
192:206	This score is only 0:4% lower (absolute) than the best reported result to date of 83:8.
193:206	After tagging, we have fed 100 best lists into a support vector reranking system, which performs global optimization to choose a good bracketing.
194:206	Our reranking system is able to increase the f-score of our bracketing approach from 83:4 to 86:1, improving our performance beyond the best reported system to date.
195:206	As we can see from Table 1, by comparing the output of our system to that of COL00Full, there is much in the way of recall to be gained by using a full syntactic parser.
196:206	However, this gain comes at two expenses.
197:206	First, full syntactic parsers are computationally more expensive to run.
198:206	Moreover, performance of Collins parser degrades significantly (from 87:9 to 68:7 in f-score) when it cannot take advantage of other constituent information.
199:206	This has a strong influence when one is faced with the task of moving to a new domain.
200:206	On the one hand, our system (as well as the other bracketing systems cited) requires data to only be annotated at the NP level in order to achieve high performance.
201:206	Conversely, without full parses, using a parser for learning NPs is inadequate.
202:206	Despite these successes, there is still much that can be improved upon.
203:206	While the reranking is very efficient in the classification phase, training a support vector reranking system is computationally very expensive.
204:206	Other well grounded statistical learning systems might allow us to train this component on more data and using more features.
205:206	We also hope to be able to improve our systems performance from its current rate of 86:1 (on official data) and 87:4 (on all data) closer to the n-best optimal, depicted in Figure 3.
206:206	7 Acknowledgments This work was partially supported by DARPA-ITO grant N66001-00-1-9814, NSF grant IIS-0097846, and a USC Dean Fellowship to Hal Daume III.

