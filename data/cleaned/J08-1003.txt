Wide-CoverageDeepStatisticalParsing
UsingAutomaticDependency
StructureAnnotation
AoifeCahill
∗
DublinCityUniversity
MichaelBurke
∗∗,†
DublinCityUniversity
IBMCenterforAdvancedStudies
RuthO’Donovan
∗∗
DublinCityUniversity
StefanRiezler
‡
PaloAltoResearchCenter
JosefvanGenabith
∗∗,†
DublinCityUniversity
IBMCenterforAdvancedStudies
AndyWay
∗∗,†
DublinCityUniversity
IBMCenterforAdvancedStudies
A number of researchers have recently conducted experiments comparing “deep” hand-crafted
wide-coverage with “shallow” treebankand machine-learning-based parsers at the level of
dependencies, using simple and automatic methods to convert tree output generated by the
shallow parsers into dependencies. In this article, we revisit such experiments, this time using
sophisticated automatic LFG f-structure annotation methodologies with surprising results. We
compare various PCFG and history-based parsers to ﬁnd a baseline parsing system that ﬁts
best into our automatic dependency structure annotation technique. This combined system of
syntactic parser and dependency structure annotation is compared to two hand-crafted, deep
constraint-basedparsers,RASPandXLE.Weevaluateusingdependency-basedgoldstandards
∗ NowattheInstitutf¨urMaschinelleSprachverarbeitung,Universit¨atStuttgart,Germany.E-mail:aoife.
cahill@ims.uni-stuttgart.de.
∗∗ NationalCentreforLanguageTechnology,DublinCityUniversity,Dublin9,Ireland.
† IBMDublinCenterforAdvancedStudies(CAS),Dublin15,Ireland.
‡ NowatGoogleInc.,MountainView,CA.
Submissionreceived:24August2005;revisedsubmissionreceived:20March2007;acceptedforpublication:
2June2007.
©2008AssociationforComputationalLinguistics
ComputationalLinguistics Volume34,Number1
and use the Approximate Randomization Test to test the statistical signiﬁcance of the results.
Our experiments show that machine-learning-based shallow grammars augmented with so-
phisticated automatic dependency annotation technology outperform hand-crafted, deep, wide-
coverageconstraintgrammars.Currentlyourbestsystemachievesanf-scoreof82.73%against
the PARC 700 Dependency Bank, a statistically signiﬁcant improvement of 2.18% over the
most recent results of 80.55% for the hand-crafted LFG grammar and XLE parsing system
and an f-score of 80.23% against the CBS 500 Dependency Bank, a statistically signiﬁcant
3.66%improvementoverthe76.57%achievedbythehand-craftedRASPgrammarandparsing
system.
1.Introduction
Wide-coverage parsers are often evaluated against gold-standard CFG trees (e.g.,
Penn-IIWSJSection23trees)reportingtraditionalPARSEVALmetrics(Blacketal.1991)
of labeled and unlabeled bracketing precision, recall and f-score measures, number of
crossingbrackets,completematches,andsoforth.Althoughtree-basedparserevalua-
tionprovidesvaluableinsightsintotheperformanceofgrammarsandparsingsystems,
itissubjecttoanumberof(related)drawbacks:
1. BracketedtreesdonotalwaysprovideNLPapplicationswithenough
informationtocarryouttherequiredtasks:Manyapplicationsinvolve
adeeperanalysisoftheinputintheformofsemanticallymotivated
informationsuchasdeepdependencyrelations,predicate–argument
structures,orsimplelogicalforms.
2. Anumberofalternative,butequallyvalidtreerepresentationscan
potentiallybegivenforthesameinput.Togivejustafewexamples:In
English,VPscontainingmodalsandauxiliariescanbeanalyzedusing
(predominantly)binarybranchingrules(Penn-II[Marcusetal.1994]),or
employﬂatteranalyseswheremodalsandauxiliariesaresistersofthe
mainverb(APtreebank[LeechandGarside1991]),orindeeddowithout
adesignatedVPconstituentatall(SUSANNE[Sampson1995]).Treebank
bracketingguidelinescanuse“traditional”CFGcategoriessuchasS,NP,
andsoon(Penn-II)oramaximalprojection-inspiredanalysiswithIPs
andDPs(ChinesePennTreebank[Xueetal.2004]).
3. Becauseatree-basedgoldstandardforparserevaluationmustadopta
particularstyleoflinguisticanalysis(reﬂectedinthegeometryand
nomenclatureofthenodesinthetrees),evaluationofstatisticalparsers
andgrammarsthatarederivedfromparticulartreebankresources(as
wellashand-craftedgrammars/parsers)cansufferundulyifthegold
standarddeviatessystematicallyfromthe(possibly)equallyvalidstyle
oflinguisticanalysisprovidedbytheparser.
Problems such as these have motivated research on more abstract, dependency-
based parser evaluation (e.g., Lin 1995; Carroll, Briscoe, and Sanﬁlippo 1998; Carroll
et al. 2002; Clark and Hockenmaier 2002; King et al. 2003; Preiss 2003; Kaplan et al.2004; Miyao and Tsujii 2004). Dependency-based linguistic representations are ap-
proximations of abstract predicate-argument-adjunct (or more basic head-dependent)
82
Cahilletal. StatisticalParsingUsingAutomaticDependencyStructures
structures, providing a more normalized representation abstracting away from the
particularsofsurfacerealizationorCFG-treerepresentation,whichenablesmeaningful
cross-parserevaluation.
A related contrast holds between shallow and deep grammars and parsers.
1
In
additiontodeﬁningalanguage(asasetofstrings),deepgrammarsrelatestringstoin-
formation/meaning,oftenintheformofpredicate–argumentstructure,dependencyre-
lations,
2
orlogicalforms.Bycontrast,ashallowgrammarsimplydeﬁnesalanguageand
mayassociatesyntactic(e.g.,CFGtree)representationswithstrings.Naturallanguages
do not always interpret linguistic material locally where the material is encountered
in the string (or tree). In order to obtain accurate and complete predicate–argument,
dependency,orlogicalformrepresentations,ahallmarkofdeepgrammarsisthatthey
usuallyinvolvealong-distancedependency(LDD)resolutionmechanism.
Traditionally, deep grammars are hand-crafted (cf. the ALVEY Natural Language
Tools [Briscoe et al. 1987], the Core Language Engine [Alshawi and Pulman 1992], the
AlpinoDutchdependencyparser[Bouma,vanNoord,andMalouf2000],theXeroxLin-
guisticEnvironment[Buttetal.2002],theRASPdependencyparser[CarrollandBriscoe
2002]andtheLinGOEnglishResourceGrammar[Flickinger2000;Baldwinetal.2004]).
Wide-coverage, deep-grammar development, particularly in rich formalisms such as
LFG (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) and HPSG (Pollard
and Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting
aninstanceofthe(in-)famous“knowledgeacquisitionbottleneck”familiarfromother
areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars
(Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002)
have,infact,beensuccessfullyscaledtounrestrictedinput.
The last 15 years have seen extensive efforts on treebank-based automatic gram-
maracquisitionusingavarietyofmachine-learningtechniques(e.g.,Gaizauskas1995;
Charniak1996;Collins1999;Johnson1999;Charniak2000;Bikel2002;Bod2003;Klein
and Manning 2003). These grammars are wide-coverage and robust and in contrast
to manual grammar development, machine-learning-based grammar acquisition in-
curs relatively low development cost. With few notable exceptions,
3
however, these
treebank-induced wide-coverage grammars are shallow: They usually do not attempt
toresolveLDDsnordotheyassociatestringswithmeaningrepresentations.
Over the last few years, addressing the knowledge acquisition bottleneck in deep
constraint-basedgrammardevelopment,agrowingbodyofresearchhasemergedtoau-
tomaticallyacquirewide-coveragedeepgrammarsfromtreebankresources(TAG[Xia
1999], CCG [Hockenmaier and Steedman 2002], HPSG [Miyao, Ninomiya, and Tsujii
2003], LFG [Cahill et al. 2002b, 2004]). To a ﬁrst approximation, these approaches can
be classiﬁed as “conversion”or “annotation”-based. TAG-based approaches convert
1 Ouruseoftheterms“shallow”and“deep”parsers/grammarsfollowsKaplanetal.(2004)where
a“shallowparser”doesnotrelatestringstomeaningrepresentations.Thisdeviatesfromamore
commonuseofthetermswhere,forexample,a“shallowparser”refersto(oftenﬁnite-state-based)
parsers(orchunkers)thatmayproducepartialbracketingsofinputstrings.
2 Bydependencyrelationswemeandeep,ﬁne-grained,labeleddependenciesthatencodelong-distance
dependenciesandpassiveinformation,forexample.Thesedifferfromthetypesofunlabeled
dependencyrelationsinotherworksuchas(McDonaldandPereira2006).
3 BothCollinsModel3(1999)andJohnson(2002)outputCFGtreerepresentationswithtraces.Collins
Model3performsLDDresolutionforwh-relativeclauseconstructions,Johnson(2002)forawiderange
ofLDDphenomenainapost-processingapproachbasedonPenn-IItreefragmentslinkingdisplaced
materialwithwhereitistobeinterpretedsemantically.TheworkofDienesandDubey(2003)and
LevyandManning(2004)issimilartothatofJohnson(2002),recoveringemptycategoriesontopof
CFG-basedparsers.Noneofthemmapstringsintodependencies.
83
ComputationalLinguistics Volume34,Number1
treebank trees into (lexicalized) elementary or adjunct trees. CCG-based approaches
converttreesintoCCGderivationsfromwhichCCGcategoriescanbeextracted.HPSG-
and LFG-based grammar induction methods automatically annotate treebank trees
with(typed)attribute-valuestructureinformationfortheextractionofconstraint-based
grammarsandlexicalresources.
Two recent papers (Preiss 2003; Kaplan et al. 2004) have started tying together
the research strands just sketched: They use dependency-based parser evaluation to
compare wide-coverage parsing systems using hand-crafted, deep, constraint-based
grammars with systems based on a simple version of treebank-based deep grammar
acquisition technology in the conversion paradigm. In the experiments, tree output
generated by Collins’s Model 1, 2, and 3 (1999) and Charniak’s (2000) parsers, for
example,areautomaticallytranslatedintodependencystructuresandevaluatedagainst
gold-standarddependencybanks.
Preiss (2003) uses the grammatical relations and the CBS 500 Dependency Bank
described in Carroll, Briscoe, and Sanﬁlippo (1998) to compare a number of parsing
systems (Briscoe and Carroll 1993; Collins’s 1997 models 1 and 2; and Charniak 2000)
usingasimpleversionoftheconversion-baseddeepgrammaracquisitionprocess(i.e.,
readingoffgrammaticalrelationsfromCFGparsetreesproducedbythetreebank-based
shallowparsers).Thearticlealsoreportsonatask-basedevaluationexperimenttorank
theparsersusingthegrammaticalrelationsasinputtoananaphoraresolutionsystem.
Preissconcludedthatparserrankingusinggrammaticalrelationsreﬂectedtheabsolute
ranking (between treebank-induced parsers) using traditional tree-based metrics, but
thatthedifferencebetweentheperformanceoftheparsingalgorithmsnarrowedwhen
they carried out the anaphora resolution task. Her results show that the hand-crafted
deep uniﬁcation parser (Briscoe and Carroll 1993) outperforms the machine-learned
parsers (Collins 1997; Charniak 2000) on the f-score derived from weighted precision
and recall on grammatical relations.
4
Kaplan et al. (2004) compare their deep, hand-
crafted,LFG-basedXLEparsingsystem(Riezleretal.2002)withCollins’s(1999)model
3 using a simple conversion-based approach, capturing dependencies from the tree
output of the machine-learned parser, and evaluating both parsers against the PARC
700 Dependency Bank (King et al. 2003). They conclude that the hand-crafted, deep
grammaroutperformsthestate-of-the-arttreebank-basedshallowparseronthelevelof
dependencyrepresentation,atthepriceofasmalldecreaseinparsingspeed.
Both Preiss (2003) and Kaplan et al. (2004) emphasize that they use rather basic
versionsoftheconversion-baseddeepgrammaracquisitiontechnologyoutlinedherein.
InthisarticlewerevisittheexperimentscarriedoutbyPreissandKaplanetal.,thistime
usingthesophisticatedandﬁne-grainedtreebank-andannotation-based,deep,probabilis-
ticLFGgrammaracquisitionmethodologydevelopedinCahilletal.(2002b),Cahilletal.
(2004),O’Donovanetal.(2004),andBurke(2006)withanumberofsurprisingresults:
1. EvaluatingagainstthePARC700DependencyBank(Kingetal.2003)
usingaretrainedversionofBikel’s(2002)parser,thebestautomatically
induced,deepLFGresourcesachieveanf-scoreof82.73%.Thisisan
improvementof3.13percentagepointsoverthepreviouslybestpublished
resultsestablishedbyKaplanetal.(2004)whouseahand-crafted,
wide-coverage,deepLFGandtheXLEparsingsystem.Thisisalsoa
4 ThenumbersgivenaredifﬁculttocompareastheresultsfortheBriscoeandCarroll(1993)parserwere
capturedforarichersetofgrammaticalrelationsthanthoseforCollins(1997)andCharniak(2000).
84
Cahilletal. StatisticalParsingUsingAutomaticDependencyStructures
statisticallysigniﬁcantimprovementof2.18percentagepointsoverthe
mostrecentimprovedresultspresentedinthisarticlefortheXLEsystem.
2. EvaluatingagainsttheCarroll,Briscoe,andSanﬁlippo(1998)CBS500
gold-standarddependencybankusingaretrainedversionofBikel’s(2002)
parser,thebestPenn-IItreebank-based,automaticallyacquired,deepLFG
resourcesachieveanf-scoreof80.23%.Thisisastatisticallysigniﬁcant
improvementof3.66percentagepointsoverCarrollandBriscoe(2002),
whouseahand-crafted,wide-coverage,deep,uniﬁcationgrammarand
theRASPparsingsystem.
Evaluationresultsonareannotatedversion(BriscoeandCarroll2006)ofthePARC
700 Dependency Bank were recently published in Clark and Curran (2007), reporting
f-scores of 81.9% for the CCG parser, and 76.3% for RASP. As Briscoe and Carroll
(2006) point out, these evaluations are not directly comparable with the Kaplan et al.(2004) style evaluation against the original PARC 700 Dependency Bank, because the
annotationschemesaredifferent.
The article is structured as follows: In Section 2, we outline the automatic LFG
f-structure annotation algorithm and the pipeline parsing architecture of Cahill et al.(2002b), Cahill et al. (2004), and Burke (2006). In Section 3, we present our experiment
design.InSection4,usingtheDCU105DependencyBankasourdevelopmentset,we
evaluateanumberoftreebank-inducedLFGparsingsystemsagainsttheautomatically
generated Penn-II WSJ Section 22 Dependency Bank test set. We use the Approximate
RandomizationTest(Noreen1989)totestforstatisticalsigniﬁcanceandchoosethebest
parsingsystemfortheevaluationsagainstthewide-coverage,hand-craftedRASPand
LFG grammars of Carroll and Briscoe (2002) and Kaplan et al. (2004) using the CBS
500andPARC700DependencyBanksinSection5.InSection6,wediscussresultsand
issuesraisedbyourmethodology,outlinerelatedandfutureresearchandconcludein
Section7.
2.Methodology
Inthissection,webrieﬂyoutlineLFGandpresentourautomaticf-structureannotation
algorithm and parsing architecture. The parsing architecture enables us to integrate
PCFG-andhistory-basedparsers,whichallowsustocomparetheseparsersatthelevel
ofdependencystructures,ratherthanjusttrees.
2.1LexicalFunctionalGrammar
LexicalFunctionalGrammar(LFG)(KaplanandBresnan1982;Bresnan2001;Dalrymple
2001) is a constraint-based theory of grammar. It (minimally) posits two levels of
representation, c(onstituent)-structure and f(unctional)-structure. C-structure is rep-
resented by context-free phrase-structure trees, and captures surface grammatical
conﬁgurationssuchaswordorder.Thenodesinthetreesareannotatedwithfunctional
equations (attribute-value structure constraints, for example (↑OBJ)=↓) which are
resolved (in the case of well-formed strings) to produce an f-structure. F-structures
arerecursive attribute-valuematrices, representing abstractsyntacticfunctions, which
85
ComputationalLinguistics Volume34,Number1
Figure1
C-andf-structuresforthesentenceU.N.signstreaty.
approximatetobasicpredicate-argument-adjunct structuresordependencyrelations.
5
Figure1showsthec-andf-structuresforthestring U.N. signs treaty.Eachnodeinthe
c-structure is annotated with f-structure equations, for example (↑ SUBJ)= ↓.The
uparrows (↑) point to the f-structure associated with the mother node, downarrows
(↓) to that of the local node. In a complete parse tree, these ↑ and ↓ meta variables are
instantiatedtouniquetreenodeidentiﬁersandasetofconstraints(asetoftermsinan
equalitylogic)isgeneratedwhich(ifsatisﬁable)generatesanf-structure.
2.2AutomaticF-StructureAnnotationAlgorithm
Deep grammars can be induced from treebank resources if the treebank encodes
enough information to support the derivation of deep grammatical information, such
as predicate–argument structures, deep dependency relations, or logical forms. Many
secondgenerationtreebankssuchasPenn-IIprovideinformationtosupportthecompi-
lationofmeaningrepresentations,forexampleintheformoftracesrelatingdisplaced
linguisticmaterialtowhereitshouldbeinterpretedsemantically.Thef-structureanno-
tation algorithm exploits conﬁgurational and categorial information, as well as traces
and the Penn-II functional tag annotations (Table 1) to automatically associate Penn-II
CFGtreeswithLFGf-structureinformation.
Given a tree, such as the Penn-II-style tree in Figure 2, the algorithm will traverse
the tree and deterministically add f-structure equations to the phrasal and leaf nodes
ofthetree,resultinginanf-structureannotatedversionofthetree.Theannotationsare
thencollectedandpassedontoaconstraintsolverwhichgeneratesanf-structure(ifthe
constraints are satisﬁable). We use a simple graph-uniﬁcation-based constraint solver
(EiseleandD¨orre1986),extendedtohandlepath,set-valued,disjunctive,andexistential
constraints.GivenparseroutputwithoutPenn-IIstyleannotationsandtraces,thesame
algorithm is used to assign annotations to each node in the tree, whereas a separate
moduleisappliedattheleveloff-structuretoresolveanylong-distancedependencies
(seeSection2.3).
5 vanGenabithandCrouch(1996,1997)providetranslationsbetweenf-structures,Quasi-LogicalForms
(QLFs),andUnderspeciﬁedDiscourseRepresentationStructures(UDRSs).
86
Cahilletal. StatisticalParsingUsingAutomaticDependencyStructures
Table1
AcompletelistofthePenn-IIfunctionallabels.
Tag Description
Form/functiondiscrepancies
-ADV clausalandNPadverbials
-NOM nonNPsthatfunctionasNPs
Grammaticalrole
-DTV dative
-LGS logicalsubjectsinpassives
-PRD nonVPpredicates
-PUT locativecomplementofput
-SBJ surfacesubject
-TPC topicalizedandfrontedconstituents
-VOC vocatives
Adverbials
-BNF benefactive
-DIR directionandtrajectory
-EXT extent
-LOC location
-MNR manner
-PRP purposeandreason
-TMP temporalphrases
Miscellaneous
-CLR closelyrelatedtoverb
-CLF trueclefts
-HLN headlinesanddatelines
-TTL titles
The f-structure annotation algorithm is described in detail in Cahill et al. (2002a),
McCarthy(2003),Cahilletal.(2004),andBurke(2006).Inbrief,thealgorithmismodular
withfourcomponents(Figure3),takingPenn-IItreesasinputandautomaticallyadding
LFGf-structureequationstoeachnodeinthetree.
Lexical Information. Lexical information is generated automatically by macros for each
of the POS classes in Penn-II. To give a simple example, third-person plural noun
Penn-II POS-word sequences of the form NNS word are automatically associated with
the equations (↑PRED)=word
prime,(↑NUM)=pland(↑PERS) = 3rd, where word
prime
is the
lemmatizedword.
Left–Right Context Annotation. The Left–Right context annotation component identiﬁes
the heads of Penn-II trees using a modiﬁed version of the head ﬁnding rules of
Magerman(1994).Thispartitionseachlocalsubtree(ofdepthone)intoalocalhead,a
left context (left sisters), and a right context (right sisters). The contexts together with
information about the local mother and daughter categories and (if present) Penn-II
87
ComputationalLinguistics Volume34,Number1
Figure2
TreesforthesentenceU.N.signstreaty,theheadlinesaidbeforeandafterautomaticf-structure
annotation,withthef-structureautomaticallyproduced.
Figure3
F-structureannotationalgorithmmodules.
88
Cahilletal. StatisticalParsingUsingAutomaticDependencyStructures
Table2
SamplefromanNPAnnotationmatrix.
Leftcontext Head Rightcontext
DT:(↑SPEC DET)=↓ NN,NNS,NNP,NNPS,NP: RRC,SBAR:(↑RELMOD)=↓
CD:(↑SPEC QUANT)=↓↑=↓ PP:↓∈(↑ADJUNCT)
ADJP,JJ,NN,NNP:↓∈(↑ADJUNCT)NP:↓∈(↑APP)
functionaltaglabels(Table1)areusedbythef-structureannotationalgorithm.Foreach
Penn-IImother(i.e.,phrasal)categoryanAnnotationmatrixexpressesgeneralizations
abouthowtoannotateimmediatedaughtersdominatedbythemothercategoryrelative
to their location in relation to the local head. To give a (much simpliﬁed) example,
theheadﬁndingrulesforNPsstatethattherightmostnominal(NN,NNS,NNP,...)
not preceded by a comma or “-”
6
is likely to be the local head. The Annotation ma-
trix for NPs states (inter alia) that heads are annotated ↑=↓, that DTs (determiners)
to the left of the head are annotated (↑ SPEC DET)=↓, NPs to the right of the head as
↓∈(↑ APP) (appositions). Table 2 provides a sample extract from the NPAnnotation
matrix. Figure 4 provides an example of the application of the NPand PPAnnotation
matricestoasimpletree.
For each phrasal category, Annotation matrices are constructed by inspecting the
mostfrequentPenn-IIruletypesexpandingthecategorysuchthatthetokenoccurrences
oftheseruletypescovermorethan85%ofalloccurrencesofexpansionsofthatcategory
in Penn-II. For NP rules, for example, this means that we analyze the most frequent
102 rule types expanding NP, rather than the complete set of more than 6,500 Penn-II
NPrule types, in order to populate the NPAnnotation matrix. Annotation matrices
generalize to unseen rule types as, in the case of NPs, these may also feature DTs to
the left of the local head and NPs to the right and similarly for rule types expanding
othercategories.
Coordination. In order to support the modularity, maintainability, and extendability of
theannotationalgorithm,theLeft–RightAnnotationmatricesapplyonlytolocaltrees
ofdepthone,whichdonotfeaturecoordination.ThiskeepsthestatementofAnnotation
matrices perspicuous and compact. The Penn-II treatment of coordination is (inten-
tionally) ﬂat. The annotation algorithm has modules for likeand unlike-constituent
coordination.Coordinatedconstituentsareelementsofa COORDsetandannotated ↓∈
(↑ COORD). The Coordination module reuses the Left–Right context Annotation ma-
trices to annotate any remaining nodes in a local subtree containing a coordinating
conjunction. Figure 5 provides a VP-coordination example (with right-node-raising).
Catch-All and Clean-Up. TheCatch-AllandClean-Upmoduleprovidesdefaultstocap-
tureremainingunannotatednodes(Catch-All)andcorrects(Clean-Up)overgeneraliza-
tionsresultingfromtheapplicationoftheLeft–RightcontextAnnotationmatrices.The
Left–RightAnnotationmatricesareallowedacertainamountofovergeneralizationas
thisfacilitatestheperspicuousstatementofgeneralizationsandaseparatestatementof
exceptions,supportingthemodularityandmaintainabilityoftheannotationalgorithm.
PPsunderVPsareacaseinpoint.TheVPAnnotationmatrixanalysesPPstotheright
ofthelocalVPheadasadjuncts: ↓∈(↑ADJUNCT).TheCatch-AllandClean-Upmodule
6 Iftherightmostnominalisprecededbyacommaor“-”,itislikelytobeanappositiontothehead.
89
ComputationalLinguistics Volume34,Number1
Figure4
AutomaticallyannotatedPenn-IItree(fragment)andf-structure(simpliﬁed)forGerryPurdy,
directorofmarketing.
usesPenn-IIfunctionaltag(Table1)information(ifpresent),forexample-CLR(closely
related to local head), to replace the original adjunct analysis by an oblique argument
analysis:(↑OBL)=↓.AnexampleofthisisprovidedbythePP-CLRintheleftVP-conjunct
inFigure5.Inothercases,argument–adjunctdistinctionsareencodedconﬁgurationally
in Penn-II (without the use of -CLR tags). To give a simple example, the NP Anno-
tation matrix indiscriminately associates SBARs to the right of the local head with
(↑ RELMOD)=↓. However, some of these SBARs are actually arguments of the local
NPhead and, unlike SBAR relative clauses which are Chomsky-adjoined to NP(i.e.,
relativeclausesaredaughtersofanNPmotherandsistersofaphrasalNPhead),SBAR
argumentsaresistersofnon-phrasalNPheads.
7
Insuchcases,theCatch-AllandClean-
Up module rewrites the original relative clause analysis into the correct complement
argument analysis (↑COMP)=↓.Figure6showstheCOMP f-structure analyses for an
exampleNPcontaininganinternalSBARargument(ratherthanrelativeclause)node.
Traces. The Traces module translates traces and coindexed material in Penn-II trees
representinglong-distancedependenciesintocorrespondingreentranciesatf-structure.
Penn-IIprovidesaricharsenaloftracetypestorelate“displaced”materialtowhereit
7 StructuralinformationofthiskindisnotencodedintheAnnotationmatrices;compareTable2.
90
Cahilletal. StatisticalParsingUsingAutomaticDependencyStructures
Figure5
AutomaticallyannotatedPenn-IItree(fragment)andresultingf-structureforaskedforand
receivedrefunds.
shouldbeinterpretedsemantically.Thef-structureannotationalgorithmcoverswh-and
wh-less relative clause constructions, interrogatives, control and raising constructions,
right-node-raising,andgeneralICH(interpretconstituenthere)traces.Figure5givesan
example that shows the interplay between coordination, right-node-raising traces and
thecorrespondingautomaticallygeneratedreentranciesatf-structure.
2.3ParsingArchitecture
ThepipelineparsingarchitectureofCahilletal.(2004)andCahill(2004)forparsingraw
textinto LFG f-structures is shown in Figure 7. In this model, PCFGs or history-based
lexicalizedparsersareextractedfromtheunannotatedtreebankandusedtoparseraw
text into trees. The resulting parse trees are then passed to the automatic f-structure
annotationalgorithmtogeneratef-structures.
8
Compared to full Penn-II treebank trees, the output of standard probabilistic
parsers is impoverished: Parsers do not normally output Penn-II functional tag an-
notations(Table1)nordotheyindicate/resolvelong-distancedependencies,recorded
8Intheintegratedmodel(Cahilletal.2004;Cahill2004),weextractf-structureannotatedPCFGs
(A-PCFGs)fromthef-structureannotatedtreebank,whereeachnon-terminalsymbolinthegrammar
hasbeenaugmentedwithLFGfunctionalequations,suchasNP[↑OBJ=↓] → DT[↑SPEC=↓]NN[↑=↓].
Wetreatanon-terminalsymbolfollowedbyannotationsasamonadiccategoryforgrammarextraction
andparsing.ParsingwithA-PCFGsresultsinannotatedparsetrees,fromwhichanf-structurecanbe
generated.Inthisarticleweonlyusethepipelineparsingarchitecture.
91
ComputationalLinguistics Volume34,Number1
Figure6
AutomaticallyannotatedPenn-IItree(fragment)andf-structureforsignsthatmanagers
expectdeclines.
in terms of a ﬁne-grained system of empty productions (traces) and coindexation in
the full Penn-II treebank trees. The f-structure annotation algorithm, as described in
Section 2.2, makes use of Penn-II functional tag information (if present) and relies on
traces and coindexation to capture LDDs in terms of corresponding reentrancies at
f-structure.
Penn-II functional labels are used by the annotation algorithm to discriminate
between adjuncts and (oblique) arguments. PP-sisters to a head verb are analyzed as
arguments iff they are labeled -CLR, -PUT, -DTV or -BNF, for example. Conversely,
functional labels (e.g., -TMP) are also used to analyze certain NPs as adjuncts, and
-LGS labels help to identify logical subjects in passive constructions. In the absence of
functional labels, the annotation algorithm will default to decisions based on simple
structural,conﬁgurational,andCFG-categoryinformation(and,forexample,conserva-
tivelyanalyzeaPPsistertoaheadverbasanadjunct,ratherthanasanargument).
InSections3and4wepresentanumberoftreebank-basedparsers(inparticularthe
PCFGsandaversionofBikel’shistory-based,lexicalizedgenerativeparser)trainedto
output CFG categories with Penn-II functional tags. We achieve this through a simple
92
Cahilletal. StatisticalParsingUsingAutomaticDependencyStructures
Figure7
Treebank-basedLFGparsingarchitecture.
masking and un-masking operation where functional tags are joined with their local
CFG category label to form a new (larger) set of (monadic) CFG category labels (e.g.,
PP-CLRgoestoPP CLR)fortrainingandparsing(forBikel,theparserhead-ﬁndingrules
arealsoadjustedtotheexpandedsetofcategories).Afterparsing,thePenn-IIfunctional
tagsareunmaskedandavailabletothef-structureannotationalgorithm.
TheTracescomponentinthef-structureannotationalgorithm(Figure3)translates
LDDsrepresentedintermsoftracesandcoindexationintheoriginalPenn-IItreebank
trees into corresponding reentrancies at f-structure. Most probabilistic treebank-based
parsers, however, do not indicate/resolve LDDs, and the Traces component of the an-
notationalgorithmdoesnotapply.Initially,thef-structuresproducedforparseroutput
treesinthearchitectureinFigure7arethereforeLDD-unresolved:Theyareincomplete
(orproto)f-structures,wheredisplacedmaterial(e.g.,thevaluesof FOCUS, TOPIC,and
TOPICREL attributes[wh-andwh-lessrelativeclauses,topicalization,andinterrogative
constructions] at f-structure) is not yet linked to the appropriate argument grammati-
cal functions (or elements of adjunct sets) for the governing local PRED. A dedicated
LDD Resolution component in the architecture in Figure 7 turns parser output proto-
f-structures into fully LDD-resolved proper f-structures, without traces and coindexa-
tioninparsetrees.
ConsiderthefollowingfragmentofaproperPenn-IItreebanktree(Figure8),where
the LDD between the WHNPin the relative clause and the embedded direct object
positionoftheverbrewardisindicatedintermsofthetrace*T*-3anditscoindexation
withtheantecedentWHNP-3.Notefurtherthatthecontrolrelationbetweenthesubject
of the verbs wanted and reward is similarly expressed in terms of traces (*T*-2) and
coindexation (NP-SBJ-2). From the treebank tree, the f-structure annotation algorithm
isabletoderiveafullyresolvedf-structurewheretheLDDandthecontrolrelationare
capturedintermsofcorrespondingreentrancies(Figure9).
93
ComputationalLinguistics Volume34,Number1
Figure8
Penn-IItreebanktreewithLDDindicatedintermsoftraces(emptyproductions)and
coindexationandf-structureannotationsgeneratedbytheannotationalgorithm.
Now consider the corresponding “impoverished” (but otherwise correct) parser
outputtree(Figure10)forthesamestring:Theparseroutputdoesnotexplicitlyrecord
thecontrolrelationnortheLDD.
Giventhisparseroutputtree,priortotheLDDresolutioncomponentintheparsing
architecture(Figure7),thef-structureannotationalgorithmwouldinitiallyconstructthe
partial(proto-)f-structureinFigure11,wheretheLDDindicatedbytheTOPICRELfunc-
tionisunresolved(i.e.,thevalueofTOPICRELisnotcoindexedwiththeOBJgrammatical
function of the embedded verb reward). The control relation (shared subject between
the two verbs in the relative clause) is in fact captured by the annotation algorithm in
terms of a default annotation (↑ SUBJ)=(↓ SUBJ) on sole argument VPs to the right of
head verbs (as often, even in the full Penn-II treebank trees, control relations are not
consistentlycapturedthroughexplicitargumenttraces).
In LFG, LDD resolution operates at the level of f-structure, using functional un-
certainty equations (regular expressions over paths in f-structure [Kaplan and Zaenen
1989] relating f-structure components in different parts of an f-structure), obviating
tracesandcoindexationinc-structuretrees.FortheexampleinFigure10,afunctional
uncertainty equation of the form (↑TOPICREL)=(↑[COMP|XCOMP]
∗
[SUBJ|OBJ]) would
beassociatedwiththeWHNPdaughternodeoftheSBARrelativeclause.Theequation
states that the value of the TOPICREL attribute is token-identical (re-entrant) with the
value of a SUBJ or OBJ function, reached through a path along any number (including
94
Cahilletal. StatisticalParsingUsingAutomaticDependencyStructures
Figure9
FullyLDD-resolvedf-structure.
Figure10
Impoverishedparseroutputtree:LDDsnotcaptured.
zero) of COMP or XCOMP attributes. This equation, together with subcategorization
frames(LFGsemanticforms)forthelocal PREDsandtheusualLFGcompletenessand
coherence conditions, resolve the partial proto-f-structure in Figure 11 into the fully
LDD-resolvedproperf-structureinFigure9.
95
ComputationalLinguistics Volume34,Number1
Figure11
Proto-f-structure:LDDsnotcaptured.
Following Cahill et al. (2004), in our parsing architecture (Figure 7) we model
LFG LDD resolution using automatically induced ﬁnite approximations of functional-
uncertainty equations and subcategorization frames from the f-structure-annotated
Penn-II treebank (O’Donovan et al. 2004) in an LDD resolution component. From the
fully LDD-resolved f-structures from the Penn-II training section treebank trees we
learn probabilistic LDD resolution paths (reentrancies in f-structure), conditional on
LDD type (Table 3), and subcategorization frames, conditional on lemma (and voice)
(Table 4). Table 3 lists the eight most probable TOPICREL paths (out of a total of 37
TOPICREL paths acquired). The totality of these paths constitutes a ﬁnite subset of the
reference language deﬁnde by the full functional uncertainty equation (↑TOPICREL)=
(↑[COMP|XCOMP]
∗
[SUBJ|OBJ]). Given an unresolved LDD type (such as TOPICREL in
the parser output for the relative clause example in Figure 11), admissible LDD res-
olutions assert a reentrancy between the value of the LDD trigger (here, TOPICREL)
and a grammatical function (or adjunct set element) of an embedded local predicate,
subjecttotheconditionsthat(i)thelocalpredicatecanbereachedfromtheLDDtrigger
using the LDD path; (ii) the grammatical function terminates the LDD path; (iii) the
grammatical function is not already present (at the relevant level of embedding in
the local f-structure); and (vi) the local predicate subcategorizes for the grammatical
function in question.
9
Solutions satisfying (i)–(iv) are ranked using the product of
LDD path and subcategorization frame probabilities and the highest ranked solution
(possibly involving multiple interacting LDDs for a single f-structure) is returned by
thealgorithm(fordetailsandcomparisonagainstalternativeLDDresolutionmethods,
seeCahilletal.2004).
10
For our example (Figure 11), the highest ranked LDD resolution is for LDD path
(↑TOPICREL)=(↑ XCOMPOBJ )andthelocalsubcatframe REWARD〈↑ SUBJ, ↑ OBJ〉.This
9 Conditions(i)–(iv)aresuitablyadaptedforLDDresolutionsterminatinginadjunctsets.
10 InourexperimentswedonotusethelimitedLDDresolutionforwh-phrasesprovidedbyCollins’sModel
3parserasbetterresultsareachievedusingthepurelyf-structure-basedLDDresolutionasshownin
Cahilletal.(2004).
96
Cahilletal. StatisticalParsingUsingAutomaticDependencyStructures
Table3
Mostfrequentwh-TOPICRELpaths.
wh-TOPICREL Probability wh-TOPICREL Probability
subj .7583 xcomp .0830
obj .0458 xcomp:obj .0338
xcomp:xcomp .0168 xcomp:subj .0109
comp .0097 comp:subj .0073
Table4
Mostfrequentsemanticformsforactiveandpassive(p)occurrencesoftheverbwantand
reward.
Semanticform Probability
want([subj,xcomp]) .6208
want([subj,obj]) .2496
want([subj,obj,xcomp]) .1008
want([subj]) .0096
want([subj,obj,obl]) .0048
want([subj,obj,part]),p) .5000
want([subj,obl]),p) .1667
want([subj,part]),p) .1667
want([subj]),p) .1667
reward([subj,obj]) .8000
reward([subj,obj,obl]) .2000
reward([subj]),p) 1.0000
(together with the subject control equation described previously) turns the parser-
output proto-f-structure (in Figure 11) into the fully LDD resolved f-structure in
(Figure9).
The full pipeline parsing architecture with the LDD resolution (rather than the
Traces component for LDD resolved Penn-II treebank trees) component (and the LDD
pathandsubcategorizationframeextraction)isgiveninFigure7.
The pipeline architecture supports ﬂexible integration of treebank-based PCFGs
orstate-of-the-art, history-based, and lexicalized parsers (Collins1999; Charniak 2000;
Bikel2002)andenablesdependency-basedevaluationofsuchparsers.
3.ExperimentDesign
In our experiments we compare four history-based parsers for integration into the
pipelineparsingarchitecturedescribedinSection2.3:
a114
Collins’s1999Models3
11
a114
Charniak’s2000maximum-entropyinspiredparser
12
11 Downloadedfromftp://ftp.cis.upenn.edu/pub/mcollins/PARSER.tar.gz.
12 Downloadedfromftp://ftp.cs.brown.edu/pub/nlparser/.
97
ComputationalLinguistics Volume34,Number1
a114
Bikel’s2002emulationofCollinsModel2
13
a114
aretrainedversionofBikel’s(2002)parserwhichretainsPenn-IIfunctional
tags
InputforCollins’sandBikel’sparserswaspre-taggedusingtheMXPOSTPOStag-
ger(Ratnaparkhi1996).Charniak’sparserprovidesitsownPOStagger.Thecombined
system of best history-based parser and automatic f-structure annotation is compared
totwoprobabilisticparsingsystemsbasedonhand-crafted,wide-coverage,constraint-
based,deepgrammars:
a114
theRASPparsingsystem(CarrollandBriscoe2002)
a114
theXLEparsingsystem(Riezleretal.2002;Kaplanetal.2004)
Both hand-crafted grammars perform their own POS tagging, resolve LDDs, and
associate strings with dependency relations (in the form of grammatical relations or
LFGf-structures).
We evaluate the parsers against a number of gold-standard dependency banks.
We use the DCU 105 Dependency Bank (Cahill et al. 2002a) as our development set
for the treebank-based LFG parsers. We use the f-structure annotation algorithm to
automatically generate a gold-standard test set from the original Section 22 treebank
trees (the f-structure annotated WSJ Section 22 Dependency Bank) to choose the best
treebank-based LFG parsing systems for the PARC 700 and CBS 500 experiments.
FollowingtheexperimentalsetupinKaplanetal.(2004),weusethePenn-IISection23-
basedPARC700DependencyBank(Kingetal.2003)toevaluatethetreebank-induced
LFG resources against the hand-crafted XLE grammar and parsing system of Riezler
etal.(2002)andKaplanetal.FollowingPreiss(2003),weusetheSUSANNEBasedCBS
500DependencyBank(Carroll,Briscoe,andSanﬁlippo1998)toevaluatethetreebank-
induced LFG resources against the hand-crafted RASPgrammar and parsing system
(CarrollandBriscoe2002)aswellasagainsttheXLEsystem(Riezleretal.2002).
For each gold standard, our experiment design is as follows: We parse automati-
cally tagged input
14
sentences with the treebankand machine-learning-based parsers
trained on WSJ Sections 02–21 in the pipeline architecture, pass the resulting parse
treestoourautomaticf-structureannotationalgorithm,collectthef-structureequations,
pass them to a constraint-solver which generates an f-structure, resolve long-distance
dependenciesatf-structurefollowingCahilletal.(2004)andconverttheresultingLDD-
resolved f-structures into dependency representations using the formats and software
of Crouch et al. (2002) (for the DCU 105, PARC 700, and WSJ Section 22 evaluations)
and the formats and software of Carroll, Briscoe, and Sanﬁlippo (1998) (for the CBS
500evaluation).Intheexperimentswedidnotuseanyadditionalannotationssuchas
-A (for argument) that can be generated by some of the history-based parsers (Collins
1999) as the f-structure annotation algorithm is designed for Penn-II trees (which do
notcontainsuchannotations). WealsodidnotusethelimitedLDDresolution for wh-
relative clauses provided by Collins’s Model 3 as better results are achieved by LDD
13 ThiswasdevelopedattheUniversityofPennsylvaniabyDanBikelandisfreelyavailabletodownload
fromhttp://www.cis.upenn.edu/∼dbikel/software.html.
14 TagswereautomaticallyassignedeitherbytheparsersthemselvesorbytheMXPOSTtagger
(Ratnaparkhi1996).
98
Cahilletal. StatisticalParsingUsingAutomaticDependencyStructures
Table5
Resultsoftree-basedevaluationonallsentencesWSJsection23,Penn-II.
Parser Labeled
f-score(%)
PCFG 73.03
Parent-PCFG 78.05
CollinsM3 88.33
Charniak 89.73
Bikel 88.32
Bikel+Tags 87.53
resolutiononf-structure(Cahilletal.2004).Acompletesetofparametersettingsforthe
parsersisprovidedintheAppendix.
In order to evaluate the treebank-induced LFG resources against the PARC 700
and the CBS 500 dependency banks, a certain amount of automatic mapping is re-
quired to account for systematic differences in linguistic analysis, feature geometry,
and nomenclature at the level of dependencies. This is discussed in Sections 5.1 and
5.2.Throughout,weusetheApproximateRandomizationTest(Noreen1989)totestthe
statisticalsigniﬁcanceoftheresults.
4.ChoosingaTreebank-BasedLFGParsingSystem
Inthissection,wechoosethebesttreebank-basedLFGparsingsystemforthecompar-
isons with the hand-crafted XLE and RASPresources in Section 5. We use the DCU
105 Dependency Bank as our development set and carry out comparative evaluation
andstatisticalsigniﬁcancetestingonthelarger,automaticallygeneratedWSJSection22
Dependency Bank as a test set. The system based on Bikel’s (2002) parser retrained to
retainPenn-IIfunctionaltags(Table1)achievesoverallbestresults.
4.1Tree-BasedEvaluationagainstWSJSection23
For reference, we include the traditional CFG-tree-based comparison for treebank-
inducedparsers.TheparsersaretrainedonSections02to21ofthePenn-IITreebankand
testedonSection23.Thepublishedresults
15
ontheseexperimentsforthehistory-based
parsersaregiveninTable5.WealsoincludeﬁguresforaPCFGandaParent-PCFG(a
PCFG which has undergone the parent transformation [Johnson 1999]). These PCFGs
areinducedfollowingstandardtreebankpreprocessingsteps,includingeliminationof
emptynodes,butfollowingCahilletal.(2004),theydoincludePenn-IIfunctionaltags
(Table1),asthesetagscontainvaluableinformationfortheautomaticf-structureanno-
tationalgorithm(Section2.2).Thesetagsareremovedforthetree-basedevaluation.
The results show that the history-based parsers produce considerably better trees
than the more basic PCFGs (with and without parent transformations). Charniak’s
(2000) parser scores best with an f-score of 89.73% on all sentences in Section 23. The
15 WheretherewerenopublishedresultsavailableforSection23,wecalculatedthemusingthe
downloadableversionsoftheparsers.
99
ComputationalLinguistics Volume34,Number1
vanilla PCFG achieves the lowest f-score of 73.03%, a difference of 16.7 percentage
points. The hand-crafted XLE and RASPgrammars achieve around 80% coverage
(measured in terms of complete spanning parse) on Section 23 and use a variety of
(longest)fragmentscombiningtechniquestogeneratedependencyrepresentations for
theremaining20%ofSection23strings.Bycontrast,thetreebank-inducedPCFGsand
history-basedparsersallachievecoverageofover99.9%.Giventhatthehistory-based
parsers score considerably better than PCFGs on trees, we would also expect them to
producedependencystructuresofsubstantiallyhigherquality.
4.2UsingDCU105asaDevelopmentSet
The DCU 105 (Cahill et al. 2002a) is a hand-crafted gold-standard dependency bank
for105sentences,randomlychosenfromSection23ofthePenn-IITreebank.
16
Thisisa
relativelysmallgoldstandard,initiallydevelopedtoevaluatetheautomaticf-structure
annotation algorithm. We parse the 105 tagged sentences into LFG f-structures with
eachofthetreebank-inducedparsersinthepipelineparsingandf-structureannotation
architecture.Thef-structuresofthegoldstandardandthef-structuresreturnedbythe
parsing systems are converted into dependency triples following Crouch et al. (2002)
and Riezler et al. (2002) and we also use their software for evaluation. The following
dependencytriplesareproducedbythef-structureinFigure1:
subj(sign∼0,U.N.∼1)
obj(sign∼0,treaty∼2)
num(U.N.∼1,sg)
pers(U.N.∼1,3)
num(treaty∼2,sg)
pers(treaty∼3,3)
tense(sign∼0,present)
Weevaluatepreds-onlyf-structures(i.e.,wherepathsinf-structuresendina PRED
value: the predicate-argument-adjunct structure skeleton) and all grammatical func-
tions(GFs)includingnumber,tense,person,andsoon.TheresultsaregiveninTable6.
With one main exception, Tables 5 and 6 conﬁrm the general expectation that
the better the trees produced by the parsers, the better the f-structures automatically
generated for those trees. The exception is Bikel+Tags. The automatic f-structure an-
notationalgorithmwillexploitPenn-IIfunctionaltaginformationifpresenttogenerate
appropriatef-structureequations(seeSection2.2).Itwilldefaulttopossiblylessreliable
conﬁgurationalandcategorialinformationifPenn-IItagsarenotpresentinthetrees.
In order to test whether the retention of Penn-II functional labels in the history-
based parser output will improve LFG f-structure-based dependency results, we use
Bikel’s (2002) training software,
17
and retrain the parser on a version of the Penn-II
treebank (Sections 02 to 21) with the Penn-II functional tag labels (Table 1) annotated
insuchawaythattheresultinghistory-basedparserwillretainthem(Section2.3).The
retrained parser (Bikel+Tags) then produces CFG-trees with Penn-II functional labels
andtheseareusedbythef-structureannotationalgorithm.Weevaluatethef-structure
dependenciesagainsttheDCU105(Table6)andachieveanf-scoreof82.92%preds-only
16 Itispubliclyavailablefordownloadfrom:http://nclt.computing.dcu.ie/gold105.txt.
17 WeuseBikel’ssoftwareratherthanCharniak’sforthisexperimentastheformerprovedmorestable
duringtheretrainingphase.
100
Cahilletal. StatisticalParsingUsingAutomaticDependencyStructures
Table6
Treebank-inducedparsers:resultsofdependency-basedevaluationagainstDCU105.
Parser Predsonly AllGFs
f-score(%) f-score(%)
PCFG 70.24 79.90
Parent-PCFG 75.84 83.58
CollinsM3 77.84 85.08
Charniak 79.61 85.66
Bikel 79.39 86.56
Bikel+Tags 82.92 88.30
Table7
Treebankinducedparsers:breakdownbydependencyrelationofpreds-onlyevaluationagainst
DCU105.
Dep. Percentoftotal F-score(%)
Parent-PCFG CollinsM3 Charniak Bikel Bikel+Tags
ADJUNCT 33.73 71 72 76 73 79
APP 0.68 61 0 55 70 65
COMP 2.31 60 61 66 61 73
COORD 5.73 64 73 77 67 76
DET 9.58 91 93 96 96 96
FOCUS 0.04 100 100 0 100 100
OBJ 16.42 82 84 86 85 90
OBJ20.07805755750
OBL 2.17 58 24 27 23 63
OBL20.075000067
OBL AG 0.43 40 96 96 92 92
POSS 2.88 80 83 82 82 79
QUANT 1.85 70 67 69 70 70
RELMOD 1.78 50 78 67 78 73
SUBJ 14.74 80 81 83 85 85
TOPIC 0.46 85 87 96 96 89
TOPICREL 1.85 61 80 80 79 74
XCOMP 5.20 90 92 79 93 93
and88.3%allGFs.AdetailedbreakdownbydependencyisgiveninTable7.Thesystem
basedontheretrainedparserisnowmuchbetterabletoidentifyobliqueargumentsand
overallpreds-onlyaccuracyhasimprovedby3.53%overtheoriginalBikelexperiment
and3.31%overCharniak’sparser,eventhoughCharniak’sparserperformsmorethan
2%betteronthetree-basedscoresinTable5andeventhoughtheretrainedparserdrops
0.79%againsttheoriginalBikelparseronthetree-basedscores.
18
Inspection of the results broken down by grammatical function (Table 7) for the
preds-onlyevaluationagainsttheDCU105showsthatjustoveronethirdofalldepen-
dencytriplesinthegoldstandardareadjuncts. SUBJ(ects)and OBJ(ects)togethermake
upafurther30%.
18 TheﬁguressuggestthatretrainingCharniak’sparsertoretainPenn-IIfunctionaltagsislikelytoproduce
evenbetterdependencyscoresthanthoseachievedbyBikel’sretrainedparser.
101
ComputationalLinguistics Volume34,Number1
Table 7 shows that the treebank-based LFG system using Collins’s Models 3 is
unable to identify APP(osition). This is due to Collins’s treatment of punctuation and
the fact that punctuation is often required to reliably identify apposition.
19
None of
the original history-based parsers produced trees which enabled the annotation algo-
rithm to identify second oblique dependencies (OBL2), and they generally performed
considerablyworsethanParent-PCFGwhenidentifying OBL(ique)dependencies.This
is because the automatic f-structure annotation algorithm is cautious to the point of
undergeneralizationwhenidentifyingobliquearguments.Inmanycases,thealgorithm
reliesonthepresenceof,forexample,a-CLRPenn-IIfunctionallabel(indicatingthatthe
phraseiscloselyrelatedtotheverb),andthehistory-based(CollinsM3,Charniak,and
Bikel)parsersdonotproducetheselabels,whereasParent-PCFG(aswellasPCFG)are
trained to retain Penn-II functional labels. Parent-PCFG, by contrast, performs poorly
forobliqueagents(OBL AG,agentiveby-phrasesinpassiveconstructions),whereasthe
history-based parsers are able to identify these with considerable accuracy. This is be-
causeParent-PCFGoftenerroneouslyﬁndsobliqueagents,evenwhenthepreposition
isnotby,asitneverhasenoughcontextinwhichtodistinguishbyprepositionalphrases
from other PPs. The history-based parsers produce trees from which the automatic
f-structureannotationalgorithmcanbetteridentify RELMODand TOPICRELdependen-
cies than Parent-PCFG. This, in turn, leads to improved long distance dependency
resolutionwhichimprovesoverallaccuracy.
TheDCU105developmentsetistoosmalltosupportreliablestatisticalsigniﬁcance
testing of the performance ranking of the six treebank-based LFG parsing systems. In
order to carry out signiﬁcance testing to select the best treebank-based LFG parsing
system for comparative evaluation against the hand-crafted deep XLE and RASPre-
sources,wemovetoalargerdependency-basedevaluationdataset:thegold-standard
dependencybankautomaticallygeneratedfromWSJSection22.
4.3EvaluationagainstWSJSection22Dependencies
In an experimental setup similar to that of Hockenmaier and Steedman (2002),
20
we
evaluate each parser against alarge automatically generated gold standard. Thegold-
standarddependencybankisautomaticallygeneratedbyannotatingthe original 1,700
treebank trees from WSJ Section 22 of the Penn-II Treebank with our f-structure an-
notation algorithm. We then evaluate the f-structures generated from the tree output
of the six parsers trained on Sections 02 to 21 resulting from parsing the Section 22
stringsagainsttheautomaticallyproducedf-structuresfortheoriginalSection22Penn-II
treebanktrees.TheresultsaregiveninTable8.
ComparedtoTable6fortheDCU105goldstandard,mostscoresareup,particularly
so for the history-based parsers. This trend is possibly due to the fact that the WSJ
19 TheannotationalgorithmreliesonPenn-II-stylepunctuationpatternswherean NPappositionfollowsa
nominalheadseparatedbyacomma([NP[NPBush],[NPthepresident]]),allthreesistersofthesame
mothernode,whilethetreesproducedbyCollins’sparserattachthecommalowinthetree([NP[NP
Bush,][NPthepresident]]).AlthoughitwouldbetrivialtocarryoutatreetransformationontheCollins
outputtoraisethepunctuationtotheexpectedlevel,wehavenotdonethishere.
20 ThiscorrespondstoexperimentswheretheoriginalPenn-IISection23treebanktreesareautomatically
convertedintoCCGderivations,whicharethenusedasagoldstandardtoevaluatetheCCGparser
trainedonSections02–21.Asimilarmethodologyisusedfortheevaluationoftreebank-basedHPSG
resources(Miyao,Ninomiya,andTsujii2003)wherePenn-IItreebanktreesareautomaticallyannotated
withHPSGtyped-featurestructureinformation.
102
Cahilletal. StatisticalParsingUsingAutomaticDependencyStructures
Table8
Resultsofdependency-basedevaluationagainsttheautomaticallygeneratedgoldstandardfor
WSJSection22.
Parser Predsonly AllGFs
f-score(%) f-score(%)
PCFG 70.76 80.44
Parent-PCFG 74.92 83.04
CollinsM3 79.30 86.00
Charniak 81.35 86.96
Bikel 81.40 87.00
Bikel+Tags 83.06 87.63
Section22goldstandardisgeneratedautomaticallyfromtheoriginal“perfect”Penn-II
treebanktreesusingtheautomaticf-structureannotationalgorithm,whereastheDCU
105 has been created manually without regard as to whether or not the f-structure
annotation algorithm could ever generate the f-structures, even given the “perfect”
trees.
The LFG system based on Bikel’s retrained parser achieves the highest f-score of
83.06% preds-only and 87.63% all GFs. Parent-PCFG achieves an f-score of 74.92%
preds-onlyand83.04%allGFs.Table9providesabreakdownbyfeatureofthepreds-
onlyevaluation.
Table 9 shows that, once again, the automatic f-structure annotation algorithm is
notabletoidentifyanycasesofappositionfromtheoutputofCollins’sModel3parser.
ApartfromBikel’sretrainedparser,noneofthehistory-basedparsersareabletoidentify
Table9
Breakdownbydependencyofresultsofpreds-onlyevaluationagainsttheautomatically
generatedSection22goldstandard.
Dep. Percentoftotal F-score(%)
Parent-PCFG CollinsM3 Charniak Bikel Bikel+Tags
ADJUNCT 33.77 70 75 78 78 80
APP 0.74 61 0 77 77 71
COMP 1.35 60 72 70 70 80
COORD 5.11 74 78 82 82 81
DET 10.72 88 91 92 92 91
FOCUS 0.02 27 67 88 88 71
OBJ 16.17 80 85 87 87 88
OBJ20.071530323271
OBL 1.92 50 19 21 21 73
OBL20.074733369
OBL AG 0.31 50 90 89 89 85
POSS 2.47 86 91 91 91 91
QUANT 2.12 89 89 93 93 92
RELMOD 1.84 51 71 72 72 69
SUBJ 15.45 75 80 81 81 82
TOPIC 0.44 81 85 84 84 76
TOPICREL 1.82 61 72 74 75 69
XCOMP 5.62 81 87 81 81 88
103
ComputationalLinguistics Volume34,Number1
Figure12
ApproximateRandomizationTestforstatisticalsigniﬁcancetesting.
OBJ2, OBL or OBL2 dependencies very well, although Parent-PCFG is able to produce
treesfromwhichitiseasiertoidentifyobliques(OBL),becauseofthePenn-IIfunctional
-CLR label. The automatic annotation algorithm is unable to identify RELMOD depen-
denciessatisfactorily fromthetreesproduced byparsingwithParent-PCFG,although
the history-based parsers score reasonably well for this function. Whereas Charniak’s
parserisabletoidentifysomedependenciesbetterthanBikel’sretrainedparser,overall
the system based on Bikel’s retrained parser performs better when evaluating against
thedependenciesinWSJSection22.
Inordertodeterminewhethertheresultsarestatisticallysigniﬁcant,weusetheAp-
proximateRandomizationTest(Noreen1989).
21
Thistestisanexampleofacomputer-
intensivestatisticalhypothesistest.Suchtestsaredesignedtoassessresultdifferences
withrespecttoateststatisticincaseswherethesamplingdistributionoftheteststatistic
is unknown. Comparative evaluations of outputs of parsing systems according to test
statistics,suchasdifferencesinf-score,areexamplesofthissituation.Theteststatistics
are computed by accumulating certain count variables over the sentences in the test
set. In the case of f-score, variable tuples consisting of the number of dependency-
relations in the parse for the system translation, the number of dependency-relations
in the parse for the reference translation, and the number of matching dependency-
relationsbetweensystemandreferenceparse,areaccumulatedoverthetestset.
Under the null hypothesis, the compared systems are not different, thus any vari-
abletupleproducedbyoneofthesystemscouldjustaslikelyhavebeenproducedby
theothersystem.Soshufﬂingthevariabletuplesbetweenthetwosystemswithequal
probability, and recomputing the test statistic, creates an approximate distribution of
the test statistic under the null hypothesis. For a test set of S sentences there are 2
S
different ways to shufﬂe the variable tuples between the two systems. Approximate
randomization produces shufﬂes by random assignments instead of evaluating all 2
S
possibleassignments.Signiﬁcancelevelsarecomputedasthepercentageoftrialswhere
the pseudo statistic, that is the test statistic computed on the shufﬂed data, is greater
thanorequaltotheactualstatistic,thatistheteststatisticcomputedonthetestdata.A
sketchofanalgorithmforapproximaterandomizationtestingisgiveninFigure12.
21 ApplicationsofthistesttonaturallanguageprocessingproblemscanbefoundinChinchoretal.(1993)
andYeh(2000).
104
Cahilletal. StatisticalParsingUsingAutomaticDependencyStructures
Table10
ComparingparsersevaluatedagainstSection22dependencies(preds-only):p-valuesfor
approximaterandomizationtestfor10,000,000randomizations.
PCFG Parent-PCFG CollinsM3 Charniak Bikel Bikel+Tags
PCFG -
Parent-PCFG <.0001 -
CollinsM3 <.0001 <.0001 -
Charniak <.0001 <.0001 <.0001 -
Bikel <.0001 <.0001 <.0001 .0003 -
Bikel+Tags <.0001 <.0001 <.0001 <.0001 <.0001 -
Table10givesthep-values(thesmallestﬁxedlevelatwhichthenullhypothesiscan
be rejected) for comparing each parser against all of the other parsers. We test for sig-
niﬁcanceatthe95%level.Becausewearedoingapairwisecomparisonofsixsystems,
giving15comparisons,thep-valueneedstobebelow.0034fortheretobeasigniﬁcant
difference at the 95% level.
22
For each parser, the values in the row corresponding to
that parser represent the p-values for those parsers that achieve a lower f-score than
thatparser.ThisshowsthatthesystembasedonBikel’sretrainedparserissigniﬁcantly
betterthanthosebasedontheotherparserswithastatisticalsigniﬁcanceof>95%.For
theXLEandRASPcomparisons,wewillusethef-structure-annotationalgorithmand
Bikelretrained-basedLFGsystem.
5.Cross-FormalismComparisonofTreebank-InducedandHand-CraftedGrammars
From the experiments in Section 4, we choose the treebank-based LFG system using
the retrained version of Bikel’s parser (which retains Penn-II functional tag labels) to
compareagainstparsingsystemsusingdeep,hand-crafted,constraint-basedgrammars
at the level of dependencies. We report on two experiments. In the ﬁrst experiment
(Section 5.1), we evaluate the f-structure annotation algorithm and Bikel retrained
parser-basedLFGsystemagainstthehand-crafted,wide-coverageLFGandXLEpars-
ingsystem(Riezleretal.2002;Kaplanetal.2004)onthePARC700DependencyBank
(Kingetal.2003).Inthesecondexperiment(Section5.2),weevaluateagainstthehand-
crafted, wide-coverage uniﬁcation grammar and RASPparsing system of Carroll and
Briscoe(2002)ontheCBS500DependencyBank(Carroll,Briscoe,andSanﬁlippo1998).
5.1EvaluationagainstPARC700
The PARC 700 Dependency Bank (King et al. 2003) provides dependency relations
(includingLDDrelations)for700sentencesrandomlyselectedfromWSJSection23of
thePenn-IITreebank.Inordertoevaluatetheparsers,wefollowtheexperimentalsetup
ofKaplanetal.(2004)withasplitof560dependencystructuresforthetestsetand140
for the development set. The set of features (Table 12, later in this article) evaluated
in the experiment form a proper superset of preds-only, but a proper subset of all
22 BasedonCohen(1995,p.190):α
e
≈1–(1–α
c
)
m,wheremisthenumberofpairwisecomparisons,α
e
is
theexperiment-wiseerror,andα
c
istheper-comparisonerror.
105
ComputationalLinguistics Volume34,Number1
Figure13
PARC700conversionsoftware.
grammaticalfunctions(preds-only⊂PARC⊂allGFs).Thisfeaturesetwasselectedin
Kaplanetal.becausethefeaturescarryimportantsemanticinformation.Therearesys-
tematicdifferencesbetweenthePARC700dependenciesandthef-structuresgenerated
in our approach as regards feature geometry, feature nomenclature, and the treatment
ofnamedentities.InordertoevaluateagainstthePARC700testset,weautomatically
map the f-structures produced by our parsers to a format similar to that of the PARC
700DependencyBank.Thisisdonewithconversionsoftwareinapost-processingstage
onthef-structureannotatedtrees(Figure13).
The conversion software is developed on the 140-sentence development set of the
PARC700,exceptfortheMulti-WordExpressionssection.Followingtheexperimental
setup of Kaplan et al. (2004), we mark up multi-word expression predicates based on
thegold-standardPARC700DependencyBank.
Multi-WordExpressions The f-structure annotation algorithm analyzes the internal
structure of all noun phrases fully. In Figure 14, for example, BT is analyzed as
an ADJUNCTmodiﬁeroftheheadsecurities,whereasPARC700analyzesthisand
other (more complex) named entities as multi-word expression predicates. The
conversionsoftwaretransformstheoutputofthef-structureannotationalgorithm
intothemulti-wordexpressionpredicateformat.
FeatureGeometry In constructions such as Figure 2, the f-structure annotation algo-
rithm analyzes say as the main PRED with what is said as the value of a COMP
argument. In the PARC 700, these constructions are analyzed in such a way that
what is said/reported provides the top level f-structure whereas other material
(who reported, etc.) is analyzed in terms of ADJUNCTs modifying the top level
f-structure.Afurthersystematicstructuraldivergenceisprovidedbytheanalysis
Figure14
Namedentityand OBL AGfeaturegeometrymapping.
106
Cahilletal. StatisticalParsingUsingAutomaticDependencyStructures
of passive oblique agent constructions (Figure 14): The f-structure annotation
algorithmgeneratesacomplexinternalanalysisoftheobliqueagentPP,whereas
thePARCanalysisencodesaﬂatrepresentation.Theconversionsoftwareadjusts
theoutputofthef-structureannotationalgorithmtothePARC-styleencodingof
linguisticinformation.
FeatureNomenclature There are a number of systematic differences between feature
namesusedbytheautomaticannotationalgorithmandPARC700:Forexample,
DETisDET FORMinthePARC700,COORDisCONJ,FOCUSisFOCUS INT.Nomen-
clature differences are treated in terms of a simple relabeling by the conversion
software.
AdditionalFeatures AnumberoffeaturesinthePARC700arenotproducedbytheau-
tomatic annotation algorithm. These include: AQUANT for adjectival quantiﬁers,
MOD for NP-internal modiﬁers, and STMT TYPE for statement type (declarative,
interrogative, etc.). Additional features (and their values) are automatically gen-
erated by the mapping software, using categorial, conﬁgurational, and already
produced f-structure annotation information, extending the original annotation
algorithm.
XCOMPFlattening The automatic annotation algorithm treats both auxiliary and
modal verb constructions in terms of hierarchically cascading XCOMPs, whereas
inPARC700thetemporalandaspectualinformationexpressedbyauxiliaryverbs
isrepresentedintermsofaﬂatanalysisandfeatures(Figure15).Theconversion
softwareautomaticallyﬂattensthef-structuresproducedbytheautomaticanno-
tationalgorithmintothePARC-styleencoding.
Forfulldetailsofthemapping,seeBurkeetal.(2004).
In our parsing experiments, we used the most up-to-date version of the hand-
crafted, wide-coverage, deep LFG resources and XLE parsing system with improved
results over those reported in Kaplan et al. (2004): This latest version achieves 80.55%
f-score, a 0.95 percentage point improvement on the previous 79.6%. The XLE parsing
system combines a large-scale, hand-crafted LFG for English and a statistical disam-
biguation component to choose the most likely analysis among those returned by
the symbolic parser. The statistical component is a log-linear model trained on 10,000
partially labeled structures from the WSJ. The results of the parsing experiments are
presented in Table 11. We also include a ﬁgure for the upper bound of each system.
23
Using Bikel’s retrained parser, the treebank-based LFG system achieves an f-score of
82.73%, and the hand-crafted grammar and XLE-based system achieves an f-score
of 80.55%. The approximate randomization test produced a p-value of .0054 for this
pairwise comparison, showing that this result difference is statistically signiﬁcant at
the95%level.Evaluationresultsonareannotatedversion(BriscoeandCarroll2006)of
the PARC 700 Dependency Bank were recently published in Clark and Curran (2007),
reporting f-scores of 81.9% for the CCG parser, and 76.3% for RASP. As Briscoe and
23 Theupperboundforthetreebank-basedLFGsystemisdeterminedbytakingtheoriginalPenn-IIWSJ
Section23treescorrespondingtothePARC700strings,automaticallyannotatingthemwiththe
f-structureannotationalgorithm,andevaluatingthef-structuresagainstthePARC700dependencies.The
upperboundfortheXLEsystemisdeterminedbyselectingtheXLEparsethatscoresbestagainstthe
PARC700dependenciesforeachofthePARC700strings.Itisinterestingtonotethattheupperbound
forthetreebank-basedsystemisonly1.18percentagepointshigherthanthatfortheXLEsystem.Apart
fromthetwodifferentmethodsforestablishingtheupperbounds,thisismostlikelyduetothefactthat
themappingrequiredforevaluatingthetreebank-basedLFGsystemagainstPARC700islossy(cf.the
discussioninSection6).
107
ComputationalLinguistics Volume34,Number1
Figure15
DCU105andPARC700analysesforthesentenceUnlike1987,interestrateshavebeenfalling
thisyear.
Carroll point out, these evaluations are not directly comparable with the Kaplan et al.(2004) style evaluation against the original PARC 700 Dependency Bank, because the
annotationschemesaredifferent.Kaplanetal.andourexperimentsuseaﬁne-grained
feature set of 34 features (Table 12), while the Briscoe and Carroll scheme uses 17
features.
A breakdown by dependency relation for each system is given in Table 12. The
treebank-induced grammarsystemcanbetteridentify DET FORM, SUBORD FORM,and
Table11
ResultsofevaluationagainstthePARC700DependencyBankfollowingtheexperimentalsetup
ofKaplanetal.(2004).
Bikel+Tags XLE p-Value
F-score 82.73 80.55 .0054
Upperbound 86.83 85.65 -
108
Cahilletal. StatisticalParsingUsingAutomaticDependencyStructures
Table12
BreakdownbydependencyrelationofresultsofevaluationagainstPARC700.
Dep. Percentofdeps. F-score(%)
Bikel+Tags XLE
ADEGREE 6.17 80 82
ADJUNCT 14.32 68 66
AQUANT 0.06 78 61
COMP 1.23 80 74
CONJ 2.64 73 69
COORD FORM 1.20 83 90
DET FORM 4.61 97 91
FOCUS 0.02 0 36
MOD 2.74 74 67
NUM 19.82 91 89
NUMBER 1.42 89 83
NUMBER TYPE 2.10 94 86
OBJ 8.92 87 78
OBJ THETA 0.05 43 31
OBL 0.83 55 69
OBL AG 0.22 82 76
OBL COMPAR 0.07 38 56
PASSIVE 1.14 80 88
PCASE 0.25 79 68
PERF 0.41 89 90
POSS 0.98 88 80
PRECOORD FORM 0.03 0 91
PROG 0.97 89 81
PRON FORM 2.54 92 94
PRON INT 0.03 0 33
PRON REL 0.57 74 72
PROPER 3.56 83 93
PRT FORM 0.22 80 41
QUANT 0.34 77 80
STMT TYPE 5.23 87 80
SUBJ 8.51 78 78
SUBORD FORM 0.93 47 42
TENSE 5.02 95 90
TOPIC REL 0.57 56 73
XCOMP 2.29 80 78
PRT FORMdependencies
24
andachieveshigherf-scoresfor OBJand POSS.However,the
hand-crafted parsing system can better identify FOCUS, OBL(ique) arguments, PRECO-
ORD FORMand TOPICRELrelations.
5.2EvaluationagainstCBS500
Wealsocomparethehand-crafted,deep,probabilisticuniﬁcationgrammar-basedRASP
parsing system of Carroll and Briscoe (2002) to our treebankand retrained Bikel
24 DET FORM, SUBORD FORM,andPRT FORM(andingeneralX FORM)dependenciesrecord(semantically
relevant)surfaceformsinf-structurefor X-typeclosedclasscategories.
109
ComputationalLinguistics Volume34,Number1
Figure16
CBS500conversionsoftware.
parser-based LFG system. The RASPparsing system is a domain-independent, robust
statisticalparsingsystemforEnglish,basedonahand-written,feature-baseduniﬁcation
grammar. A probabilistic parse selection model conditioned on the structural parse
context, degree of support for a subanalysis in the parse forest, and lexical informa-
tion(whenavailable)choosesthemostlikelyparses.Forthisexperiment,weevaluate
againsttheCBS500,
25
developedbyCarroll,Briscoe,andSanﬁlippo(1998)inorderto
evaluateaprecursoroftheRASPparsingresources.TheCBS500containsdependency
structures (including some long distance dependencies
26
) for 500 sentences chosen at
randomfromtheSUSANNEcorpus(Sampson1995),butsubjecttotheconstraintthat
theyareparsablebytheparserinCarroll,Briscoe,andSanﬁlippo.AswiththePARC700,
therearesystematicdifferencesbetweenthef-structuresproducedbyourmethodology
and the dependency structures of the CBS 500. In order to be able to evaluate against
theCBS500,weautomaticallymapourf-structuresintoaformatsimilartotheirs.We
did not split the data into a heldout and a test set when developing the mapping, so
that a comparison could be made with other systems that report evaluations against
theCBS500.ThefollowingCBS500-stylegrammaticalrelationsareproducedfromthe
f-structureinFigure1:
(ncsubj sign U.N.)
(dobj sign treaty)
Somemappingiscarriedout(asintheevaluationagainstthePARC700)onthef-
structureannotatedtrees,andtheremainingmappingiscarriedoutonthef-structures
(Figure16).AswiththePARC700mapping,allmappingsarecarriedoutautomatically.
Thefollowingphenomenaweredealtwithonthef-structureannotatedtrees:
Auxiliaryverbs(xcompﬂattening) XCOMPS were ﬂattened to promote the main verb
to the top level, while maintaining a list of auxiliary and modal verbs and their
relationtooneanother.
Treatmentoftopicalizedsentences The predicate of the topicalized sentence became
themainpredicateandanyothertoplevelmaterialbecameanadjunct.
Multi-wordexpressions Multi-word expressions (such as according to) were not
marked up in the parser input, but captured in the annotated trees and the
annotationsadjustedaccordingly.
Treatmentoftheverbsbeandbecome Our automatic annotation algorithm does not
treattheverbsbeandbecomedifferentlyfromanyotherverbswhentheyareused
transitively.ThisanalysisconﬂictedwiththeCBS500analysis,sowaschangedto
matchtheirs.
25 Thiswasdownloadedfromhttp://www.informatics.susx.ac.uk/research/nlp/carroll/greval.html.
26 Thelongdistancedependenciesincludepassive,wh-lessrelativeclauses,controlverbs,andsoforth.
110
Cahilletal. StatisticalParsingUsingAutomaticDependencyStructures
Table13
ResultsofdependencyevaluationagainsttheCBS500(Carroll,Briscoe,andSanﬁllipo1998).
Bikel+Tags RASP p-Value
F-score 80.23 76.57 <.0001
Thefollowingarethemainmappingscarriedoutonthef-structures:
EncodingofPassive Wetreatpassiveasafeatureinourautomaticf-structureannota-
tionalgorithm,whereastheCBS500triplesencodethisinformationindirectly.
ObjectsofPrepositionalPhrases No dependency was generated for these objects, as
therewasnocorrespondingdependencyintheCBS500analyses.
NomenclatureDifferences There were some trivial mappings to account for differ-
ences in nomenclature, for example OBL in our analyses became IOBJ in the
mappeddependencies.
Encodingofwh-lessrelativeclauses These are encoded by means of reentrancies in
f-structure,butwereencodedinamoreindirectwayinthemappeddependencies
tomatchtheCBS500annotationformat.
Tocarryouttheexperiments,wePOS-taggedthetokenizedCBS500sentenceswith
theMXPOSTtagger(Ratnaparkhi1996)andparsedthetagsequenceswithourPenn-II
and Bikel retrained-based LFG system. We use the evaluation software of Carroll,
Briscoe,andSanﬁlippo(1998)
27
toevaluatethegrammaticalrelationsproducedbyeach
parser.TheresultsaregiveninTable13.
Our LFG system based on Bikel’s retrained parser achieves an f-score of 80.23%,
whereas the hand-crafted RASPgrammar and parser achieves an f-score of 76.57%.
Crouch et al. (2002) report that their XLE system achieves an f-score of 76.1% for the
same experiment. A detailed breakdown by dependency is given in Table 14. The
LFG system based on Bikel’s retrained parser is able to better identify MOD(iﬁer) de-
pendency relations, ARG MOD (the relation between a head and a semantic argument
which is syntactically realized as a modiﬁer, for example by-phrases), IOBJ (indirect
object) and AUXiliary relations. RASPis able to better identify XSUBJ (clausal subjects
controlled from without), CSUBJ (clausal subjects), and COMP (clausal complement)
relations.AgainweusetheApproximateRandomizationTesttotesttheparsingresults
forstatisticalsigniﬁcance.The p-valueforthetestcomparingoursystemusingBikel’s
retrainedparseragainstRASPis <.0001.Thetreebank-basedLFGsystemusingBikel’s
retrainedparserissigniﬁcantlybetterthanthehand-crafted,deep,uniﬁcationgrammar-
basedRASPparsingsystemwithastatisticalsigniﬁcanceof >95%.
6.DiscussionandRelatedWork
At the moment, we can only speculate as to why our treebank-based LFG resources
outperformthehand-craftedXLEandRASPgrammars.
In Section 4, we observed that the treebank-induced LFG resources have consid-
erably wider coverage (>99.9% measured in terms of complete spanning parse) than
27 Thiswasdownloadedfromhttp://www.informatics.susx.ac.uk/research/nlp/carroll/greval.html.
111
ComputationalLinguistics Volume34,Number1
Table14
BreakdownbygrammaticalrelationforresultsofevaluationagainstCBS500.
Dep. Percentofdeps. F-score(%)
Bikel+Tags RASP
DEPENDANT 100.00 80.23 76.57
MOD 48.96 80.30 75.29
NCMOD 30.42 85.37 72.98
XMOD 1.60 70.05 55.88
CMOD 2.61 75.60 53.08
DETMOD 14.06 95.85 91.97
ARG MOD 0.51 80.00 64.52
ARG 43.70 78.28 77.57
SUBJ 13.10 79.84 83.57
NCSUBJ 12.99 87.84 84.32
XSUBJ 0.06 0.00 88.89
CSUBJ 0.04 0.00 22.22
SUBJ OR DOBJ 18.22 81.21 83.84
COMP 12.38 76.73 71.87
OBJ 7.34 76.05 69.53
DOBJ 5.11 84.55 84.57
OBJ2 0.25 48.00 43.84
IOBJ 1.98 59.04 47.60
CLAUSAL 5.04 77.74 75.37
XCOMP 4.03 80.00 84.11
CCOMP 1.01 69.61 75.14
AUX 4.76 94.94 88.27
CONJ 2.06 68.84 69.09
the hand-crafted grammars (∼80% for XLE and RASPgrammars on unseen treebank
text). Both XLE and RASPuse a number of (largest) fragment-combining techniques
to achieve full coverage. If coverage is a signiﬁcant component in the performance
difference observed between the hand-crafted and treebank-induced resources, then
it is reasonable to expect that the performance difference is more pronounced with
increasing sentence length (with shorter sentences being simpler and more likely to
be within the coverage of the hand-crafted grammars). In other words, we expect the
hand-crafted,deep,precisiongrammarstodobetteronshortersentences(morelikelyto
bewithintheircoverage),whereasthetreebank-inducedgrammarsshouldshowbetter
performanceonlongerstrings(lesslikelytobewithinthecoverageofthehand-crafted
grammars).
Inordertotestthishypothesis,wecarriedoutanumberofexperiments:
First, we plotted the sentence length distribution for the 560 PARC 700 test sen-
tences and the 500 CBS 500 sentences (Figures 17 and 18). Both gold standards are
approximately normally distributed, with the CBS 500 distribution possibly showing
theeffectsofbeingchosensubjecttotheconstraintthatthestringsareparsablebythe
parserinCarroll,Briscoe,andSanﬁlippo(1998).Foreachcaseweusethemean,µ,and
twostandarddeviations,2σ,totheleftandrightofthemeantoexcludesentencelengths
notsupportedbysufﬁcientobservations:ForPARC700,µ=23.27, µ−2σ=2.17,and
µ+2σ=44.36; for CBS 500, µ=17.27, µ−2σ=1.59, and µ+2σ=32.96. Both the
PARC700andtheCB500distributionsarepositivelyskewed.ForthePARC700,µ−2σ
isactuallyoutsidetheobserveddatarange,whereasforCB500,µ−2σalmostcoincides
112
Cahilletal. StatisticalParsingUsingAutomaticDependencyStructures
Figure17
DistributionofsentencefrequencybysentencelengthinthePARC700testsetwithBezier
interpolation.Verticallinesmarktwostandarddeviationsfromthemean.
with the left border. It is therefore useful to further constrain the ±2σ range by a
sentence count threshold of ≥ 5.
28
This results in a sentence length range of 4–41 for
PARC700and4–32forCBS500.
Second,inordertotestwhetherfragmentparsesincreasewithsentencelength,we
plottedthepercentageoffragmentparsesoversentencelengthfortheXLEparsesofthe
560-sentencetestsetofthePARC700(wedidnotdothisfortheCBS500asitsstrings
areselectedtobefullyparsablebyRASP).Figure19showsthatthenumberoffragment
parsestendstoincreasewithsentencelength.
Third, we plotted the average dependency f-score for the hand-crafted and the
treebank-induced resources against sentence lengths. Figure 20 shows the results for
PARC700,Figure21forCBS500.
In both cases, contrary to our (perhaps somewhat naive) assumption, the graphs
show that the treebank-induced resources outperform the hand-crafted resources
within(mostof)the4–41and4–32sentencelengthbounds,withtheresultsforthevery
shortandtheverylongstringsoutsidethoseboundsnotbeingsupportedbysufﬁcient
datapoints.
In the parsing literature, results are often also provided for strings with lengths
≤40. Below we give those results and statistical signiﬁcance testing for the PARC 700
and CBS 500 (Tables 15 and 16). The results show that the Bikel retrained–based LFG
system achieves a higher dependency f-score on sentences of length ≤40 than on all
sentences, whereas the XLE system achieves a slightly lower score on sentences of
length ≤40. The Bikel-retrained system achieves an f-score of 83.18%, a statistically
28 NotethatbecausethedistributionsinFigures17and18areBezierinterpolated,theconstraintdoesnot
guaranteethateverysentencelengthwithintherangeoccursﬁveormoretimes.
113
ComputationalLinguistics Volume34,Number1
Figure18
DistributionofsentencefrequencybysentencelengthintheCB500testsetwithBezier
interpolation.Verticallinesmarktwostandarddeviationsfromthemean.
Figure19
PercentageoffragmentsentencesforXLEparsingsystempersentencelengthwithBezier
interpolation.
signiﬁcantimprovementof2.67percentagepointsovertheXLEsystemonsentencesof
length≤40.AgainsttheCBS500,Bikel’sretrainedsystemachievesaweightedf-scoreof
82.58%,astatisticallysigniﬁcantimprovementof3.87percentagepointsovertheRASP
systemwhichachievesaweightedf-scoreof78.81%onsentencesoflength≤40.
114
Cahilletal. StatisticalParsingUsingAutomaticDependencyStructures
Figure20
Averagef-scorebysentencelengthforPARC700testsetwithBezierinterpolation.
Figure21
Averagef-scorebysentencelengthforCB500testsetwithBezierinterpolation.
Finally, we test whether the strict preds-only dependency evaluation has an ef-
fect on the results for the PARC 700 experiments. Recall that following Kaplan et al.(2004)forthePARC700evaluationweusedasetofsemanticallyrelevantgrammatical
functions that is a superset of preds-only and a subset of all-GFs. A preds-only based
evaluationis“stricter”andtendstoproducelowerscoresasitdirectlyreﬂectstheeffects
115
ComputationalLinguistics Volume34,Number1
Table15
Evaluationandsigniﬁcancetestingofsentenceslength≤40againstthePARC700.
Allsentencelengths Length≤40
f-score f-score
Bikel+Tags 82.73 83.18
XLE 80.55 80.51
p-value .0054 .0010
Table16
Evaluationandsigniﬁcancetestingofsentenceslength≤40againsttheCBS500.
Allsentencelengths Length≤40
f-score f-score
Bikel+Tags 80.23 82.58
RASP76.57 78.81
p-value <.0001 <.0001
Table17
Preds-onlyevaluationagainstthePARC700DependencyBank.
AllGFs Predsonly
f-score f-score
Bikel+Tags 82.73 77.40
XLE 80.55 74.31
ofpredicate–argument/adjunctmisattachmentsintheresultingdependencyrepresen-
tations (while local functions such as NUM(ber), for example, can score properly even
if the local predicate is misattached). Table 17
29
below gives the results for preds-only
evaluation
30
onthePARC700forallsentencelengths.TheresultsshowthattheBikel-
retrained treebank-based LFG resource achieves an f-score of 77.40%, 5.33 percentage
points lower than the score for all the PARC dependencies. The XLE system achieves
an f-score of 74.31%, 6.24 percentage points lower than the score for all the PARC
dependencies and a 3.09 percentage point drop against the results obtained by the
Bikel-retrainedtreebank-basedLFGresources.TheperformanceoftheBikelretrained–
basedLFGsystemsufferslessthantheXLEsystemwhenpreds-onlydependenciesare
evaluated.
It is important to note that in our f-structure annotation algorithm and treebank-
based LFG parsing architectures, we do not claim to provide fully adequate statistical
models.Itiswellknown(Abney1997)thatPCFG-orhistory-basedparserapproxima-
tions to general constraint-based grammars can yield inconsistent probability models
29 Wedonotincludeap-valuehereasthebreakdownbyfunctionpersentencewasnotavailabletousfor
theXLEdata.
30 Thedependencyrelationsweincludeinpreds-onlyevaluationare: ADJUNCT, AQUANT, COMP, CONJ,
COORD FORM, DET FORM, FOCUS INT, MOD, NUMBER, OBJ, OBJ THETA, OBL, OBL AG, OBL COMPAR,
POSS, PRON INT, PRON REL, PRT FORM, QUANT, SUBJ, SUBORD FORM, TOPIC REL, XCOMP.
116
Cahilletal. StatisticalParsingUsingAutomaticDependencyStructures
due to loss of probability mass: The parser successfully returns the highest ranked
parse tree but the constraint solver cannot resolve the f-structure equations and the
probability mass associated with that tree is lost. Research on adequate probability
models for constraint-based grammars is important (Bouma, van Noord, and Malouf
2000;MiyaoandTsujii2002;Riezleretal.2002;ClarkandCurran2004).Inthiscontext,
it is interesting to compare parser performance against upper bounds. For the PARC
700evaluation,theupperboundfortheXLE-basedresourcesis85.65%,against86.83%
for the treebank-based LFG resources. XLE-based parsing currently achieves 94.05%
(f-score 80.55%) of its upper bound using a discriminative disambiguation method,
whereasthetreebank-basedLFGresourceachieves95.28%(f-score82.73%)ofitsupper
bound.
Althoughthisseemstoindicatethatthetwodisambiguationmodelsachievesimilar
results, the ﬁgures are actually very difﬁcult to compare. In the case of the XLE, the
upper bound is established by unpacking all parses for a string and scoring the best
match against the gold standard (rather than letting the probability model select a
parse).Bycontrast,inthecaseofthetreebank-basedLFGresources,weusetheoriginal
“perfect” Penn-II treebank trees (rather than the trees produced by the parser), auto-
maticallyannotatethosetreeswiththef-structureannotationalgorithm,andscorethe
results against the PARC 700 (it is not feasible to generate all parses for a string, there
are simply too many for treebank-induced resources). The upper bound computed in
this fashion for the treebank-based LFG resource (86.83%) is relatively low. The main
reasonisthattheautomaticmappingrequiredtorelatethef-structuresgeneratedbythe
treebank-basedLFGresourcestothePARC700dependenciesislossy.Thisisindicated
bycomparingtheupperboundforthetreebank-basedLFGresourcesforthePARC700
against the upper bound for the DCU 105 gold standard, where little or no mapping
(apartfromthefeature-structuretodependency-tripleconversion)isrequired:Scoring
thef-structureannotationsfortheoriginaltreebanktreesresultsin86.83%againstPARC
700versus96.80%againstDCU105.
Our discussion shows how delicate it can be to compare parsing systems and
theirdisambiguationmodels.Ultimatelywhatisrequiredisanevaluationstrategythat
separates out and clearly distinguishes between the grammar, parsing algorithm, and
disambiguationmodelandiscapableofassessingdifferentcombinationsofthesecore
components.Ofcourse,thiswillnotalwaysbepossibleandmovingtowardsitispartof
amuchmoreextendedresearchagenda,wellbeyondthescopeoftheresearchreported
in the present article. Our approach, and previous approaches, evaluate systems at
thehighestlevelofgranularity,thatofthecompletepackage:thecombinedgrammar-
parser-disambiguationmodel.Theresultsshowthatmachine-learning-basedresources
canoutperformdeep,state-of-the-arthand-craftedresourceswithrespecttothequality
ofdependenciesgenerated.
Treebank-based, deep and wide-coverage constraint-based grammar acquisition
has become an important research topic: Starting with the seminal TAG-based work
of Xia (1999), there are now also HPSG-, CCGand LFG-based approaches. Miyao,
Ninomiya, and Tsujii (2003) and Tsuruoka, Miyao, and Tsujii (2004) present re-
searchoninducingPenn-IItreebank-basedHPSGswithlog-linearprobabilitymodels.
HockenmaierandSteedman(2002)andHockenmaier(2003)providetreebank-induced
CCG-basedmodelsincludingLDDresolution.Itwouldbeinterestingtoconductacom-
parativeevaluationinvolvingtreebank-basedHPSG,CCG,andLFGresourcesagainst
thePARC700andCBS500DependencyBankstoenablemorecomprehensivecompar-
isonofmachine-learning-basedwithhand-crafted,deep,wide-coverageresourcessuch
asthoseusedintheXLEorRASPparsingsystems.
117
ComputationalLinguistics Volume34,Number1
Gildea and Hockenmaier (2003) and Miyao and Tsujii (2004) present PropBank
(Kingsbury,Palmer,andMarcus2002)-basedevaluationsoftheirautomaticallyinduced
CCG and HPSG resources. PropBank-based evaluations provide valuable information
to rank parsing systems. Currently, however, PropBank-based evaluations are some-
whatpartial:Theyonlyrepresent(andhencescore)verbalargumentsanddisregarda
raft of other semantically important dependencies (e.g., temporal and aspectual infor-
mation,dependencieswithinnominalclusters,etc.)ascapturedinthePARC700orCBS
500DependencyBanks.
31
7.Conclusions
Parser comparison is a non-trivial and time-consuming exercise. We extensively eval-
uated four machine-learning-based shallow parsers and two hand-crafted, wide-
coverage deep probabilistic parsers involving four gold-standard dependency banks,
usingtheApproximateRandomizationTest(Noreen1989)totestforstatisticalsigniﬁ-
cance.Weusedasophisticatedmethodforautomaticallyproducingdeepdependency
relations from Penn-II-style CFG-trees (Cahill et al. 2002b, 2004) to compare shallow
parseroutputatthelevelofdependencyrelationandrevisitexperimentscarriedoutby
Preiss(2003)andKaplanetal.(2004).
Ourmainﬁndingsaretwofold:
1.UsingourCFG-tree-to-dependencyannotationtechnology,togetherwithtreebank-and
machine-learning-basedparsers,wecanoutperformhand-crafted,wide-coverage,deep,
probabilistic,constraint-basedgrammarsandparsers.
This result is surprising for two reasons. First, it is established against two externally-
provideddependencybanks(thePARC700andtheCBS500goldstandards),originally
designed using the hand-crafted, wide-coverage, probabilistic grammars for the XLE
(Riezler et al. 2002) and RASP(Carroll and Briscoe 2002) parsing systems to evaluate
those systems. What is more, the SUSANNE corpus-based CBS 500 constitutes an
instanceofdomainvariationforthePenn-II-trainedLFGresources,likelytoadversely
affectscores.Second,thetreebank-andmachine-learning-basedLFGresourcesrequire
automatic mapping to relate f-structure output of the treebank-based parsing systems
to the representation format in the PARC 700 and CBS 500 Dependency Banks. These
mappings are partial and lossy: That is, they do not cover all of the systematic dif-
ferences between f-structure and dependency bank representations and introduce a
certain amount of error in what they are designed to capture, that is they both over-
andundergeneralize,againadverselyaffectingscores.Improvementsofthemappings
shouldleadtoafurtherimprovementinthedependencyscores.
2.Parserevaluationatthelevelofdependencyrepresentationstillrequiresnon-trivial
mappingsbetweendifferentdependencyrepresentationformats.
31 Inasense,PropBankdoesnotyetprovideasingleagreedupongoldstandard:Roleinformationis
providedindirectlyandanevaluationgold-standardhastobecomputedfromthis.Indoingso,choices
havetobemadeasregardstherepresentationofsharedarguments,theanalysisofcoordinatestructures,
andsoforth,anditisnotclearthatthesamechoicesarecurrentlymadeforevaluationscarriedoutby
differentresearchgroups.
118
Cahilletal. StatisticalParsingUsingAutomaticDependencyStructures
Earlier we criticized tree-based parser evaluation on the grounds that equally valid
different tree-typologies can be associated with strings, and identiﬁed this as a major
obstacle to fair and unbiased parser evaluation. Dependency-based parser evaluation,
asitturnsout,isnotentirelyfreeofthiscriticismeither:Therearesigniﬁcantsystematic
differencesbetweenthePARC700dependencyandtheCBS500dependencyrepresen-
tations;therearesigniﬁcantsystematicdifferencesbetweentheLFGf-structuresgener-
atedbythehand-crafted, wide-coverage grammars ofRiezler etal.(2002) andKaplan
et al. (2004) and those of the treebank-induced and f-structure annotation algorithm
basedresourcesofCahilletal.(2004).Thesedifferencesrequirecarefulimplementation
of mappings if parsers are not to be unduly penalized for systematic and motivated
differences at the level of dependency representation. By and large, these differences
are, however, less pronounced than differences on CFG tree representations, making
dependency-basedparserevaluationaworthwhileandrewardingexercise.
Summarizing our results, we ﬁnd that against the DCU 105 development set, the
treebank-andf-structureannotationalgorithm-basedLFGsystemusingBikel’sparser
retrained to retain Penn-II functional tag labels performs best, achieving f-scores of
82.92% preds-only and 88.3% all grammatical functions. Against the automatically
generatedWSJSection22DependencyBank,thesystemusingBikel’sretrainedparser
achieves the highest results, achieving f-scores of 83.06% preds-only and 87.861% all
GFs. This is statistically signiﬁcantly better than all other parsers. In order to evaluate
againstthePARC700andCBS500goldstandards,weautomaticallymapthedependen-
ciesproducedbyourtreebank-basedLFGsystemintoaformatcompatiblewiththegold
standards. Against the PARC 700 Dependency Bank, the treebank-based LFG system
using Bikel’s retrained parser achieves an f-score of 82.73%, a statistically signiﬁcant
improvement of 2.18% against the most up-to-date results of the hand-crafted XLE-
based parsing resources. Against the CBS 500, the treebank-based LFG system using
Bikel’sretrainedparserachievedthehighestf-scoreof80.23%,astatisticallysigniﬁcant
improvement of 3.66 percentage points on the highest previously published results
for the same experiment with the hand-crafted RASPresources in Carroll and Briscoe
(2002).
AppendixA.ParserParameterSettings
This section provides a complete list of the parameter settings used for each of the
parsersdescribedinthisarticle.
Parser Parameters
CollinsModel3 We used the Collins parser with its default settings of a
beamsizeof10,000andwherethevaluesofthefollowing
ﬂags are set to 1: punctuation-ﬂag, distaﬂag, distvﬂag,
and npbﬂag. Input was pre-tagged using the MXPOST
POS tagger (Ratnaparkhi 1996). We parse the input ﬁle
with all three models and use the scripts provided to
merge the outputs into the ﬁnal parser output ﬁle. Note
thatthisﬁlehasbeencleanedofall-Afunctionaltagsand
tracenodes.
Charniak We used the parser dated August 2005 and ran the
parser using the data provided in the download on pre-
tokenizedsentencesoflength ≤200.Inputwasautomati-
callytaggedbytheparser.
119
ComputationalLinguistics Volume34,Number1
BikelEmulationof We used version 0.9.9b of the parser trained on the ﬁle
CollinsModel2 of observed events made available on the downloads
page.Weusedthecollins.propertiesﬁleandamaximum
heap size of 1,500 MB. Input was pre-tagged using the
MXPOSTPOStagger(Ratnaparkhi1996).
Bikel+FunctionalTags Weusedversion0.9.9boftheparsertrainedonaversion
ofSections02–21ofthePenn-IItreebankwherefunctional
tags werenot ignored by the parser. We updated thede-
fault head-ﬁnding rules to deal with the new categories.
We also trained on all sentences from the training set
(the default collins.properties ﬁle is set to ignore trees of
more than 500 tokens). Input was pre-tagged using the
MXPOST POS tagger (Ratnaparkhi 1996) and the maxi-
mumheapsizewas1,500MB.
RASP Weusedaﬁleofparseroutputprovidedthroughpersonal
communicationwithJohnCarroll.(Taggingiscarriedout
automaticallybytheparser.)
XLE We used a ﬁle of parser output provided by the Natural
LanguageTheoryandTechnologygroupatthePaloAlto
ResearchCenter.(Taggingiscarriedoutautomaticallyby
theparser.)
Acknowledgments
Wearegratefultoouranonymousreviewers
whoseinsightfulcommentshaveimproved
thearticlesigniﬁcantly.Wewouldliketo
thankJohnCarrollfordiscussionandhelp
withreproducingtheRASPparsingresults;
MichaelCollins,EugeneCharniak,Dan
Bikel,andHelmutSchmidformakingtheir
parsingenginesavailable;andRonKaplan
andtheteamatPARCfordiscussion,
feedback,andsupport.Partoftheresearch
presentedherehasbeensupportedby
ScienceFoundationIrelandgrants
04/BR/CS0370and04/IN/I527,Enterprise
IrelandBasicResearchGrantSC/2001/0186,
anIrishResearchCouncilforScience,
EngineeringandTechnologyPh.D.
studentship,anIBMPh.D.studentshipand
supportfromIBM’sCentreforAdvanced
Studies(CAS)inDublin.
References
Abney,Stephen.1997.Stochastic
attribute-valuegrammars.Computational
Linguistics,23(4):597–618.
Alshawi,HiyanandStephenPulman,1992.
Ellipsis,Comparatives,andGeneration,
chapter13.TheMITPress,Cambridge,
MA.
Baldwin,Timothy,EmilyBender,Dan
Flickinger,AraKim,andStephanOepen.
2004.Road-testingtheEnglishResource
GrammarovertheBritishNational
Corpus.InProceedingsoftheFourth
InternationalConferenceonLanguage
ResourcesandEvaluation(LREC2004),
pages2047–2050,Lisbon,Portugal.
Bikel,Dan.2002.Designofamulti-lingual,
parallel-processingstatisticalparsing
engine.InProceedingsofHLT2002,
pages24–27,SanDiego,CA.
Black,Ezra,StevenAbney,DanFlickenger,
ClaudiaGdaniec,RalphGrishman,Philip
Harrison,DonaldHindle,RobertIngria,
FredJelineck,JudithKlavans,Mark
Liberman,MitchellMarcus,SalimRoukos,
BeatriceSantorini,andTomek
Strzalkowski.1991.Aprocedurefor
quantitativelycomparingthesyntactic
coverageofenglishgrammars.In
ProceedingsoftheSpeechandNatural
LanguageWorkshop,pages306–311,Paciﬁc
Grove,CA.
Bod,Rens.2003.Anefﬁcientimplementation
ofanewDOPmodel.In Proceedingsofthe
TenthConferenceoftheEuropeanChapterof
theAssociationforComputationalLinguistics
(EACL’03),pages19–26,Budapest,
Hungary.
Bouma,Gosse,GertjanvanNoord,and
RobertMalouf.2000.Alpino:
Wide-coveragecomputationalanalysisof
dutch.InWalterDaelemans,Khalil
Sima’an,JornVeenstra,andJakubZavrel,
editors,ComputationalLinguisticsinThe
Netherlands2000.Rodopi,Amsterdam,
pages45–59.
120
Cahilletal. StatisticalParsingUsingAutomaticDependencyStructures
Bresnan,Joan.2001.Lexical-FunctionalSyntax.
Blackwell,Oxford,England.
Briscoe,EdwardandJohnCarroll.1993.
GeneralizedprobabilisticLRparsingof
naturallanguage(corpora)with
uniﬁcation-basedgrammars.Computational
Linguistics,19(1):25–59.
Briscoe,TedandJohnCarroll.2006.
Evaluatingtheaccuracyofan
unlexicalizedstatisticalparseronthe
PARCDepBank.InProceedingsofthe
COLING/ACL2006MainConferencePoster
Sessions,pages41–48,Sydney,Australia.
Briscoe,Edward,ClaireGrover,Bran
Boguraev,andJohnCarroll.1987.A
formalismandenvironmentforthe
developmentofalargegrammarof
English.InProceedingsofthe10th
InternationalJointConferenceonAI,
pages703–708,Milan,Italy.
Burke,Michael.2006.AutomaticTreebank
AnnotationfortheAcquisitionofLFG
Resources.Ph.D.thesis,Schoolof
Computing,DublinCityUniversity,
Dublin,Ireland.
Burke,Michael,AoifeCahill,Ruth
O’Donovan,JosefvanGenabith,andAndy
Way.2004.Evaluationofanautomatic
annotationalgorithmagainstthePARC
700DependencyBank.InProceedingsofthe
NinthInternationalConferenceonLFG,
pages101–121,Christchurch,NewZealand.
Butt,Miriam,HelgeDyvik,TracyHolloway
King,HiroshiMasuichi,andChristian
Rohrer.2002.TheParallelGrammar
Project.InProceedingsofCOLING2002,
WorkshoponGrammarEngineeringand
Evaluation,pages1–7,Taipei,Taiwan.
Cahill,Aoife.2004.ParsingwithAutomatically
Acquired,Wide-Coverage,Robust,
ProbabilisticLFGApproximations.Ph.D.
thesis,SchoolofComputing,DublinCity
University,Dublin,Ireland.
Cahill,Aoife,MichaelBurke,Ruth
O’Donovan,JosefvanGenabith,andAndy
Way.2004.Long-distancedependency
resolutioninautomaticallyacquired
wide-coveragePCFG-basedLFG
approximations.InProceedingsofthe42nd
AnnualMeetingoftheAssociationfor
ComputationalLinguistics,pages320–327,
Barcelona,Spain.
Cahill,Aoife,Mair´eadMcCarthy,Josefvan
Genabith,andAndyWay.2002a.
AutomaticannotationofthePenn
TreebankwithLFGf-structure
information.InProceedingsoftheLREC
WorkshoponLinguisticKnowledge
AcquisitionandRepresentation:Bootstrapping
AnnotatedLanguageData,pages 8–15,Las
Palmas,CanaryIslands,Spain.
Cahill,Aoife,Mair´eadMcCarthy,Josefvan
Genabith,andAndyWay.2002b.Parsing
withPCFGsandautomaticf-structure
annotation.InProceedingsoftheSeventh
InternationalConferenceonLFG,
pages76–95,PaloAlto,CA.
Carroll,JohnandEdwardBriscoe.2002.
Highprecisionextractionofgrammatical
relations.InProceedingsofthe19th
InternationalConferenceonComputational
Linguistics(COLING),pages134–140,
Taipei,Taiwan.
Carroll,John,EdwardBriscoe,andAntonio
Sanﬁlippo.1998.Parserevaluation:A
surveyandnewproposal.InProceedingsof
theInternationalConferenceonLanguage
ResourcesandEvaluation,pages447–454,
Granada,Spain.
Carroll,John,AnetteFrank,DekangLin,
DetlefPrescher,andHansUszkoreit,
editors.2002.HLTWorkhop:‘Beyond
PARSEVAL—Towardsimprovedevaluation
measuresforparsingsystems’,LasPalmas,
CanaryIslands,Spain.
Charniak,Eugene.1996.Tree-bank
grammars.InProceedingsofthe
ThirteenthNationalConferenceon
ArtiﬁcialIntelligence,pages1031–1036,
MenloPark,CA.
Charniak,Eugene.2000.Amaximum
entropyinspiredparser.InProceedings
oftheFirstAnnualMeetingoftheNorth
AmericanChapteroftheAssociationfor
ComputationalLinguistics(NAACL2000),
pages132–139,Seattle,WA.
Chinchor,Nancy,LynetteHirschman,and
DavidD.Lewis.1993.Evaluatingmessage
understandingsystems:Ananalysisofthe
ThirdMessageUnderstandingConference
(MUC-3).ComputationalLinguistics,
19(3):409–449.
Clark,StephenandJamesCurran.2004.
ParsingtheWSJusingCCGandlog-linear
models.InProceedingsofthe42ndAnnual
MeetingoftheAssociationforComputational
Linguistics(ACL-04),pages104–111,
Barcelona,Spain.
Clark,StephenandJamesCurran.2007.
Formalism-independentparserevaluation
withCCGandDepBank.InProceedingsof
the45thAnnualMeetingoftheAssociation
forComputationalLinguistics(ACL2007),
pages248–255,Prague,CzechRepublic
http://www.aclweb-org/anthology/P/
P07/P07-1032.
Clark,StephenandJuliaHockenmaier.2002.
Evaluatingawide-coverageCCGparser.
121
ComputationalLinguistics Volume34,Number1
InProceedingsoftheLREC2002Beyond
ParsevalWorkshop,pages60–66,Las
Palmas,CanaryIslands,Spain.
Cohen,PaulR.1995.EmpiricalMethodsfor
ArtiﬁcialIntelligence.TheMITPress,
Cambridge,MA.
Collins,Michael.1997.Threegenerative,
lexicalizedmodelsforstatisticalparsing.
InProceedingsofthe35thAnnualMeetingof
theAssociationforComputationalLinguistics,
pages16–23,Madrid,Spain.
Collins,Michael.1999.Head-DrivenStatistical
ModelsforNaturalLanguageParsing.Ph.D.
thesis,UniversityofPennsylvania,
Philadelphia,PA.
Crouch,Richard,RonKaplan,TracyHolloway
King,andStefanRiezler.2002.A
comparisonofevaluationmetricsfora
broadcoverageparser.InProceedingsof
theLRECWorkshop:BeyondPARSEVAL—
TowardsImprovedEvaluationMeasuresfor
ParsingSystems,pages67–74,LasPalmas,
CanaryIslands,Spain.
Dalrymple,Mary.2001.Lexical-Functional
Grammar.London,AcademicPress.
Dienes,P´eterandAmitDubey.2003.
Antecedentrecovery:Experimentswitha
tracetagger.InProceedingsofthe2003
ConferenceonEmpiricalMethodsinNatural
LanguageProcessing,pages33–40,Sapporo,
Japan.
Eisele,AndreasandJochenD¨orre.1986.A
lexicalfunctionalgrammarsystemin
Prolog.InProceedingsofthe11thInternational
ConferenceonComputationalLinguistics
(COLING1986),pages551–553,Bonn.
Flickinger,Dan.2000.Onbuildingamore
efﬁcientgrammarbyexploitingtypes.
NaturalLanguageEngineering,6(1):15–28.
Gaizauskas,Rob.1995.Investigationsinto
thegrammarunderlyingthePenn
TreebankII.ResearchMemorandum
CS-95-25,DepartmentofComputer
Science,UniveristyofShefﬁeld,UK.
Gildea,DanielandJuliaHockenmaier.
2003.Identifyingsemanticrolesusing
combinatorycategorialgrammar.
InProceedingsofthe2003Conference
onEmpiricalMethodsinNatural
LanguageProcessing,pages57–64,
Sapporo,Japan.
Hockenmaier,Julia.2003.Parsingwith
generativemodelsofpredicate–argument
structure.InProceedingsofthe41stAnnual
ConferenceoftheAssociationfor
ComputationalLinguistics,pages359–366,
Sapporo,Japan.
Hockenmaier,JuliaandMarkSteedman.
2002.Generativemodelsforstatistical
parsingwithcombinatorycategorial
grammar.InProceedingsofthe40th
AnnualMeetingoftheAssociationfor
ComputationalLinguistics,pages335–342,
Philadelphia,PA.
Johnson,Mark.1999.PCFGmodelsof
linguistictreerepresentations.
ComputationalLinguistics,24(4):613–632.
Johnson,Mark.2002.Asimple
pattern-matchingalgorithmfor
recoveringemptynodesandtheir
antecedents.InProceedingsofthe40th
AnnualMeetingoftheAssociationfor
ComputationalLinguistics,pages136–143,
Philadelphia,PA.
Kaplan,RonandJoanBresnan.1982.
Lexicalfunctionalgrammar,aformal
systemforgrammaticalrepresentation.
InJoanBresnan,editor,TheMental
RepresentationofGrammaticalRelations.
MITPress,Cambridge,MA,
pages173–281.
Kaplan,Ron,StefanRiezler,TracyHolloway
King,JohnT.Maxwell,Alexander
Vasserman,andRichardCrouch.2004.
Speedandaccuracyinshallowanddeep
stochasticparsing.InProceedingsofthe
HumanLanguageTechnologyConferenceand
the4thAnnualMeetingoftheNorth
AmericanChapteroftheAssociationfor
ComputationalLinguistics(HLT-NAACL’04),
pages97–104,Boston,MA.
Kaplan,RonaldandAnnieZaenen.1989.
Long-distancedependencies,constituent
structureandfunctionaluncertainty.In
MarkBaltinandAnthonyKroch,editors,
AlternativeConceptionsofPhraseStructure,
pages17–42,UniversityofChicagoPress,
Chicago.
King,TracyHolloway,RichardCrouch,
StefanRiezler,MaryDalrymple,andRon
Kaplan.2003.ThePARC700dependency
bank.InProceedingsoftheEACL03:4th
InternationalWorkshoponLinguistically
InterpretedCorpora(LINC-03),pages1–8,
Budapest,Hungary.
Kingsbury,Paul,MarthaPalmer,and
MitchMarcus.2002.Addingsemantic
annotationtothePennTreeBank.In
ProceedingsoftheHumanLanguage
TechnologyConference,pages252–256,
SanDiego,CA.
Klein,DanandChristopherD.Manning.
2003.Accurateunlexicalizedparsing.In
Proceedingsofthe41stAnnualMeetingofthe
AssociationforComputationalLinguistics,
pages423–430,Sapporo,Japan.
Leech,GeoffreyandRogerGarside.1991.
Runningagrammarfactory:Onthe
122
Cahilletal. StatisticalParsingUsingAutomaticDependencyStructures
compilationofparsedcorpora,or
‘treebanks’.InStigJohanssonand
Anna-BritaStenstr¨om,editors,English
ComputerCorpora:SelectedPapers.Mouton
deGruyter,Berlin,pages15–32.
Levy,RogerandChristopherD.Manning.
2004.Deepdependenciesfromcontext-free
statisticalparsers:Correctingthesurface
dependencyapproximation.InProceedings
ofthe42ndAnnualMeetingoftheAssociation
forComputationalLinguistics(ACL2004),
pages328–335,Barcelona,Spain.
Lin,Dekang.1995.Adependency-based
methodforevaluatingbroad-coverage
parsers.InProceedingsoftheInternational
JointConferenceonAI,pages1420–1427,
Montr´eal,Canada.
Magerman,David.1994.NaturalLanguage
ParsingasStatisticalPatternRecognition.
Ph.D.thesis,DepartmentofComputer
Science,StanfordUniversity,CA.
Marcus,Mitchell,GraceKim,MaryAnn
Marcinkiewicz,RobertMacIntyre,
AnnBies,MarkFerguson,KarenKatz,
andBrittaSchasberger.1994.The
PennTreebank:Annotatingpredicate
argumentstructure.InProceedings
oftheARPAWorkshoponHuman
LanguageTechnology,pages110–115,
Princeton,NJ.
McCarthy,Mair´ead.2003.Designand
EvaluationoftheLinguisticBasisofan
AutomaticF-StructureAnnotationAlgorithm
forthePenn-IITreebank.Master’sthesis,
SchoolofComputing,DublinCity
University,Dublin,Ireland.
McDonald,RyanandFernandoPereira.2006.
Onlinelearningofapproximate
dependencyparsingalgorithms.In
Proceedingsofthe11thConferenceofthe
EuropeanChapteroftheAssociationfor
ComputationalLinguistics,pages81–88,
Trento,Italy.
Miyao,Yusuke,TakashiNinomiya,and
Jun’ichiTsujii.2003.Probabilisticmodeling
ofargumentstructuresincludingnon-local
dependencies.InProceedingsofthe
ConferenceonRecentAdvancesinNatural
LanguageProcessing(RANLP),
pages285–291,Borovets,Bulgaria.
Miyao,YusukeandJun’ichiTsujii.2002.
Maximumentropyestimationforfeature
forests.InProceedingsofHumanLanguage
TechnologyConference(HLT2002),
pages292–297,SanDiego,CA.
Miyao,YusukeandJun’ichiTsujii.
2004.Deeplinguisticanalysis
fortheaccurateidentiﬁcationof
predicate–argumentrelations.In
Proceedingsofthe18thInternational
ConferenceonComputationalLinguistics
(COLING2004),pages1392–1397,
Geneva,Switzerland.
Noreen,EricW.1989.ComputerIntensive
MethodsforTestingHypotheses:An
Introduction.Wiley,NewYork.
O’Donovan,Ruth,MichaelBurke,Aoife
Cahill,JosefvanGenabith,andAndy
Way.2004.Large-scaleinductionand
evaluationoflexicalresourcesfromthe
Penn-IItreebank.InProceedingsofthe42nd
AnnualMeetingoftheAssociationfor
ComputationalLinguistics,pages368–375,
Barcelona,Spain.
Pollard,CarlandIvanSag.1994.Head-driven
PhraseStructureGrammar.CSLI
Publications,Stanford,CA.
Preiss,Judita.2003.Usinggrammatical
relationstocompareparsers.InProceedings
oftheTenthConferenceoftheEuropean
ChapteroftheAssociationforComputational
Linguistics(EACL’03),pages291–298,
Budapest,Hungary.
Ratnaparkhi,Adwait.1996.Amaximum
entropypart-of-speechtagger.In
ProceedingsoftheEmpiricalMethodsin
NaturalLanguageProcessingConference,
pages133–142,Philadelphia,PA.
Riezler,Stefan,TracyKing,RonaldKaplan,
RichardCrouch,JohnT.Maxwell,
andMarkJohnson.2002.Parsing
theWallStreetJournalusinga
lexical-functionalgrammarand
discriminativeestimationtechniques.
InProceedingsofthe40thAnnual
ConferenceoftheAssociationfor
ComputationalLinguistics(ACL-02),
pages271–278,Philadelphia,PA.
Sampson,Geoffrey.1995.Englishforthe
Computer:TheSUSANNECorpusand
AnalyticScheme.ClarendonPress,
Oxford,England.
Tsuruoka,Yoshimasa,YusukeMiyao,
andJun’ichiTsujii.2004.Towards
efﬁcientprobabilisticHPSGparsing:
Integratingsemanticandsyntactic
preferencetoguidetheparsing.
InProceedingsofIJCNLP-04Workshop:
Beyondshallowanalyses—Formalisms
andstatisticalmodelingfordeep
analyses,HainanIsland,China.[No
pagenumbers].
vanGenabith,JosefandRichardCrouch.
1996.Directandunderspeciﬁed
interpretationsofLFGf-structures.In16th
InternationalConferenceonComputational
Linguistics(COLING96),pages262–267,
Copenhagen,Denmark.
123
ComputationalLinguistics Volume34,Number1
vanGenabith,JosefandRichard
Crouch.1997.Oninterpreting
f-structuresasUDRSs.InProceedings
ofACL-EACL-97,pages402–409,
Madrid,Spain.
Xia,Fei.1999.Extractingtreeadjoining
grammarsfrombracketedcorpora.In
Proceedingsofthe5thNaturalLanguage
ProcessingPaciﬁcRimSymposium
(NLPRS-99),pages398–403,Beijing,
China.
Xue,Nianwen,FeiXia,Fu-DongChiou,and
MarthaPalmer.2004.ThePennChinese
treebank:Phrasestructureannotationofa
largecorpus.NaturalLanguageEngineering,
10(4):1–30.
Yeh,Alexander.2000.Moreaccuratetests
forthestatisticalsigniﬁcanceofresult
differences.InProceedingsofthe18th
InternationalConferenceonComputational
Linguistics(COLING,2000),pages947–953,
Saarbr¨ucken,Germany.
124

