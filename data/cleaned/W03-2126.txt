Dialog Input Ranking in a Multi-Domain Environment Using
Transferable Belief Model
Hong-I Ng
Department of Computer Science
School of Computing
National University of Singapore
nghi@comp.nus.edu.sg
Kim-Teng Lua
Department of Computer Science
School of Computing
National University of Singapore
luakt@comp.nus.edu.sg
Abstract
This paper presents results of using be-
lief functions to rank the list of candi-
date information provided in a noisy di-
alogue input. The information under con-
sideration is the intended task to be per-
formed and the information provided for
the completion of the task. As an exam-
ple, we use the task of information ac-
cess in a multi-domain dialogue system.
Currently, the system contains knowledge
of ten different domains. Callers calling
in are greeted with an open-ended “How
may I help you?” prompt (Thomson and
Wisowaty, 1999; Chu-Carroll and Carpen-
ter, 1999; Gorin et al., 1997). After re-
ceiving a reply from the caller, we extract
word evidences from the recognized utter-
ances. By using transferable belief model
(TBM), we in turn determine the task that
the caller intends to perform as well as any
information provided.
1 Introduction
Touch-tone menus are prevalent in call centers for
accessing personal records and pre-recorded infor-
mation. However, it can sometimes be very frus-
trating when we need to listen to a long list of op-
tions. Moreover, the information that we are look-
ing for may not seem to be relevant to any of the
given options. Recently, systems that allow people
to access information based on spoken inputs have
been built. They require a speech recognizer that
is trained on a specific set of key words and speech
grammars to understand the spoken inputs. Callers
are guided through a series of prompts. At each
prompt, the callers are supposed to speak out their
choices in a way that is easy for the systems to un-
derstand. However, new callers may not know what
should they say at different prompts and how should
they say it. They might have spoken their choices
too early, or the way they say it is not encoded in the
systems grammar. Thus, we are motivated to work
on the problem of accessing information using nat-
urally spoken dialogue. We allow callers to speak
in a natural way. Our ultimate aim is to provide the
caller with the exact piece of information that s/he is
looking for through a series of dialogue interaction.
The work reported in this paper is our first attempt
toward our ultimate aim, i.e., to determine what the
callers want and find out the information the callers
have provided that are useful for the task. To achieve
that, we use Smets’ (1988) TBM.
TBM is the concept used to justify the use of be-
lief functions (BFs), Dempster’s rule of condition-
ing and Dempster’s rule of combination to model
someone’s belief (Smets, 1988). Since early 1980’s,
BFs have generated considerable interest in the Ar-
tificial Intelligence community. In Smets (1999),
Denœux (2000) and Zouhal and Denœux (1998),
BFs are used to provide sound and elegant solu-
tions to real life problems where some information
is missing. As in Bayesian model, given the avail-
able evidences, parts of the amount of belief are al-
located to each object in our problem domain. How-
ever, some evidences might support something other
than only one of the various domain objects. In this
case, Principle of Insufficient Reason (Smets, 1988)
is invoked to decide that the belief mass must be split
equally among the domain objects involved. TBM
does not evoke this principle and leaves the belief
mass allocated to the disjunct of the domain objects
involved. Examples of the use of BFs include dis-
criminant analysis using a learning set where classes
are only partially known; determine the number of
sources in a multi-sensor environment by studying
the inter-sensors contradiction and pattern classifi-
cation. As far as we know, nobody has used BFs to
solve problems related to human-computer conver-
sational dialogue. However, we belief that BFs can
be applied on problems related to human-computer
conversational dialogue, where the recognized utter-
ances contain insertion, deletion and substitution er-
rors. Currently, our multi-domain dialogue system
contains knowledge of ten different domains. They
are phone directory service (Ta0 ), train schedule in-
quiry (Ta1 ), flight status inquiry (Ta2 ), travel book-
ing (Ta3 ), Bus Service inquiry (Ta4 ), financial plan-
ning (Ta5 ), phone banking (Ta6 ), checking of the em-
ployee’s account (Ta7 ), concert ticket booking (Ta8 )
and course registration (Ta0a10a9 ).
Similar works have been reported is the past.
However, their main aim is to do call routing instead
of information access. Their approaches include the
use of a vector-based information retrieval technique
(Lee et al., 2000; Chu-Carroll and Carpenter, 1999)
/bin/bash: line 1: a: command not found Our do-
mains are more varied, which may results in more
recognition errors. In addition, we do not have a
training corpus. However, we have a knowledge
base that provides partial information based on word
evidences. For examples, the occurrence of word
evidence account indicates that the user wants to ac-
cess her/his employee’s account or bank account, the
occurrence of a person name indicates that the user
is not checking for a flight status or bus service, the
occurrence of word evidence time indicates that the
user probably wants to check the train schedules or
flight status.
Due to space limitation, readers are advised to re-
fer to Smets (1988; 1997; 1989) for more detailed
discussions on BFs, combination of BFs, decision
making using BFs and TBM.
2 Ranking
Information from the
Recognized Utterance of Naturally
Spoken Input
Our aim is to use TBM in dialogue management.
First, TBM is used to rank the information identi-
fied from the recognized input. Then, the rank list
is used in clarification dialogues if necessary. Other-
wise, the best result is treated as the user input. Our
experiments are done using Sphinx II speech recog-
nition system (Huang et al., 1992). Using a test cor-
pus of 1977 words, we find that the word recognition
accuracy is 54.5%. In our experiments, we use 139
naturally spoken responses to an open-ended “How
may I help you prompt” prompt. The callers are told
in advance the list of tasks that the system can per-
form. As notations, let U denotes a recognized ut-
terance, n the length of U in number of words and
a11a13a12a15a14a17a16a19a18a21a20a23a22a24a22a26a25 the word evidences from U.
2.1 Identifying
the Intended Task
In this experiment, we show whether TBM can be
used to identify the caller’s intended tasks. First, we
need to identify our problem domain or frame of dis-
cernment, a27 (Smets, 1988). For task identification,
a27
a18a29a28a31a30a33a32a35a34a36a37a18a38a20a23a22a24a22a39a20a31a40a42a41 , i.e., the list of tasks presented
in Section 1. a43a45a44a47a46a49a48 a30a51a50 , i.e., the basic belief mass
(bbm) of a11a13a12 given to a30 where a30a53a52a55a54a35a56 is calculated
based on the occurrence frequency of word evidence
a11a13a12 in the knowledge-bases of a30a47a32a57a14a10a36a58a18a21a20a23a22a24a22a39a20a31a40 .
Currently the knowledge base a59
a32 of each a30a33a32a60a14a10a36a61a18
a20a23a22a24a22a39a20a31a40 consists of (a) a task specification; (b) infor-
mation schemas for a30a47a32 ; and (c) information for task,
i.e., the database records, facts and rules, and remote
tables and databases used in a30a47a32 . A task specification
specifies the goal of the task and the list of steps re-
quired to attain the goal. Each step is linked to either
a basic operation, for examples, fetch some records
from a database and ask the caller for information,
or a sub-task. Information schemas specify the high-
level formats of the information used in a30a62a32 . They in-
clude database schemas, XML schemas of facts and
rules, and format descriptions of some remote tables
and databases used in a30 a32 .
We do indexing for each a59 a32a23a14a10a36a37a18a21a20a23a22a24a22a39a20a31a40 so that it is
easy to calculate the bbm’s a43 a44a47a46 a48 a30a63a50 where a16a64a18a65a20a23a22a24a22a26a25
and a30 a52a66a54a57a56 . We then do the following adjust-
ments to make sure that a67a69a68a71a70a68a73a72
a56
a43 a44a47a46 a48a75a74
a50a37a18a76a20 : if
a67a69a68a73a70a68a71a72
a56
a43 a44 a46 a48a75a74
a50
a1
a20 , then the BF
a43 a44 a46 is scaled to
one; otherwise, a43 a44 a46 a48a3a2
a56
a50a51a18 a20a5a4
a67 a68a73a70a68a71a72
a56
a43 a44 a46 a48a75a74
a50
where a2 a56 a18 a6a8a7a10a9
a56
a30 .
a43 a44a47a46 a48a3a2
a56
a50 is also called
the ignorance value relative to a27 (Smets, 1988).
Larger a43 a44 a46a49a48a3a2
a56
a50 implies that it is harder to decide
which is the intended task of the caller by looking
at evidence a11 a12 . The BF’s a43 a44 a46 a14a17a16 a18 a20a23a22a24a22a26a25 are then
combined using Dempster’s rule of combination,
a43 a44 a46a24a44a12a11 a48a75a74
a50 a18
a67a14a13a16a15a18a17a20a19 a68 a43 a44a47a46 a48a22a21
a50
a43 a44a23a11 a48a25a24
a50 where
a21
a14
a24
a14
a74a27a26 a27 and
a16a29a28a18a76a36 .
a43 a44a47a46a24a44a23a11a17a44a31a30 is computed
by combining a43 a44a47a46a24a44a23a11 and a43 a44 a30 . Lastly, a30a33a32a60a14a10a36 a18
a20a23a22a24a22a39a20a31a40 are ranked in descending order according
to their pignistic probability measure a32a34a33a36a35a38a37 a48 a30 a32 a50 a18
a67a40a39 a72
a56a8a41
a7
a11
a9
a39 a42a8a43
a39a10a44
a43
a0a46a45
a42a8a43a48a47a50a49
a44a51a44a53a52a39a54a52
with the top of the rank
being the most probable target task. Experiment re-
sults will be presented in Section 3.1.
2.2 Identifying
the Provided Information
In this experiment, we show whether TBM can be
used to identify the information provided by the
caller in U. Here, the frame of discernment a27 a12a48a55a57a56a59a58
consists of the objects in the information schemas
for a specific task. As in Section 2.1, we use the
indices of a59
a32a57a14a10a36 a52 a20a23a22a24a22a39a20a31a40 to compute the bbm’s of
a11 a12 a14a17a16 a18 a20a23a22a24a22a26a25 given to each object disjunct
a60a62a61
a52
a54a57a56 a46a64a63a34a65a3a66 . Lastly, we combine the BFs
a43 a44 a46
a14a17a16 a18 a20a23a22a24a22a26a25
and compute the pignistic probability measures of
each object a60 a52 a27 a12a48a55a67a56a59a58 . Experiment results will be
presented in Section 3.2.
3 Experiment
Results
3.1 Identifying
the Intended Task
10
20
30
40
50
60
70
80
90
100
1 2 3 4 5 6 7 8 9 10
Correct rate
n
taskschema
infotask+schema
task+infoschema+info
all
Figure 1: Percentage of time the correct task is in-
cluded when considering the top a25 ranked tasks.
Figure 1 shows the results of selecting top-n-tasks
in the ranked list of a30a47a32a60a14a10a36 a18 a20a23a22a24a22a39a20a31a40 . The labels task,
schema and info denote that only knowledge in the
task specifications, information schemas and basic
information respectively are included in the calcula-
tion of bbm’s. ’+’ denotes a combination of some
and all denotes the combination of all. The graphs
show that we obtain the best ranking of candidate
tasks when knowledge from task specifications and
information schemas are used to calculate the BF’s.
This is intuitive because callers will often say her/his
goal and mention the name of the piece of informa-
tion s/he’s looking for, e.g., “I want to buy a movie
ticket please.”
10
20
30
40
50
60
70
80
90
100
1 2 3 4 5 6 7 8 9 10
Correct rate
n
taskschema
infotask+schema
task+infoschema+info
all
Figure 2: Percentage of time the correct task is in-
cluded when considering the top a25 ranked tasks, tak-
ing similar words into considerations.
Next, we examine the result of taking similar
words into considerations. This is because callers
may use words different from those occurring in our
knowledge base. Thus, for each word evidence a11 a12
in a68 , we use WordNet (Fellbaum, 1997) to look for
similar words a11a70a69a12a32 a14a10a36a45a18 a20a23a22a24a22a71 in our knowledge base.
For each a11 a69a12a26a32 a14a10a36a37a18a29a20a23a22a24a22a71 , we calculate the BF a43 a44a31a72
a46a11
as
discussed in Section 2.1. This time, we also multi-
ply the bbm’s in a43 a44a31a72
a46a11
by the distance measure be-
tween a11 a69a12a32 and a11a13a12 . The distance measures fall in the
range [0:1]. These results are shown in Figure 2.
Again, the results show that we obtain the best rank-
ing of candidate tasks when knowledge from task
specifications and information schemas are used to
calculate the BF’s. However, there is a decrease in
correct rate when only the best (-6.25%) and 2-best
(-1.58%) tasks in the ranked list are used to allow
10
20
30
40
50
60
70
80
90
100
1 2 3 4 5 6 7 8 9 10
Correct rate
n
taskschema
infotask+schema
task+infoschema+info
all
Figure 3: Percentage of time the correct task is in-
cluded when considering the top a25 ranked tasks, tak-
ing similar words and correlation measures into con-
sideration.
the callers to select. The correct rate is increased
only when more than 2 top-ranking tasks are used
for callers’ selection, i.e., 4.38%, 1.32%, 2.66% and
12.32% when n = 3, 4, 5 and 6 respectively.
From the results, we found that some words oc-
cur commonly across multiple domains. This phe-
nomena is common in problems related to natural
language processing. To alleviate the problem, we
have used words that only occur commonly in few
domains. We use correlation coefficient (Ng et al.,
1997) to measure the correlations of all words to all
domains. After that, we scale the correlation mea-
sures to 1. In calculating the bbm’s, we multiply
the original bbm’s with the corresponding correla-
tion measures. Figure 3 shows the results when sim-
ilar words and correlation measures are considered
in the calculation of BF’s. This time, the results
show that we obtain the best ranking of candidate
tasks when knowledge from task specifications and
basic information are used to calculate the BF’s. In
addition, there is a 67.31% improvement when the
top task in the ranked list is taken as the caller’s in-
tended tasks. When top-n tasks are used for callers’
selection, the improvements are 40.5%, 29.53% and
16.76% for n = 2, 3 and 4 respectively.
For the purpose of comparison, we show the re-
sults of task identification based on dialogue tran-
scripts, similar words and correlation measures in
Figure 4. The results show that with the use of only
basic information in the calculation of BF’s, a re-
sult of 99.1% can be achieved by select the top task
30
40
50
60
70
80
90
100
1 2 3 4 5 6 7 8 9 10
Correct rate
n
taskschema
infotask+schema
task+infoschema+info
all
Figure 4: Percentage of time the correct task is in-
cluded when considering the top a25 ranked tasks us-
ing dialogue transcripts, similar words and correla-
tion measures.
in the ranked list. Thus, when the word accuracy
of the speech recognizer is high, basic information
is sufficient to identify the callers’ intended tasks.
Otherwise, knowledge from task specifications and
information schema are required in target task iden-
tifications. We have shown that TBM can be used
for task identification in a noisy and multi-domain
environment. It would be interesting to compare
these results when we have enough corpus to train
a vector-based task identifier.
3.2 Identifying
the Provided Information
Figure 5 shows the percentage of time the cor-
rect information is included in the top-n selected
information after they have been sorted according
to their pignistic probability measures. SR-best-1
(SR-best-2) indicates that the best (respectively, two
best) speech recognition results are used for infor-
mation identification. The results show that there is
a 14.25% (10.54%) improvement when the best (re-
spectively, two best) speech recognition results are
used for information identification. ‘Transcript’ in-
dicates that the dialogue transcripts are used for in-
formation identification. The results show that there
is an average of 63.79% information lost between
’transcript’ and ‘SR-best-2’.
4 Conclusion
A new naturally spoken dialogue processing based
on the TBM has been presented. This approach
0
20
40
60
80
100
1 2 3 4 5 6 7 8
Correct rate
n
’SR-best-1’
’SR-best-2’
’transcript’
Figure 5: Correct identification rate using the top n
information in the rank.
can be viewed as looking for evidences from noisy
speech inputs to identify the tasks that the callers
want to perform and the information that they have
provided. Our experiments are tested on a multi-
domain environment. The speech recognizer that we
use has a word accuracy of around 55%. The exper-
iment results show that there is some initial success
in using TBM to aid in task and information identi-
fication when the recognized input is noisy.
In order to improve users’ satisfaction, we are
looking into dialogue processing methods that are
able to improve the results of task and information
identification. In particular, instead of using word
evidences from the recognized inputs, we are look-
ing into the use other evidences such as phonemes.
We are also looking into dialogue strategies that are
able to collaborate with the callers to correct the
identified information. In particular, if the ignorance
value a43a69a48a3a2 a56 a50 is high, our system should employ sys-
tem initiative strategies to disambiguate the identi-
fied information. If a43a69a48a1a0 a56 a50 is high, which means
that the evidences do not point strongly to any object
in a27 , then our system should employ system initia-
tive strategies to learn new task-related information.
If both a43a69a48a1a0
a56
a50 and
a43a69a48a3a2 a56
a50 are low, out system can
employ a mixed initiative dialogue strategy.
Acknowledgments
Our thanks go to the undergraduate students who
have contributed their valuable time to help us in the
recordings without asking for any rewards.

References

Chu-Carroll, Jeniffer and Bob Carpenter. 1999. Vector-
based natural language call routing. Computational
Linguistics, 25(3):361–388.

Denœux, Thierry. 2000. A neural network classifier based
on Dempster-Shafer theory. IEEE Transactions on
Systems, Man, and Cybernetics – Part A: Systems and
Humans, 30(2):131–150, March.

Fellbaum, Christiane (Ed). 1997. WordNet: An Electronic
Lexical Database. Imprint Cambridge, Mass: MIT
Press.

Gorin, Allen L., Giuseppe Riccardi and Jeremy H.
Wright. 1997. How may I help you? Speech Commu-
nication, 23:113–127.

Huang, Xuedong, Fileno Alleva, Hsiao-Wuen Hon, Mei-
Yuh Hwang, Ronald Rosenfeld. 1992. The SPHINX-
II speech recognition system: an overview. Computer
Speech and Language, 7(2):137–148.

Lee, Chin-Hui, Bob Carpenter, Wu Chou, Jennifer Chu-
Carroll, Wolfgang Reichl, Antoine Saad and Qiru
Zhou. 2000. On natural language call routing. Speech
Communication, 31(4):309-320, Aug.

Ng, Hwee Tou, Goh Wei Boon and Low Kok Leong.
1997. Feature selection, perceptron learning, and a us-
ability case study for text categorization. In Proceed-
ings of the 20th International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, 67-73. Philadelphia, Pennsylvania, USA.

Smets, Philippe. 1999. Practical uses of belief functions.
Uncertainty in Artificial Intelligence: Proceedings of
the Fifteenth Conference (UAI-1999), Morgan Kauf-
mann Publishers, San Francisco, CA, 612–621.

Smets, Philippe. 1989. Constructing the pignistic proba-
bility function in a context of uncertainty. Uncertainty
in Artificial Intelligence 5. Henrion M., Shachter R.
D., Kanal L. N. and Lemmer J. F. (Eds). North Hol-
land, Amsterdam, 29–40.

Smets, Philippe. 1988. Belief functions. Non-standard
Logic for Automated Reasoning. P. Smets, A. Mam-
dani, D. Dubois, and H. Prade (Eds). New York: Aca-
demic, 252–286.

Smets, Philippe. 1997. The axiomatic justification of
the transferable belief model. Artificial Intelligence,
92:229–242.

Thomson, David L. and Jack J. Wisowaty. 1999. User
confusion in natural language services. In Proc. ESCA
Workshop on Interactive Dialogue in Multi-Modal
Systems, Kloster Irsee, Germany, June, 189–196,
keynote address.

Zouhal, Lalla Merieme and Thierry Denœux. 1998. An
evidence-theoretic k-NN rule with parameter opti-
mization. IEEE Transactions on Systems, Man and Cy-
bernetics – Part C, 28(2):263-271.

