<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-fine n-best parsing and maxent discriminative reranking</title>
<date>2005</date>
<booktitle>In Proc. ACL</booktitle>
<contexts>
<context> and Matsumoto, 2000) and the Stanford BART. Figure 1: Example system configuration Named Entity Recognizer (Finkel et al., 2005). • The parsing pipeline uses Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005) to assignPOStagsandusesbaseNPsaschunkequivalents, whilealsoprovidingsyntactictreesthatcanbeusedby feature extractors. • The Carafe pipeline uses the parser in conjunction with an ACE mention tagger </context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Charniak, E. and Johnson, M. (2005). Coarse-to-fine n-best parsing and maxent discriminative reranking. In Proc. ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Culotta</author>
<author>M Wick</author>
<author>A McCallum</author>
</authors>
<title>Firstorder probabilistic models for coreference resolution</title>
<date>2007</date>
<booktitle>In Proc. HLT/NAACL</booktitle>
<contexts>
<context> et al., 2007). Developing a full coreference system, however, is a considerable engineering effort, which is why a large body of research concerned with feature engineering or learning methods (e.g. Culotta et al. 2007; Denis and Baldridge 2007) use a simpler but non-realistic setting, i.e. pre-identified mentions. Besides, the use of coreference information in summarization or question answering techniques is not </context>
</contexts>
<marker>Culotta, Wick, McCallum, 2007</marker>
<rawString>Culotta, A., Wick, M., and McCallum, A. (2007). Firstorder probabilistic models for coreference resolution. In Proc. HLT/NAACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Denis</author>
<author>J Baldridge</author>
</authors>
<title>A ranking approach to pronoun resolution</title>
<date>2007</date>
<booktitle>In Proc. IJCAI</booktitle>
<contexts>
<context>oping a full coreference system, however, is a considerable engineering effort, which is why a large body of research concerned with feature engineering or learning methods (e.g. Culotta et al. 2007; Denis and Baldridge 2007) use a simpler but non-realistic setting, i.e. pre-identified mentions. Besides, the use of coreference information in summarization or question answering techniques is not as widespread as it could </context>
<context>on the same task for the basic feature set, whereas performance using the extended feature set with tree kernels gives 73.4% recall on MUC, coming near specialized pronoun resolution systems such as (Denis and Baldridge, 2007). As in Uryupina (2006), we can 2http://riso.sourceforge.net Recall Precision F1 train (sec.) test (sec.) J48 55.0 72.6 62.6 30 76 SVMLight (linear) 51.0 74.1 60.4 44 90 MaxEnt (plain) 52.4 73.4 61.2</context>
</contexts>
<marker>Denis, Baldridge, 2007</marker>
<rawString>Denis, P. and Baldridge, J. (2007). A ranking approach to pronoun resolution. In Proc. IJCAI 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>T Grenager</author>
<author>C Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by Gibbs sampling</title>
<date>2005</date>
<booktitle>In Proc. ACL</booktitle>
<pages>363--370</pages>
<contexts>
<context>mbination, with the Stanford POS tagger (Toutanova et al., 2003), the YamCha chunker (Kudoh and Matsumoto, 2000) and the Stanford BART. Figure 1: Example system configuration Named Entity Recognizer (Finkel et al., 2005). • The parsing pipeline uses Charniak and Johnson’s reranking parser (Charniak and Johnson, 2005) to assignPOStagsandusesbaseNPsaschunkequivalents, whilealsoprovidingsyntactictreesthatcanbeusedby fe</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Finkel, J. R., Grenager, T., and Manning, C. (2005). Incorporating non-local information into information extraction systems by Gibbs sampling. In Proc. ACL 2005, pages 363–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale SVM learning practical</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods Support Vector Learning</booktitle>
<editor>In Sch¨olkopf, B., Burges, C., and Smola, A., editors</editor>
<contexts>
<context>re sets. Learning Interfaces to several machine learning libraries have been realized: • The WEKA machine learning toolkit (Witten and Frank, 2005); all classifiers from WEKA can be used. • SVMLight (Joachims, 1999), or SVMLight/TK (Moschitti, 2006), a modified version of SVMLight that can beusedwithtree-valuedfeatures. Classificationusesa Java Native Interface-based wrapper replacing SVMLight/TK’s svm classify</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Joachims, T. (1999). Making large-scale SVM learning practical. In Sch¨olkopf, B., Burges, C., and Smola, A., editors, Advances in Kernel Methods Support Vector Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudoh</author>
<author>Y Matsumoto</author>
</authors>
<title>Use of Support Vector Machines for chunk identification</title>
<date>2000</date>
<booktitle>In Proc. CoNLL</booktitle>
<contexts>
<context>on of possible designs yielded the following pipelines: • The chunking pipeline uses a classical tagger/chunker combination, with the Stanford POS tagger (Toutanova et al., 2003), the YamCha chunker (Kudoh and Matsumoto, 2000) and the Stanford BART. Figure 1: Example system configuration Named Entity Recognizer (Finkel et al., 2005). • The parsing pipeline uses Charniak and Johnson’s reranking parser (Charniak and Johnson</context>
</contexts>
<marker>Kudoh, Matsumoto, 2000</marker>
<rawString>Kudoh, T. and Matsumoto, Y. (2000). Use of Support Vector Machines for chunk identification. In Proc. CoNLL 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Liu</author>
<author>J Nocedal</author>
</authors>
<title>On the limited memory method for large scale optimization</title>
<date>1989</date>
<journal>Mathematical Programming B</journal>
<volume>45</volume>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Liu, D. C. and Nocedal, J. (1989). On the limited memory method for large scale optimization. Mathematical Programming B, 45(3):503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>N Kambhatla</author>
<author>S Roukos</author>
</authors>
<title>A mention-synchronous coreference resolution algorithm based on the bell tree</title>
<date>2004</date>
<booktitle>In ACL</booktitle>
<marker>Luo, Ittycheriah, Jing, Kambhatla, Roukos, 2004</marker>
<rawString>Luo, X., Ittycheriah, A., Jing, H., Kambhatla, N., and Roukos, S. (2004). A mention-synchronous coreference resolution algorithm based on the bell tree. In ACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F McCarthy</author>
<author>W G Lehnert</author>
</authors>
<title>Using decision trees for coreference resolution</title>
<date>1995</date>
<booktitle>In Proc. IJCAI</booktitle>
<marker>McCarthy, Lehnert, 1995</marker>
<rawString>McCarthy, J. F. and Lehnert, W. G. (1995). Using decision trees for coreference resolution. In Proc. IJCAI 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T S Morton</author>
</authors>
<title>Coreference for NLP applications</title>
<date>2000</date>
<booktitle>In Proc. ACL</booktitle>
<contexts>
<context>Coreference information has been shown to be beneficial in many high-level Natural Language Processing (NLP) processingtaskssuchasinformationextraction(McCarthyand Lehnert, 1995), question answering (Morton, 2000) and summarization (Steinberger et al., 2007). Developing a full coreference system, however, is a considerable engineering effort, which is why a large body of research concerned with feature engine</context>
</contexts>
<marker>Morton, 2000</marker>
<rawString>Morton, T. S. (2000). Coreference for NLP applications. In Proc. ACL 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
</authors>
<title>Making tree kernels practical for natural language learning</title>
<date>2006</date>
<booktitle>In Proc. EACL</booktitle>
<contexts>
<context>everal machine learning libraries have been realized: • The WEKA machine learning toolkit (Witten and Frank, 2005); all classifiers from WEKA can be used. • SVMLight (Joachims, 1999), or SVMLight/TK (Moschitti, 2006), a modified version of SVMLight that can beusedwithtree-valuedfeatures. Classificationusesa Java Native Interface-based wrapper replacing SVMLight/TK’s svm classify program to improve the classifica</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Moschitti, A. (2006). Making tree kernels practical for natural language learning. In Proc. EACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M¨uller</author>
<author>M Strube</author>
</authors>
<title>Multi-level annotation of linguistic data with MMAX2</title>
<date>2006</date>
<booktitle>Corpus Technology and Language Pedagogy: New Resources</booktitle>
<editor>In Braun, S., Kohn, K., and Mukherjee, J., editors</editor>
<location>New Tools, New Methods. Peter Lang, Frankfurt a.M., Germany</location>
<marker>M¨uller, Strube, 2006</marker>
<rawString>M¨uller, C. and Strube, M. (2006). Multi-level annotation of linguistic data with MMAX2. In Braun, S., Kohn, K., and Mukherjee, J., editors, Corpus Technology and Language Pedagogy: New Resources, New Tools, New Methods. Peter Lang, Frankfurt a.M., Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
</authors>
<title>Shallow semantics for coreference resolution</title>
<date>2007</date>
<booktitle>In Proc. IJCAI</booktitle>
<contexts>
<context> in GUITAR. Future work includes improvements to mention detection algorithms and a more comprehensive evaluation of features including thoserecently proposed by otherresearchers (e.g. Uryupina 2006; Ng 2007). Acknowledgments We thank the CLSP at Johns Hopkins, NSF and the Department of Defense for ensuring funding for the workshop and to EML Research, MITRE, the Center for Excellence in HLT, and FBK-IRS</context>
</contexts>
<marker>Ng, 2007</marker>
<rawString>Ng, V. (2007). Shallow semantics for coreference resolution. In Proc. IJCAI 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S P Ponzetto</author>
<author>M Strube</author>
</authors>
<title>Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution</title>
<date>2006</date>
<booktitle>In Proc. HLT/NAACL</booktitle>
<marker>Ponzetto, Strube, 2006</marker>
<rawString>Ponzetto, S. P. and Strube, M. (2006). Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution. In Proc. HLT/NAACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Qiu</author>
<author>M-Y Kan</author>
<author>T-S Chua</author>
</authors>
<title>A public reference implementation of the RAP anaphora resolution algorithm</title>
<date>2004</date>
<booktitle>In Proc. LREC</booktitle>
<marker>Qiu, Kan, Chua, 2004</marker>
<rawString>Qiu, L., Kan, M.-Y., and Chua, T.-S. (2004). A public reference implementation of the RAP anaphora resolution algorithm. In Proc. LREC 2004.</rawString>
</citation>
</citationList>
</algorithm>

