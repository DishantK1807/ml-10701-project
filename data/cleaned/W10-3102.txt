Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 5–13,
Uppsala, July 2010.
CreatingandEvaluatinga ConsensusforNegatedandSpeculative Words
ina SwedishClinicalCorpus
HerculesDalianis,MariaSkeppstedt
Departmentof Computerand SystemsSciences(DSV)
StockholmUniversity
Forum 100
SE-16440 Kista, Sweden
{hercules, mariask}@dsv.su.se
Abstract
In this paper we describe the creation
of a consensus corpus that was obtained
through combining three individual an-
notations of the same clinical corpus in
Swedish. We used a few basic rules that
were executed automaticallyto create the
consensus. The corpus contains nega-
tion words, speculative words, uncertain
expressions and certain expressions. We
evaluated the consensususing it for nega-
tion and speculation cue detection. We
used StanfordNER, whichis based on the
machine learning algorithm Conditional
RandomFields for the trainingand detec-
tion. For comparison we also used the
clinical part of the BioScope Corpus and
trainedit with StanfordNER.For our clin-
ical consensus corpus in Swedish we ob-
taineda precisionof 87.9 percentand a re-
call of 91.7 percentfor negation cues, and
for English with the Bioscope Corpus we
obtaineda precisionof 97.6 percentand a
recall of 96.7 percentfor negation cues.
1 Introduction
How we use languageto expressour thoughts,and
how we interpretthe languageof others,variesbe-
tween different speakers of a language. This is
true for various aspects of a language, and also
for the topic of this article; negations and spec-
ulations. The differences in interpretationare of
course most relevant when a text is used for com-
munication,but it also appliesto the task of anno-
tation. When the same text is annotatedby more
than one annotator, given that the annotatingtask
is non-trivial, the resultingannotatedtexts will not
be identical. This will be the result of differences
in how the text is interpreted, but also of differ-
ences in how the instructions for annotation are
interpreted. In order to use the annotated texts,
it must first be decidedif the interpretationsby the
differentannotatorsare similarenoughfor the pur-
pose of the text, and if so, it must be decidedhow
to handlethe non-identicalannotations.
In the study described in this article, we have
used a Swedish clinical corpus that was anno-
tated for certainty and uncertainty, as well as for
negation and speculationcues by three Swedish-
speakingannotators.The articledescribes an eval-
uationof a consensusannotationobtainedthrough
a few basic rules for combiningthe three different
annotationsinto one annotatedtext.1
2 Relatedresearch
2.1 Previousstudiesondetectionofnegation
andspeculationinclinicaltext
Clinicaltext oftencontainsreasoning,and thereby
many uncertain or negated expressions. When,
for example,searchingfor patientswith a specific
symptomin a clinical text, it is thus important to
be able to detectif a statementaboutthis symptom
is negated, certainor uncertain.
The first approach to identifying negations in
Swedishclinical text was carried out by Skeppst-
edt (2010),by whomthe well-known NegEx algo-
rithm (Chapmanet al., 2001), created for English
clinicaltext, was adaptedto Swedishclinicaltext.
Skeppstedtobtaineda precisionof 70 percentand
a recall of 81 percent in identifyingnegated dis-
eases and symptomsin Swedishclinicaltext. The
NegEx algorithmis purely rule-based,using lists
of cue wordsindicatingthat a precedingor follow-
ing disease or symptom is negated. The English
versionof NegEx (Chapmanet al., 2001)obtained
a precisionof 84.5percentand a recallof 82.0per-
cent.
1This research has been carried out after approval from
the Regional Ethical Review Board in Stockholm(Etikprvn-
ingsnmnden i Stockholm), permission number 2009/1742-
31/5.
5
Another example of negation detection in En-
glish is the approach used by Huang and Lowe
(2007). They used both parse trees and regu-
lar expressionsfor detecting negated expressions
in radiology reports. Their approach could de-
tect negated expressions both close to, and also
at some distancefrom, the actual negation cue (or
what they call negation signal). They obtained a
precisionof 98.6 percent and a recall of 92.6 per-
cent.
Elkinet al. (2005)used the termsin SNOMED-
CT (Systematized Nomenclature of Medicine-
Clinical Terms), (SNOMED-CT, 2010) and
matched them to 14 792 concepts in 41 health
records. Of these concepts,1 823 were identified
as negated by humans. The authors used Mayo
Vocabulary Server Parsing Engineand lists of cue
words triggering negation as well as words in-
dicating the scope of these negation cues. This
approach gave a precision of 91.2 percent and
a recall of 97.2 percent in detecting negated
SNOMED-CTconcepts.
In Rokach et al. (2008), they used clinical nar-
rative reportscontaining1 766instancesannotated
for negation. The authors tried several machine
learningalgorithmsfor detectingnegated findings
and diseases, including hidden markov models,
conditionalrandomfields and decisiontrees. The
best results were obtainedwith cascadeddecision
trees,with nodesconsistingof regularexpressions
for negation patterns. The regular expressions
were automaticallylearnt, using the LCS (longest
common subsequence)algorithm on the training
data. The cascadeddecisiontrees, built with LCS,
gave a precision of 94.4 percent, a recall of 97.4
percentand an F-scoreof 95.9 percent.
Szarvas (2008)describesa trial to automatically
identifyspeculative sentencesin radiologyreports,
using MaximumEntropy Models. Advanced fea-
ture selection mechanismswere used to automat-
ically extract cue words for speculation from an
initial seed set of cues. This, combinedwith man-
ual selection of the best extracted candidates for
cue words, as well as with outer dictionaries of
cue words, yielded an F-score of 82.1 percent for
detecting speculations in radiology reports. An
evaluation was also made on scientific texts, and
it could be concludedthat cue words for detecting
speculationwere domain-specific.
Moranteand Daelemans(2009) describea ma-
chinelearningsystemdetectingthe scopeof nega-
tions, which is based on meta-learning and is
trainedand testedon the annotatedBioScopeCor-
pus. In the clinical part of the corpus, the au-
thors obtained a precision of 100 percent, a re-
call of 97.5 percentand finally an F-scoreof 98.8
percent on detection of cue words for negation.
The authorsused TiMBL(Tilburg MemoryBased
Learner), which based its decision on features
such as the words annotatedas negation cues and
the two words surrounding them, as well as the
part of speech and word forms of these words.
For detection of the negation scope, the task was
to decide whether a word in a sentence contain-
ing a negation cue was either the word starting
or ending a negation scope, or neither of these
two. Three different classifiers were used: sup-
port vector machines, conditional random fields
and TiMBL. Featuresthat were used includedthe
word and the two words precedingand following
it, the part of speech of these words and the dis-
tance to the negation cue. A fourth classifier, also
based on conditionalrandom fields, used the out-
put of the other three classifiers,amongother fea-
tures, for the final decision. The result was a pre-
cision of 86.3 percentand a recall of 82.1 percent
for clinicaltext. It couldalsobe concludedthat the
system was portable to other domains, but with a
lower result.
2.2 TheBioScopeCorpus
Annotated clinical corpora in English for nega-
tion and speculationare describedin Vincze et al.(2008), where clinical radiology reports (a sub-
set of the so calledBioScopeCorpus)encompass-
ing 6 383 sentenceswere annotatedfor negation,
speculation and scope. Henceforth, when refer-
ring to the BioScopeCorpus,we only refer to the
clinical subset of the BioScope Corpus. The au-
thors found 877 negation cues and 1 189 specu-
lation cues, (or what we call speculative cues) in
the corpora in 1 561 sentences. This means that
fully 24 percent of the sentences containedsome
annotationfor negation or uncertainty. However,
of the original 6 383 sentences, 14 percent con-
tained negations and 13 percent contained spec-
ulations. Hence some sentences contained both
negationsand speculations.The corpuswas anno-
tated by two studentsand their work was led by a
chief annotator. The studentswere not allowed to
discusstheirannotationswitheachother, exceptat
regularmeetings,but they were allowed to discuss
6
with the chief annotator. In the cases where the
two student annotators agreed on the annotation,
that annotationwas chosenfor the final corpus. In
the cases where they did not agree, an annotation
made by the chief annotatorwas chosen.
2.3 TheStanfordNERbasedonCRF
The StanfordNamed Entity Recognizer(NER) is
based on the machine learning algorithm Condi-
tional RandomFields(Finkel et al., 2005)and has
been used extensively for identifyingnamed enti-
tiesin news text. For examplein the CoNLL-2003,
where the topic was language-independentnamed
entity recognition, Stanford NER CRF was used
both on English and German news text for train-
ing and evaluation. Wherethe best results for En-
glish with StanfordNER CRF gave a precisionof
86.1 percent, a recall of 86.5 percent and F-score
of 86.3 percent, for German the best results had
a precision of 80.4 percent, a recall of 65.0 per-
cent and an F-score of 71.9 percent, (Klein et al.,
2003). We have used the Stanford NER CRF for
trainingand evaluationof our consensus.
2.4 TheannotatedSwedishclinicalcorpus
fornegationandspeculation
A process to create an annotated clinical corpus
for negation and speculationis described in Dalia-
nis and Velupillai (2010). A total of 6 740 ran-
domly extractedsentencesfrom a very large clin-
ical corpus in Swedish were annotated by three
non-clinical annotators. The sentences were ex-
tractedfromthe text fieldAssessment(Bed¨omning
in Swedish). Each sentence and its context from
the text field Assessmentwerepresentedto the an-
notators who could use five different annotation
classes to annotate the corpora. The annotators
had discussions every two days on the previous
days’ work led by the experimentleader.
As described in Velupillai (2010), the anno-
tation guidelines were inspired by the BioScope
Corpus guidelines. There were, however, some
differences, such as the scope of a negation or of
an uncertaintynot being annotated. It was instead
annotatedif a sentenceor clause was certain, un-
certain or undefined. The annotators could thus
choose to annotate the entire sentence as belong-
ing to one of thesethreeclasses,or to breakup the
sentenceinto subclauses.
Pairwise inter-annotator agreement was also
measuredin the article by Dalianisand Velupillai
(2010). Theaverageinter-annotatoragreementin-
creasedafter the first annotationrounds,but it was
lower than the agreementbetween the annotators
of the BioScopeCorpus.
The annotationclassesused were thus negation
and speculativewords, but also certainexpression
anduncertainexpressionas wellas undefined. The
annotated subset contains a total of 6 740 sen-
tencesor 71 454 tokens, includingits context.
3 Methodforconstructingtheconsensus
We constructeda consensus annotationout of the
threedifferentannotationsof the sameclinicalcor-
pus that is described in Dalianis and Velupillai
(2010). The consensus was constructedwith the
generalidea of choosing,as far as possible,an an-
notationfor whichthere existedan identicalanno-
tation performedby at least two of the annotators,
and thus to find a majorityannotation.In the cases
whereno majoritywas found,other methodswere
used.
Otheroptionswouldbe to let the annotatorsdis-
cuss the sentences that were not identically an-
notated, or to use the method of the BioScope
Corpus, where the sentences that were not iden-
tically annotatedwere resolved by a chief annota-
tor (Vincze et al., 2008). A third solution, which
might, however, lead to a very biased corpus,
would be to not include the sentences for which
there was not a unanimous annotation in the re-
sultingconsensuscorpus.
3.1 Thecreationofa
consensus
The annotationclasses that were used for annota-
tion can be divided into two levels. The first level
consistedof the annotationclasses for classifying
the type of sentenceor clause. This level thus in-
cluded the annotation classes uncertain, certain
and undefined. The second level consisted of
the annotation classes for annotating cue words
for negation and speculation,thus the annotation
classes negation and speculativewords. The an-
notationclasses on the first level were considered
as moreimportantfor the consensus, since if there
was no agreement on the kind of expression, it
could perhaps be said to be less importantwhich
cue phrases these expressions contained. In the
following constructedexample,the annotationtag
Uncertainis thus an annotationon the first level,
while the annotation tags Negation and Specula-
tive words are on the secondlevel.
7
<Sentence>
<Uncertain>
<Speculative_words>
<Negation>Not</Negation>
really
</Speculative_words>
much worse than before
</Uncertain>
<Sentence>
When constructing the consensus corpus, the
annotated sentences from the first rounds of an-
notation were considered as sentences annotated
before the annotators had fully learnt to apply
the guidelines. The first 1 099 of the annotated
sentences,which also had a lower inter-annotator
agreement,were thereforenot includedwhencon-
structingthe consensus.Thereby, 5 641 sentences
were left to compare.
The annotationswere compared on a sentence
level, where the three versions of each sentence
were compared. First, sentences for which there
existed an identical annotation performed by at
least two of the annotatorswere chosen. This was
the casefor 5 097 sentences,thus 90 percentof the
sentences.
For the remaining544 sentences, only annota-
tion classes on the first level were comparedfor a
majority. For the 345 sentenceswhere a majority
was found on the first level, a majorityon the sec-
ond level was found for 298 sentences when the
scope of these tags was disregarded. The annota-
tion with the longest scope was then chosen. For
the remaining 47 sentences, the annotation with
the largest number of annotated instances on the
secondlevel was chosen.
The 199 sentences that were still not resolved
were then once again comparedon the first level,
this time disregardingthe scope. Thereby, 77 sen-
tences were resolved. The annotation with the
longest scopes on the first-level annotations was
chosen.
The remaining 122 sentences were removed
from the consensus. Thus, of the 5 641 sentences,
2 percent could not be resolved with these basic
rules. In the resulting corpus, 92 percent of the
sentences were identically annotated by at least
two persons.
3.2 Differencesbetweentheconsensusand
theindividualannotations
Aspectsof how the consensusannotationdiffered
from the individual annotations were measured.
The number of occurrences of each annotation
class was counted, and thereafter normalised on
the number of sentences,since the consensusan-
notationcontainedfewer sentencesthan the origi-
nal, individualannotations.
The resultsin Table 1 show that there are fewer
uncertainexpressionsin the consensusannotation
than in the average of the individual annotations.
The reason for this could be that if the annotation
is not completeley free of randomness, the class
with a higher probabilitywill be more frequentin
a majorityconsensus,than in the individual anno-
tations. In the cases where the annotatorsare un-
sureof how to classifya sentence,it is not unlikely
that the sentencehas a higherprobabilityof being
classified as belongingto the majority class, that
is, the class certain.
The class undefined is also less common in
the consensusannotation,and the same reasoning
holds true for undefinedas for uncertain, perhaps
to an even greater extent, since undefined is even
less common.
Alsothe speculativewordsare fewer in the con-
sensus. Most likely, this follows from the uncer-
tain sentencesbeing less common.
The words annotatedas negations, on the other
hand, are more common in the consensus anno-
tation than in the individual annotations. This
could be partly explainedby the choice of the 47
sentences with an annotation that contained the
largest number of annotatedinstanceson the sec-
ond level, and it is an indicationthat the consensus
containssomeannotationsfor negationcueswhich
have only been annotatedby one person.
TypeofAnnot.class Individ. Consens.
Negation 853 910
Speculative words 1 174 1 077
Uncertainexpression 697 582
Certainexpression 4 787 4 938
Undefinedexpression 257 146
Table1: Comparisonof the numberof occurrences
of each annotationclass for the individualannota-
tions and the consensus annotation. The figures
for the individual annotationsare the mean of the
threeannotators,normalisedon the numberof sen-
tencesin the consensus.
Table 2 shows how often the annotators have
divided the sentences into clauses and annotated
eachclausewitha separateannotationclass. From
the table we can see that annotatorA and also an-
8
notator H broke up sentencesinto more than one
type of the expressions Certain, Uncertainor Un-
defined expressions more often than annotator F.
Thereby, the resultingconsensusannotationhas a
lower frequency of sentencesthat containedthese
annotationsthan the average of the individual an-
notations. Many of the more granularannotations
that break up sentencesinto certain and uncertain
clausesare thus not includedin the consensusan-
notation. There are instead more annotationsthat
classify the entire sentenceas either Certain, Un-
certainor Undefined.
Annotators A F H Cons.
No. sentences 349 70 224 147
Table 2: Numberof sentencesthat containedmore
than one instance of either one of the annotation
classes Certain, Uncertain or Undefined expres-
sions or a combinationof these three annotation
classes.
3.3 Discussionofthemethod
The constructedconsensusannotationis thus dif-
ferentfromthe individualannotations,andit could
at least in somesensebe said to be better, since 92
percentof the sentenceshave been identicallyan-
notatedby at leasttwo persons.However, sincefor
example some expressions of uncertainty, which
do not have to be incorrect,have been removed, it
can also be said that some informationcontaining
possible interpretationsof the text, has also been
lost.
The appliedheuristicsare in mostcasesspecific
to this annotatedcorpus. The methodis, however,
describedin order to exemplify the more general
idea to use a majority decision for selecting the
correctannotations.Whatis testedwhenusingthe
majority method described in this article for de-
ciding which annotationis correct, is the idea that
a possible alternative to a high annotator agree-
ment would be to ask many annotators to judge
what they considerto be certainor uncertain.This
could perhaps be based on a very simplifiedidea
of language,that the use and interpretationof lan-
guageis nothingmore than a majoritydecisionby
the speakers of that language.
A similar approach is used in Steidl et al.(2005),wherethey studyemotionin speech. Since
there are no objective criteria for deciding with
what emotion somethingis said, they use manual
classificationby five labelers, and a majorityvot-
ing for decidingwhichemotionlabelto use. If less
than three labelers agreed on the classification,it
was omittedfrom the corpus.
It could be argued that this is also true for un-
certainty, that if there is no possibilityto ask the
author of the text, there are no objective criteria
for decidingthe level of certaintyin the text. It is
always dependent on how it is perceived by the
reader, and therefore a majority method is suit-
able. Even if the majority approachcanbe usedfor
subjective classifications, it has some problems.
For example,to increasevalidity more annotators
are needed, which complicatesthe process of an-
notation. Also, the same phenomenon that was
observed when constructingthe consensuswould
probably also arise, that a very infrequent class
such as uncertain, would be less frequent in the
majorityconsensusthan in the individual annota-
tions. Finally, therewouldprobablybe many cases
where there is no clear majority for either com-
pletelycertainor uncertain:in these cases, having
many annotatorswill not help to reach a decision
and it can only be concludedthat it is difficult to
classify this part of a text. Different levels of un-
certaintycould then be introduced,where the ab-
sence of a clear majoritycould be an indicationof
weakcertaintyor uncertainty, anda very weakma-
jority could result in an undefinedclassification.
However, even though different levels of cer-
tainty or uncertainty are interesting when study-
ing how uncertaintiesare expressedand perceived,
they would complicatethe processof information
extraction. Thus, if the final aim of the annota-
tion is to createa systemthat automaticallydetects
what is certain or uncertain,it would of course be
moredesirableto have an annotationwith a higher
inter-annotatoragreement. One way of achieving
a this would be to provide more detailed annota-
tion guidelinesfor what to define as certaintyand
uncertainty. However, when it comes to such a
vagueconceptas uncertainty, thereis always a thin
line betweenhaving guidelinescapturingthe gen-
eral perceptionof uncertaintyin the languageand
capturinga definitionof uncertaintythatis specific
to the writers of the guidelines. Also, there might
perhapsbe a risk that the complex conceptof cer-
tainty and uncertaintybecomes overly simplified
when it has to be formulated as a limited set of
guidelines. Therefore,a more feasible method of
achieving higher agreementis probablyto instead
9
ClassNeg-Spec Relevant Retrieved Corpus Precision Recall F-score
Negation 782 890 853 0.879 0.917 0.897
Speculative words 376 558 1061 0.674 0.354 0.464
Total 1 158 1 448 1 914 0.800 0.605 0.687
Table3: Theresultsfor negationand speculationon consensuswhenexecutingStanfordNERCRFusing
ten-foldcross validation.
ClassCert-Uncertain Relevant Retrieved Corpus Precision Recall F-score
Certainexpression 4 022 4 903 4 745 0.820 0.848 0.835
Uncertainexpression 214 433 577 0.494 0.371 0.424
Undefinedexpression 2 5 144 0.400 0.014 0.027
Total 4 238 5 341 5 466 0.793 0.775 0.784
Table 4: The results for certain and uncertainon consensuswhen executing StanfordNER CRF using
ten-foldcross validation.
simplifywhatis beingannotated,and not annotate
for such a broadconceptas uncertaintyin general.
Amongother suggestionsfor improving the an-
notationguidelinesfor the corpusthat the consen-
sus is basedon, Velupillai(2010)suggeststhat the
guidelinesshould also include instructionson the
focus of the uncertainties,that is, what concepts
are to be annotatedfor uncertainty.
The task couldthus, for example,be tailoredto-
wards the informationthat is to be extracted, and
thereby be simplified by only annotating for un-
certaintyrelatingto a specificconcept. If diseases
or symptomsthat are presentin a patientare to be
extracted,the most relevant conceptto annotateis
whether a finding is present or not present in the
patient,or whetherit is uncertainif it is presentor
not. This approach has, for example, achieved a
very high inter-annotator agreement in the anno-
tation of the evaluation data used by Chapmanet
al. (2001). Even though this approach is perhaps
linguisticallyless interesting,not givingany infor-
mationon uncertaintiesin general,if the aim is to
search for diseases and symptoms in patients, it
shouldbe sufficient.
In light of the discussionabove, the questionto
whatextentthe annotationsin the constructedcon-
sensuscapturea generalperception of certaintyor
uncertaintymust be posed. Since it is constructed
using a majority method with three annotators,
who had a relatively low pairwise agreement,the
corpus could probablynot be said to be a precise
captureof what is a certaintyor uncertainty. How-
ever, as Artstein and Poesio (2008) point out, it
cannot be said that there is a fixed level of agree-
ment that is valid for all purposesof a corpus,but
the agreementmust be high enough for a certain
purpose. Therefore,if the informationon whether
there was a unanimousannotationof a sentenceor
not is retained,servingas an indicatorof how typ-
ical an expression of certainty or uncertainty is,
the constructed corpus can be a useful resource.
Both for studyinghow uncertaintyin clinical text
is constructedand perceived, and as one of the re-
sources that is used for learning to automatically
detectcertaintyand uncertaintyin clinicaltext.
4 ResultsoftrainingwithStanfordNER
CRF
As a first indicationof whetherit is possibleto use
the annotated consensuscorpus for finding nega-
tion and speculationin clinicaltext, we trainedthe
StanfordNERCRF, (Finkel et al., 2005)on the an-
notateddata. Artsteinand Poesio(2008)writethat
the fact that annotateddata can be generalizedand
learnt by a machine learning system is not an in-
dicationthat the annotationscapturesome kind of
reality. If it would be shown that the constructed
consensusis easilygeneralizable,this can thus not
be usedas an evidenceof its quality. However, if it
would be shown that the data obtainedby the an-
notations cannot be learnt by a machine learning
system, this can be used as an indicationthat the
data is not easily generalizableand that the task
to learn perhapsshould,if possible,be simplified.
Of course, it could also be an indication that an-
other learning algorithm should be used or other
featuresselected.
We created two training sets of annotatedcon-
sensusmaterial.
The first training set contained annotationson
the second level, thus annotationsthat contained
the classesSpeculativewords and Negation. In 76
cases, the tag for Negation was inside an annota-
tion for Speculativewords, and these occurrences
10
ClassNeg-SpecBio Relevant Retrieved Corpus Precision Recall F-score
Negation 843 864 872 0.976 0.967 0.971
Speculative words 1 021 1 079 1 124 0.946 0.908 0.927
Scope1 1 295 1 546 1 5952 0.838 0.812 0.825
Table 5: The results for negations,speculationcues and scopeson the BioScopeCorpuswhen executing
StanfordNER CRF using ten-foldcross validation.
ClassNeg-Spec Relevant Retrieved Corpus Precision Recall F-score
Negation A 791 1 005 896 0.787 0.883 0.832
Speculative words 684 953 1 699 0.718 0.403 0.516
Negation F 938 1097 1023 0.855 0.916 0.884
Speculative words 464 782 1 496 0.593 0.310 0.407
Negation H 722 955 856 0.756 0.843 0.797
Speculative words 552 853 1 639 0.647 0.336 0.443
Table 6: The resultsfor negationsand speculationcues and scopesfor annotatorA, F and H respectively
when executingStanfordNER CRF using ten-foldcross validation.
of the tag Negation were removed. It is detecting
this difference between a real negation cue and a
negation word inside a cue for speculationthat is
one of the difficultiesthat distinguishes the learn-
ing task from a simplestring matching.
The secondtrainingset only containedthe con-
sensusannotationson the first level, thusthe anno-
tation classesCertain, Uncertainand Undefined.
We used the default settings on Stanford NER
CRF. The results of the evaluation using ten-fold
crossvalidation(Kohavi, 1995)are shown in Table
3 and Table 4.
As a comparison, and to verify the suitabil-
ity of the chosen machine learning method, we
also trained and evaluated the BioScope Corpus
using Stanford NER CRF for negation, specula-
tion and scope. The results can be seen in Ta-
ble 5. When training the detectionof scope, only
BioScope sentences that contained an annotation
for negation and speculationwere selected for the
training and evaluation material for the Stanford
NER CRF. This divisioninto two trainingsets fol-
lows the methodused by Moranteand Daelemans
(2009), where sentencescontaininga cue are first
detected, and then, among these sentences, the
scope of the cue is determined.
We also trained and evaluated the annotations
that were carried out by each annotator A, F and
H separately, i.e. the source of consensus.The re-
sults can be seen in Table 6.
We also comparedthe distribution of Negation
and Speculativewords in the consensusversus the
BioScope Corpus and we found that the consen-
sus, in Swedish, used about the same number of
(types) for negation as the BioScope Corpus in
English (see Table 7), but for speculative words
the consensuscontainedmany moretypesthan the
BioScope Corpus. In the constructedconsensus,
72 percentof the Speculativewords occurredonly
once,whereasin the BioScopeCorpusthiswas the
case for only 24 percentof the Speculativewords.
Typeofword Cons. Bio
Uniquewords (Types)
annotatedas Negation 13 19
Negationsthat
occurredonly once 5 10
Uniquewords (Types)
annotatedas Speculative 408 79
Speculativewords that
occurredonly once 294 19
Table7: Numberof uniquewordsbothin the Con-
sensus and in the BioScopeCorpus that were an-
notatedas Negationand as Speculativewords, and
how many of these that occurredonly once.
5 Discussion
The training results using our clinical consensus
corpusin Swedishgave a precisionof 87.9percent
and a recallof 91.7percentfor negation cuesand a
precisionof 67.4 percent and a recall of 35.4 per-
cent for speculationcues. The results for detecting
negation cues are thus much higher than for de-
tecting cues for speculationusing Stanford NER
CRF. This difference is not very surprising,given
1The scopes were trained and evaluated separetely from
the negationsand speculations.
2Theoriginalnumberof annotatedscopesin the BioScope
Corpusis 1 981. Of these, 386 annotationsfor nested scopes
were removed.
11
the datain Table7, whichshows thatthereare only
a very limited number of negation cues, whereas
there exist over 400 different cue words for spec-
ulation. One reason why the F-score for negation
cues is not even higher, despite the fact that the
numberof cuesfor negationsis very limited,could
be that a negationword insidea tag for speculative
words is not counted as a negation cue. There-
fore, the word not in, for example,not reallycould
have been classifiedas a negation cue by Stanford
NER CRF, even though it is a cue for speculation
and not for negation. Anotherreasoncouldbe that
the word meaningwithout in Swedish(utan) also
meansbut, whichonlysometimesmakes it a nega-
tion cue.
We can also observe in Table 4, that the results
for detectionof uncertainexpressionsare very low
(F-score 42 percent). For undefined expressions,
due to scarce training material, it is not possible
to interpretthe results. For certainexpressionsthe
results are acceptable,but since the instances are
in majority, the resultsare not very useful.
Regarding the BioScope Corpus we can ob-
serve (see Table 5) that the training results both
for detecting cues for negation and for specula-
tions are very high, with an F-score of 97 and 93
percent, respectively. For scope detection,the re-
sult is lower but acceptable, with an F-score of
83 percent. These results indicatethat the chosen
methodis suitablefor the learningtask.
The main reason for the differences in F-score
between the Swedish consensus corpus and the
BioScopeCorpus, when it comes to the detection
of speculationcues, is probablythat the variation
of words that were annotatedas Speculativeword
is much larger in the constructedconsensus than
in the BioScopeCorpus.
As can be seen in Table 7, there are many more
types of speculative words in the Swedishconsen-
sus than in the BioScopeCorpus. We believe that
one reasonfor this differenceis that the sentences
in the constructed consensus are extracted from
a very large number of clinics (several hundred),
whereasthe BioScopeCorpuscomesfrom one ra-
diologyclinic. Thisis supportedby the findingsof
Szarvas (2008), who writes that cues for specula-
tionare domain-specific.In this case,however, the
texts are still withinthe domainof clinicaltexts.
Anotherreasonfor the larger varietyof cues for
speculation in the Swedish corpus could be that
the guidelines for annotating the BioScope Cor-
pus and the methodfor creatinga consensuswere
different.
When comparingthe results for the individual
annotatorswiththe constructedconsensus,the fig-
ures in Tables 3 and 6 indicate that there are no
big differencesin generalizability. Whendetecting
cues for negation, the precisionfor the consensus
is better than the precision for the individual an-
notations. However, the results for the recall are
only slightlybetteror equivalentfor the consensus
than for the individual annotations. If we analyse
the speculative cues we can observe that the con-
sensusand the individualannotationshave similar
results.
The low results for learning to detect cues for
speculationalso serve as an indicatorthat the task
should be simplifiedto be more easily generaliz-
able. For example, as previously suggested for
increasingthe inter-annotatoragreement,the task
could be tailoredtowards the specificinformation
that is to be extracted, such as the presence of a
diseasein a patient.
6 Future
work
To further investigate if a machine learning algo-
rithm such as ConditionalRandom Fields can be
used for detectingspeculative words, more infor-
mation needs to be provided for the Conditional
Random Fields, such as part of speech or if any
of the words in the sentencecan be classifiedas a
symptomor a disease. One ConditionalRandom
Fields system that can treat nested annotationsis
CRF++(CRF++,2010). CRF++is usedby several
research groups and we are interestedin trying it
out for the negation and speculation detection as
well as scope detection.
7 Conclusion
A consensus clinical corpus was constructed by
applyinga few basic rules for combiningthree in-
dividual annotations into one. Compared to the
individual annotations, the consensus contained
fewer annotations of uncertaintiesand fewer an-
notations that divided the sentences into clauses.
It also containedfewer annotationsfor speculative
words, and more annotations for negations. Of
the sentencesin the constructedcorpus,92 percent
were identicallyannotatedby at least two persons.
In comparison with the BioScope Corpus, the
constructed consensus contained both a larger
number and a larger variety of speculative cues.
12
This might be one of the reasons why the results
for detectingcues for speculative words using the
Stanford NER CRF are much better for the Bio-
Scope Corpus than for the constructedconsensus
corpus;the F-scoresare 93 percentversus 46 per-
cent.
Both the BioScopeCorpus and the constructed
consensuscorpus had high values for detectionof
negation cues, F-scores97 and 90 percent,respec-
tively.
As is suggestedby Velupillai(2010),the guide-
lines for annotation should include instructions
on the focus of the uncertainties. To focus the
decision of uncertainty on, for instance, the dis-
ease of a patient, might improve both the inter-
annotator agreement and the possibility of auto-
maticallylearning to detect the concept of uncer-
tainty.
Acknowledgments
We are very gratefulfor the valuablecommentsby
the three anonymousreviewers.
References
Ron Artstein and MassimoPoesio. 2008. Inter-coder
agreementfor computationallinguistics. Computa-
tional Linguistics, 34(4):555–596.
Wendy W. Chapman, Will Bridewell, Paul Hanbury,
Gregory F. Cooper, and Bruce G. Buchanan. 2001.
A simple algorithmfor identifyingnegated findings
and diseases in discharge summaries. Journal of
biomedicalinformatics, 34(5):301–310.
CRF++. 2010. CRF++: Yet anotherCRF toolkit,May
8. http://crfpp.sourceforge.net//.
Hercules Dalianis and Sumithra Velupillai. 2010.
How certain are clinical assessments? Annotating
Swedish clinical text for (un)certainties, specula-
tions and negations. In Proceedingsof the Seventh
conference on International Language Resources
and Evaluation(LREC’10), Valletta,Malta,May.
Peter L. Elkin, Steven H. Brown, Brent A. Bauer,
Casey S. Husser, William Carruth, Larry R.
Bergstrom, and Dietlind L. Wahner-Roedler. 2005.
A controlled trial of automated classification of
negation from clinical notes. BMC Medical Infor-
maticsand DecisionMaking, 5(1):13.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporatingnon-local informa-
tion into information extraction systems by gibbs
sampling. In ACL ’05: Proceedingsof the 43rd An-
nualMeetingon Associationfor ComputationalLin-
guistics, pages 363–370.
Yang Huangand HenryJ. Lowe. 2007. A novel hybrid
approachto automatednegation detectionin clinical
radiologyreports. Journalof the AmericanMedical
InformaticsAssociation, 14(3):304.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D. Manning. 2003. Named entity recognition
with character-level models. In Proceedingsof the
seventh conference on Natural language learningat
HLT-NAACL 2003, pages 180–183.Associationfor
ComputationalLinguistics.
Ron Kohavi. 1995. A study of cross-validation and
bootstrap for accuracy estimationand model selec-
tion. In InternationalJoint Conference on Artificial
Intelligence, volume14, pages 1137–1145.
Roser Morante and Walter Daelemans. 2009. A met-
alearningapproachto processingthe scope of nega-
tion. In CoNLL ’09: Proceedings of the Thir-
teenth Conference on ComputationalNatural Lan-
guage Learning, pages21–29.Associationfor Com-
putationalLinguistics.
Lior Rokach, Roni Romano,and Oded Maimo. 2008.
Negation recognition in medical narrative reports.
InformationRetrieval, 11(6):499–538.
Maria Skeppstedt. 2010. Negation detection in
Swedish clinical text. In Louhi’10 Second Louhi
Workshopon Text and Data Mining of Health Doc-
uments,held in conjunctionwith NAACL HLT 2010,
Los Angeles,June.
SNOMED-CT. 2010. Systematized nomen-
clature of medicine-clinical terms, May 8.
http://www.ihtsdo.org/snomed-ct/.
Stefan Steidl, Michael Levit, Anton Batliner, Elmar
N¨oth, and Heinrich Niemann. 2005. ”Off all
things the measureis man” Automaticclassification
of emotions and inter-labeler consistency. In Pro-
ceedingof the IEEE ICASSP,2005, pages 317–320.
Gy¨orgy Szarvas. 2008. Hedge classification in
biomedical texts with a weakly supervised selec-
tion of keywords. In Proceedingsof ACL-08: HLT,
pages 281–289,Columbus, Ohio, June. Association
for ComputationalLinguistics.
Sumithra Velupillai. 2010. Towards a better un-
derstanding of uncertainties and speculations in
swedish clinical text – analysis of an initial anno-
tation trial. To be published in the proceedingsof
the Negation and Speculationin Natural Language
ProcessingWorkshop,July 10, 2010,Uppsala,Swe-
den.
Veronika Vincze, Gy¨orgy Szarvas, Rich´ard Farkas,
Gy¨orgy M´ora, and J´anos Csirik. 2008. The bio-
scope corpus: biomedicaltexts annotatedfor uncer-
tainty, negationand theirscopes. BMCBioinformat-
ics, 9(S-11).
13

