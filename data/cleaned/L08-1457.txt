<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>C˘at˘alina Barbu</author>
<author>Ruslan Mitkov</author>
</authors>
<title>Evaluation tool for rule-based anaphora resolution methods</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, pages 34 – 41</booktitle>
<volume>9</volume>
<location>Toulouse, France</location>
<contexts>
<context>ights future directions for the evaluation exercise. 2. Related work The need to have a consistent evaluation for anaphora and coreference has been repeatedly highlighted by researchers in the field (Barbu and Mitkov, 2001; Mitkov, 2000) but the only evaluation conferences which focused explicitly on coreference and anaphora were the Message Understanding Conferences (MUC)3 which included a coreference track. The Autom</context>
</contexts>
<marker>Barbu, Mitkov, 2001</marker>
<rawString>C˘at˘alina Barbu and Ruslan Mitkov. 2001. Evaluation tool for rule-based anaphora resolution methods. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, pages 34 – 41, Toulouse, France, July 9 – 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Cristea</author>
<author>Oana Postolache</author>
</authors>
<title>Anaphora resolution: framework, creation of resources, and evaluation</title>
<date>2006</date>
<booktitle>In Svetla Koeva and Mila DimitrovaVulchanova, editors, Proceedings of the Fifth International Conference on Formal Approaches to South Slavic and Balkan Languages, pages 1 – 5</booktitle>
<editor>18 – 20. Laura Hasler, Constantin Or˘asan, and Karin Naumann</editor>
<location>Sofia, Bulgaria</location>
<contexts>
<context>entified by the automatic systems did not match those in the gold standard. As a result, an overlap measure was introduced to indicate how successfully the automatic system can identify the entities (Cristea and Postolache, 2006). The overlap between two entities E1 and E2 is defined as: overlap(E1,E2) = length(overlapstring)max(length(E 1),length(E2) where: • length(overlapstring) represents the length in words of the strin</context>
</contexts>
<marker>Cristea, Postolache, 2006</marker>
<rawString>Dan Cristea and Oana Postolache. 2006. Anaphora resolution: framework, creation of resources, and evaluation. In Svetla Koeva and Mila DimitrovaVulchanova, editors, Proceedings of the Fifth International Conference on Formal Approaches to South Slavic and Balkan Languages, pages 1 – 5, Sofia, Bulgaria, October 18 – 20. Laura Hasler, Constantin Or˘asan, and Karin Naumann.</rawString>
</citation>
<citation valid="true">
<title>NPs for Events: Experiments in Coreference Annotation</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th edition of the International Conference on Language Resources and Evaluation (LREC2006</booktitle>
<volume>24</volume>
<pages>1167--1172</pages>
<location>Genoa, Italy</location>
<marker>2006</marker>
<rawString>2006. NPs for Events: Experiments in Coreference Annotation. In Proceedings of the 5th edition of the International Conference on Language Resources and Evaluation (LREC2006), pages 1167 – 1172, Genoa, Italy, 24 – 26 May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Therese Firmin</author>
<author>David House</author>
<author>Michael Chrzanowski</author>
<author>Gary Klein</author>
<author>Lynette Hirshman</author>
<author>Beth Sundheim</author>
<author>Leo Obrst</author>
</authors>
<title>The TIPSTER SUMMAC text summarisation evaluation: Final report</title>
<date>1998</date>
<tech>Technical Report MTR 98W0000138</tech>
<publisher>The MITRE Corporation</publisher>
<contexts>
<context> term goals of ARE are to identify a large set of linguistically motivated relations between a variety of entities, in different languages. Evaluation conferences are common in other domains. SUMMAC (Mani et al., 1998) and DUC (TIDES, 2000) were organised in the field of automatic summarisation, TREC conferences5 evaluate different aspects of text retrieval, whilst CLEF campaigns6 attempt the similar tasks in a mu</context>
</contexts>
<marker>Mani, Firmin, House, Chrzanowski, Klein, Hirshman, Sundheim, Obrst, 1998</marker>
<rawString>Inderjeet Mani, Therese Firmin, David House, Michael Chrzanowski, Gary Klein, Lynette Hirshman, Beth Sundheim, and Leo Obrst. 1998. The TIPSTER SUMMAC text summarisation evaluation: Final report. Technical Report MTR 98W0000138, The MITRE Corporation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
</authors>
<title>Towards more comprehensive evaluation in anaphora resolution</title>
<date>2000</date>
<booktitle>In Proceedings of the Second International Conference on Language Resources and Evaluation, volume III</booktitle>
<pages>1309--1314</pages>
<location>Athens, Greece</location>
<contexts>
<context>for the evaluation exercise. 2. Related work The need to have a consistent evaluation for anaphora and coreference has been repeatedly highlighted by researchers in the field (Barbu and Mitkov, 2001; Mitkov, 2000) but the only evaluation conferences which focused explicitly on coreference and anaphora were the Message Understanding Conferences (MUC)3 which included a coreference track. The Automatic Content E</context>
</contexts>
<marker>Mitkov, 2000</marker>
<rawString>Ruslan Mitkov. 2000. Towards more comprehensive evaluation in anaphora resolution. In Proceedings of the Second International Conference on Language Resources and Evaluation, volume III, pages 1309 – 1314, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
</authors>
<title>Outstanding issues in anaphora resolution</title>
<date>2001</date>
<booktitle>Computational Linguistics and Intelligent Text Processing</booktitle>
<pages>110--125</pages>
<editor>In Al. Gelbukh, editor</editor>
<publisher>Springer</publisher>
<contexts>
<context>re directions for the evaluation exercise. 2. Related work The need to have a consistent evaluation for anaphora and coreference has been repeatedly highlighted by researchers in the field (Barbu and Mitkov, 2001; Mitkov, 2000) but the only evaluation conferences which focused explicitly on coreference and anaphora were the Message Understanding Conferences (MUC)3 which included a coreference track. The Autom</context>
<context> that need to be resolved and their candidates are known by the systems. As can be seen these goals largely address the difference made between the evaluation of algorithms and evaluation of systems (Mitkov, 2001). 3.1.1. Task 1: Pronominal anaphora resolution on pre-annotated texts The main purpose of the first task was to evaluate to what extent pronoun resolution algorithms work. In order to achieve this, </context>
</contexts>
<marker>Mitkov, 2001</marker>
<rawString>Ruslan Mitkov. 2001. Outstanding issues in anaphora resolution. In Al. Gelbukh, editor, Computational Linguistics and Intelligent Text Processing, pages 110– 125. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
</authors>
<title>Anaphora resolution</title>
<date>2002</date>
<contexts>
<context>phora resolution is the process of resolving an anaphoric expression to the expression it refers to. If the antecedent and the anaphor have the same referent in the real world they are coreferential (Mitkov, 2002). The process of building chains of coreferential entities is called coreference resolution. Both anaphora resolution and coreference resolution are vital to a number of applications such as machine </context>
</contexts>
<marker>Mitkov, 2002</marker>
<rawString>Ruslan Mitkov. 2002. Anaphora resolution. Longman. Tony G. Rose, Mark Stevenson, and Miles Whitehead.</rawString>
</citation>
<citation valid="true">
<title>The Reuters Corpus Volume 1 from Yesterday’s News to Tomorrow’s Language Resources</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC-2002</booktitle>
<pages>827</pages>
<marker>2002</marker>
<rawString>2002. The Reuters Corpus Volume 1 from Yesterday’s News to Tomorrow’s Language Resources. In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC-2002), pages 827 – 833, Las Palmas de Gran Canaria, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TIDES</author>
</authors>
<title>Translingual Information Detection, Extraction and Summarization</title>
<date>2000</date>
<contexts>
<context>entify a large set of linguistically motivated relations between a variety of entities, in different languages. Evaluation conferences are common in other domains. SUMMAC (Mani et al., 1998) and DUC (TIDES, 2000) were organised in the field of automatic summarisation, TREC conferences5 evaluate different aspects of text retrieval, whilst CLEF campaigns6 attempt the similar tasks in a multilingual environment</context>
</contexts>
<marker>TIDES, 2000</marker>
<rawString>TIDES. 2000. Translingual Information Detection, Extraction and Summarization (TIDES). http://www.darpa.mil/iao/TIDES.htm.</rawString>
</citation>
</citationList>
</algorithm>

