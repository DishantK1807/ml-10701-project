CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 243–247
Manchester, August 2008
TheIntegrationofDependencyRelationClassificationandSemanticRole
LabelingUsingBilayerMaximumEntropyMarkovModels
WeiweiSun and HongzhanLi and ZhifangSui
InstituteofComputationalLinguistics
PekingUniversity
{weiwsun,lihongzhan.pku}@gmail.com,szf@pku.edu.cn
Abstract
This paper describesa system to solve
thejointlearningof syntacticandseman-
tic dependencies. An directedgraphical
modelis put forward to integratedepen-
dency relationclassificationandsemantic
role labeling. We presenta bilayerdi-
rected graph to expressprobabilisticre-
lationshipsbetweensyntacticand seman-
tic relations. MaximumEntropy Markov
Modelsareimplementedto estimatecon-
ditionalprobabilitydistributionandto do
inference. The submittedmodel yields
76.28% macro-average F1 performance,
forthejointtask,85.75%syntacticdepen-
denciesLASand66.61%semanticdepen-
denciesF1.
1 Introduction
Dependencyparsingandsemanticrolelabelingare
becomingimportantcomponentsinmanykindsof
NLPapplications.Givenasentence,thetaskofde-
pendency parsingis to identifythesyntactichead
ofeachwordinthesentenceandclassifytherela-
tionbetweenthedependentandits head;thetask
ofsemanticrolelabelingconsistsofanalyzingthe
propositionsexpressedby sometargetpredicates.
Theintegrationof syntacticandsemanticparsing
interestsmany researchersand someapproaches
hasbeenproposed(Yi andPalmer, 2005;Geand
Mooney, 2005). CoNLL2008 sharedtask pro-
posesthemergingof bothsyntacticdependencies
andsemanticdependenciesundera uniqueunified
representation(Surdeanuetal.,2008).Weexplore
c©2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Somerightsreserved.
theintegrationproblemandevaluateourapproach
usingdataprovidedonCoNLL2008.
This paperexploresthe integrationof depen-
dency relationclassificationandsemanticrolela-
beling,usingadirectedgraphicalmodelthatisalso
knownasBayesianNetworks.Thedirectedgraph
of oursystemcanbe seenas onechainof obser-
vationswithtwo labellayers:theobservationsare
argumentcandidates;onelayer’s labelsetis syn-
tacticdependencyrelations;theother’sissemantic
dependency relations.To estimatetheprobability
distributionof eacharcanddo inference,weim-
plementaMaximumEntropyMarkovModel(Mc-
Callumet al., 2000). Specially, a logisticregres-
sionmodelisusedtogettheconditionalprobabil-
ity of eacharc; dynamicprogrammingalgorithm
isappliedtosolvethe”argmax”problem.
2 SystemDescription
OurDP-SRLsystemconsistsof5stages:
1. dependencyparsing;
2. predicateprediction;
3. syntacticdependency relationclassification
andsemanticdependency relationidentifica-
tion;
4. semanticdependencyrelationclassification;
5. semanticdependencyrelationinference.
2.1 DependencyParsing
In dependency parsingstage, MSTParser1 (Mc-
Donaldet al., 2005), a dependency parser that
searchesfor maximumspanningtrees over di-
rectedgraphs,isused.weuseMSTParser’sdefault
1http://www.seas.upenn.edu/strctlrn/MSTParser/MSTParser.html
243
LemmaanditsPOStag
Numberofchildren
SequentialPOStagsofchildren
LemmaandPOSofNeighboringwords
LemmaandPOSofparent
IsthewordinwordlistofNomBank
IsthewordinwordlistofPropBank
IsPOSofthewordisVB*orNN*
Table1: Featuresusedtopredicttargetpredicates
parametersto traina parsingmodel. In the third
stageofoursystem,dependencyrelationsbetween
argumentcandidatesandtargetpredicatesareup-
dated,if therearedependency betweenthecandi-
datesandthepredicates.
2.2 PredicatePrediction
DifferentfromCoNLL-2005sharedtask,thetar-
getpredicatesarenotgiven as input. Oursystem
formulatesthepredicatepredicationproblemas a
two-classclassificationproblemusingmaximum
entropy classifierMaxEnt2 (Berger et al., 1996).
Table 1 lists featuresused. We use a empirical
thresholdtofilterwords:ifthe”beingtarget”prob-
abilityofawordisgreaterthan0.075,itisseenas
atargetpredicate.Thisstrategyachievesa79.96%
precisionanda98.62%recall.
2.3 SyntacticDependencyRelation
ClassificationandSemanticDependency
RelationIdentification
We integrate dependency parsingand semantic
rolelabelingtosomeextentinthisstage.Somede-
pendency parsingsystemsprefertwo-stagearchi-
tecture: unlabeledparsingand dependency clas-
sification(Nivreet al., 2007). Previoussemantic
rolelabelingapproachesalsoprefertwo-stagear-
chitecture:argumentidentificationand argument
classification.Oursystemdoessyntacticrelations
classificationandsemanticrelationsidentification
at the sametime. Specially, usinga pruningal-
gorithm,wecollecta setof argumentcandidates;
thenweclassifydependency relationsbetweenar-
gumentcandidatesandthepredicatesandpredict
whethera candidateis an argument. A directed
graphicalmodelis usedto representtherelations
betweensyntacticandsemanticrelations.
2http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.h
tml
Lemma,POStagvoiceofpredicates
POSpatternofpredicate’s children
IsthepredicatefromNomBankorPropBank
Predicateclass.Thisinformationisextracted
formframefileofeachpredicate.
Position:whetherthecandidateisbeforeor
afterthepredicate
LemmaandPOStagofthecandidate
LemmaandPOSofNeighboringwordsofthe
candidate
LemmaandPOSofsiblingwordsofthe
candidate
Lengthoftheconstituentheadedbythe
candidate
LemmaandPOSoftheleftandrightmost
wordsoftheconstituentofthecandidate
Punctuationbeforeandafterthecandidate
POSpath:thechainofPOSfromcandidateto
predicate
SingleCharacterPOSpath:eachPOSinapath
isclusteredtoacategorydefinedbyits
firstcharacter
POSPattern(stringofPOStags)ofall
candidates
SingleCharacterPOSPatternofallcandidates
Table2: Featuresusedforsemanticrolelabeling
2.4 SemanticDependencyRelation
Classification
Thisstageassignsthefinalargumentlabelstothe
argumentcandidatessupplied from the previous
stage.Amulti-classclassifieristrainedtoclassify
thetypesoftheargumentssuppliedbytheprevious
stage.Table2liststhefeaturesused.Itisclearthat
the generaltypeof featuresusedhereis strongly
basedon previousworkon the SRLtask(Gildea
andJurafsky, 2002;Pradhanetal.,2005;Xueand
Palmer, 2004). DifferentfromCoNLL-2005,the
senseof predicatesshouldbe labeledas a partof
thetask. Oursystemassigns01 to all predicates.
Thisis a harshtacticsinceit do nottake the lin-
guisticmeaningoftheargument-structureintoac-
count.
2.5 SemanticDependencyRelationInference
The purposeof inferencestageis to incorporate
some prior linguisticand structuralknowledge,
suchas”eachpredicatetakesatmostoneargument
ofeachtype.” We usetheinferenceprocessintro-
244
ducedby(Punyakanoketal.,2004;Koomenetal.,
2005).Theprocessis modeledas anintegerLin-
earProgrammingProblem(ILP).It takes thepre-
dictedprobabilityover eachtypeofthearguments
asinputs,andtakestheoptimalsolutionthatmax-
imizesthelinearsumoftheprobabilitysubjectto
linguisticconstraintsas outputs. Theconstraints
area subsetofconstraintsraisedbyKoomenetal.
(2005)andencodedas following:1) Nooverlap-
pingorembeddingarguments;2)Noduplicatear-
gumentclassesforA0-A5;3) If thereis anR-arg
argument,thentherehas to be an arg argument;
4) If thereis a C-arg argument,theremustbe an
arg argument;moreover, theC-arg argumentmust
occurafterarg;5)Giventhepredicate,someargu-
menttypesareillegal. Thelistofillegalargument
typesisextractedfromframefile.
TheILPprocesscanimproveSRLperformance
on constituent-basedparsing (Punyakanoket al.,
2004). In our experiment, it also works on
dependency-basedparsing.
3 BilayerMaximumEntropyMarkov
Models
3.1 Sequentialization
Thesequentializationofaargument-structureissi-
miliartothepruningalgorithmraisedby(Xueand
Palmer, 2004).Givena constituent-basedparsing
tree,therecursivepruningprocessstartsfromatar-
get predicate. It first collectsthe siblingsof the
predicate;thenit movesto theparentofthepred-
icate,and collectsthe siblingsof the parent. In
addition,if a constituentis a prepositionalphrase,
itschildrenarealsocollected.
Oursystemusesa similarpruningalgorithmto
filterout very unlikely argumentcandidatesin a
dependency-basedparsingtree. Given a depen-
dency parsingtree,thepruningprocessalsostarts
froma targetpredicate.It firstcollectsthedepen-
dentsofthepredicate;thenit movesto theparent
of the predicate,and collectsall the dependents
again. Notethat,the predicateis alsotaken into
account.If thetargetpredicateis a verb,thepro-
cessgoeson recursively untilit reachesthe root.
Theprocessof a nountargetendswhenit seesa
PMOD, NMOD, SBJorOBJdependency relation.
Ifaprepositionisreturnedasacandidate,itschild
isalsocollected.Whenthepredicateisa verb,the
setofconstituentsheadedbysurvivorsofourprun-
ingalgorithmisasupersetofthesetofsurvivorsof
thepreviouspruningalgorithmonthecorrespond-
Figure1: DirectedgraphicalModelofThesystem
ing constituent-basedparsingtree. Thispruning
algorithmwillrecall99.08%argumentsof verbs,
andthecandidatesare3.75timesoftherealargu-
ments.IfthestoprelationsuchasPMODofanoun
isnottakenintoaccount,therecallis97.67%and
the candidatesis 6.28timesof arguments.If the
harshstopconditionis implemented,therecallis
just80.29%.SincetheSRLperformanceofnouns
isverylow,theharshpruningalgorithmworksbet-
terthantheoriginalone.
Afterpruning,oursystemsequentializesallar-
gumentcandidatesof thetargetpredicateaccord-
ingtotheirlinearorderinthegivensentence.
3.2 GraphicalModel
Figure 1 is the directedgraph of our system.
There is a chain of candidatesx = (x0 =
BOS,x1,...,xn) in thegraphwhichareobserva-
tions.Therearetwotaglayersinthegraph:theup
layeris informationofsemanticdependency rela-
tions;the down layeris informationof syntactic
dependencyrelations.
Givenx, denotethecorrespondingsyntacticde-
pendency relationsd = (d0 = BOS,d1,...,dn)
andthecorrespondingsemanticdependency rela-
tionss = (s0 = BOS,s1,...,sn). Our system
labelsthesyntacticandsemanticrelationsaccord-
ing to the conditionalprobabilityin argmaxfla-
vor. Formally, labelsthe systemassignedmake
thescorep(d,s|x) reachesitsmaximum.We de-
composetheprobabilityp(d,s|x)accordingtothe
directedgraphmodeledasfollowing:
p(d,s|x) =p(s1|s0,d1;x)p(d1|s0,d0;x)···
p(si+1|si,di+1;x)p(di+1|si,di;x)···
p(sn|sn−1,dn;x)p(dn|sn−1,dn−1;x)
=
nproductdisplay
i=1
p(si|si−1,di;x)p(di|si−1,di−1;x)
245
Lemma,POStagvoiceofpredicates
POSpatternofpredicate’s children
LemmaandPOStagofthecandidate
LemmaandPOSofNeighboringwordsofthe
candidate
LemmaandPOSofsiblingwordsofthe
candidate
Lengthoftheconstituentheadedbythe
candidate
LemmaandPOSoftheleftandrightmost
wordsoftheconstituentofthecandidate
Conjunctionoflemmaofcandidatesand
predicates;ConjunctionofPOSofcandidates
andpredicates
POSPatternofallcandidates
Table3: Featuresusedtopredictsyntacticdepen-
dencyparsing
3.3 ProbabilityEstimation
The system defines the conditionalprobability
p(si|si−1,di;x) and p(di|si−1,di−1;x) by using
themaximumentropy (Bergeretal.,1996)frame-
work Denotethe tag set of syntacticdependency
relationsDandthetagsetofsemanticdependency
relationsS. Formally, givena featuremapφs and
aweightvectorws,
pws(si|si−1,di;x) = exp{ws ·φs(x,si,si−1,di)}Z
x,si−1,di;ws
where,
Zx,si−1,di;ws =summationdisplay
s∈S
exp{ws ·φs(x,s,si−1,di)}
Similarly, given a feature map φd and
a weight vector wd, (pwd(di) is short for
pwd(di|si−1,di−1;x)
pwd(di) = exp{wd ·φd(x,di,si−1,di−1)}Z
x,si−1,di−1;wd
where,
Zx,si−1,di−1;wd =summationdisplay
d∈D
exp{wd ·φd(x,d,si−1,di−1)}
For differentcharacteristicpropertiesbetween
syntacticparsingand semanticparsing,different
feature maps are taken into account. Table 2
liststhe featuresusedto predictsemanticdepen-
dency relations,whereastable3 liststhefeatures
usedtopredictthesyntacticdependencyrelations.
The featuresusedfor syntacticdependency rela-
tionclassificationare stronglybasedon previous
works(McDonaldetal.,2006;Nakagawa,2007).
We just integrate syntacticdependency Rela-
tionclassification andsemanticdependency rela-
tionhere.Ifonecombinesidentificationandclas-
sificationofsemanticrolesasonemulti-classclas-
sification,the tag set of the secondlayercan be
substitutedbythe tagsetof semanticrolesplusa
NULL(”notanargument”)label.
3.4 Inference
The”argmaxproblem”in structuredpredictionis
nottractableinthegeneralcase.However, thebi-
layergraphicalmodelpresentedin formsections
admitsefficientsearchusing dynamicprogram-
mingsolution.Searchingforthehighest probabil-
ityofagraphdependsonthefactorizationchosen.
Accordingtotheformoftheglobalscore
p(d,s|x) =
nproductdisplay
i=1
p(si|si−1,di;x)p(di|si−1,di−1;x), wedefineforwardprobabilitiesαt(s,d) tobethe
probabilityof semanticrelationbeings andsyn-
tacticrelationbeingd at timet given observation
sequenceuptotimet. Therecursivedynamicpro-
grammingstepis
αt+1(d,s) = arg maxd∈D,s∈S summationdisplay
dprime∈D,sprime∈S
αt(dprime,sprime)·
p(si|si−1,di;x)p(di|si−1,di−1;x)
Finally, to computethe globallymost proba-
ble assignment(ˆd,ˆs) = argmaxd,sp(d,s|x), a
Viterbirecursionworkswell.
4 Results
We trainedour systemusing positive examples
extractedfromall trainingdata of CoNLL2008
sharedtask. Table4 shows the overallsyntactic
parsingresultsobtainedontheWSJtestset(Sec-
tion23)andtheBrowntestset(Sectionck/01-03).
Table5 showstheoverallsemanticparsingresults
obtainedontheWSJtestset(Section23)andthe
Browntestset(Sectionck/01-03).
246
TestSet UAS LAS LabelAccuracy
WSJ 89.25% 86.37% 91.25%
Brown 86.12% 80.75% 87.14%
Table4: Overallsyntacticparsingresults
Task Precision Recall Fβ=1
WSJ ID 73.76% 85.24% 79.08
ID&CL 63.07% 72.88% 67.62
Brown ID 70.77% 80.50% 75.32
ID&CL 54.74% 62.26% 58.26
Table5: Overallsemanticparsingresults
TestWSJ Precision(%) Recall(%) Fβ=1
SRLofVerbs
All 73.53 73.28 73.41
Core-Arg 78.83 76.93 77.87
AM-* 62.51 64.83 63.65
SRLofNouns
All 62.06 45.49 52.50
Core-Arg 61.47 46.56 52.98
AM-* 66.19 39.93 49.81
Table6: Semanticrole labelingresultson verbs
andnouns.Core-Argmeansnumberedargument.
Table6 showsthedetailedsemanticparsingre-
sults obtainedon the WSJ test set (Section23)
of verbsandnounsrespectively. Thecomparison
suggeststhat SRL on NomBankis muchharder
thanPropBank.
Acknowlegements
The work is supportedby the NationalNatural
ScienceFoundationof China under GrantsNo.
60503071,863theNationalHighTechnologyRe-
searchand DevelopmentProgramof Chinaun-
derGrantsNo.2006AA01Z144,andtheProjectof
Toshiba(China)Co.,Ltd.R&DCenter.
References
Berger, Adam,StephenDellaPietra,andVincentDella
Pietra. 1996. A MaximumEntropy Approachto
NaturalLanguageProcessing. ComputionalLin-
guistics, 22(1):39–71.
Ge,RuifangandRaymondJ.Mooney. 2005.AStatis-
ticalSemanticParserthatIntegratesSyntaxandSe-
mantics.InProceedingsof the Conferenceof Com-
putationalNaturalLanguage Learning.
Gildea,DanielandDanielJurafsky. 2002. Automatic
Labelingof SemanticRoles. ComputionalLinguis-
tics, 28(3):245–288.
Koomen,Peter, VasinaPunyakanok,Dan Roth,and
Wen-tauYih. 2005. GeneralizedInferencewith
MultipleSemanticRoleLabelingSystems.In Pro-
ceedingsofConferenceonNaturalLanguageLearn-
ing.
McCallum,Andrew, DayneFreitag,and Fernando
Pereira. 2000. MaximumEntropy Markov Mod-
els for InformationExtractionand Segmentation.
In Proceedingsof InternationalConferenceon Ma-
chineLearning.
McDonald,Ryan,FernandoPereira,KirilRibarov, and
JanHajiˇc. 2005. Non-projective dependency pars-
ingusingspanningtreealgorithms.In Proceedings
of the conferenceon HumanLanguage Technology
and EmpiricalMethodsin Natural Language Pro-
cessing.
McDonald, Ryan, Kevin Lerman, and Fernando
Pereira. 2006. MultilingualDependency Analysis
withaTwo-StageDiscriminativeParser. InProceed-
ingsofConferenceonNaturalLanguage Learning.
Nakawa, Tetsuji. 2007. MultilingualDependency
ParsingusingGlobal Features. In Proceedingsof
ConferenceonNaturalLanguage Learning.
Nivre,Joakim,JohanHall,SandraK¨ubler, RyanMc-
Donald,JensNilsson,SebastianRiedel,andDeniz
Yuret. The CoNLL2007SharedTask on Depen-
dency Parsing.2007. InProceedingsoftheCoNLL
SharedTaskSessionof EMNLP-CoNLL2007, 915–
932,
Pradhan,Sameer, KadriHacioglu,Valerie Krugler,
Wayne Ward, JamesMartin,and DanielJurafsky.
2005. SupportVectorLearningforSemanticArgu-
mentClassification.In Proceedingsof Conference
onAssociationforComputationalLinguistics.
Punyakanok,Vasin, DanRoth,Wen-tauYih,andDav
Zimak. 2004. SemanticRoleLabelingviaInteger
LinearProgrammingInference. In Proceedingsof
the20thInternationalConferenceonComputational
Linguistics.
Surdeanu,Mihai,RichardJohansson,AdamMeyers,
Llu´ıs M`arquez, and Nivre, Joakim. 2008. The
CoNLL-2008SharedTaskon JointParsingof Syn-
tacticandSemanticDependencies.In Proceedings
of the 12th Conference on ComputationalNatural
Language Learning(CoNLL-2008).
Xue,Nianwenand MarthaPalmer. 2004. Calibrat-
ing Featuresfor SemanticRoleLabeling. In Pro-
ceedingsofEmpiricalMethodsinNaturalLanguage
Processing.
Yi, Szu-tingandMarthaPalmer. 2005. TheIntegra-
tionof SyntacticParsingandSemanticRoleLabel-
ing. In Proceedingsof theConferenceof Computa-
tionalNaturalLanguage Learning.
247

