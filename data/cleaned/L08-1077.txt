<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Didier Bourigault</author>
</authors>
<title>Cécile Fabre, Cécile Frérot, Marie-Paule Jacques, and Sylwia Ozdowska</title>
<date>2005</date>
<booktitle>In EASy Workshop, Conférence Traitement Automatique des Langues Naturelles</booktitle>
<location>Dourdan, France</location>
<marker>Bourigault, 2005</marker>
<rawString>Didier Bourigault, Cécile Fabre, Cécile Frérot, Marie-Paule Jacques, and Sylwia Ozdowska. 2005. SYNTEX, analyseur syntaxique de corpus. In EASy Workshop, Conférence Traitement Automatique des Langues Naturelles, Dourdan, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stéphane Chaudiron</author>
</authors>
<title>La place de l’usager dans l’évaluation des systèmes de recherche d’informations</title>
<date>2004</date>
<booktitle>In Stéphane Chaudiron, editor, Évaluation des systèmes de traitement de l’information</booktitle>
<pages>287--310</pages>
<publisher>Hermès</publisher>
<location>Paris</location>
<contexts>
<context>a, 2006)—throughout its development. System oriented evaluation aims to assess a system’s intrinsic potential as a technology irrespective of its capabilities as a real-world operational application (Chaudiron, 2004). Intrinsic performance is evaluated using standard metrics such as precision, recall and f-measure, by comparing a system’s output to human-annotated reference data. For word alignment, output and r</context>
</contexts>
<marker>Chaudiron, 2004</marker>
<rawString>Stéphane Chaudiron. 2004. La place de l’usager dans l’évaluation des systèmes de recherche d’informations. In Stéphane Chaudiron, editor, Évaluation des systèmes de traitement de l’information, pages 287–310. Hermès, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun-Chuang Chiao</author>
<author>Olivier Kraif</author>
<author>Dominique Laurent</author>
</authors>
<title>Thi Minh Huyen Nguyen, Nasredine Semmar, François Stuck, Jean Véronis, and Wajdi Zaghouani</title>
<date>2006</date>
<booktitle>In 5th Conference on Language Resources and Evaluation (LREC’06</booktitle>
<pages>1975--1978</pages>
<location>Genoa, Italy</location>
<contexts>
<context>rence data can be maximised through the use of an annotation guide and the creation of multiple annotations for the same data (Melamed, 1998b; Mihalcea and Pedersen, 2003; Véronis and Langlais, 2000; Chiao et al., 2006). Word alignment systems are basically evaluated on one particular type of corpus. Cross-corpus evaluation is still relatively rare in NLP (Kilgarriff and Grefenstette, 2003) probably because it is d</context>
<context>d 8,759 aligned sentences with an average sentence length of 23 words for English and 27.2 words for French. JOC was provided within the framework of the ARCADE compaigns (Véronis and Langlais, 2000; Chiao et al., 2006)2. HANSARD is a corpus of Canadian parliamentary debates containing about 250,000 word tokens and 8,000 aligned sentences with an average sentence length of 15 words for English and 16.6 words for Fr</context>
</contexts>
<marker>Chiao, Kraif, Laurent, 2006</marker>
<rawString>Yun-Chuang Chiao, Olivier Kraif, Dominique Laurent, Thi Minh Huyen Nguyen, Nasredine Semmar, François Stuck, Jean Véronis, and Wajdi Zaghouani. 2006. Evaluation of multilingual text alignment systems: the ARCADE II project. In 5th Conference on Language Resources and Evaluation (LREC’06), pages 1975–1978, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fathi Debili</author>
<author>Adnane Zribi</author>
</authors>
<title>Les dépendances syntaxiques au service de l’appariement de mots</title>
<date>1996</date>
<booktitle>In 10ème Congrès Reconnaissance des Formes et Intelligence Artificielle</booktitle>
<pages>81--90</pages>
<location>Rennes, France</location>
<contexts>
<context>sults obtained, and conclude in section 6. 2. Context 2.1. System ALIBI is a rule-based word alignment system. It has been developed according to the following analogy-based hypothesis formulated in (Debili and Zribi, 1996): if there is a pair of words that are mutual translations within aligned sentences (i.e. anchor words such as Community and Communauté in figure 1) then the translational equivalence link (alignment</context>
</contexts>
<marker>Debili, Zribi, 1996</marker>
<rawString>Fathi Debili and Adnane Zribi. 1996. Les dépendances syntaxiques au service de l’appariement de mots. In 10ème Congrès Reconnaissance des Formes et Intelligence Artificielle, pages 81–90, Rennes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
<author>Inderjeet Mani</author>
</authors>
<date>2003</date>
<booktitle>The Oxford Handbook of Computational Linguistics</booktitle>
<pages>414--429</pages>
<editor>Evaluation. In Ruslan Mitkov, editor</editor>
<publisher>Oxford University Press</publisher>
<contexts>
<context>s. 1. Introduction Depending on the stage of its life cycle, the performance of an NLP system can be assessed through system oriented evaluation, task oriented evaluation or user oriented evaluation (Hirschman and Mani, 2003; Paroubek, 2004). This paper focuses on a system oriented evaluation experience set up to monitor the performance of a syntax-based word alignment system —ALIBI (Ozdowska, 2006)—throughout its develo</context>
</contexts>
<marker>Hirschman, Mani, 2003</marker>
<rawString>Lynette Hirschman and Inderjeet Mani. 2003. Evaluation. In Ruslan Mitkov, editor, The Oxford Handbook of Computational Linguistics, pages 414–429. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Gregory Grefenstette</author>
</authors>
<title>Introduction to the special issue of web as a corpus</title>
<date>2003</date>
<journal>Computational Linguistics</journal>
<volume>29</volume>
<contexts>
<context>ersen, 2003; Véronis and Langlais, 2000; Chiao et al., 2006). Word alignment systems are basically evaluated on one particular type of corpus. Cross-corpus evaluation is still relatively rare in NLP (Kilgarriff and Grefenstette, 2003) probably because it is difficult to set up. Nevertheless, evaluating NLP systems from a cross-corpus perspective is crucial as it makes it possible to assess the influence of corpus type on performa</context>
</contexts>
<marker>Kilgarriff, Grefenstette, 2003</marker>
<rawString>Adam Kilgarriff and Gregory Grefenstette. 2003. Introduction to the special issue of web as a corpus. Computational Linguistics, 29(3):333–338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Kraif</author>
</authors>
<title>Constitution et exploitation de bitextes pour l’aide à la traduction</title>
<date>2001</date>
<tech>Phd. Thesis</tech>
<institution>University Nice Sophia Antipolis</institution>
<location>France</location>
<contexts>
<context>uage pair, there exist several reference sets for word alignment that are built out of corpora such as the Bible (Melamed, 1998a), the Hansards (Och and Ney, 2003) or JOC (Véronis and Langlais, 2000; Kraif, 2001). However, these reference sets were produced according to different manual annotation schemes and hence they do not constitute a well-suited resource for the purposes of cross-corpus evaluation. To </context>
</contexts>
<marker>Kraif, 2001</marker>
<rawString>Olivier Kraif. 2001. Constitution et exploitation de bitextes pour l’aide à la traduction. Phd. Thesis, University Nice Sophia Antipolis, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Annotation style guide for the blinker project</title>
<date>1998</date>
<booktitle>Technical repport, Institute for Research in Congnitive Science</booktitle>
<institution>University of Pennsylvania</institution>
<location>Philadelphia, USA</location>
<contexts>
<context>l translations within pairs of aligned sentences. The reliability of reference data can be maximised through the use of an annotation guide and the creation of multiple annotations for the same data (Melamed, 1998b; Mihalcea and Pedersen, 2003; Véronis and Langlais, 2000; Chiao et al., 2006). Word alignment systems are basically evaluated on one particular type of corpus. Cross-corpus evaluation is still relat</context>
<context>en, 2003)3. 3. Evaluation procedures 3.1. Human annotation For the English–French language pair, there exist several reference sets for word alignment that are built out of corpora such as the Bible (Melamed, 1998a), the Hansards (Och and Ney, 2003) or JOC (Véronis and Langlais, 2000; Kraif, 2001). However, these reference sets were produced according to different manual annotation schemes and hence they do no</context>
</contexts>
<marker>Melamed, 1998</marker>
<rawString>I. Dan Melamed. 1998a. Annotation style guide for the blinker project. Technical repport, Institute for Research in Congnitive Science, University of Pennsylvania, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Manual annotation of translational equivalence: The blinker project</title>
<date>1998</date>
<booktitle>Technical repport, Institute for Research in Congnitive Science</booktitle>
<institution>University of Pennsylvania</institution>
<location>Philadelphia, USA</location>
<contexts>
<context>l translations within pairs of aligned sentences. The reliability of reference data can be maximised through the use of an annotation guide and the creation of multiple annotations for the same data (Melamed, 1998b; Mihalcea and Pedersen, 2003; Véronis and Langlais, 2000; Chiao et al., 2006). Word alignment systems are basically evaluated on one particular type of corpus. Cross-corpus evaluation is still relat</context>
<context>en, 2003)3. 3. Evaluation procedures 3.1. Human annotation For the English–French language pair, there exist several reference sets for word alignment that are built out of corpora such as the Bible (Melamed, 1998a), the Hansards (Och and Ney, 2003) or JOC (Véronis and Langlais, 2000; Kraif, 2001). However, these reference sets were produced according to different manual annotation schemes and hence they do no</context>
</contexts>
<marker>Melamed, 1998</marker>
<rawString>I. Dan Melamed. 1998b. Manual annotation of translational equivalence: The blinker project. Technical repport, Institute for Research in Congnitive Science, University of Pennsylvania, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Ted Pedersen</author>
</authors>
<title>An Evaluation Exercise for Word Alignment</title>
<date>2003</date>
<booktitle>In HLT-NAACL 2003 Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond</booktitle>
<pages>1--10</pages>
<location>Edmonton, Canada</location>
<contexts>
<context>ithin pairs of aligned sentences. The reliability of reference data can be maximised through the use of an annotation guide and the creation of multiple annotations for the same data (Melamed, 1998b; Mihalcea and Pedersen, 2003; Véronis and Langlais, 2000; Chiao et al., 2006). Word alignment systems are basically evaluated on one particular type of corpus. Cross-corpus evaluation is still relatively rare in NLP (Kilgarriff </context>
<context>16.6 words for French. The Hansards have been widely exploited in alignment and statistical machine translation, e.g. they were used for word alignment system evaluation in the HLT-NAACL’03 campaign (Mihalcea and Pedersen, 2003)3. 3. Evaluation procedures 3.1. Human annotation For the English–French language pair, there exist several reference sets for word alignment that are built out of corpora such as the Bible (Melamed,</context>
</contexts>
<marker>Mihalcea, Pedersen, 2003</marker>
<rawString>Rada Mihalcea and Ted Pedersen. 2003. An Evaluation Exercise for Word Alignment. In HLT-NAACL 2003 Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond, pages 1–10, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models</title>
<date>2003</date>
<journal>Computational Linguistics</journal>
<volume>1</volume>
<contexts>
<context>dures 3.1. Human annotation For the English–French language pair, there exist several reference sets for word alignment that are built out of corpora such as the Bible (Melamed, 1998a), the Hansards (Och and Ney, 2003) or JOC (Véronis and Langlais, 2000; Kraif, 2001). However, these reference sets were produced according to different manual annotation schemes and hence they do not constitute a well-suited resource</context>
<context>ces are fuzzy and not necessarily straightforward and it is often difficult even for a human to determine which source word or sequence of words corresponds to which target word or sequence of words (Och and Ney, 2003). Reliability of the reference sets was measured by computing inter-annotator agreement rates, which made it possible to guarantee that reference alignments were consistent enough across annotators. </context>
<context>formance obtained with ALIBI on each of the three corpora are compared to a baseline consisting of the intersection of Giza++ IBM 4 alignments in both source-to-target and target-tosource directions (Och and Ney, 2003). The baseline alignment corresponds to the anchor alignments in the experiments reported in this paper. In addition to P, R and F, absolute and relative contributions are also shown (absolute / rela</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 1(29):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylwia Ozdowska</author>
</authors>
<title>ALIBI, un système d’ALIgnement BIlingue à base de règles de propagation syntaxique</title>
<date>2006</date>
<tech>Phd. Thesis</tech>
<institution>University Toulouse-Le Mirail</institution>
<location>France</location>
<contexts>
<context> evaluation (Hirschman and Mani, 2003; Paroubek, 2004). This paper focuses on a system oriented evaluation experience set up to monitor the performance of a syntax-based word alignment system —ALIBI (Ozdowska, 2006)—throughout its development. System oriented evaluation aims to assess a system’s intrinsic potential as a technology irrespective of its capabilities as a real-world operational application (Chaudir</context>
</contexts>
<marker>Ozdowska, 2006</marker>
<rawString>Sylwia Ozdowska. 2006. ALIBI, un système d’ALIgnement BIlingue à base de règles de propagation syntaxique. Phd. Thesis, University Toulouse-Le Mirail, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Paroubek</author>
</authors>
<title>L’évaluation des systèmes d’analyse morphosyntaxique et syntaxique</title>
<date>2004</date>
<booktitle>In Stéphane Chaudiron, editor, Évaluation des systèmes de traitement de l’information</booktitle>
<pages>101--125</pages>
<publisher>Hermès</publisher>
<location>Paris</location>
<contexts>
<context>ng on the stage of its life cycle, the performance of an NLP system can be assessed through system oriented evaluation, task oriented evaluation or user oriented evaluation (Hirschman and Mani, 2003; Paroubek, 2004). This paper focuses on a system oriented evaluation experience set up to monitor the performance of a syntax-based word alignment system —ALIBI (Ozdowska, 2006)—throughout its development. System or</context>
</contexts>
<marker>Paroubek, 2004</marker>
<rawString>Patrick Paroubek. 2004. L’évaluation des systèmes d’analyse morphosyntaxique et syntaxique. In Stéphane Chaudiron, editor, Évaluation des systèmes de traitement de l’information, pages 101–125. Hermès, Paris.</rawString>
</citation>
</citationList>
</algorithm>

