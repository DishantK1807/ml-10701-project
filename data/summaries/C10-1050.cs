Hierarchical phrase-based model (Chiang, 2007)
J07-2003
et al., 2007; Tromble and Eisner, 2009) in that it
D09-1105
Och, F. and H. Ney. 2003. A systematic comparison
J03-1002 P03-1021
Collins, M., P. Koehn, and I. Kucerova. 2005. Clause
P05-1066
error rate training (MERT) (Och, 2003). Test data
J03-1002 P03-1021
Li, Z. and S. Khudanpur. 2008. A scalable decoder
W08-0402
source sentence S′ as well as target sentence T. The “move-to-front” means Tromble and Eisner (2009)
D09-1105
Shen, L., J. Xu, and R. Weischedel. 2008. A new
P08-1066
Chiang, D. 2007. Hierachical phrase-based transla-
J07-2003
Tromble, R. and J. Eisner. 2009. Learning linear
D09-1105
as Tromble and Eisner (2009), which reorders
D09-1105
’s algorithm and the “attach” means Al-Onaizan and Papineni (2006) ’s algorithm.
P06-1067
2008; Shen et al., 2009) in that syntax annotation
D09-1008
Xia, F. and M. McCord. 2004. Improving a statis-
C04-1073
(Xia and McCord, 2004; Collins et al., 2005; Li
C04-1073 P05-1066
Och, F. J. 2003. Minimum error rate training in sta-
J03-1002 P03-1021
Och, F. J. and H. Ney. 2002. Discriminative train-
P02-1038
ing. To overcome the problem, Li et al. (2007)
P07-1091
Watanabe, T., H. Tsukada, and H. Isozaki. 2006. Left-
P06-1098
Shen et al. (2008,2009) proposed a way to inte-
D09-1008 P08-1066
2004; Collins et al., 2005; Li et al., 2007; Tromble
P05-1066 P07-1091
(Chiang, 2007; Watanabe et al., 2006) is one of
J07-2003 P06-1098
proaches (Brown et al., 1993). Its model is for-
J93-2003
originally proposed by Tromble and Eisner (2009)
D09-1105
al., 2007; Tromble and Eisner, 2009). However,
D09-1105
(Xia and McCord, 2004; Collins et al., 2005; Li et
C04-1073 P05-1066
2005; Nagata et al., 2006) are extensively used
P06-1090
Chiang, D., K. Knight, and W. Wang. 2009. 11,001
N09-1025
ϕ as used in (Tromble and Eisner, 2009), which
D09-1105
