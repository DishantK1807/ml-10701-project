Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1190–1199,
Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
An Affect-Enriched Dialogue Act Clasification Model 
for Task-Oriented Dialogue 
Kristy  
Elizabeth 
Boyer 
Joseph F. 
Grafsgaard 
Eun Young  
Ha 
Robert  
Phillips
*
 
James C.  
Lester 
 
Department of Computer Science 
North Carolina State University 
Raleigh, NC, USA 
 
* 
Dual Affiliation with Applied Research Associates, Inc. 
Raleigh, NC, USA 
 
{keboyer, jfgrafsg, eha, rphilli, lester}@ncsu.edu 
 
 
Abstract 
Dialogue act clasification is a central chal-
lenge for dialogue systems. Although the im-
portance of emotion in human dialogue is 
widely recognized, most dialogue act clasifi-
cation models make limited or no use of affec-
tive channels in dialogue act classification. 
This paper presents a novel afect-enriched 
dialogue act clasifier for task-oriented dia-
logue that models facial expressions of users, 
in particular, facial expresions related to con-
fusion. The findings indicate that the afect-
enriched classifiers perform significantly bet-
ter for distinguishing user requests for fed-
back and grounding dialogue acts within 
textual dialogue. The results point to ways in 
which dialogue systems can effectively lever-
age affective channels to improve dialogue act 
classification. 
1 Introduction

Dialogue systems aim to engage users in rich, 
adaptive natural language conversation. For these 
systems, understanding the role of a user’s utter-
ance in the broader context of the dialogue is a key 
challenge (Sridhar, Bangalore, & Narayanan, 
2009). Central to this endeavor is dialogue act 
classification, which categorizes the intention be-
hind the user’s move (e.g., asking a question, 
providing declarative information). Automatic dia-
logue act clasification has ben the focus of a 
large body of research, and a variety of approach-
es, including sequential models (Stolcke et al., 
2000), vector-based models (Sridhar, Bangalore, & 
Narayanan, 209), and most recently, feature-
enhanced latent semantic analysis (Di Eugenio, 
Xie, & Serafin, 2010), have shown promise. These 
models may be further improved by leveraging 
regularities of the dialogue from both linguistic 
and extra-linguistic sources. Users’ expresions of 
emotion are one such source. 
Human interaction has long been understood to 
include rich phenomena consisting of verbal and 
nonverbal cues, with facial expresions playing a 
vital role (Knapp & Hal, 206; McNeil, 192; 
Mehrabian, 207; Rusel, Bachorowski, & 
Fernandez-Dols, 203; Schmidt & Cohn, 201). 
While the importance of emotional expresions in 
dialogue is widely recognized, the majority of dia-
logue act clasification projects have focused either 
peripheraly (or not at al) on emotion, such as by 
leveraging acoustic and prosodic features of spo-
ken utterances to aid in online dialogue act clasi-
fication (Sridhar, Bangalore, & Narayanan, 209). 
Other research on emotion in dialogue has in-
volved detecting affect and adapting to it within a 
dialogue system (Forbes-Riley, Rotaru, Litman, & 
Tetreault, 209; López-Cózar, Silovsky, & Griol, 
2010), but this work has not explored leveraging 
affect information for automatic user dialogue act 
classification. Outside of dialogue, sentiment anal-
ysis within discourse is an active area of research 
(López-Cózar et al., 2010), but it is generally lim-
1190
ited to modeling textual features and not multi-
modal expresions of emotion such as facial ac-
tions. Such multimodal expresions have only just 
begun to be explored within corpus-based dialogue 
research (Calvo & D'Melo, 2010; Cavichio, 
2009).  
This paper presents a novel affect-enriched dia-
logue act clasification aproach that leverages 
knowledge of users’ facial expresions during 
computer-mediated textual human-human dia-
logue. Intuitively, the user’s afective state is a 
promising source of information that may help to 
distinguish betwen particular dialogue acts (e.g., a 
confused user may be more likely to ask a ques-
tion). We focus specificaly on occurrences of stu-
dents’ confusion-related facial actions during task-
oriented tutorial dialogue.  
Confusion was selected as the focus of this 
work for several reasons. First, confusion is known 
to be prevalent within tutoring, and its implications 
for student learning are thought to run deep 
(Graeser, Lu, Olde, Coper-Pye, & Whiten, 
2005). Second, while identifying the “ground 
truth” of emotion based on any external display by 
a user presents challenges, prior research has 
demonstrated a correlation betwen particular faci-
al action units and confusion during learning 
(Craig, D'Melo, Witherspon, Sulins, & Graeser, 
2004; D' elo, Craig, Sullins, & Graeser, 206; 
McDaniel et al., 207). Finally, automatic facial 
action recognition technologies are developing rap-
idly, and confusion-related facial action events are 
among those that can be reliably recognized auto-
maticaly (Bartlet et al., 206; Cohn, Red, 
Ambadar, Xiao, & Moriyama, 204; Pantic & 
Bartlet, 207; Zeng, Pantic, Roisman, & Huang, 
2009). This promising development bodes well for 
the feasibility of automatic real-time confusion 
detection within dialogue systems. 
2 Background
and Related Work 
2.1 Dialogue
Act Clasification 
Because of the importance of dialogue act classifi-
cation within dialogue systems, it has ben an ac-
tive area of research for some time. Early work on 
automatic dialogue act classification modeled dis-
course structure with hiden Markov models, ex-
perimenting with lexical and prosodic features, and 
applying the dialogue act model as a constraint to 
aid in automatic speech recognition (Stolcke et al., 
2000). In contrast to this sequential modeling ap-
proach, which is best suited to offline procesing, 
recent work has explored how lexical, syntactic, 
and prosodic features perform for online dialogue 
act tagging (when only partial dialogue sequences 
are available) within a maximum entropy frame-
work (Sridhar, Bangalore, & Narayanan, 209). A 
recently proposed alternative approach involves 
treating dialogue utterances as documents within a 
latent semantic analysis framework, and applying 
feature enhancements that incorporate such infor-
mation as speaker and utterance duration (Di 
Eugenio et al., 2010). Of the approaches noted 
above, the modeling framework presented in this 
paper is most similar to the vector-based maximum 
entropy approach of Sridhar et al. (209). Howev-
er, it takes a step beyond the previous work by in-
cluding multimodal affective displays, specifically 
facial expresions, as features available to an af-
fect-enriched dialogue act clasification model. 
2.2 Detecting
Emotions in Dialogue 
Detecting emotional states during spoken dialogue 
is an active area of research, much of which focus-
es on detecting frustration so that a user can be 
automatically transferred to a human dialogue 
agent (López-Cózar et al., 2010). Research on spo-
ken dialogue has leveraged lexical features along 
with discourse cues and acoustic information to 
classify user emotion, sometimes at a coarse grain 
along a positive/negative axis (Le & Narayanan, 
2005). Recent work on an afective companion 
agent has examined user emotion classification 
within conversational spech (Cavaza et al., 
2010). In contrast to that spoken dialogue research, 
the work in this paper is situated within textual 
dialogue, a widely used modality of communica-
tion for which a deper understanding of user af-
fect may substantialy improve system 
performance. 
While many projects have focused on linguistic 
cues, recent work has begun to explore numerous 
channels for affect detection including facial ac-
tions, electrocardiograms, skin conductance, and 
posture sensors (Calvo & D'Melo, 2010). A recent 
project in a map task domain investigates some of 
these sources of affect data within task-oriented 
dialogue (Cavichio, 209). Like that work, the 
curent project utilizes facial action tagging, for 
1191
which promising automatic technologies exist 
(Bartlet et al., 206; Pantic & Bartlet, 207; 
Zeng, Pantic, Roisman, & Huang, 209). However, 
we leverage the recognized expressions of emotion 
for the task of dialogue act clasification. 
2.3 Categorizing
Emotions within Dialogue 
and Discourse 
Sets of emotion taxonomies for discourse and dia-
logue are often aplication-specific, for example, 
focusing on the frustration of users who are inter-
acting with a spoken dialogue system (López-
Cózar et al., 2010), or on uncertainty expresed by 
students while interacting with a tutor (Forbes-
Riley, Rotaru, Litman, & Tetreault, 207). In con-
trast, the most widely utilized emotion frameworks 
are not application-specific; for example, Ekman’s 
Facial Action Coding System (FACS) has ben 
widely used as a rigorous technique for coding fa-
cial movements based on human facial anatomy 
(Ekman & Friesen, 1978).  Within this framework, 
facial movements are categorized into facial action 
units, which represent discrete movements of mus-
cle groups. Aditionally, facial action descriptors 
(for movements not derived from facial muscles) 
and movement and visibility codes are included. 
Ekman’s basic emotions (Ekman, 199) have been 
used in recent work on clasifying emotion ex-
presed within blog text (Das & Bandyopadhyay, 
2009), while other recent work (Nguyen, 2010) 
utilizes Russel’s core afect model (Rusel, 203) 
for a similar task. 
During tutorial dialogue, students may not fre-
quently experience Ekman’s basic emotions of 
happines, sadness, anger, fear, surprise, and dis-
gust. Instead, students appear to more frequently 
experience cognitive-affective states such as flow 
and confusion (Calvo & D'Melo, 2010). Our work 
leverages Ekman’s facial taging scheme to identi-
fy a particular facial action unit, Action Unit 4 
(AU4), that has been observed to correlate with 
confusion (Craig, D'Melo, Witherspon, Sulins, 
& Graeser, 204; D'elo, Craig, Sulins, & 
Graeser, 206; McDaniel et al., 207).  
2.4 Importance
of Confusion in Tutorial Dia-
logue 
Among the afective states that students experience 
during tutorial dialogue, confusion is prevalent, 
and its implications for student learning are signif-
icant. Confusion is asociated with cognitive dise-
quilibrium, a state in which students’ existing 
knowledge is inconsistent with a novel learning 
experience (Graeser, Lu, Olde, Coper-Pye, & 
Whiten, 205). Students may expres such confu-
sion within dialogue as uncertainty, to which hu-
man tutors often adapt in a context-dependent 
fashion (Forbes-Riley et al., 207). Moreover, im-
plementing adaptations to student uncertainty with-
in a dialogue system can improve the efectivenes 
of the system (Forbes-Riley et al., 209).  
For tutorial dialogue, the importance of under-
standing student utterances is paramount for a sys-
tem to positively impact student learning 
(Dzikovska, More, Steinhauser, & Campbel, 
2010). The importance of frustration as a cogni-
tive-affective state during learning sugests that 
the presence of student confusion may serve as a 
useful constraining feature for dialogue act clasi-
fication of student uterances. This paper explores 
the use of facial expresion features in this way.  
3 Task-Oriented Dialogue Corpus 
The corpus was colected during a textual human-
human tutorial dialogue study in the domain of 
introductory computer science (Boyer, Philips, et 
al., 2010). Students solved an introductory com-
puter programing problem and caried on textual 
dialogue with tutors, who viewed a synchronized 
version of the students’ problem-solving work-
space. The original corpus consists of 48 dia-
logues, one per student. Each student interacted 
with one of two tutors. Facial videos of students 
were colected using built-in webcams, but were 
not shown to the tutors. Video quality was ranked 
based on factors such as obscured foreheads due to 
hats or hair, and improper camera position result-
ing in students’ faces not being fully captured on 
the video. The highest-quality set contained 14 
videos, and these videos were used in this analysis. 
They have a total running time of 11 hours and 55 
minutes, and include dialogues with thre female 
subjects and eleven male subjects. 
3.1 Dialogue
act anotation 
The dialogue act annotation scheme (Table 1) was 
applied manually. The kapa statistic for inter-
annotator agreement on a 10% subset of the corpus 
was κ=0.80, indicating god reliability.  
 
1192
Table 1. Dialogue act tags and relative frequencies 
acros fourteen dialogues in video corpus 
Student Dialogue 
Act 
Example 
Rel. 
Freq. 
EXTRA-DOMAIN 
(EX) 
Litle sleep deprived 
today 
.08 
GROUNDING (G) Ok or Thanks 
.21 
NEGATIVE 
FEEDBACK WITH 
ELABORATION (NE) 
I’m stil confused on 
what this next for lop 
is doing. 
.02 
NEGATIVE 
FEEDBACK (N) 
I don’t se the diff. 
.04 
POSITIVE 
FEEDBACK WITH 
ELABORATION (PE) 
It makes sense now 
that you explained it, 
but I never used an 
else if in any of my 
other programs 
.04 
POSITIVE 
FEEDBACK (P) 
Second part complete. 
.11 
QUESTION (Q) 
Why couldn’t I have 
said if (i<5) 
.11 
STATEMENT (S) i is my only index 
.07 
REQUEST FOR 
FEEDBACK (RF) 
So I ned to create a 
new method that sees 
how many elements 
are in my array? 
.16 
RESPONSE (RSP) 
You mean not the 
length but the contents 
.14 
UNCERTAIN 
FEEDBACK WITH 
ELABORATION (UE) 
I’m trying to remember 
how to copy arrays 
.008 
UNCERTAIN 
FEEDBACK (U) 
Not quite yet 
.008 
 
3.2 Task
action anotation 
The tutoring sesions were task-oriented, focusing 
on a computer programing exercise. The task had 
several subtasks consisting of programming mod-
ules to be implemented by the student. Each of 
those subtasks also had numerous fine-grained 
goals, and student task actions either contributed or 
did not contribute to the goals. Therefore, to obtain 
a rich representation of the task, a manual annota-
tion along two dimensions was conducted (Boyer, 
Philips, et al., 2010). First, the subtask structure 
was anotated hierarchicaly, and then each task 
action was labeled for corectnes according to the 
requirements of the asignment. Inter-annotator 
agreement was computed on 20% of the corpus at 
the leaves of the subtask taging scheme, and re-
sulted in a simple kappa of κ=.56. However, the 
leaves of the anotation scheme feature an implicit 
ordering (subtasks were completed in order, and 
adjacent subtasks are semantically more similar 
than subtasks at a greater distance); therefore, a 
weighted kapa is also meaningful to consider for 
this anotation. The weighted kapa is κ
weighted
=.80. 
An annotated excerpt of the corpus is displayed in 
Table 2.  
 
Table 2. Excerpt from corpus ilustrating annota-
tions and interplay betwen dialogue and task 
13:38:09 Student: How do I know where to 
end? [RF] 
13:38:26 Tutor: Wel you told me how to get 
how many elements in an 
array by using .length right? 
13:38:26 Student: [Task action: 
Subtask 1-a-iv, Bugy] 
13:38:56 Tutor: Great 
13:38:56 Student: [Task action: 
Subtask 1-a-v, Correct] 
13:39:35 Student: Wel is it "aray.length"? 
[RF]  
**Facial Expresion: AU4 
13:39:46 Tutor: You just ned to use the 
corect aray name 
13:39:46 Student: [Task action: 
Subtask 1-a-iv, Bugy] 
3.3 Lexical
and Syntactic Features 
In adition to the manualy anotated dialogue and 
task features described above, syntactic features of 
each uterance were automaticaly extracted using 
the Stanford Parser (De Marnefe et al., 206). 
From the phrase structure tres, we extracted the 
top-most syntactic node and its first two children. 
In the case where an uterance consisted of more 
than one sentence, only the phrase structure tre of 
the first sentence was considered. Individual word 
tokens in the utterances were further procesed 
with the Porter Stemer (Porter, 1980) in the 
NLTK package (Loper & Bird, 204). Our prior 
work has shown that these lexical and syntactic 
features are highly predictive of dialogue acts dur-
ing task-oriented tutorial dialogue (Boyer, Ha et al. 
2010).  
1193
4 Facial
Action Tagging 
An annotator who was certified in the Facial Ac-
tion Coding System (FACS) (Ekman, Friesen, & 
Hager, 202) taged the video corpus consisting of 
fourten dialogues. The FACS certification proces 
requires anotators to pas a test designed to ana-
lyze their agrement with reference coders on a set 
of spontaneous facial expresions (Ekman & 
Rosenberg, 205). This anotator viewed the vide-
os continuously and paused the playback whenever 
notable facial displays of Action Unit 4 (AU4: 
Brow Lowerer) were sen. This action unit was 
chosen for this study based on its corelations with 
confusion in prior research (Craig, D'Melo, 
Witherspon, Sulins, & Graeser, 204; D'elo, 
Craig, Sulins, & Graeser, 206; McDaniel et al., 
2007). 
To establish reliability of the anotation, a se-
cond FACS-certified annotator independently an-
notated 36% of the video corpus (5 of 14 
dialogues), chosen randomly after stratification by 
gender and tutor. This anotator folowed the same 
method as the first anotator, pausing the video at 
any point to tag facial action events. At any given 
time in the video, the coder was first identifying 
whether an action unit event existed, and then de-
scribing the facial movements that were present. 
The anotators also specified the beginning and 
ending time of each event. In this way, the action 
unit event tags spanned discrete durations of vary-
ing length, as specified by the coders. Because the 
two coders were not required to tag at the same 
point in time, but rather were permitted the free-
dom to stop the video at any point where they felt a 
notable facial action event occurred, calculating 
agreement between annotators required discretiz-
ing the continuous facial action time windows 
acros the tutoring sessions. This discretization 
was performed at granularities of 1/4, 1/2, 3/4, and 
1 second, and inter-rater reliability was calculated 
at each level of granularity (Table 3). Windows in 
which both anotators agred that no facial action 
event was present were tagged by default as neu-
tral. Figure 1 illustrates facial expresions that dis-
play facial Action Unit 4. 
 
 
Table 3. Kapa values for inter-annotator agree-
ment on facial action events 
 Granularity 
 ¼ sec ½ sec ¾ sec 1 sec 
Presence of AU4 
(Brow Lowerer)  
.84 .87 .86 .86 
 
 
  
  
Figure 1. Facial expresions displaying AU4 
(Brow Lowerer) 
 
Despite the fact that promising automatic ap-
proaches exist to identifying many facial action 
units (Bartlet et al., 206; Cohn, Red, Ambadar, 
Xiao, & Moriyama, 204; Pantic & Bartlet, 207; 
Zeng, Pantic, Roisman, & Huang, 209), manual 
annotation was selected for this project for two 
reasons. First, manual anotation is more robust 
than automatic recognition of facial action units, 
and manual annotation facilitated an exploratory, 
comprehensive view of student facial expressions 
during learning through task-oriented dialogue. 
Although a detailed discusion of the other emo-
tions present in the corpus is beyond the scope of 
this paper, Figure 2 illustrates some other sponta-
neous student facial expresions that difer from 
those asociated with confusion.  
  
1194
 
  
  
Figure 2. Other facial expresions from the corpus 
5 Models

The goal of the modeling experiment was to de-
termine whether the adition of confusion-related 
facial expresion features significantly bosts dia-
logue act classification accuracy for student uter-
ances. 
5.1 Features

We take a vector-based approach, in which the fea-
tures consist of the following: 
 
Utterance Features 
• Dialogue act features: Manually annotated 
dialogue act for the past thre utterances. 
These features include tutor dialogue acts, 
annotated with a scheme analogous to that 
used to annotate student uterances (Boyer 
et al., 209). 
• Speaker: Speaker for past thre utterances 
• Lexical features: Word unigrams 
• Syntactic features: Top-most syntactic 
node and its first two children 
 
Task-based Features 
• Subtask: Hierarchical subtask structure for 
past thre task actions (semantic pro-
graming actions taken by student) 
• Corectnes: Correctnes of past thre task 
actions taken by student 
• Preceded by task: Indicator for whether the 
most recent task action immediately pre-
ceded the target uterance, or whether it 
was immediately preceded by the last dia-
logue move 
 
Facial Expresion Features 
• AU4_1sec: Indicator for the display of the 
brow lowerer within 1 second prior to this 
utterance being sent, for the most recent 
thre utterances 
•  AU4_5sec: Indicator for the display of the 
brow lowerer within 5 seconds prior to this 
utterance being sent, for the most recent 
thre utterances 
• AU4_10sec: Indicator for the display of 
the brow lowerer within 10 seconds prior 
to this utterance being sent, for the most 
recent three uterances 
 
5.2 Modeling
Aproach 
A logistic regresion aproach was used to clasify 
the dialogue acts based on the above feature vec-
tors. The Weka machine learning toolkit (Hal et 
al., 209) was used to learn the models and to first 
perform feature selection in a best-first search. Lo-
gistic regresion is a generalized maximum likeli-
hood model that discriminates betwen pairs of 
output values by calculating a feature weight vec-
tor over the predictors.  
The goal of this work is to explore the utility of 
confusion-related facial features in the context of 
particular dialogue act types. For this reason, a 
specialized classifier was learned by dialogue act. 
5.3 Clasification
Results 
The clasification acuracy and kapa for each 
specialized classifier is displayed in Table 4. Note 
that kapa statistics adjust for the acuracy that 
would be expected by majority-baseline chance; a 
kappa statistic of zero indicates that the clasifier 
performed equal to chance, and a positive kappa 
statistic indicates that the clasifier performed bet-
ter than chance. A kapa of 1 constitutes perfect 
agreement. As the table ilustrates, the feature se-
lection chose to utilize the AU4 feature for every 
dialogue act except STATEMENT (S). When consid-
ering the accuracy of the model across the ten 
folds, two of the affect-enriched classifiers exhibit-
ed statistically significantly better performance. 
For GROUNDING (G) and REQUEST FOR FEEDBACK 
(RF), the facial expression features significantly 
1195
improved the clasification accuracy compared to a 
model that was learned without affective features.  
6 Discusion

Dialogue act clasification is an esential task for 
dialogue systems, and it has been addresed with a 
variety of modeling approaches and feature sets. 
We have presented a novel approach that treats 
facial expresions of students as constraining fea-
tures for an affect-enriched dialogue act classifica-
tion model in task-oriented tutorial dialogue. The 
results sugest that knowledge of the student’s 
confusion-related facial expressions can signifi-
cantly enhance dialogue act classification for two 
types of dialogue acts, GROUNDING and REQUEST 
FOR FEEDBACK.  
 
Table 4. Clasification acuracy and kapa for spe-
cialized DA classifiers. Statistically significant 
diferences (acros ten folds, one-tailed t-test) are 
shown in bold. 
 
 
Clasifier 
with AU4 
Clasifier 
without 
AU4  
Dialogue 
Act 
% 
acc κ 
% 
acc κ 
p-
value 
EX 90.7 .62 89.0 .28 >.05 
G 92.6 .76 91 .71 .018 
P 93 .49 92.2 .40 >.05 
Q 94.6 .72 94.2 .72 >.05 
S 
Not chosen 
in feat. sel. 
93 .22 n/a 
RF 90.7 .62 88.3 .53 .003 
RSP 93 .68 95 .75 >.05 
NE * * 
 
N * * 
PE * * 
U * * 
UE * * 
*Too few instances for ten-fold cros-validation. 
6.1 Features
Selected for Clasification 
Out of more than 150 features available during 
feature selection, each of the specialized dialogue 
act classifiers selected betwen 30 and 50 features 
in each condition (with and without affect fea-
tures). To gain insight into the specific features 
that were useful for clasifying these dialogue acts, 
it is useful to examine which of the AU4 history 
features were chosen during feature selection. 
For GROUNDING, features that indicated the 
presence of absence of AU4 in the imediately 
preceding utterance, either at the 1 second or 5 se-
cond granularity, were selected. Absence of this 
confusion-related facial action unit was asociated 
with a higher probability of a grounding act, such 
as an acknowledgement. This finding is consistent 
with our understanding of how students and tutors 
interacted in this corpus; when a student experi-
enced confusion, she would be unlikely to then 
make a simple grounding dialogue move, but in-
stead would tend to inspect her computer program, 
ask a question, or wait for the tutor to explain 
more. 
For REQUEST FOR FEEDBACK, the predictive 
features were presence or absence of AU4 within 
ten seconds of the longest available history (thre 
turns in the past), as well as the presence of AU4 
within five seconds of the curent uterance (the 
utterance whose dialogue act is being clasified). 
This finding sugests that there may be some lag 
betwen the student experiencing confusion and 
then chosing to make a request for fedback, and 
that the confusion-related facial expresions may 
re-emerge as the student is making a request for 
feedback, since the five-second window prior to 
the student sending the textual dialogue mesage 
would overlap with the student’s construction of 
the mesage itself.   
Although the improvements seen with AU4 fea-
tures for QUESTION, POSITIVE FEEDBACK, and 
EXTRA-DOMAIN acts were not statisticaly reliable, 
examining the AU4 features that were selected for 
classifying these moves points toward ways in 
which facial expresions may influence clasifica-
tion of these acts (Table 5). 
 
 
  
1196
Table 5. Number of features, and AU4 features 
selected, for specialized DA classifiers 
 
Dialogue 
Act 
# fea-
tures 
selected AU4 features selected 
G 43 
One uterance ago: 
AU4_1sec, AU4_5sec 
RF 37 
Thre uterances ago: 
AU4_10sec 
Target uterance: 
AU4_5sec 
EX 50 
Thre uterances ago: 
AU4_1sec 
P 36 
Curent uterance: 
AU4_10sec 
Q 30 
One uterance ago: 
AU4_5sec 
 
6.2 Implications

The results presented here demonstrate that lever-
aging knowledge of user affect, in particular of 
spontaneous facial expressions, may improve the 
performance of dialogue act clasification models. 
Perhaps most interestingly, displays of confusion-
related facial actions prior to a student dialogue 
move enabled an afect-enriched classifier to rec-
ognize requests for fedback with significantly 
greater acuracy than a classifier that did not have 
access to the facial action features. Feedback is 
known to be a key component of effective tutorial 
dialogue, through which tutors provide adaptive 
help (Shute, 208). Requesting fedback also 
seems to be an important behavior of students, 
characteristically engaged in more frequently by 
women than men, and more frequently by students 
with lower incoming knowledge than by students 
with higher incoming knowledge (Boyer, Vouk, & 
Lester, 207). 
6.3 Limitations

The experiments reported here have several nota-
ble limitations. First, the time-consuming nature of 
manual facial action taging restricted the number 
of dialogues that could be tagged. Although the 
highest quality videos were selected for annotation, 
other medium quality videos would have been suf-
ficiently clear to permit taging, which would have 
increased the sample size and likely revealed sta-
tistically significant trends. For example, the per-
formance of the affect-enriched classifier was bet-
ter for dialogue acts of interest such as positive 
feedback and questions, but this difference was not 
statisticaly reliable. 
An aditional limitation stems from the more 
fundamental question of which afective states are 
indicated by particular external displays. The field 
is only just beginning to understand facial expres-
sions during learning and to corelate these facial 
actions with emotions. Additional research into the 
“ground truth” of emotion expresion wil shed 
additional light on this area. Finaly, the results of 
manual facial action annotation may constitute up-
per-bound findings for aplying automatic facial 
expression analysis to dialogue act clasification. 
7 Conclusions
and Future Work 
Emotion plays a vital role in human interactions. In 
particular, the role of facial expresions in human-
human dialogue is widely recognized. Facial ex-
presions offer a promising channel for under-
standing the emotions experienced by users of 
dialogue systems, particularly given the ubiquity of 
webcam technologies and the increasing number of 
dialogue systems that are deployed on webcam-
enabled devices. This paper has reported on a first 
step toward using knowledge of user facial expres-
sions to improve a dialogue act clasification mod-
el for tutorial dialogue, and the results demonstrate 
that facial expresions hold great promise for dis-
tinguishing the pedagogically relevant dialogue act 
REQUEST FOR FEEDBACK, and the conversational 
moves of GROUNDING. 
These early findings highlight the importance 
of future work in this area. Dialogue act clasifica-
tion models have not fuly leveraged some of the 
techniques emerging from work on sentiment anal-
ysis. These approaches may prove particularly use-
ful for identifying emotions in dialogue uterances. 
Another important direction for future work in-
volves more fully exploring the ways in which af-
fect expresion differs betwen textual and spoken 
dialogue. Finaly, as automatic facial taging tech-
nologies mature, they may prove powerful enough 
to enable broadly deployed dialogue systems to 
feasibly leverage facial expresion data in the near 
future. 
  
1197
Acknowledgments 
This work is supported in part by the North Caroli-
na State University Department of Computer Sci-
ence and by the National Science Foundation 
through Grants REC-0632450, IS-0812291, DRL-
1007962 and the STARS Aliance Grant CNS-
0739216. Any opinions, findings, conclusions, or 
recomendations expresed in this report are those 
of the participants, and do not necesarily represent 
the oficial views, opinions, or policy of the Na-
tional Science Foundation. 
 
References 
A. Andrevskaia and S. Bergler. 2008. When specialists 
and generalists work together: Overcoming do-
main dependence in sentiment taging. Proced-
ings of the Anual Meting of the Asociation for 
Computational Linguistics and Human Language 
Technologies (ACL HLT), 290-298.  
M.S. Bartlet, G. Litlewort, M. Frank, C. Lainscsek, I. 
Fasel, and J. Movelan. 2006. Fully Automatic 
Facial Action Recognition in Spontaneous Behav-
ior. 7th International Conference on Automatic 
Face and Gesture Recognition (FGR06), 223-230.  
K.E. Boyer, M. Vouk, and J.C. Lester. 2007. The influ-
ence of learner characteristics on task-oriented tu-
torial dialogue. Procedings of the International 
Conference on Artificial Intelligence in Educa-
tion, 365–372.  
K.E. Boyer, E.Y. Ha, R. Philips, M.D. Walis, M. 
Vouk, and J.C. Lester. 2010. Dialogue act model-
ing in a complex task-oriented domain. Proced-
ings of the 11th Anual Meting of the Special 
Interest Group on Discourse and Dialogue 
(SIGDIAL), 297-305.  
K.E. Boyer, R. Philips, E.Y. Ha, M.D. Wallis, M.A. 
Vouk, and J.C. Lester. 2009. Modeling dialogue 
structure with adjacency pair analysis and hiden 
Markov models. Procedings of the Anual Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics: Short 
Papers, 49-52. 
K.E. Boyer, R. Philips, E.Y. Ha, M.D. Walis, M.A. 
Vouk, and J.C. Lester. 2010. Leveraging hidden 
dialogue state to select tutorial moves. Proced-
ings of the NACL HLT 2010 Fifth Workshop on 
Innovative Use of NLP for Building Educational 
Applications, 66-73. 
R.A. Calvo and S. D’Melo. 2010. Affect Detection: An 
Interdisciplinary Review of Models, Methods, and 
Their Aplications. IEE Transactions on Afec-
tive Computing, 1(1): 18-37. 
M. Cavaza, R.S.D.L. Cámara, M. Turunen, J. Gil, J. 
Hakulinen, N. Crok, et al. 2010. How was your 
day? An afective companion ECA prototype. 
Procedings of the 1th Anual Meting of the 
Special Interest Group on Discourse and Dialogue 
(SIGDIAL), 277-280.  
F. Cavichio. 2009. The modulation of coperation and 
emotion in dialogue: the REC Corpus. Proced-
ings of the ACL-IJCNLP 209 Student Research 
Workshop, 43-48.  
J.F. Cohn, L.I. Red, Z. Ambadar, J. Xiao, and T. Mori-
yama. 2004. Automatic Analysis and Recognition 
of Brow Actions and Head Motion in Spontaneous 
Facial Behavior. IEE International Conference 
on Systems, Man and Cybernetics, 610-616. 
S.D. Craig, S. D’elo, A. Witherspon, J. Sulins, and 
A.C. Graeser. 204. Emotions during learning: 
The first steps toward an afect sensitive inteli-
gent tutoring system. In J. Nal and R. Robson 
(Eds.), E-learn 204: World conference on E-
learning in Corporate, Government, Healthcare, & 
Higher Education, 241-250.  
D. Das and S. Bandyopadhyay. 2009. Word to sentence 
level emotion taging for Bengali blogs. Proced-
ings of the ACL-IJCNLP Conference, Short Pa-
pers, 149-152.  
S. Dasgupta and V. Ng. 2009. Mine the easy, clasify 
the hard: a semi-supervised aproach to automatic 
sentiment classification. Procedings of the 46th 
Annual Meting of the ACL and the 4th IJCNLP, 
701-709.  
B. Di Eugenio, Z. Xie, and R. Serafin. 2010. Dialogue 
Act Clasification, Higher Order Dialogue Struc-
ture, and Instance-Based Learning. Dialogue & 
Discourse, 1(2): 1-24. 
M. Dzikovska, J.D. Moore, N. Steinhauser, and G. 
Campbel. 2010. The impact of interpretation 
problems on tutorial dialogue. Procedings of the 
48th Annual Meting of the Asociation for Com-
putational Linguistics, Short Papers, 43-48.  
S. D’Melo, S.D. Craig, J. Sulins, and A.C. Graeser. 
2006. Predicting Afective States expresed 
through an Emote-Aloud Procedure from AutoTu-
tor’s MixedInitiative Dialogue. International 
Journal of Artificial Inteligence in Education, 
16(1): 3-28. 
P. Ekman. 199. Basic Emotions. In T. Dalgleish and 
M. J. Power (Eds.), Handbok of Cognition and 
Emotion. New York: Wiley. 
P. Ekman, W.V. Friesen. 1978. Facial Action Coding 
System. Palo Alto, CA: Consulting Psychologists 
Pres. 
P. Ekman, W.V. Friesen, and J.C. Hager. 202. Facial 
Action Coding System: Investigator’s Guide. Salt 
Lake City, USA: A Human Face. 
1198
P. Ekman and E.L. Rosenberg (Eds.). 2005. What the 
Face Reveals: Basic and Aplied Studies of Spon-
taneous Expresion Using the Facial Action Cod-
ing System (FACS) (2nd ed.). New York: Oxford 
University Pres. 
K. Forbes-Riley, M. Rotaru, D.J. Litman, and J. 
Tetreault. 2007. Exploring afect-context depend-
encies for adaptive system development. The Con-
ference of the North American Chapter of the 
Association for Computational Linguistics and 
Human Language Technologies (NACL HLT), 
Short Papers, 41-44.  
K. Forbes-Riley, M. Rotaru, D.J. Litman, and J. 
Tetreault. 209. Adapting to student uncertainty 
improves tutoring dialogues. Proceedings of the 
14th International Conference on Artificial Inteli-
gence in Education (AIED), 33-40.  
A.C. Graeser, S. Lu, B. Olde, E. Cooper-Pye, and S. 
Whiten. 2005. Question asking and eye tracking 
during cognitive disequilibrium: comprehending 
illustrated texts on devices when the devices break 
down. Memory & Cognition, 33(7): 1235-1247. 
S. Grene and P. Resnik. 209. More than words: Syn-
tactic packaging and implicit sentiment. Proced-
ings of the 209 Annual Conference of the North 
American Chapter of the ACL and Human Lan-
guage Technologies (NACL HLT), 503-511.  
M. Hal, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
man, and I.H. Witen. 209. The WEKA data 
mining software: An update. SIGKD Explora-
tions, 11(1): 10–18. 
R. Iida, S. Kobayashi, and T. Tokunaga. 2010. Incorpo-
rating extra-linguistic information into reference 
resolution in colaborative task dialogue. Proced-
ings of the 48th Anual Meting of the Asocia-
tion for Computational Linguistics, 1259-1267.  
M.L. Knap and J.A. Hal. 206. Nonverbal Comuni-
cation in Human Interaction (6th ed.). Belmont, 
CA: Wadsworth/Thomson Learning. 
C.M. Le, S.S. Narayanan. 205. Toward detecting 
emotions in spoken dialogs. IEE Transactions on 
Spech and Audio Processing, 13(2): 293-303. 
R. López-Cózar, J. Silovsky, and D. Griol. 2010. F2–
New Technique for Recognition of User Emotion-
al States in Spoken Dialogue Systems. Proced-
ings of the 11th Anual Meting of the Special 
Interest Group on Discourse and Dialogue 
(SIGDIAL), 281-288.  
B.T. McDaniel, S. D’Melo, B.G. King, P. Chipman, K. 
Tap, and A.C. Graeser. 2007. Facial Features 
for Affective State Detection in Learning Envi-
ronments. Procedings of the 29th Anual Cogni-
tive Science Society, 467-472. 
D. McNeil. 192. Hand and mind: What gestures reveal 
about thought. Chicago: University of Chicago 
Pres. 
A. Mehrabian. 2007. Nonverbal Comunication. New 
Brunswick, NJ: Aldine Transaction. 
T. Nguyen. 2010. Mood paterns and afective lexicon 
access in weblogs. Procedings of the ACL 2010 
Student Research Workshop, 43-48.  
M. Pantic and M.S. Bartlet. 207. Machine Analysis of 
Facial Expresions. In K. Delac and M. Grgic 
(Eds.), Face Recognition, 377-416. Viena, Aus-
tria: I-Tech Education and Publishing. 
J.A. Rusel. 2003. Core afect and the psychological 
construction of emotion. Psychological Review, 
110(1): 145-172. 
J.A. Rusel, J.A. Bachorowski, and J.M. Fernandez-
Dols. 2003. Facial and vocal expresions of emo-
tion. Annual Review of Psychology, 54, 329-49. 
K.L. Schmidt and J.F. Cohn. 201. Human Facial Ex-
presions as Adaptations: Evolutionary Questions 
in Facial Expresion Research. Am J Phys An-
thropol, 33: 3-24. 
V.J. Shute. 208. Focus on Formative Fedback. Re-
view of Educational Research, 78(1): 153-189. 
V.K.R Sridar, S. Bangalore, and S.S. Narayanan. 209. 
Combining lexical, syntactic and prosodic cues for 
improved online dialog act taging. Computer 
Spech & Language, 23(4): 407-422. Elsevier Ltd. 
A. Stolcke, K. Ries, N. Cocaro, E. Shriberg, R. Bates, 
D. Jurafsky, et al. 200. Dialogue Act Modeling 
for Automatic Taging and Recognition of Con-
versational Speech. Computational Linguistics, 
26(3): 339-373. 
C. Toprak, N. Jakob, and I. Gurevych. 2010. Sentence 
and expression level annotation of opinions in us-
er-generated discourse. Procedings of the 48th 
Annual Meting of the Asociation for Computa-
tional Linguistics, 575-584.  
T. Wilson, J. Wiebe, and P. Hofman. 209. Recogniz-
ing Contextual Polarity: An Exploration of Fea-
tures for Phrase-Level Sentiment Analysis. 
Computational Linguistics, 35(3): 399-433. 
Z. Zeng, M. Pantic, G.I. Roisman, and T.S. Huang. 
2009. A Survey of Affect Recognition Methods: 
Audio, Visual, and Spontaneous Expresions. 
IEE Transactions on Patern Analysis and Ma-
chine Intelligence, 31(1): 39-58. 
1199

