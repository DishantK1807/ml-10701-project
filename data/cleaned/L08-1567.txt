<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>L Anderson</author>
</authors>
<title>Simultaneous Interpretation: Contextual and Translation Aspects. In Bridging the Gap: Empirical research in simultaneous interpretation</title>
<date>1994</date>
<pages>101--120</pages>
<contexts>
<context> of J-E and E-J interpretations is 4.532 seconds and 2.446 seconds, respectively. The average delay of E-J interpretation is close to other results derived from European language pairs (Barik, 1973) (Anderson, 1994) (Christoffels and de Groot, 2004), while J-E interpretation has larger delay. The difference of verb positions between Japanese and English might have effects. The standard deviation of J-E and E-J </context>
</contexts>
<marker>Anderson, 1994</marker>
<rawString>L. Anderson. 1994. Simultaneous Interpretation: Contextual and Translation Aspects. In Bridging the Gap: Empirical research in simultaneous interpretation, pages 101–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H C Barik</author>
</authors>
<title>Simultaneous Interpretation: Temporal and Quantitative Data. Language and Speech</title>
<date>1973</date>
<contexts>
<context> average delay of J-E and E-J interpretations is 4.532 seconds and 2.446 seconds, respectively. The average delay of E-J interpretation is close to other results derived from European language pairs (Barik, 1973) (Anderson, 1994) (Christoffels and de Groot, 2004), while J-E interpretation has larger delay. The difference of verb positions between Japanese and English might have effects. The standard deviatio</context>
</contexts>
<marker>Barik, 1973</marker>
<rawString>H. C. Barik. 1973. Simultaneous Interpretation: Temporal and Quantitative Data. Language and Speech, 18(3):272–287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Brugnara</author>
<author>D Falavigna</author>
<author>M Omologo</author>
</authors>
<title>Automatic Segmentation and Labeling of Speech based on Hidden Markov Models</title>
<date>1993</date>
<journal>Speech Communication</journal>
<volume>12</volume>
<contexts>
<context>mation of Word Utterance Timing Given speech and its corresponding transcription as input, beginning time and end time of each word are estimated using Hidden Markov Model based phoneme segmentation (Brugnara et al., 1993). The temporal information is estimated in the following steps (Figure 4). 1. Feature vectors are extracted from the speech. Features are 12th order MFCC, ∆MFCC, and ∆log energy under the condition s</context>
</contexts>
<marker>Brugnara, Falavigna, Omologo, 1993</marker>
<rawString>F. Brugnara, D. Falavigna, and M. Omologo. 1993. Automatic Segmentation and Labeling of Speech based on Hidden Markov Models. Speech Communication, 12(4):357–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<date>2000</date>
<note>A Maximum-Entropy-Inspired Parser</note>
<contexts>
<context>-speech (noun or verb) and grammatical roles (subject or object) of the source-speaker’s words. 4.2.1. Parts-of-speech Parts-of-speech were esimated with ChaSen (Matsumoto et al., 1999) and nlparser (Charniak, 2000) for Japanese and English, respectively. Figure 6 shows the result of J-E interpretation. Although nouns have larger delay than verbs, there is not a large difference. Figure 7 shows the result in E-</context>
<context>a’ have smaller delay than ‘wa’ and ‘ni’, that is subjects have smaller delay than objects in J-E interpretation. Grammatical roles of English were estimated using parsed trees derived with nlparser (Charniak, 2000). 151 nouns were found as objects and their average delay was 2.195 seconds. The average delay of other nouns was 1.957. There was no significant difference between them. Table 5: Attached particles </context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A Maximum-Entropy-Inspired Parser.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of the NAACL-2000</booktitle>
<pages>132--139</pages>
<marker></marker>
<rawString>In Proceedings of the NAACL-2000, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I K Christoffels</author>
<author>A M B de Groot</author>
</authors>
<title>Components of Simultaneous Interpreting: Comparing Interpreting with Shadowing and Paraphrasing. Bilingualism: Language and Cognition</title>
<date>2004</date>
<volume>7</volume>
<marker>Christoffels, de Groot, 2004</marker>
<rawString>I. K. Christoffels and A. M. B. de Groot. 2004. Components of Simultaneous Interpreting: Comparing Interpreting with Shadowing and Paraphrasing. Bilingualism: Language and Cognition, 7(3):227–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CMU</author>
</authors>
<title>CMU Pronunciation Dictionary version 0.6. http://www.speech.cs.cmu.edu/cgi-bin</title>
<date>1998</date>
<journal>Eijiro</journal>
<note>http://www.eijiro.jp</note>
<contexts>
<context>labary and it can be converted into phoneme sequences by rules. English transcriptions are split into words by white spaces and pronunciations are given with CMU Pronunciation Dictionary version 0.6 (CMU, 1998). 3. Following the pronunciation, phoneme HMMs are concatenated to build the large HMM corresponding to the whole transcript. For Japanese, the speaker independent 16 mixture monophone model of Juliu</context>
</contexts>
<marker>CMU, 1998</marker>
<rawString>CMU. 1998. CMU Pronunciation Dictionary version 0.6. http://www.speech.cs.cmu.edu/cgi-bin/. Eijiro. 2001. http://www.eijiro.jp/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Garofolo</author>
<author>L F Lamel</author>
<author>W M Fisher</author>
<author>J G Fiscus D S Pallett</author>
<author>N L Dahlgren</author>
</authors>
<date>1993</date>
<journal>DARPA TIMIT Acoustic-Phonetic Continuous Speech</journal>
<note>http://julius.sourceforge.jp</note>
<contexts>
<context>ctation Kit v3.1 (Julius, 2005) was used. For English, speaker independent 2 mixture monophone model are constructed from the 6,300 utterances of the TIMIT Acoustic Phonetic Continuous Speech Corpus (Garofolo et al., 1993) using HTK (Young et al., 2006). Three-state left-to-right HMMs are trained for 39 phonemes of CMU Pronouncing Dictionary and 1 silence. 4. The maximum likelihood state sequence of the transcription </context>
</contexts>
<marker>Garofolo, Lamel, Fisher, Pallett, Dahlgren, 1993</marker>
<rawString>J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus D. S. Pallett, and N. L. Dahlgren. 1993. DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus CD-ROM. Julius. 2005. http://julius.sourceforge.jp/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Maekawa</author>
<author>H Koiso</author>
<author>S Furui</author>
<author>H Isahara</author>
</authors>
<date>2000</date>
<contexts>
<context> separated into manageable segments, or utterance units, by 200-millisecond or longer pauses. All utterance units are transcribed manually in compliance with the Corpus of Spontaneous Japanese (CSJ) (Maekawa et al., 2000); Phenomena found in spontaneous language such as fillers, hesitations, etc. are tagged with the discourse tags, and beginning time and end time are provided to each utterance unit. Translation align</context>
</contexts>
<marker>Maekawa, Koiso, Furui, Isahara, 2000</marker>
<rawString>K. Maekawa, H. Koiso, S. Furui, and H. Isahara. 2000.</rawString>
</citation>
<citation valid="true">
<title>Spontaneous Speech Corpus of Japanese</title>
<booktitle>In Proceedings of the LREC-2000</booktitle>
<pages>947--952</pages>
<marker></marker>
<rawString>Spontaneous Speech Corpus of Japanese. In Proceedings of the LREC-2000, pages 947–952.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Matsubara</author>
<author>A Takagi</author>
<author>N Kawaguchi</author>
<author>Y Inagaki</author>
</authors>
<marker>Matsubara, Takagi, Kawaguchi, Inagaki, </marker>
<rawString>S. Matsubara, A. Takagi, N. Kawaguchi, and Y. Inagaki.</rawString>
</citation>
<citation valid="true">
<title>Bilingual Spoken Monologue Corpus for Simultaneous Machine Interpretation Research</title>
<date>2002</date>
<booktitle>In Proceedings of the LREC-2002</booktitle>
<pages>167--174</pages>
<marker>2002</marker>
<rawString>2002. Bilingual Spoken Monologue Corpus for Simultaneous Machine Interpretation Research. In Proceedings of the LREC-2002, pages 167–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Matsumoto</author>
<author>A Kitauchi</author>
<author>T Yamashita</author>
<author>Y Hirano</author>
</authors>
<title>Japanese Morphological Analysis System Chasen version 2.0 Manual. In</title>
<date>1999</date>
<tech>NAIST Techinical Report, NAISTIS-TR99009</tech>
<contexts>
<context>under the condition shown in Table 3. CMS is done for each utterance unit. 2. Word boundaries and phoneme pronunciation are provided to the transcription. For Japanese, morphological analyzer ChaSen (Matsumoto et al., 1999) is utilized to identify the morpheme boundaries and Katakana pronunciation. Katakana is a Japanese syllabary and it can be converted into phoneme sequences by rules. English transcriptions are split</context>
<context>rrelation of the delay against partsof-speech (noun or verb) and grammatical roles (subject or object) of the source-speaker’s words. 4.2.1. Parts-of-speech Parts-of-speech were esimated with ChaSen (Matsumoto et al., 1999) and nlparser (Charniak, 2000) for Japanese and English, respectively. Figure 6 shows the result of J-E interpretation. Although nouns have larger delay than verbs, there is not a large difference. F</context>
</contexts>
<marker>Matsumoto, Kitauchi, Yamashita, Hirano, 1999</marker>
<rawString>Y. Matsumoto, A. Kitauchi, T. Yamashita, and Y. Hirano. 1999. Japanese Morphological Analysis System Chasen version 2.0 Manual. In NAIST Techinical Report, NAISTIS-TR99009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mizuno</author>
</authors>
<title>Process Model for Simultaneous Interpreting and Working Memory</title>
<date>2005</date>
<location>META</location>
<contexts>
<context>interpreter’s utterances always delay behind the speaker’s utterances to grasp the speaker’s message. Since large delay burdens the interpreter’s memory, which could lower the interpretation quality (Mizuno, 2005), it is essential for interpreters to control the delay properly. The delay is heavily affected by the source and target languages. Because Japanese and English have quite different word order, it is</context>
</contexts>
<marker>Mizuno, 2005</marker>
<rawString>A. Mizuno. 2005. Process Model for Simultaneous Interpreting and Working Memory. META, 50(2):739–794.</rawString>
</citation>
</citationList>
</algorithm>

