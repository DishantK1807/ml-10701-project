Book Reviews
Semisupervised Learning for Computational Linguistics
Steven Abney
(UniversityofMichigan)
BocaRaton,FL:Chapman&Hall/CRC(Computerscienceanddataanalysisseries,
editedbyDavidMadiganetal.),2007,xi+308pp;hardbound,ISBN978-1-58488-559-7,
$79.95,£44.99
Reviewed by
Vincent Ng
University of Texas at Dallas
Semi-supervised learning is by no means an unfamiliar concept to natural language
processingresearchers.Labeleddatahasbeenusedtoimproveunsupervisedparameter
estimation procedures such as the EM algorithm and its variants since the beginning
of the statistical revolution in NLP (e.g., Pereira and Schabes 1992). Unlabeled data
has also been used to improve supervised learning procedures, the most notable ex-
amplesbeingthesuccessfulapplicationsofself-trainingandco-trainingtowordsense
disambiguation (Yarowsky 1995) and named entity classiﬁcation (Collins and Singer
1999).
Despite its increasing importance, semi-supervised learning is not a topic that is
typicallydiscussedinintroductorymachinelearningtexts(e.g.,Mitchell1997;Alpaydin
2004) or NLP texts (e.g., Manning and Sch¨utze 1999; Jurafsky and Martin 2000).
1
Consequently,tolearnaboutsemi-supervisedlearningresearch,onehastoconsultthe
machine-learningliterature.ThiscanbeadauntingtaskforNLPresearcherswhohave
littlebackgroundinmachinelearning.StevenAbney’sbook Semisupervised Learning for
Computational Linguistics is targeted precisely at such researchers, aiming to provide
themwitha“broadandaccessiblepresentation”oftopicsinsemi-supervisedlearning.
According to the preamble, the reader is assumed to have taken only an introductory
course in NLP “that include[s] statistical methods—concretely the material contained
in Jurafsky and Martin (2000) and Manning and Sch¨utze (1999).” Nonetheless, I agree
with the author that any NLP researcher who has a solid background in machine
learningisreadyto“tackletheprimaryliteratureonsemisupervisedlearning,andwill
probablynotﬁndthisbookparticularlyuseful”(page11).
As the author promises, the book is self-contained and quite accessible to those
who have little background in machine learning. In particular, of the 12 chapters
in the book, three are devoted to preparatory material, including: a brief introduc-
tion to machine learning, basic unconstrained and constrained optimization tech-
niques (e.g., gradient descent and the method of Lagrange multipliers), and relevant
linear-algebra concepts (e.g., eigenvalues, eigenvectors, matrix and vector norms,
1 AlthoughManningandSch¨utze(1999)andJurafskyandMartin(2000)dodiscussself-training,theydo
soonlyinthecontextofYarowsky’swordsensedisambiguationalgorithm.
ComputationalLinguistics Volume34,Number3
diagonalization).Theremainingchaptersfocusroughlyonsixtypesofsemi-supervised
learningmethods:
2
a114
Self-training. Afterintroducingtheself-trainingalgorithmanditsvariants,
theauthordiscussesitsapplicationsinNLPanditsrelationshiptoother
semi-supervisedlearningalgorithms.
a114
Agreement-based methods. Theco-trainingalgorithm,alongwitha
theoreticalanalysisofitsconditionalindependenceassumptionandits
applicationsinNLP,arepresented.Additionally,arandomﬁeld,which
penalizesdisagreementamongneighboringnodes,isintroducedasan
alternativewayofenforcingagreement.
a114
Clustering algorithms. Basichardclusteringalgorithms(e.g., k-means,
graphmincuts,hierarchicalclustering),EM(asasoftclustering
algorithm),andtheirroleinsemi-supervisedlearningarediscussed.
a114
Boundary-oriented methods. Twodiscriminativelearningalgorithms,
boostingandsupportvectormachines,areintroducedasameansto
facilitatethediscussionoftheirsemi-supervisedcounterparts:co-boosting
andtransductiveSVMs.
a114
Label propagation in graphs. Ingraph-basedapproachestosemi-supervised
learning,thelabelsofthelabelednodesarepropagatedtotheunlabeled
nodes,withthegoalofmaximizingtheagreementofthelabelsof
proximatenodes.Theauthorshowsthatthisgoalisequivalenttoﬁndinga
harmonic function giventhelabelednodes,andpresentsseveralalgorithms,
includingthemethodofrelaxation,forcomputingthisfunction.
a114
Spectral methods. Spectralmethodsforsemi-supervisedlearningcanbe
viewedasinterpolationacrossapartiallylabeledgraphasdescribed
previouslyusinga“standingwave.”Theauthorexplainstheconnection
betweensuchawaveandthespectrumofamatrix,andestablishesthe
relationshipofspectralclusteringalgorithmstoothersemi-supervised
learners,includinggraphmincuts,randomwalks,andlabelpropagation.
Thebookisrichintheoryandalgorithms,andalthoughitistargetedatthosewho
lack relevant mathematical background, each theory and algorithm is presented in a
rigorousmanner.
Anothernicefeatureofthebookisthatitrevealstheconnectionamongseemingly
disparate ideas. As mentioned earlier, it shows that many semi-supervised learners
can in fact be viewed as self-training; also, the description of the connection between
spectralclusteringandothersemi-supervisedlearnersisinsightful.
In addition, I like the organization of the book. One reason is the presentation of
co-training:AlthoughthealgorithmispresentedinChapter2,itstheoreticalunderpin-
nings are not described until Chapter 9. This enables the reader to see its applications
inNLP(inChapter3)beforegoingthroughthemathematics,whichcouldbeimportant
forresearcherswhoarelinguisticallybutnotmathematicallyoriented.Anotherreason
2 Thepresentationofthemethodsheredoesnotreﬂecttheorderinwhichtheyareintroducedinthe
book;rather,itismotivatedbythebook’sSection1.3,whichgivesanoverviewofthe“leadingideas”
ofthebook.
450
BookReviews
is that the preparatory material is presented on a need-to-know basis. This allows the
discussionofalgorithmicideasassoonasthereadergraspstherelevantfundamentals.
For instance, function optimization and basic linear algebra concepts are presented
in separate chapters, with the latter being deferred to Chapter 11, right before the
discussionofspectralclusteringinChapter12.
Whereas the discussion of self-training and co-training is complemented by their
application to NLP problems, the same is not true for the remaining semi-supervised
learners described in the book. The reader is often left to imagine the potential NLP
applicationsoftheselearners,andasaconsequenceisunabletogainanunderstanding
ofthestateoftheartofsemi-supervised learningforNLP.Infact,givenitsscarcityof
NLPapplications,thebookperhapsdoesnotmerititscurrenttitle.Itdoeshavearich
bibliography on semi-supervised learning for NLP, but most of the references are not
citedinthetext.
The book also lacks a discussion of the practical issues in applying the semi-
supervised learners. For instance, the author does not mention that in practice it is
not easy to choose k in k-means clustering, merely describing k as a parameter of the
clustering algorithm. As another example, when introducing the EM algorithm, the
author applies it to a generative model that can be expressed in exponential form,
withoutacknowledgingthatoneofthemostdifﬁcultissuessurroundingtheapplication
of EM concerns the design of the right generative model given the data. The lack of
NLP applications in the book has unfortunately enabled the author to sidestep these
practicalissues.Onarelatednote,onecanhardlyﬁndanydiscussionsofthestrengths
andweaknessesofthesemi-supervisedlearnersinthebook.Thiscouldleavethereader
withouttheabilitytochoosethebestlearnerforagivenNLPproblem,andisprobably
anotherundesirableconsequenceofthebook’sreluctancetodiscussNLPapplications.
The author’s decision to focus exclusively on semi-supervised classiﬁcation prob-
lemseffectivelylimitsthescopeofthebook.Oneconsequenceofthisdecisionisthatthe
readermaynotbeabletoapplytheEMalgorithmtotrainahiddenMarkovmodelfor
solving sequence-learning problemsasbasicaspart-of-speechtagginguponcompletion
of this book. Given the recent surge of interest in structure prediction in the NLP
community, and the fact that co-training and semi-supervised EM have been applied
tostructure-predictionproblemssuchasstatisticalparsingandpart-of-speechtagging,
thebook’ssolefocusonclassiﬁcationproblemisperhapsoneofitsweaknesses.
There are a few occasions on which the reader might not get a complete picture
of the capability of an algorithm. For instance, the reader might think that spectral
methodscanbeappliedonlytobinaryclassiﬁcationtasks,owingtothebook’sexclusive
focus on such tasks in its discussion of spectral clustering. Similarly for the treatment
of support vector machines: The reader may get the impression that SVMs cannot be
used to learn non-linear functions, as the discussion of kernels is deliberately omitted
due to their irrelevance to transductive learning. Although it is important to keep the
presentationfocused,Ibelievethattheauthorcouldeasilyhaveremovedpotentialcon-
fusionsbyexplicitlystatingthefullcapabilityofanalgorithmandreferringthereader
totherelevantpapersfordetails.
Given the rapid growth of semi-supervised learning research in the past decade,
there is currently a need for a broad and accessible reference to this area of research.
Abney’s book serves this purpose in spite of the aforementioned weaknesses, and I
believe that it is a useful starting point for any non–machine-learning experts who
intendtoapplysemi-supervisedlearningtechniquestotheirresearch.Assomeonewho
hassomepriorknowledgeofsemi-supervisedlearning,Istillﬁndthisbookinsightful:
Itrevealsdeepconnectionsamongapparentlydisparateideas.IfIweretoteachacourse
451
ComputationalLinguistics Volume34,Number3
onsemi-supervisedlearningforNLP,Iwouldundoubtedlyusethisbookasaprimary
reference.
References
Alpaydin,Ethem.2004. Introduction to
Machine Learning.TheMITPress,
Cambridge,MA.
Collins,MichaelandYoramSinger.1999.
Unsupervisedmodelsfornamedentity
classiﬁcation.In Proceedings of the 1999
Joint Conference on Empirical Methods in
Natural Language Processing and Very
Large Corpora,pages100–110,College
Park,MD.
Jurafsky,DanielandJamesH.Martin.2000.
Speech and Language Processing.Prentice
Hall,UpperSaddleRiver,NJ.
Manning,ChristopherD.andHinrich
Sch¨utze.1999. Foundations of Statistical
Natural Language Processing.TheMIT
Press,Cambridge,MA.
Mitchell,TomM.˙1997. Machine Learning.
McGrawHill,Columbus,OH.
Pereira,FernandoandYvesSchabes.1992.
Inside-outsidereestimationfrompartially
bracketedcorpora.In Proceedings of the
30th Annual Meeting of the Association for
Computational Linguistics,pages128–135,
Newark,DE.
Yarowsky,David.1995.Unsupervisedword
sensedisambiguationrivalingsupervised
methods.In Proceedings of the 33rd
Annual Meeting of the Association for
Computational Linguistics,pages189–196,
Cambridge,MA.
Vincent Ng isanassistantprofessorintheDepartmentofComputerScienceattheUniversityof
TexasatDallas.Heisalsoafﬁliatedwiththeuniversity’sHumanLanguageTechnologyResearch
Institute,whereheconductsresearchonstatisticalnaturallanguageprocessingandteachesun-
dergraduateandgraduatecoursesinmachinelearning.Ng’saddressis:DepartmentofComputer
Science,UniversityofTexasatDallas,Richardson,TX75080-0688;e-mail:vince@hlt.utdallas.edu.
452

