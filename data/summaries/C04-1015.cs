Translation Model and Language Model We used a lexicon model of IBM Model 4 learned by GIZA++ (Och and Ney, 2003) and word bigram and trigram models learned by CMU-Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997).
J03-1002
For example, Marcu (2001) proposed a method in which initial translations are constructed by combining bilingual phrases from translation memory, which is followed by modifying the translations by greedy decoding (Germann et al., 2001).
P01-1030 P01-1050
For instance, example-based MT can be improved by applying an optimization algorithm that uses an automatic evaluation of MT quality (Imamura et al., 2003).
P03-1057
BLEU: Automatic evaluation by BLEU score (Papineni et al., 2002).
P02-1040
On the other hand, statistical MT employing IBM models (Brown et al., 1993) translates an input sentence by the combination of word transfer and word re-ordering.
J93-2003
Specifically, the semantic distance (Sumita and Iida, 1991) is calculated between the source examples and the headwords of the input sentence, and the transfer rules that contain the nearest example are used to construct the target tree structure.
P91-1024
Using this feature, Akiba et al.(2002) achieved selection of the best translation among those output by multiple MT engines.
C02-1076
