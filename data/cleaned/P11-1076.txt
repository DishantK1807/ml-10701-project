Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 752–762,
Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
Learning to Grade Short Answer Questions using Semantic Similarity
Measures and Dependency Graph Alignments
Michael Mohler
Dept. of Computer Science
University of North Texas
Denton, TX
mgm0038@unt.edu
Razvan Bunescu
School of EECS
Ohio University
Athens, Ohio
bunescu@ohio.edu
Rada Mihalcea
Dept. of Computer Science
University of North Texas
Denton, TX
rada@cs.unt.edu
Abstract
In this work we address the task of computer-
assisted assessment of short student answers.
We combine several graph alignment features
with lexical semantic similarity measures us-
ing machine learning techniques and show
that the student answers can be more accu-
rately graded than if the semantic measures
were used in isolation. We also present a first
attempt to align the dependency graphs of the
student and the instructor answers in order to
make use of a structural component in the au-
tomatic grading of student answers.
1 Introduction
One of the most important aspects of the learning
process is the assessment of the knowledge acquired
by the learner. In a typical classroom assessment
(e.g., an exam, assignment or quiz), an instructor or
a grader provides students with feedback on their
answers to questions related to the subject matter.
However, in certain scenarios, such as a number of
sites worldwide with limited teacher availability, on-
line learning environments, and individual or group
study sessions done outside of class, an instructor
may not be readily available. In these instances, stu-
dents still need some assessment of their knowledge
of the subject, and so, we must turn to computer-
assisted assessment (CAA).
While some forms of CAA do not require sophis-
ticated text understanding (e.g., multiple choice or
true/false questions can be easily graded by a system
if the correct solution is available), there are also stu-
dent answers made up of free text that may require
textual analysis. Research to date has concentrated
on two subtasks of CAA: grading essay responses,
which includes checking the style, grammaticality,
and coherence of the essay (Higgins et al., 2004),
and the assessment of short student answers (Lea-
cock and Chodorow, 2003; Pulman and Sukkarieh,
2005; Mohler and Mihalcea, 2009), which is the fo-
cus of this work.
An automatic short answer grading system is one
that automatically assigns a grade to an answer pro-
vided by a student, usually by comparing it to one
or more correct answers. Note that this is different
from the related tasks of paraphrase detection and
textual entailment, since a common requirement in
student answer grading is to provide a grade on a
certain scale rather than make a simple yes/no deci-
sion.
In this paper, we explore the possibility of im-
proving upon existing bag-of-words (BOW) ap-
proaches to short answer grading by utilizing ma-
chine learning techniques. Furthermore, in an at-
tempt to mirror the ability of humans to understand
structural (e.g. syntactic) differences between sen-
tences, we employ a rudimentary dependency-graph
alignment module, similar to those more commonly
used in the textual entailment community.
Specifically, we seek answers to the following
questions. First, to what extent can machine learn-
ing be leveraged to improve upon existing ap-
proaches to short answer grading. Second, does the
dependency parse structure of a text provide clues
that can be exploited to improve upon existing BOW
methodologies?
752
2 Related
Work
Several state-of-the-art short answer grading sys-
tems (Sukkarieh et al., 2004; Mitchell et al., 2002)
require manually crafted patterns which, if matched,
indicate that a question has been answered correctly.
If an annotated corpus is available, these patterns
can be supplemented by learning additional pat-
terns semi-automatically. The Oxford-UCLES sys-
tem (Sukkarieh et al., 2004) bootstraps patterns by
starting with a set of keywords and synonyms and
searching through windows of a text for new pat-
terns. A later implementation of the Oxford-UCLES
system (Pulman and Sukkarieh, 2005) compares
several machine learning techniques, including in-
ductive logic programming, decision tree learning,
and Bayesian learning, to the earlier pattern match-
ing approach, with encouraging results.
C-Rater (Leacock and Chodorow, 2003) matches
the syntactical features of a student response (i.e.,
subject, object, and verb) to that of a set of correct
responses. This method specifically disregards the
BOW approach to take into account the difference
between “dog bites man” and “man bites dog” while
still trying to detect changes in voice (i.e., “the man
was bitten by the dog”).
Another short answer grading system, AutoTutor
(Wiemer-Hastings et al., 1999), has been designed
as an immersive tutoring environment with a graph-
ical “talking head” and speech recognition to im-
prove the overall experience for students. AutoTutor
eschews the pattern-based approach entirely in favor
of a BOW LSA approach (Landauer and Dumais,
1997). Later work on AutoTutor(Wiemer-Hastings
et al., 2005; Malatesta et al., 2002) seeks to expand
upon their BOW approach which becomes less use-
ful as causality (and thus word order) becomes more
important.
A text similarity approach was taken in (Mohler
and Mihalcea, 2009), where a grade is assigned
based on a measure of relatedness between the stu-
dent and the instructor answer. Several measures are
compared, including knowledge-based and corpus-
based measures, with the best results being obtained
with a corpus-based measure using Wikipedia com-
bined with a “relevance feedback” approach that it-
eratively augments the instructor answer by inte-
grating the student answers that receive the highest
grades.
In the dependency-based classification compo-
nent of the Intelligent Tutoring System (Nielsen et
al., 2009), instructor answers are parsed, enhanced,
and manually converted into a set of content-bearing
dependency triples or facets. For each facet of the
instructor answer each student’s answer is labelled
to indicate whether it has addressed that facet and
whether or not the answer was contradictory. The
system uses a decision tree trained on part-of-speech
tags, dependency types, word count, and other fea-
tures to attempt to learn how best to classify an an-
swer/facet pair.
Closely related to the task of short answer grading
is the task of textual entailment (Dagan et al., 2005),
which targets the identification of a directional in-
ferential relation between texts. Given a pair of two
texts as input, typically referred to as text and hy-
pothesis, a textual entailment system automatically
finds if the hypothesis is entailed by the text.
In particular, the entailment-related works that are
most similar to our own are the graph matching tech-
niques proposed by Haghighi et al. (2005) and Rus
et al. (2007). Both input texts are converted into a
graph by using the dependency relations obtained
from a parser. Next, a matching score is calculated,
by combining separate vertexand edge-matching
scores. The vertex matching functions use word-
level lexical and semantic features to determine the
quality of the match while the the edge matching
functions take into account the types of relations and
the difference in lengths between the aligned paths.
Following the same line of work in the textual en-
tailment world are (Raina et al., 2005), (MacCartney
et al., 2006), (de Marneffe et al., 2007), and (Cham-
bers et al., 2007), which experiment variously with
using diverse knowledge sources, using a perceptron
to learn alignment decisions, and exploiting natural
logic.
3 Answer
Grading System
We use a set of syntax-aware graph alignment fea-
tures in a three-stage pipelined approach to short an-
swer grading, as outlined in Figure 1.
In the first stage (Section 3.1), the system is pro-
vided with the dependency graphs for each pair of
instructor (nulli) and student (nulls) answers. For each
753
Figure 1: Pipeline model for scoring short-answer pairs.
node in the instructor’s dependency graph, we com-
pute a similarity score for each node in the student’s
dependency graph based upon a set of lexical, se-
mantic, and syntactic features applied to both the
pair of nodes and their corresponding subgraphs.
The scoring function is trained on a small set of man-
ually aligned graphs using the averaged perceptron
algorithm.
In the second stage (Section 3.2), the node simi-
larity scores calculated in the previous stage are used
to weight the edges in a bipartite graph representing
the nodes in nulli on one side and the nodes in nulls on
the other. We then apply the Hungarian algorithm
to find both an optimal matching and the score asso-
ciated with such a matching. In this stage, we also
introduce question demoting in an attempt to reduce
the advantage of parroting back words provided in
the question.
In the final stage (Section 3.4), we produce an
overall grade based upon the alignment scores found
in the previous stage as well as the results of several
semantic BOW similarity measures (Section 3.3).
Using each of these as features, we use Support Vec-
tor Machines (SVM) to produce a combined real-
number grade. Finally, we build an Isotonic Regres-
sion (IR) model to transform our output scores onto
the original [0,5] scale for ease of comparison.
3.1 Node
to Node Matching
Dependency graphs for both the student and in-
structor answers are generated using the Stanford
Dependency Parser (de Marneffe et al., 2006) in
collapse/propagate mode. The graphs are further
post-processed to propagate dependencies across the
“APPOS” (apposition) relation, to explicitly encode
negation, part-of-speech, and sentence ID within
each node, and to add an overarching ROOT node
governing the main verb or predicate of each sen-
tence of an answer. The final representation is a
list of (relation,governor,dependent) triples, where
governor and dependent are both tokens described
by the tuple (sentenceID:token:POS:wordPosition).
For example: (nsubj, 1:provide:VBZ:4, 1:pro-
gram:NN:3) indicates that the noun “program” is a
subject in sentence 1 whose associated verb is “pro-
vide.”
If we consider the dependency graphs output by
the Stanford parser as directed (minimally cyclic)
graphs,1 we can define for each nodenulla set of nodes
nullx that are reachable from null using a subset of the
relations (i.e., edge types)2. We variously define
“reachable” in four ways to create four subgraphs
defined for each node. These are as follows:
null null0x : All edge types may be followed
null null1x : All edge types except for subject types,
ADVCL, PURPCL, APPOS, PARATAXIS,
ABBREV, TMOD, and CONJ
null null2x : All edge types except for those in null1x plus
object/complement types, PREP, and RCMOD
null null3x : No edge types may be followed (This set
is the single starting node null)
Subgraph similarity (as opposed to simple node
similarity) is a means to escape the rigidity involved
in aligning parse trees while making use of as much
of the sentence structure as possible. Humans intu-
itively make use of modifiers, predicates, and subor-
dinate clauses in determining that two sentence en-
tities are similar. For instance, the entity-describing
phrase “men who put out fires” matches well with
“firemen,” but the words “men” and “firemen” have
1The standard output of the Stanford Parser produces rooted
trees. However, the process of collapsing and propagating de-
pendences violates the tree structure which results in a tree
with a few cross-links between distinct branches.
2For more information on the relations used in this experi-
ment, consult the Stanford Typed Dependencies Manual at
http://nlp.stanford.edu/software/dependencies manual.pdf
754

