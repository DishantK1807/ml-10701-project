1:185	Phrase Pair Rescoring with Term Weightings for Statistical Machine Translation Bing Zhao Stephan Vogel Alex Waibel Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA, 15213, USA {bzhao, vogel+, ahw}@cs.cmu.edu Abstract We propose to score phrase translation pairs for statistical machine translation using term weight based models.
2:185	These models employ tf.idf to encode the weights of content and non-content words in phrase translation pairs.
3:185	The translation probability is then modeled by similarity functions defined in a vector space.
4:185	Two similarity functions are compared.
5:185	Using these models in a statistical machine translation task shows significant improvements.
6:185	1 Introduction Words can be classified as content and functional words.
7:185	Content words like verbs and proper nouns are more informative than function words like "to'' and "the''.
8:185	In machine translation, intuitively, the informative content words should be emphasized more for better adequacy of the translation quality.
9:185	However, the standard statistical translation approach does not take account how informative and thereby, how important a word is, in its translation model.
10:185	One reason is the difficulty to measure how informative a word is. Another problem is to integrate it naturally into the existing statistical machine translation framework, which typically is built on word alignment models, like the well-known IBM alignment models (Brown et al 1993).
11:185	In recent years there has been a strong tendency to incorporate phrasal translation into statistical machine translation.
12:185	It directly translates an n-gram from the source language into an mgram in the target language.
13:185	The advantages are obvious: It has built-in local context modeling, and provides reliable local word reordering.
14:185	It has multi-word translations, and models a words conditional fertility given a local context.
15:185	It captures idiomatic phrase translations and can be easily enriched with bilingual dictionaries.
16:185	In addition, it can compensate for the segmentation errors made during preprocessing, i.e. word segmentation errors of Chinese.
17:185	The advantage of using phrase-based translation in a statistical framework has been shown in many studies such as (Koehn et al. 2003; Vogel et al. 2003; Zens et al. 2002; Marcu and Wong, 2002).
18:185	However, the phrase translation pairs are typically extracted from a parallel corpus based on the Viterbi alignment of some word alignment models.
19:185	The leads to the question what probability should be assigned to those phrase translations.
20:185	Different approaches have been suggested as using relative frequencies (Zens et al. 2002), calculate probabilities based on a statistical word-to-word dictionary (Vogel et al. 2003) or use a linear interpolation of these scores (Koehn et al. 2003).
21:185	In this paper we investigate a different approach with takes the information content of words better into account.
22:185	Term weighting based vector models are proposed to encode the translation quality.
23:185	The advantage is that term weights, such as tf.idf, are useful to model the informativeness of words.
24:185	Highly informative content words usually have high tf.idf scores.
25:185	In information retrieval this has been successfully applied to capture the relevance of a document to a query, by representing both query and documents as term weight vectors and use for example the cosine distance to calculate the similarity between query vector and document vector.
26:185	The idea now is to consider the source phrase as a query, and the different target phrases extracted from the bilingual corpus as translation candidates as a relevant documents.
27:185	The cosine distance is then a natural choice to model the translation probability.
28:185	Our approach is to apply term weighting schemes to transform source and target phrases into term vectors.
29:185	Usually content words in both source and target languages will be emphasized by large term weights.
30:185	Thus, good phrase translation pairs will share similar contours, or, to express it in a different way, will be close to each other in the term weight vector space.
31:185	A similarity function is then defined to approximate translation probability in the vector space.
32:185	The paper is structured as follows: in Section 2, our phrase-based statistical machine translation system is introduced; in Section 3, a phrase translation score function based on word translation probabilities is explained, as this will be used as a baseline system; in Section 4, a vector model based on tf.idf is proposed together with two similarity functions; in Section 5, length regularization and smoothing schemes are explained briefly; in Section 6, the translation experiments are presented; and Section 7 concludes with a discussion.
33:185	2 Phrase-based Machine Translation In this section, the phrase-based machine translation system used in the experiments is briefly described: the phrase based translation models and the decoding algorithm, which allows for local word reordering.
34:185	2.1 Translation Model The phrase-based statistical translation systems use not only word-to-word translation, extracted from bilingual data, but also phrase-to phrase translations.
35:185	Different types of extraction approaches have been described in the literature: syntax-based, word-alignment-based, and genuine phrase alignment models.
36:185	The syntax-based approach has the advantage to model the grammar structures using models of more or less structural richness, such as the syntax-based alignment model in (Yamada and Knight, 2001) or the Bilingual Bracketing in (Wu, 1997).
37:185	Popular word-alignment-based approaches usually rely on initial word alignments from the IBM and HMM alignment models (Och and Ney, 2000), from which the phrase pairs are then extracted.
38:185	(Marcu and Wong 2002) and (Zhang et al. 2003) do not rely on word alignment but model directly the phrase alignment.
39:185	Because all statistical machine translation systems search for a globally optimal translation using the language and translation model, a translation probability has to be assigned to each phrase translation pair.
40:185	This score should be meaningful in that better translations have a higher probability assigned to them, and balanced with respect to word translations.
41:185	Bad phrase translations should not win over better word for word translations, only because they are phrases.
42:185	Our focus here is not phrase extraction, but how to estimate a reasonable probability (or score) to better represent the translation quality of the extracted phrase pairs.
43:185	One major problem is that most phrase pairs are seen only several times, even in a very large corpus.
44:185	A reliable and effective estimation approach is explained in section 3, and the proposed models are introduced in section 4.
45:185	In our system, a collection of phrase translations is called a transducer.
46:185	Different phrase extraction methods result in different transducers.
47:185	A manual dictionary can be added to the system as just another transducer.
48:185	Typically, one source phrase is aligned with several candidate target phrases, with a score attached to each candidate representing the translation quality.
49:185	2.2 Decoding Algorithm Given a set of transducers as the translation model (i.e. phrase translation pairs together with the scores of their translation quality), decoding is divided into several steps.
50:185	The first step is to build a lattice by applying the transducers to the input source sentence.
51:185	We start from a lattice, which has as its only path the source sentence.
52:185	Then for each word or sequence of words in the source sentence for which we have an entry in the transducer new edges are generated and inserted into the lattice, spanning over the source phrase.
53:185	One new edge is created for each translation candidate, and the translation score is assigned to this edge.
54:185	The resulting lattice has then all the information available from the translation model.
55:185	The second step is search for a best path through this lattice, but not only based on the translation model scores but applying also the language model.
56:185	We start with an initial special sentence begin hypothesis at the first node in the lattice.
57:185	Hypotheses are then expanded over the edges, applying the language model to the partial translations attached to the edges.
58:185	The following algorithm summarizes the decoding process when not considering word reordering: Current node n, previous node n; edge e Language model state L, L Hypothesis h, h Foreach node n in the lattice Foreach incoming edge e in n phrase = word sequence at e n = FromNode(e) foreach L in n foreach h with LMstate L LMcost = 0.0 foreach word w in phrase LMcost += -log p(w|L) L = NewState(L,w) L = L end Cost= LMcost+TMcost(e) TotalCost=TotalCost(h)+Cost h = (L,e,h,TotalCost) store hin Hypotheses(n,L) The updated hypothesis h at the current node stores the pointer to the previous hypothesis and the edge (labeled with the target phrase) over which it was expanded.
59:185	Thus, at the final step, one can trace back to get the path associated with the minimum cost, i.e. the best hypothesis.
60:185	Other operators such as local word reordering are incorporated into this dynamic programming search (Vogel, 2003).
61:185	3 Phrase Pair Translation Probability As stated in the previous section, one of the major problems is how to assign a reasonable probability for the extracted phrase pair to represent the translation quality.
62:185	Most of the phrase pairs are seen only once or twice in the training data.
63:185	This is especially true for longer phrases.
64:185	Therefore, phrase pair cooccurrence counts collected from the training corpus are not reliable and have little discriminative power.
65:185	In (Vogel et al. 2003) a different estimation approach was proposed.
66:185	Similar as in the IBM models, it is assumed that each source word s i in the source phrase ),,( 21 I ssss L v = is aligned to every target word t j in the target phrase ),,( 21 J tttt L v = with probability )|Pr( ij st. The total phrase translation probability is then calculated according to the following generative model:  = = = I i J j ji tsts 1 1 ))|Pr(()|Pr( v v (1) This is essentially the lexical probability as calculated in the IBM1 alignment model, without considering position alignment probabilities.
67:185	Any statistical translation can be used in (1) to calculate the phrase translation probability.
68:185	However, in our experiment we typically see now significant difference in translation results when using lexicons trained from different alignment models.
69:185	Also Equation (1) was confirmed to be robust and effective in parallel sentence mining from a very large and noisy comparable corpus (Zhao and Vogel, 2002).
70:185	Equation (1) does not explicitly discriminate content words from non-content words.
71:185	As noncontent words such as high frequency functional words tend to occur in nearly every parallel sentence pair, they co-occur with most of the source words in the vocabulary with non-trivial translation probabilities.
72:185	This noise propagates via (1) into the phrase translations probabilities, increasing the chance that non-optimal phrase translation candidates get high probabilities and better translations are often not in the top ranks.
73:185	We propose a vector model to better distinguish between content words and non-content words with the goal to emphasize content words in the translation.
74:185	This model will be used to rescore the phrase translation pairs, and to get a normalized score representing the translation probability.
75:185	4 Vector Model for Phrase Translation Probability Term weighting models such as tf.idf are applied successfully in information retrieval.
76:185	The duality of term frequency (tf) and inverse document frequency (idf), document space and collection space respectively, can smoothly predict the probability of terms being informative (Roelleke, 2003).
77:185	Naturally, tf.idf is suitable to model content words as these words in general have large tf.idf weights.
78:185	4.1 Phrase Pair as Bag-of-Words Our translation model: (transducer, as defined in 2.1), is a collection of phrase translation pairs together with scores representing the translation quality.
79:185	Each phrase translation pair, which can be represented as a triple },{ pts v v , is now converted into a Bag-of-Words D consisting of a collection of both source and target words appearing in the phrase pair, as shown in (2): },,,,,{},{ 2121 JI tttsssDpts LL v v = (2) Given each phrase pair as one document, the whole transducer is a collection of such documents.
80:185	We can calculate tf.idf for each i s and j t, and represent source and target phrases by vectors of s v v and t v v as in Equation (3): },,,{ 21 I ssss wwwv L v = },,,{ 21 J tttt wwwv L v = (3) where i s w and j t w are tf.idf for i s or j t respectively.
81:185	This vector representation can be justified by word co-occurrence considerations.
82:185	As the phrase translation pairs are extracted from parallel sentences, the source words i s and target words j t in the source and target phrases must co-occur in the training data.
83:185	The co-occurring words should share similar term frequency and document frequency statistics.
84:185	Therefore, the vectors s v v and t v v have similar term weight contours corresponding to the co-occurring word pairs.
85:185	So the vector representations of a phrase translation pair can reflect the translation quality.
86:185	In addition, the content words and non-content words are modeled explicitly by using term weights.
87:185	An over-simplified example would be that a rare word in the source language usually translates into a rare word in the target language.
88:185	4.2 Term Weighting Schemes Given the transducer, it is straightforward to calculate term weights for source and target words.
89:185	There are several versions of tf.idf.
90:185	The smooth ones are preferred, because phrase translation pairs are rare events collected from training data.
91:185	The idf model selected is as in Equation (4): ) 5.0 5.0 log( + + = df dfN idf (4) where N is the total number of documents in the transducer, i.e. the total number of translation pairs, and df is the document frequency, i.e. in how many phrase pairs a given word occurs.
92:185	The constant of 0.5 acts as smoothing.
93:185	Because most of the phrases are short, such as 2 to 8 words, the term frequency in the bag of words representation is usually 1, and some times 2.
94:185	This, in general, does not bring much discrimination in representing translation quality.
95:185	The following version of tf is chosen, so that longer target phrases with more words than average will be slightly down-weighted: )(/)(5.15.0 ' vavglenvlentf tf tf vv ++ = (5) where tf is the term frequency, )(vlen v is the length in words of the phrase v v, and )(vavglen v is the average length of source or target phrase calculated from the transducer.
96:185	Again, the values of 0.5 and 1.5 are constants used in IR tasks acting as smoothing.
97:185	Thus after a transducer is extracted from a parallel corpus, tf and df are counted from the collection of the bag-of-words'' phrase alignment representations.
98:185	For each word in the phrase pair translation its tf.idf weight is assigned and the source and target phrase are transformed into vectors as shown in Equation (3).
99:185	These vectors reserve the translation quality information and also model the content and non-content words by the term weighting model of tf.idf.
100:185	4.3 Vector Space Alignment Given the vector representations in Equation (3), a similarity between the two vectors can not directly be calculated.
101:185	The dimensions I and J are not guaranteed to be the same.
102:185	The goal is to transform the source vector into a vector having the same dimensions as the target vector, i.e. to map the source vector into the space of the target vector, so that a similarity distance can be calculated.
103:185	Using the same reasoning as used to motivate Equation (1), it is assumed that every source word i s contributes some probability mass to each target word j t . That is to say, given a term weight for j t, all source term weights are aligned to it with some probability.
104:185	So we can calculate a transformed vector from the source vectors by calculating weights j t a w using a translation lexicon )|Pr( st as in Equation (6):  = = I i sij t a i j wstw 1 )|Pr( (6) Now the target vector and the mapped vector a v v have the same dimensions as shown in (7): },,,{ 21 J t a t a t aa wwwv L v = },,,{ 21 J tttt wwwv L v = (7) 4.4 Similarity Functions As explained in section 4.1, intuitively, if s v and t v is a good translation pair, then the corresponding vectors of a v v and t v v should be similar to each other in the vector space.
105:185	Cosine distance The standard cosine distance is defined as the inner product of the two vectors a v v and t v v normalized by their norms.
106:185	Based on Equation (6), it is easy to derive the similarity as follows: )()( )|( )|( 1 1),( ),( 1 2 1 2 11 11 1 cos     == == == = = = == J j t a J j t J j I i sijt J j I i sijt t t a J j t t a t t at t a t t a t t a j j ij ij j j wsqrtwsqrt wstPw wstPw vv ww vvvv vv vvd (8) where I and J are the length of the source and target phrases; i s w and j t w are term weights for source word and target words; j t a w is the transformed weight mapped from all source words to the target dimension at word j t . BM25 distance TREC tests show that bm25 (Robertson and Walker, 1997) is one of the best-known distance schemes.
107:185	This distance metric is given in Equation (9).
108:185	The constants of 31,, kbk are set to be 1, 1 and 1000 respectively.
109:185	)( )1( )( )1( 3 3 1 1 25 j j j j t a t a J j t t bm wk wk wK wk wd + + + + =  = )5.0/()5.0( ++== jjj ttt dfdfNidfw ))(/)1(( 1 lavgJbkK += (9) where avg(l) is the average target phrase length in words given the same source phrase.
110:185	Our experiments confirmed the bm25 distance is slightly better than the cosine distance, though the difference is not really significant.
111:185	One advantage of bm25 distance is that the set of free parameters 31,, kbk can be tuned to get better performance e.g. via n-fold cross validation.
112:185	4.5 Integrated Translation Score Our goal is to rescore the phrase translation pairs by using additional evidence of the translation quality in the vector space.
113:185	The vector based scores (8) & (9) provide a distinct view of the translation quality in the vector space.
114:185	Equation (1) provides a evidence of the translation quality based on the word alignment probability, and can be assumed to be different from the evidences in vector space.
115:185	Thus, a natural way of integrating them together is a geometric interpolation shown in (10) or equivalently a linear interpolation in the log domain.
116:185	)|(Pr),( 1 int tsstdd vec v vv v   = (10) where ),( std vec v v is the score from the cosine or bm25 vector distance, normalized within [0, 1], like a probability.
117:185	0.1),( =  t vec std v v v The parameter  can be tuned using held-out data.
118:185	In our cross validation experiments 5.0= gave the best performance in most cases.
119:185	Therefore, Equation (10) can be simplified into: )|Pr(),( int tsstdd vec v vv v = (11) The phrase translation score functions in (1) and (11) are non-symmetric.
120:185	This is because the statistical lexicon Pr(s|t) is non-symmetric.
121:185	One can easily re-write all the distances by using Pr(t|s).
122:185	But in our experiments this reverse direction of using Pr(t|s) gives trivially difference.
123:185	So in all the experimental results reported in this paper, the distances defined in (1) and (11) are used.
124:185	5 Length Regularization Phrase pair extraction does not work perfectly and sometimes a short source phrase is aligned to a long target phrase or vice versa.
125:185	Length regularization can be applied to penalize too long or too short candidate translations.
126:185	Similar to the sentence alignment work in (Gale and Church, 1991), the phrase length ratio is assumed to be a Gaussian distribution as given in Equation (12): ) ))(/)(( 5.0exp(),( 2 2    sltl stl v v v v (12) where l(t) is the target sentence length.
127:185	Mean  and variance  can be estimated using a parallel corpus using a Maximum Likelihood criteria.
128:185	The regularized score is the product of (11) and (12).
129:185	6 Experiments Experiments were carried out on the so-called large data track Chinese-English TIDES translation task, using the June 2002 test data.
130:185	The training data used to train the statistical lexicon and to extract the phrase translation pairs was selected from a 120 million word parallel corpus in such a way as to cover the phrases in test sentences.
131:185	The restricted training corpus contained then approximately 10 million words A trigram model was built on 20 million words of general newswire text, using the SRILM toolkit (Stolcke, 2002).
132:185	Decoding was carried out as described in section 2.2.
133:185	The test data consists of 878 Chinese sentences or 24,337 words after word segmentation.
134:185	There are four human translations per Chinese sentence as references.
135:185	Both NIST score and Bleu score (in percentage) are reported for adequacy and fluency aspects of the translation quality.
136:185	6.1 Transducers Four transducers were used in our experiments: LDC, BiBr, HMM, and ISA.
137:185	LDC was built from the LDC Chinese-English dictionary in two steps: first, morphological variations are created.
138:185	For nouns and noun phrases plural forms and entries with definite and indefinite determiners were generated.
139:185	For verbs additional word forms with -s -ed and -ing were generated, and the infinitive form with 'to'.
140:185	Second, a large monolingual English corpus was used to filter out the new word forms.
141:185	If they did not appear in the corpus, the new entries were not added to the transducer (Vogel, 2004).
142:185	BiBr extracts sub-tree mappings from Bilingual Bracketing alignments (Wu, 1997); HMM extracts partial path mappings from the Viterbi path in the Hidden Markov Model alignments (Vogel et.
143:185	al., 1996).
144:185	ISA is an integrated segmentation and alignment for phrases (Zhang et.al, 2003), which is an extension of (Marcu and Wong, 2002).
145:185	LDC BiBr HMM ISA )(KN 425K 137K 349K 263K )/( srctgt llavg 1.80 1.11 1.09 1.20 Table-1 statistics of transducers Table-1 shows some statistics of the four transducers extracted for the translation task.
146:185	N is the total number of phrase pairs in the transducer.
147:185	LDC is the largest one having 425K entries, as the other transducers are restricted to useful entries, i.e. those translation pairs where the source phrase matches a sequence of words in one of the test sentence.
148:185	Notice that the LDC dictionary has a large number of long translations, leading to a high source to target length ratio.
149:185	6.2 Cosine vs BM25 The normalized cosine and bm25 distances defined in (8) and (9) respectively, are plugged into (11) to calculate the translation probabilities.
150:185	Initial experiments are reported on the LDC transducer, which gives already a good translation, and therefore allows for fast and yet meaningful experimentation.
151:185	Four baselines (Uniform, Base-m1, Base-m4, and Base-m4S) are presented in Table-2.
152:185	NIST Bleu Uniform 6.69 13.82 Base-m1 7.08 14.84 Base-m4 7.04 14.91 Base-m4S 6.91 14.44 cosine 7.17 15.30 bm25 7.19 15.51 bm25-len 7.21 15.64 Table-2 Comparisons of different score functions In the first uniform probabilities are assigned to each phrase pair in the transducer.
153:185	The second one (Base-m1) is using Equation (1) with a statistical lexicon trained using IBM Model-1, and Base-m4 is using the lexicon from IBM Model-4.
154:185	Base-m4S is using IBM Model-4, but we skipped 194 high frequency English stop words in the calculation of Equation (1).
155:185	Table-2 shows that the translation score defined by Equation (1) is much better than a uniform model, as expected.
156:185	Base-m4 is slightly worse than Base-m1.on NIST score, but slightly better using the Bleu metric.
157:185	Both differences are not statistically significant.
158:185	The result for Base-m4S shows that skipping English stop words in Equation (1) gives a disadvantage.
159:185	One reason is that skipping ignores too much nontrivial statistics from parallel corpus especially for short phrases.
160:185	These high frequency words actually account already for more than 40% of the tokens in the corpus.
161:185	Using the vector model, both with the cosine cos d and the bm25 25bm d distance, is significantly better than Base-m1 and Base-m4 models, which confirms our intuition of the vector model as an additional useful evidence for translation quality.
162:185	The length regularization (12) helps only slightly for LDC.
163:185	Since bm25s parameters could be tuned for potentially better performance, we selected bm25 with length regularization as the model tested in further experiments.
164:185	A full-loaded system is tested using the LM020 with and without word-reordering in decoding.
165:185	The results are presented in Table-3.
166:185	Table-3 shows consistent improvements on all configurations: the individual transducers, combinations of transducers, and different decoder settings of word-reordering.
167:185	Because each phrase pair is treated as a bag-of-words, the grammar structure is not well represented in the vector model.
168:185	Thus our model is more tuned towards the adequacy aspect, corresponding to NIST score improvement.
169:185	Because the transducers of BiBr, HMM, and ISA are extracted from the same training data, they have significant overlaps with each other.
170:185	This is why we observe only small improvements when adding more transducers.
171:185	The final NIST score of the full system is 8.24, and the Bleu score is 22.37.
172:185	This corresponds to 3.1% and 11.8% relative improvements over the baseline.
173:185	These improvements are statistically significant according to a previous study (Zhang et.al., 2004), which shows that a 2% improvement in NIST score and a 5% improvement in Bleu score is significant for our translation system on the June 2002 test data.
174:185	6.3 Mean Reciprocal Rank To further investigate the effects of the rescoring function in (11), Mean Reciprocal Rank (MRR) experiments were carried out.
175:185	MRR for a labeled set is the mean of the reciprocal rank of the individual phrase pair, at which the best candidate translation is found (Kantor and Voorhees, 1996).
176:185	Totally 9,641 phrase pairs were selected containing 216 distinct source phrases.
177:185	Each source phrase was labeled with its best translation candidate without ambiguity.
178:185	The rank of the labeled candidate is calculated according to translation scores.
179:185	The results are shown in Table-4.
180:185	baseline cosine bm25 MRR 0.40 0.58 0.75 Table-4 Mean Reciprocal Rank The rescore functions improve the MRR from 0.40 to 0.58 using cosine distance, and to 0.75 using bm25.
181:185	This confirms our intuitions that good translation candidates move up in the rank after the rescoring.
182:185	Decoder settings without word reordering with word reordering baseline bm25 baseline bm25 Scores (%) NIST Bleu NIST Bleu NIST Bleu NIST Bleu LDC 7.08 14.84 7.21 15.64 7.13 15.10 7.26 15.98 LDC+ISA 7.73 19.60 7.99 19.58 7.86 20.80 8.13 20.93 LDC+ISA+HMM 7.86 19.08 8.14 20.70 7.95 19.84 8.19 21.60 LDC+ISA+HMM+BiBr 7.87 19.23 8.14 21.48 7.99 20.01 8.24 22.37 Table-3 Translation using bm25 rescore function with different decoder settings 7 Conclusion and Discussion In this work, we proposed a way of using term weight based models in a vector space as additional evidences for translation quality, and integrated the model into an existing phrase-based statistical machine translation system.
183:185	The model shows significant improvements when using it to score a manual dictionary as well as when using different phrase transducers or a combination of all available translation information.
184:185	Additional experiments also confirmed the effectiveness of the proposed model in terms of of improved Mean Reciprocal Rank of good translations.
185:185	Our future work is to explore alternatives such as the reranking work in (Collins, 2002) and include more knowledge such as syntax information in rescoring the phrase translation pairs.

