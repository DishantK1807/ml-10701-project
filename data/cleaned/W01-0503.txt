Learning Within-Sentence Semantic Coherence by Elena Eneva, Rose Hoberman, and Lucian Lita References C.
Cai, R.
Rosenfeld, and L.
Wasserman. 2000.
Exponential language models, logistic regression, and semantic coherence.
Proc. of NIST/DARPA Speech Transcription Workshop.
I. Dagan, S.
Marcus, and S.
Markovitch. 1995.
Contextual word similarity and estimation from sparse data.
Computer Speech and Language.
Reinhard Kneser and Hermann Ney.
1995. Improved backing-off for m-gram language modeling.
Proc. of IEEE International Conference on Acoustics, Speech, and Signal Processing.
S. Della Pietra, V.
Della Pietra, and J.
Lafferty. 1997.
Inducing features of random fields.
IEEE Transactions on Pattern Analysis and Machine Intelligence.
J. R.
Quinlan. 1993.
C4.5: Programs for Machine Learning.
Morgan Kaufmann, San Mateo, CA.
Ronald Rosenfeld, Stanley F.
Chen, and Xiaojin Zhu.
2001. Whole-sentence exponential language models: a vehicle for linguistic-statistical integration.
R. Rosenfeld.
1997. A whole sentence maximum entropy language model.
Proc. of IEEE Workshop on Automated Speech Recognition and Understanding.
Robert E.
Schapire. 1999.
A brief introduction to boosting. IJCAI.

