We therefore follow previous work (e.g., Miltsakaki and Kukich 2000) in assuming
P00-1052
concept-to-text generation (Marciniak and Strube 2005; Barzilay and Lapata 2006),
N06-1046 W05-0618
Riedel, Sebastian and James Clarke. 2006.
P06-1048 W06-1616
of Roark’s (2001) parser. McDonald uses Charniak’s (2000) parser, which performs
J01-2004
Wolf, Florian and Edward Gibson. 2004.
P04-1049
of the grammatical roles in the sentence (Tetreault 2001). We identify grammatical roles
J01-4003
Barzilay, Regina and Mirella Lapata. 2006.
N06-1046
structure. Inspired by Centering Theory (Grosz, Weinstein, and Joshi 1995)—a theory
J95-2003
readable (Barzilay and Lapata 2008).
J08-1001
erative approaches (Knight and Marcu 2002; Turner and Charniak 2005; Galley and
P05-1036
repetitive (Mani, Gates, and Bloedorn 1999).
E99-1011 P99-1072
from a parse tree (Riezler et al. 2003; Nguyen et al. 2004; McDonald 2006; Clarke and
C04-1107 E06-1038 N03-1026
(Morris and Hirst 1991). Computing global discourse structure robustly and accurately
J91-1002
Barzilay, R. and M. Elhadad. 1997. Using
W97-0703
Barzilay and Elhadad’s (1997) scoring function aims to identify sentences (for inclusion
W97-0703
McDonald (2006) formalizes sentence compression as a classification task in a dis-
E06-1038
Tetreault, Joel R. 2001. A corpus-based
J01-4003
Kibble, Rodger and Richard Power. 2004.
J04-4001
McDonald, Ryan. 2006. Discriminative
E06-1038
Sentence) and McDonald (2006) on a sentence-by-sentence basis. Table 1 shows the
E06-1038
exceptions. Jing (2000) uses information from the local context as evidence for and
A00-1043
mance comparable to syntactic parsers.) Teufel and Moens (2002) identify discourse
J02-4002
Miltsakaki, Eleni and Karen Kukich. 2000.
P00-1052
We follow Grosz, Weinstein, and Joshi (1995) in ranking entities according to their
J95-2003
Martins, Andr´e and Noah A. Smith. 2009.
P09-1039 W09-1801
Lin, Dekang. 2001. LaTaT: Language and text
H01-1046
Jing, Hongyan. 2000. Sentence reduction
A00-1043
(Kupiec, Pedersen, and Chen 1995; Barzilay and Elhadad 1997; Marcu 2000; Teufel and
W97-0703
sions (Knight and Marcu 2002; Turner and Charniak 2005; Galley and McKeown 2007;
N07-1023 P05-1036
sented as a graph (Wolf and Gibson 2004) and sentence importance is determined in
P04-1049
Morris, Jane and Graeme Hirst. 1991. Lexical
J91-1002
compression model with sentence extraction (see Martins and Smith [2009] for an ILP
P09-1039 W09-1801
dependency parsing (Riedel and Clarke 2006; Martins, Smith, and Xing 2009), and
P06-1048 P09-1039 W06-1616 W09-1801
It improves on Barzilay and Elhadad’s (1997) original algorithm by
W97-0703
coreference resolution (Denis and Baldridge 2007).
N07-1030
to Centering Theory (Grosz, Weinstein, and Joshi 1995) and adopt an entity-based
J95-2003
uation measures have also been proposed. Riezler et al. (2003) compare the grammatical
N03-1026
programming (see McDonald [2006] for details).
E06-1038
of discourse, Centering Theory (Grosz, Weinstein, and Joshi 1995) and lexical chains
J95-2003
Teufel, Simone and Marc Moens. 2002.
J02-4002
state-of-the-art compression models (McDonald 2006; Clarke and Lapata 2008).
E06-1038
Clarke, James and Mirella Lapata. 2006.
P06-1048
Denis, Pascal and Jason Baldridge. 2007.
N07-1030
Centering Theory (Grosz, Weinstein, and Joshi 1995) is an entity-orientated theory of
J95-2003
Barzilay and Elhadad (1997) describe a technique for building lexical chains for
W97-0703
Barzilay, Regina and Mirella Lapata. 2008.
J08-1001
more highly ranked than the rest (Grosz, Weinstein, and Joshi 1995). The C
J95-2003
of lexical cohesion as sequences of semantically related words (Morris and Hirst 1991).
J91-1002
Roth, Dan and Wen-tau Yih. 2004. A linear
W04-2401
tion extraction (Roth and Yih 2004), semantic role labeling (Punyakanok et al. 2004),
C04-1197 W04-2401
previous sentence). Unlike Jing (2000) we explicitly identify topically important words
A00-1043
Thompson 1988; Corston-Oliver 1998; Ono, Sumita, and Miike 1994; Carlson et al. 2001)
C94-1056
McDonald’s (2006) (61.0% vs. 60.1%) and slightly shorter than the Sentence ILP model
E06-1038
(61.0% vs. 62.1%). The Discourse ILP model is significantly better than McDonald (2006)
E06-1038
It is important to note that McDonald (2006) is not a straw-man system. It achieves
E06-1038
applications (Scott and de Souza 1990; Kibble and Power 2004). In creating a context-
J04-4001
without increasing the summary length (Lin 2003; Zajic et al. 2007). It could also be
W03-1101
following, and refer the interested reader to McDonald (2006) for details.
E06-1038
Roark, Brian. 2001. Probabilistic top–down
J01-2004
We trained McDonald’s (2006) model on the full training set (48 documents, 962
E06-1038
adigms (e.g., Nguyen et al. 2004; McDonald 2006; Cohn and Lapata 2009). The same
C04-1107 E06-1038
a topical unit (Morris and Hirst 1991). Besides repetition, they also examine semantic
J91-1002
McDonald [2006] for examples).
E06-1038
margin learning (McDonald 2006; Cohn and Lapata 2009). These models are trained
E06-1038
Lin, Chin-Yew. 2003. Improving
W03-1101
heuristics or approximations during decoding (see Turner and Charniak [2005] and
P05-1036
(Morris and Hirst 1991). Both approaches capture local coherence—the way adjacent
J91-1002
model and McDonald (2006).
E06-1038
tropy (Riezler et al. 2003), support vector machines (Nguyen et al. 2004), and large-
C04-1107 N03-1026
other parser with broadly similar output (e.g., Lin 2001) could also serve our purposes.
H01-1046
McDonald [2006]) on an equal footing, we ensured that their compression rates were
E06-1038
tween adjacent utterances. Grosz, Weinstein, and Joshi (1995) distinguish between three
J95-2003
and Elhadad 1997; Marcu 2000; Teufel and Moens 2002) and other text generation
J02-4002
