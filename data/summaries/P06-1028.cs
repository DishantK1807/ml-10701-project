These tasks are generally treated as sequential labeling problems incorporating the IOB tagging scheme (Ramshaw and Marcus, 1995).
W95-0107
The details of actual optimization procedures for linear chain CRFs, which are typical CRF applications, have already been reported (Sha and Pereira, 2003).
N03-1028
We need to define a segment-wise loss, in contrast to the standard CRF loss, which is sometimes referred to as an (entire) sequential loss (Kakade et al., 2002; Altun et al., 2003).
W03-1019
Methods and Parameters For ML and MAP, we performed exactly the same training procedure described in (Sha and Pereira, 2003) with L-BFGS optimization.
N03-1028
Moreover, an F-score optimization method for logistic regression has also been proposed (Jansche, 2005).
H05-1087
As regards the basic feature set for Chunking, we followed (Kudo and Matsumoto, 2001), which is the same feature set that provided the best result in CoNLL-2000.
N01-1025
The modeling power of CRFs has been of great benefit in several applications, such as shallow parsing (Sha and Pereira, 2003) and information extraction (McCallum and Li, 2003).
N03-1028 W03-0430
Following the definitions of (Sha and Pereira, 2003), a log-linear combination of weighted features, Φc(y,x; λ) = exp(λ fc(y,x)), is used as individual potential functions, where fc represents a feature vector obtained from the corresponding clique c.
N03-1028
3 with a variant of the forwardbackward algorithm (Sha and Pereira, 2003).
N03-1028
With Chunking, (Kudo and Matsumoto, 2001) reported the best F-score of 93.91 with the voting of several models trained by Support Vector Machine in the same experimental settings and with the same feature set.
N01-1025
The maximum a posteriori (MAP) criterion over parameters, λ, given x and y is the natural choice for reducing over-fitting (Sha and Pereira, 2003).
N03-1028
Several non-linear objective functions, such as F-score for text classification (Gao et al., 2003), and BLEU-score and some other evaluation measures for statistical machine translation (Och, 2003), have been introduced with reference to the framework of MCE criterion training.
P03-1021
5.2. However, MCE-F showed the better performance of 85.29 compared with (McCallum and Li, 2003) of 84.04, which used the MAP training of CRFs with a feature selection architecture, yielding similar results to the MAP results described here.
W03-0430
This point-wise discriminant function is different from that described in (Kakade et al., 2002; Altun et al., 2003), which is calculated based on marginals.
W03-1019
12, by using the variant of the forward-backward and Viterbi algorithm described in (Sha and Pereira, 2003).
N03-1028
Och. 2003.
P03-1021
Work Various loss functions have been proposed for designing CRFs (Kakade et al., 2002; Altun et al., 2003).
W03-1019
