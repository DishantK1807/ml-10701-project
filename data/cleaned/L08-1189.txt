<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>J Atserias</author>
<author>L Villarejo</author>
<author>G Rigau</author>
<author>E Agirre</author>
<author>J Carroll</author>
<author>B Magnini</author>
<author>P Vossen</author>
</authors>
<title>The meaning multilingual central repository. In</title>
<date>2004</date>
<booktitle>In Proceedings of GWC</booktitle>
<location>Brno, Czech Republic</location>
<contexts>
<context> the chosen knowledge base. The method is general, in the sense that it is not tied to any particular knowledge base, but in this work we have applied it to the Multilingual Central Repository (MCR, (Atserias et al., 2004)). The evaluation has been performed on the Senseval-3 all-words task (Snyder and Palmer, 2004). The main contributions of the paper are twofold: (1) We have evaluated the separate and combined perfo</context>
<context>forming unsupervised WSD. The method is general, in the sense that it is not tied to any particular lexical resource, but in this work we have applied it to the Multilingual Central Repository (MCR, (Atserias et al., 2004)). The MCR is based on the EuroWordNet architecture to represent tightly connected wordnets for various languages. It also contains thousands of automatically acquired relations. The main goal of thi</context>
<context>d Wi the associated concept in Synsetsi which has maximum rank (in case of ties, we assign all the concepts with maximum rank). 3. Multilingual Central Repository The Multilingual Central Repository (Atserias et al., 2004) is a lexical knowledge base built within the MEANING project1, and acts as a multilingual interface for integrating and distributing all the knowledge acquired in the project. The MCR constitutes a </context>
<context>ending on treatment of multiwords and hyphenated words. GAMBL: best supervised system. any particular knowledge base, but in this work we have applied it to the Multilingual Central Repository (MCR, (Atserias et al., 2004)). The evaluation has been performed on the Senseval-3 allwords task (Snyder and Palmer, 2004). One of the contributions of this paper is that we have evaluated the separate and combined performance </context>
</contexts>
<marker>Atserias, Villarejo, Rigau, Agirre, Carroll, Magnini, Vossen, 2004</marker>
<rawString>J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Carroll, B. Magnini, and P. Vossen. 2004. The meaning multilingual central repository. In In Proceedings of GWC, Brno, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brin</author>
<author>L Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual web search engine</title>
<date>1998</date>
<journal>Computer Networks and ISDN Systems</journal>
<pages>30--1</pages>
<contexts>
<context>the most relevant concepts and relations in the MCR for the particular input context. Exploiting structural properties of the disambiguation graph Once the GD graph is built, we compute the PageRank (Brin and Page, 1998) algorithm over it. The intuition behind this step is that the vertices representing the correct synsets will be more relevant inGD than the rest of the possible synsets of the context words, which s</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>S. Brin and L. Page. 1998. The anatomy of a large-scale hypertextual web search engine. Computer Networks and ISDN Systems, 30(1-7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Daude</author>
<author>L Padro</author>
<author>G Rigau</author>
</authors>
<title>Mapping WordNets using structural information</title>
<date>2000</date>
<booktitle>In 38th Anual Meeting of the Association for Computational Linguistics (ACL’2000</booktitle>
<location>Hong Kong</location>
<contexts>
<context>he resources integrated on it where built linked to that version of WordNet. Unfortunately new versions of WordNet have been produced in later years, and although automatic mapping technology exists (Daude et al., 2000) and porting the relations among versions is possible, some errors are introduced, as we will see. In this work we have used the following relations from the MCR: WN1.6: English WordNet 1.6 synsets a</context>
</contexts>
<marker>Daude, Padro, Rigau, 2000</marker>
<rawString>J. Daude, L. Padro, and G. Rigau. 2000. Mapping WordNets using structural information. In 38th Anual Meeting of the Association for Computational Linguistics (ACL’2000), Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database</title>
<date>1998</date>
<publisher>MIT Press</publisher>
<contexts>
<context>ion of the MCR contains more than 1;500;000 semantic relations between synsets, most of them acquired by automatic means. This represents almost seven times more relations than the Princeton WordNet (Fellbaum, 1998) (235;402 unique semantic relations in WN 3.0). The MCR follows the model proposed by the EWN project, whose architecture includes the Inter-Lingual-Index (ILI), a Domain ontology and a Top Concept o</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Harabagiu</author>
<author>D I Moldovan</author>
</authors>
<title>Enriching the wordnet taxonomy with contextual knowledge acquired from text</title>
<date>2000</date>
<pages>301--333</pages>
<contexts>
<context>Spanish wordnets; (iii) 1http://nipadio.lsi.upc.es/nlp/meaning Large collections of semantic preferences, acquired both from SemCor and from BNC; (iv) disambiguated glosses from the eXtended WordNet (Harabagiu and Moldovan, 2000); (v) Instances, including named entities. The MCR is mainly based on WordNet 1.6. The reason is that most of the resources integrated on it where built linked to that version of WordNet. Unfortunate</context>
</contexts>
<marker>Harabagiu, Moldovan, 2000</marker>
<rawString>S. M. Harabagiu and D. I. Moldovan. 2000. Enriching the wordnet taxonomy with contextual knowledge acquired from text. pages 301–333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone</title>
<date>1986</date>
<booktitle>In SIGDOC ’86: Proceedings of the 5th annual international conference on Systems documentation</booktitle>
<pages>24--26</pages>
<publisher>ACM</publisher>
<location>New York, NY, USA</location>
<contexts>
<context>ign a sense to an ambiguous word by comparing each of its senses with those of the surrounding context. Typically, some semantic similarity metric is used for calculating the relatedness among senses(Lesk, 1986; McCarthy et al., 2004). One of the major drawbacks on these systems comes from the fact that the WSD process is done in a word-by-word basis, i.e., the words are disambiguated individually. Recently</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>M. Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In SIGDOC ’86: Proceedings of the 5th annual international conference on Systems documentation, pages 24–26, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
<author>R Koeling</author>
<author>J Weeds</author>
<author>J Carroll</author>
</authors>
<title>Finding predominant word senses in untagged text</title>
<date>2004</date>
<booktitle>In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics</booktitle>
<pages>279</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics</institution>
<location>Morristown, NJ, USA</location>
<contexts>
<context>to an ambiguous word by comparing each of its senses with those of the surrounding context. Typically, some semantic similarity metric is used for calculating the relatedness among senses(Lesk, 1986; McCarthy et al., 2004). One of the major drawbacks on these systems comes from the fact that the WSD process is done in a word-by-word basis, i.e., the words are disambiguated individually. Recently, graph-based methods f</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2004. Finding predominant word senses in untagged text. In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 279, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
</authors>
<title>Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling</title>
<date>2005</date>
<booktitle>In Proceedings of HLT05</booktitle>
<location>Morristown, NJ, USA</location>
<contexts>
<context>., the words are disambiguated individually. Recently, graph-based methods for knowledge-based WSD have gained much attention in the NLP community (Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Mihalcea, 2005). These methods use well-known graph-based techniques to find and exploit the structural properties of the graph underlying the LKB. Because the graph is analyzed as a whole, these techniques can hel</context>
<context>6 introduces considerable noise. 5. Comparison to related work We have compared our results with similar work which also use graph techniques for performing unsupervised, knowledge based WSD, namely (Mihalcea, 2005; Sinha and Mihalcea, 2007; Navigli and Lapata, 2007). Table 3 shows a comparison of their results with ours. We only depict the results of out best unsupervised experiment M17, as the rest of the sys</context>
<context>ord is also included, as well as the result of the best system of Senseval-3 all word task (GAMBL). Note that GAMBL is a supervised system which learns from Semcor information. The TexRank algorithm (Mihalcea, 2005) for WSD creates a complete weighted graph (e.g. a graph where every pair of distinct vertices is connected by an weighted edge) formed by the synsets of the words in the input context. The weight of</context>
<context>ty algorimth ranks yields the results on row Sin07 on Table 3. Here, the Senseval-3 corpus was used as a development data set, and we can thus see the results as the upperbound of their method. Both (Mihalcea, 2005) and (Sinha and Mihalcea, 2007) use WordNet as a LKB, but neither of them specify which particular version did they use. We can see in Table 3 that our method clearly outperforms both Mih05 and Sin07</context>
<context>see that storing intermediate LKB nodes from the shortest paths between synsets adds useful information when performing WSD. The results of various inhouse made experiments with complete graphs a-la (Mihalcea, 2005) also confirm this observation. Note also that our method is simpler than the combination strategy used in (Sinha and Mihalcea, 2007), and that we did not perform any parameter tuning as they did. Th</context>
</contexts>
<marker>Mihalcea, 2005</marker>
<rawString>R. Mihalcea. 2005. Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling. In Proceedings of HLT05, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>C Leacock</author>
<author>R Tengi</author>
<author>R Bunker</author>
</authors>
<title>A semantic concordance</title>
<date>1993</date>
<booktitle>In Proc. of the ARPA HLT workshop</booktitle>
<contexts>
<context>lver and normal) sPref: Selectional preference relations sCooc: Coocurrence relations The latter two types of relations (sPref and sCooc) are extracted from Semcor, a semantically hand-tagged corpus (Miller et al., 1993). They are thus essentially different from the other relations of the MCR; because they are extracted from a hand-tagged corpus, the system benefits from a “supervised” kind of information when explo</context>
</contexts>
<marker>Miller, Leacock, Tengi, Bunker, 1993</marker>
<rawString>G.A. Miller, C. Leacock, R. Tengi, and R.Bunker. 1993. A semantic concordance. In Proc. of the ARPA HLT workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
<author>M Lapata</author>
</authors>
<title>Graph connectivity measures for unsupervised word sense disambiguation</title>
<date>2007</date>
<booktitle>In IJCAI</booktitle>
<contexts>
<context> a word-by-word basis, i.e., the words are disambiguated individually. Recently, graph-based methods for knowledge-based WSD have gained much attention in the NLP community (Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Mihalcea, 2005). These methods use well-known graph-based techniques to find and exploit the structural properties of the graph underlying the LKB. Because the graph is analyzed as a whole, these te</context>
<context>ison to related work We have compared our results with similar work which also use graph techniques for performing unsupervised, knowledge based WSD, namely (Mihalcea, 2005; Sinha and Mihalcea, 2007; Navigli and Lapata, 2007). Table 3 shows a comparison of their results with ours. We only depict the results of out best unsupervised experiment M17, as the rest of the systems are also of unsupervised nature. A baseline whi</context>
<context> that our method is simpler than the combination strategy used in (Sinha and Mihalcea, 2007), and that we did not perform any parameter tuning as they did. The work presented here is very similar to (Navigli and Lapata, 2007). They also perform a two stage process for WSD, the first one consisting on the extraction of a subgraph from the LKB which better suits the concepts involved in a particular input context. Then, th</context>
<context>ity algorithms deciding the relevance of the nodes on the subgraph. The algorithm which performs best yields the results shown in row Nav07 on table 3. The main difference between the method used in (Navigli and Lapata, 2007) and our method lies on the initial method for extracting the context subgraph. Whereas we rely on shortest paths between word synsets, they apply a depthfirst search algorithm over the LKB graph, an</context>
</contexts>
<marker>Navigli, Lapata, 2007</marker>
<rawString>R. Navigli and M. Lapata. 2007. Graph connectivity measures for unsupervised word sense disambiguation. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
<author>P Velardi</author>
</authors>
<title>Structural semantic interconnections: A knowledge-based approach to word sense disambiguation</title>
<date>2005</date>
<journal>IEEE Trans. Pattern Anal. Mach. Intell</journal>
<volume>27</volume>
<contexts>
<context>thfirst search algorithm over the LKB graph, and restrict the deep of the subtree to a value of 3. As for the underlying LKB, they use WordNet 2.0 enriched with several new relations as described in (Navigli and Velardi, 2005). Unfortunately, those new relations are not publicly available, so we can’t perform a direct comparison. Navigli and Lapata don’t report overall results and therefore, we can’t directly compare our </context>
</contexts>
<marker>Navigli, Velardi, 2005</marker>
<rawString>R. Navigli and P. Velardi. 2005. Structural semantic interconnections: A knowledge-based approach to word sense disambiguation. IEEE Trans. Pattern Anal. Mach. Intell., 27(7):1075–1086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sinha</author>
<author>R Mihalcea</author>
</authors>
<title>Unsupervised graphbased word sense disambiguation using measures of word semantic similarity</title>
<date>2007</date>
<booktitle>In Proceedings of the IEEE International Conference on Semantic Computing (ICSC 2007</booktitle>
<location>Irvine, CA, USA</location>
<contexts>
<context>the WSD process is done in a word-by-word basis, i.e., the words are disambiguated individually. Recently, graph-based methods for knowledge-based WSD have gained much attention in the NLP community (Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Mihalcea, 2005). These methods use well-known graph-based techniques to find and exploit the structural properties of the graph underlying the LKB. Because the graph is ana</context>
<context>siderable noise. 5. Comparison to related work We have compared our results with similar work which also use graph techniques for performing unsupervised, knowledge based WSD, namely (Mihalcea, 2005; Sinha and Mihalcea, 2007; Navigli and Lapata, 2007). Table 3 shows a comparison of their results with ours. We only depict the results of out best unsupervised experiment M17, as the rest of the systems are also of unsupervi</context>
<context>Once the complete graph is built, PageRank algorithm is executed over it and words are assigned to the most relevant synset. Their results on Senseval-3 dataset are depicted in row Mih05 on Table 3. (Sinha and Mihalcea, 2007) extends the previous work by using a collection of semantic similarity measures when assigning a weight to the links across synsets. They also compare different graph-based centrality algorithms to </context>
<context>elds the results on row Sin07 on Table 3. Here, the Senseval-3 corpus was used as a development data set, and we can thus see the results as the upperbound of their method. Both (Mihalcea, 2005) and (Sinha and Mihalcea, 2007) use WordNet as a LKB, but neither of them specify which particular version did they use. We can see in Table 3 that our method clearly outperforms both Mih05 and Sin07, which indicates that the init</context>
<context>. The results of various inhouse made experiments with complete graphs a-la (Mihalcea, 2005) also confirm this observation. Note also that our method is simpler than the combination strategy used in (Sinha and Mihalcea, 2007), and that we did not perform any parameter tuning as they did. The work presented here is very similar to (Navigli and Lapata, 2007). They also perform a two stage process for WSD, the first one con</context>
</contexts>
<marker>Sinha, Mihalcea, 2007</marker>
<rawString>R. Sinha and R. Mihalcea. 2007. Unsupervised graphbased word sense disambiguation using measures of word semantic similarity. In Proceedings of the IEEE International Conference on Semantic Computing (ICSC 2007), Irvine, CA, USA.</rawString>
</citation>
</citationList>
</algorithm>

