<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Vibhu O Mittal Banko</author>
<author>Michael J Witbrock</author>
</authors>
<title>Applying support vector machines to imbalanced datasets</title>
<date>2004</date>
<booktitle>In Proceedings of ECML 2004</booktitle>
<pages>39--50</pages>
<publisher>Michele</publisher>
<marker>Banko, Witbrock, 2004</marker>
<rawString>2004. Applying support vector machines to imbalanced datasets. In Proceedings of ECML 2004, pages 39–50. Michele Banko, Vibhu O. Mittal, and Michael J. Witbrock.</rawString>
</citation>
<citation valid="true">
<title>Headline generation based on statistical translation</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics</booktitle>
<pages>318--325</pages>
<marker>2000</marker>
<rawString>2000. Headline generation based on statistical translation. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pages 318–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Sentence fusion for multidocument news summarization</title>
<date>2005</date>
<journal>Computational Linguistics</journal>
<volume>31</volume>
<contexts>
<context>as also discussed in an overview of DUC-2005 (Dang, 2005). Producing human-like summaries would seem to require radical advances in both text comprehension and generation. On the contrary, as others (Barzilay and McKeown, 2005; Soricut and Marcu, 2006), we believe it is possible to produce more human-like summaries via textto-text generation techniques. In this paper, we present our approach to generating quasi-abstractive</context>
<context>t use the patterns found during the decomposition process to predict what fragments shall be used to compose a sentence in the summary, as we do here. Our approach is very similar to Sentence Fusion (Barzilay and McKeown, 2005): a new sentence is generated by finding the common theme from a set of similar sentences coming from different documents and computing how the sentences can be fused, based on their dependency tree </context>
</contexts>
<marker>Barzilay, McKeown, 2005</marker>
<rawString>Regina Barzilay and Kathleen R. McKeown. 2005. Sentence fusion for multidocument news summarization. Computational Linguistics, 31(3):297–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Borgelt</author>
</authors>
<title>A naive bayes classifier plug-in for dataengine</title>
<date>1999</date>
<booktitle>In Proceedings of the 3rd Data Analysis Symposium</booktitle>
<pages>87--90</pages>
<contexts>
<context>a. The augmented data set was input to three classifiers: Na¨ıve Bayes (NB), Support Vector Machines (SVMs) and decision trees (DTs). We used three freely available packages, Borgelt’s NB classifier (Borgelt, 1999), SVM-light (Joachims, 2002) and YaDT (Ruggieri, 2004) respectively. We ran three way cross validation, where in each run the classifier is trained on 2/3 of the data and tested on the remaining thir</context>
</contexts>
<marker>Borgelt, 1999</marker>
<rawString>Christian Borgelt. 1999. A naive bayes classifier plug-in for dataengine. In Proceedings of the 3rd Data Analysis Symposium, pages 87–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitesh V Chawla</author>
<author>Kevin W Bowyer</author>
<author>Lawrence O Hall</author>
<author>W Philip Kegelmeyer</author>
</authors>
<title>Smote: Synthetic minority over-sampling technique</title>
<date>2002</date>
<journal>Journal of Artificial Intelligence and Research</journal>
<pages>16--321</pages>
<contexts>
<context>Chawla et al., 2004) have been devoted to this problem. A common technique to address this problem in machine learning is to replicate instances of the minority class to reduce the unfair preference (Chawla et al., 2002; Japkowicz, 2000a; Akbani et al., 2004). This is the approach we followed as well, hence, we replicated the instances in the minority class to reduce the unfair preference. In the experiments reporte</context>
</contexts>
<marker>Chawla, Bowyer, Hall, Kegelmeyer, 2002</marker>
<rawString>Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall, and W. Philip Kegelmeyer. 2002. Smote: Synthetic minority over-sampling technique. Journal of Artificial Intelligence and Research, 16:321–357.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitesh V Chawla</author>
</authors>
<title>Nathalie Japkowicz, and Aleksander Kotcz</title>
<date>2003</date>
<booktitle>Proceedings of the ICML’2003 Workshop on Learning from Imbalanced Data Sets</booktitle>
<location>editors</location>
<marker>Chawla, 2003</marker>
<rawString>Nitesh V. Chawla, Nathalie Japkowicz, and Aleksander Kotcz, editors. 2003. Proceedings of the ICML’2003 Workshop on Learning from Imbalanced Data Sets.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitesh V Chawla</author>
<author>Nathalie Japkowicz</author>
<author>Aleksander Kotcz</author>
</authors>
<title>Editorial: special issue on learning from imbalanced data sets</title>
<date>2004</date>
<journal>SIGKDD Explorations Newsletter</journal>
<volume>6</volume>
<contexts>
<context>ed data sets cause the dominant class of the target features to be favored. Two whole workshops (Japkowicz, 2000b; Chawla et al., 2003) and a full special issue of the SIGKDD Explorations Newsletter (Chawla et al., 2004) have been devoted to this problem. A common technique to address this problem in machine learning is to replicate instances of the minority class to reduce the unfair preference (Chawla et al., 2002</context>
</contexts>
<marker>Chawla, Japkowicz, Kotcz, 2004</marker>
<rawString>Nitesh V. Chawla, Nathalie Japkowicz, and Aleksander Kotcz. 2004. Editorial: special issue on learning from imbalanced data sets. SIGKDD Explorations Newsletter, 6(1):1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa Trang Dang</author>
</authors>
<title>Overview of duc 2005</title>
<date>2005</date>
<booktitle>In Proceedings of the Document Understanding Conference (DUC05</booktitle>
<contexts>
<context>ion systems already show impressive results, state-of-the-art has not progressed beyond summaries composed of whole sentences extracted from the sources, as also discussed in an overview of DUC-2005 (Dang, 2005). Producing human-like summaries would seem to require radical advances in both text comprehension and generation. On the contrary, as others (Barzilay and McKeown, 2005; Soricut and Marcu, 2006), we</context>
</contexts>
<marker>Dang, 2005</marker>
<rawString>Hoa Trang Dang. 2005. Overview of duc 2005. In Proceedings of the Document Understanding Conference (DUC05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Domingos</author>
</authors>
<title>Metacost: a general method for making classifiers cost-sensitive</title>
<date>1999</date>
<booktitle>In Proceedings of the Fifth ACM SIGKDD</booktitle>
<pages>155--164</pages>
<publisher>ACM Press</publisher>
<location>New York, NY, USA</location>
<contexts>
<context>ng additional methods for each of the four substeps in our approach. For example, in step 1b, to obtain a better CSS’s classifier, we are investigating other ways, such as cost-sensitive classifiers (Domingos, 1999; Ting, 2002; Turney, 2000), to improve the performance of classifier without changing the class distribution. We are also investigating the features used for classification in step 1b, by considering</context>
</contexts>
<marker>Domingos, 1999</marker>
<rawString>Pedro Domingos. 1999. Metacost: a general method for making classifiers cost-sensitive. In Proceedings of the Fifth ACM SIGKDD, pages 155–164, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Elhadad</author>
</authors>
<title>FUF user manual version 5.0</title>
<date>1991</date>
<tech>Technical Report CUCS-038-91</tech>
<contexts>
<context>kind of information that generic NLG systems require as input (Soricut and Marcu, 2005), e.g., deep subject-verb or verb-object relations as required by Penman (Matthiessen and Bateman, 1991) or FUF (Elhadad, 1991), or the shallow syntactic relations needed by HALogen (Langkilde, 1998). We present each of our methods in turn. 4.2.1. SQa realization The SQa method uses n-gram probabilities to generate the actua</context>
</contexts>
<marker>Elhadad, 1991</marker>
<rawString>Michael Elhadad. 1991. FUF user manual version 5.0. Technical Report CUCS-038-91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathalie Japkowicz</author>
</authors>
<title>The class imbalance problem: Significance and strategies</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 International Conference on Artificial Intelligence (ICAI’2000</booktitle>
<volume>1</volume>
<pages>111--117</pages>
<contexts>
<context>stances is extremely imbalanced, since the positive instances only amount to 0.398%. Extremely imbalanced data sets cause the dominant class of the target features to be favored. Two whole workshops (Japkowicz, 2000b; Chawla et al., 2003) and a full special issue of the SIGKDD Explorations Newsletter (Chawla et al., 2004) have been devoted to this problem. A common technique to address this problem in machine le</context>
</contexts>
<marker>Japkowicz, 2000</marker>
<rawString>Nathalie Japkowicz. 2000a. The class imbalance problem: Significance and strategies. In Proceedings of the 2000 International Conference on Artificial Intelligence (ICAI’2000), volume 1, pages 111–117.</rawString>
</citation>
<citation valid="false">
<booktitle>2000b. Proceedings of the AAAI’2000 Workshop on Learning from Imbalanced Data Sets, AAAI Tech Report WS-00-05</booktitle>
<editor>Nathalie Japkowicz, editor</editor>
<publisher>AAAI</publisher>
<marker></marker>
<rawString>Nathalie Japkowicz, editor. 2000b. Proceedings of the AAAI’2000 Workshop on Learning from Imbalanced Data Sets, AAAI Tech Report WS-00-05. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
</authors>
<title>Using hidden markov modeling to decompose human-written summaries</title>
<date>2002</date>
<journal>Computational Linguistics</journal>
<volume>28</volume>
<contexts>
<context>ases. If counted in words, the overlap comprises only 21% of the abstract. However, in general there are many sentences in the document that more closely correspond to the sentences in the abstract. (Jing, 2002) showed that six operations are often seen in human-written abstracts: reduction, combination, syntactic transformation, lexical paraphrasing, generalization or specification, and reordering. Those o</context>
<context> 3.1. Step 1a: Label To train the CSS generator, we first need to create our training data, namely, the CSS’s for our corpus. We build on the detailed study of decomposition of abstract sentences in (Jing, 2002). Jing trained a HMM to identify fragments. We, however, adopt a simpler approach based on string overlap, since our goal is to find phrases which cover a summary sentence as much as possible, not to</context>
<context> plan construction task different knowledge about the domain and the desirable characteristics of the resulting domain plan. Figure 3: An Example Sentence Generated by the QaST Model 6. Related Work (Jing, 2002) is the first study of decomposition of abstract sentences which goes well beyond previous approaches based on sentence alignment (Kupiec et al., 1995; Marcu, 1999; Teufel and Moens, 1997), where an </context>
</contexts>
<marker>Jing, 2002</marker>
<rawString>Hongyan Jing. 2002. Using hidden markov modeling to decompose human-written summaries. Computational Linguistics, 28(4):527–543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Learning to Classify Text using Support Vector Machines</title>
<date>2002</date>
<contexts>
<context>as input to three classifiers: Na¨ıve Bayes (NB), Support Vector Machines (SVMs) and decision trees (DTs). We used three freely available packages, Borgelt’s NB classifier (Borgelt, 1999), SVM-light (Joachims, 2002) and YaDT (Ruggieri, 2004) respectively. We ran three way cross validation, where in each run the classifier is trained on 2/3 of the data and tested on the remaining third. Table 1 reports the avera</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Thorsten Joachims. 2002. Learning to Classify Text using Support Vector Machines.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistics-based summarization step one: Sentence compression</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence</booktitle>
<pages>703--710</pages>
<publisher>AAAI Press / The MIT Press</publisher>
<contexts>
<context>to build a new sentence without considering how to obtain the input. Other approaches relevant to ours, since they all aim at deriving summaries that are not extractive, include sentence compression (Knight and Marcu, 2000), and models for headline generation (Banko et al., 2000; Soricut and Marcu, 2006). Our approach differs from theirs in that we try to simulate the process of composing a new sentence by finding the </context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>Kevin Knight and Daniel Marcu. 2000. Statistics-based summarization step one: Sentence compression. In Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence, pages 703– 710. AAAI Press / The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
<author>Jan O Pedersen</author>
<author>Francine Chen</author>
</authors>
<title>A trainable document summarizer</title>
<date>1995</date>
<booktitle>In Proceedings of the 18th ACM-SIGIR Conference</booktitle>
<pages>68--73</pages>
<contexts>
<context>ple Sentence Generated by the QaST Model 6. Related Work (Jing, 2002) is the first study of decomposition of abstract sentences which goes well beyond previous approaches based on sentence alignment (Kupiec et al., 1995; Marcu, 1999; Teufel and Moens, 1997), where an abstract sentence is considered to be re-written from a single sentence from the text. Jing showed that an abstract sentence may consist of fragments f</context>
</contexts>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>Julian Kupiec, Jan O. Pedersen, and Francine Chen. 1995. A trainable document summarizer. In Proceedings of the 18th ACM-SIGIR Conference, pages 68–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
</authors>
<title>An empirical verification of coverage and correctness for a general-purpose sentence generator</title>
<date>1998</date>
<contexts>
<context> and Marcu, 2005), e.g., deep subject-verb or verb-object relations as required by Penman (Matthiessen and Bateman, 1991) or FUF (Elhadad, 1991), or the shallow syntactic relations needed by HALogen (Langkilde, 1998). We present each of our methods in turn. 4.2.1. SQa realization The SQa method uses n-gram probabilities to generate the actual sentences in the abstract. A simple quasi-abstractive summary is gener</context>
</contexts>
<marker>Langkilde, 1998</marker>
<rawString>Irene Langkilde. 1998. An empirical verification of coverage and correctness for a general-purpose sentence generator.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: a package for automatic evaluation of summaries</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL04 Workshop on Text Summarization Branches Out (WAS</booktitle>
<pages>74--81</pages>
<contexts>
<context>tive summaries, and more importantly, 13.9% better than the extractive summaries generated by ADAMS, our adaptive summarizer based on gene expression programming (Xie et al., 2006). The ROUGE scores (Lin, 2004) are higher as well: the ROUGE-1 and ROUGE-2 are 31.5% and 64.3% better than the lead-based summaries, and 2.8% and 19.3% better 1The work reported here was completed while the first author attended </context>
<context>y improve cosine similarity, which is a bag-of-words metric, since the more overlap two texts have, the higher their similarity value. The summarization community has also been using ROUGE-n metrics (Lin, 2004). More overlap between abstract and summary will also improve the ROUGE-n metrics, since ROUGEn are recall measures. Our approach to generating quasiabstractive summaries consists of the following st</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: a package for automatic evaluation of summaries. In Proceedings of the ACL04 Workshop on Text Summarization Branches Out (WAS 2004), pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The automatic construction of largescale corpora for summarization research</title>
<date>1999</date>
<booktitle>In SIGIR ’99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval</booktitle>
<pages>137--144</pages>
<contexts>
<context>d by the QaST Model 6. Related Work (Jing, 2002) is the first study of decomposition of abstract sentences which goes well beyond previous approaches based on sentence alignment (Kupiec et al., 1995; Marcu, 1999; Teufel and Moens, 1997), where an abstract sentence is considered to be re-written from a single sentence from the text. Jing showed that an abstract sentence may consist of fragments found in multi</context>
</contexts>
<marker>Marcu, 1999</marker>
<rawString>Daniel Marcu. 1999. The automatic construction of largescale corpora for summarization research. In SIGIR ’99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 137–144. Christian M. I. M. Matthiessen and John A. Bateman.</rawString>
</citation>
<citation valid="true">
<title>Text generation and systemic-functional linguistics: experiences from English and Japanese</title>
<date>1991</date>
<marker>1991</marker>
<rawString>1991. Text generation and systemic-functional linguistics: experiences from English and Japanese.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Salvatore Ruggieri</author>
</authors>
<title>Yadt: Yet another decision tree builder</title>
<date>2004</date>
<booktitle>In Proceedings of 16th IEEE International Conference on Tools with Artificial Intelligence (ICTAI</booktitle>
<pages>260--265</pages>
<contexts>
<context>ers: Na¨ıve Bayes (NB), Support Vector Machines (SVMs) and decision trees (DTs). We used three freely available packages, Borgelt’s NB classifier (Borgelt, 1999), SVM-light (Joachims, 2002) and YaDT (Ruggieri, 2004) respectively. We ran three way cross validation, where in each run the classifier is trained on 2/3 of the data and tested on the remaining third. Table 1 reports the average results across the thre</context>
</contexts>
<marker>Ruggieri, 2004</marker>
<rawString>Salvatore Ruggieri. 2004. Yadt: Yet another decision tree builder. In Proceedings of 16th IEEE International Conference on Tools with Artificial Intelligence (ICTAI 2004), pages 260–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Daniel Marcu</author>
</authors>
<title>Towards developing generation algorithms for text-to-text applications</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics</booktitle>
<pages>66--74</pages>
<contexts>
<context>ted generation approach because, as often happens in a text-to-text generation circumstance, our application does not have access to the kind of information that generic NLG systems require as input (Soricut and Marcu, 2005), e.g., deep subject-verb or verb-object relations as required by Penman (Matthiessen and Bateman, 1991) or FUF (Elhadad, 1991), or the shallow syntactic relations needed by HALogen (Langkilde, 1998)</context>
</contexts>
<marker>Soricut, Marcu, 2005</marker>
<rawString>Radu Soricut and Daniel Marcu. 2005. Towards developing generation algorithms for text-to-text applications. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 66–74, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Daniel Marcu</author>
</authors>
<title>Stochastic language generation using widl-expressions and its application in machine translation and summarization</title>
<date>2006</date>
<booktitle>In Proceedings of the 44rd Annual Meeting of the Association for Computational Linguistics</booktitle>
<pages>1105--1112</pages>
<contexts>
<context>view of DUC-2005 (Dang, 2005). Producing human-like summaries would seem to require radical advances in both text comprehension and generation. On the contrary, as others (Barzilay and McKeown, 2005; Soricut and Marcu, 2006), we believe it is possible to produce more human-like summaries via textto-text generation techniques. In this paper, we present our approach to generating quasi-abstractive summaries, a new type of</context>
<context>ches relevant to ours, since they all aim at deriving summaries that are not extractive, include sentence compression (Knight and Marcu, 2000), and models for headline generation (Banko et al., 2000; Soricut and Marcu, 2006). Our approach differs from theirs in that we try to simulate the process of composing a new sentence by finding the correspondence between fragments of sentences in the abstracts and sentences in th</context>
</contexts>
<marker>Soricut, Marcu, 2006</marker>
<rawString>Radu Soricut and Daniel Marcu. 2006. Stochastic language generation using widl-expressions and its application in machine translation and summarization. In Proceedings of the 44rd Annual Meeting of the Association for Computational Linguistics, pages 1105–1112, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Marc Moens</author>
</authors>
<title>Sentence extraction as a classification task</title>
<date>1997</date>
<booktitle>In Proceedings of Workshop on Intelligent Scalable Text Summarization (ACL97/EACL97</booktitle>
<pages>58--65</pages>
<contexts>
<context> Model 6. Related Work (Jing, 2002) is the first study of decomposition of abstract sentences which goes well beyond previous approaches based on sentence alignment (Kupiec et al., 1995; Marcu, 1999; Teufel and Moens, 1997), where an abstract sentence is considered to be re-written from a single sentence from the text. Jing showed that an abstract sentence may consist of fragments found in multiple sentences, and devel</context>
</contexts>
<marker>Teufel, Moens, 1997</marker>
<rawString>Simone Teufel and Marc Moens. 1997. Sentence extraction as a classification task. In Proceedings of Workshop on Intelligent Scalable Text Summarization (ACL97/EACL97), pages 58–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Ming Ting</author>
</authors>
<title>An instance-weighting method to induce cost-sensitive trees</title>
<date>2002</date>
<journal>IEEE Transactions on Knowledge and Data Engineering</journal>
<volume>14</volume>
<contexts>
<context>thods for each of the four substeps in our approach. For example, in step 1b, to obtain a better CSS’s classifier, we are investigating other ways, such as cost-sensitive classifiers (Domingos, 1999; Ting, 2002; Turney, 2000), to improve the performance of classifier without changing the class distribution. We are also investigating the features used for classification in step 1b, by considering the selecti</context>
</contexts>
<marker>Ting, 2002</marker>
<rawString>Kai Ming Ting. 2002. An instance-weighting method to induce cost-sensitive trees. IEEE Transactions on Knowledge and Data Engineering, 14(3):659–665.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Types of cost in inductive concept learning</title>
<date>2000</date>
<booktitle>In Proceedings of the ICML’2000 Workshop on Cost-Sensitive Learning</booktitle>
<pages>15--21</pages>
<contexts>
<context>ch of the four substeps in our approach. For example, in step 1b, to obtain a better CSS’s classifier, we are investigating other ways, such as cost-sensitive classifiers (Domingos, 1999; Ting, 2002; Turney, 2000), to improve the performance of classifier without changing the class distribution. We are also investigating the features used for classification in step 1b, by considering the selection metrics dis</context>
</contexts>
<marker>Turney, 2000</marker>
<rawString>Peter D. Turney. 2000. Types of cost in inductive concept learning. In Proceedings of the ICML’2000 Workshop on Cost-Sensitive Learning, pages 15–21.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Stephen Wan</author>
<author>Robert Dale</author>
<author>Mark Dras</author>
<author>C´ecile Paris</author>
</authors>
<marker>Wan, Dale, Dras, Paris, </marker>
<rawString>Stephen Wan, Robert Dale, Mark Dras, and C´ecile Paris.</rawString>
</citation>
<citation valid="true">
<title>Global revision in summarisation: Generating novel sentences with prim’s algorithm</title>
<date>2007</date>
<booktitle>In Proceedings of PACLING 2007 10th Conference of the Pacific Association for Computational Linguistics</booktitle>
<pages>226--235</pages>
<location>Melbourne, Australia</location>
<marker>2007</marker>
<rawString>2007. Global revision in summarisation: Generating novel sentences with prim’s algorithm. In Proceedings of PACLING 2007 10th Conference of the Pacific Association for Computational Linguistics, pages 226–235, Melbourne, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Wasserman</author>
<author>Katherine Faust</author>
</authors>
<title>Social Network Analysis : Methods and Applications</title>
<date>1994</date>
<publisher>Cambridge University Press</publisher>
<contexts>
<context>created from the CSS’s. In the second step, we enhance simple n-gram probabilities by anchoring them via the most salient NPs, as predicted by using several features of the NPs, including centrality (Wasserman and Faust, 1994). The innovative aspects of our approach are: generating candidate sentence sets and using fragments from sentences in those sets to generate the summary; and approximating the unfolding of topics in</context>
</contexts>
<marker>Wasserman, Faust, 1994</marker>
<rawString>Stanley Wasserman and Katherine Faust. 1994. Social Network Analysis : Methods and Applications. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhuli Xie</author>
<author>Barbara Di Eugenio</author>
<author>Peter C Nelson</author>
</authors>
<title>Adaptive learning in machine summarization</title>
<date>2006</date>
<booktitle>In Proceedings of the Nineteenth International FLAIRS Conference</booktitle>
<pages>180--181</pages>
<marker>Xie, Di Eugenio, Nelson, 2006</marker>
<rawString>Zhuli Xie, Barbara Di Eugenio, and Peter C. Nelson. 2006. Adaptive learning in machine summarization. In Proceedings of the Nineteenth International FLAIRS Conference, pages 180–181, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhuli Xie</author>
</authors>
<title>Centrality measures in text mining: Prediction of noun phrases that appear in abstracts</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Student Research Workshop</booktitle>
<pages>103--108</pages>
<contexts>
<context>include the most salient information from the source and exclude the redundant. For a text document to be informative, it should address some topics and regard them as the foci. In our previous work (Xie, 2005), we proposed a new text feature, noun phrase centrality, which represents the prominence of a noun phrase within a certain text. The value of the NP centrality is obtained by applying centrality mea</context>
<context>e several features of NPs, including NP centrality. Here we extend the basic summary generator by anchoring the generation of a new summary sentence in a salient NP predicted in the way described in (Xie, 2005). Our augmented summary generator comprises: 1. Generate salient NPs via topic prediction 2. Sort predicted salient NPs according to their lengths. Longer NPs cause less variation in the sentence gen</context>
</contexts>
<marker>Xie, 2005</marker>
<rawString>Zhuli Xie. 2005. Centrality measures in text mining: Prediction of noun phrases that appear in abstracts. In Proceedings of the ACL Student Research Workshop, pages 103–108, June.</rawString>
</citation>
</citationList>
</algorithm>

