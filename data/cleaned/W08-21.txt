1:259	CoNLL 2008 Proceedings of the Twelfth Conference on Computational Natural Language Learning Conference chairs: Alexander Clark and Kristina Toutanova 1617 August 2008 Manchester, UK c2008 The Coling 2008 Organizing Committee Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Nonported license http://creativecommons.org/licenses/by-nc-sa/3.0/ Some rights reserved Order copies of this and other Coling proceedings from: Association for Computational Linguistics (ACL) 209 N. Eighth Street Stroudsburg, PA 18360 USA Tel: +1-570-476-8006 Fax: +1-570-476-0860 acl@aclweb.org ISBN 978-1-905593-48-4 Design by Chimney Design, Brighton, UK Production and manufacture by One Digital, Brighton, UK ii Introduction The 2008 Conference on Computational Natural Language Learning is the twelfth in the series of yearly meetings organized by SIGNLL, the ACL special interest group on natural language learning.
2:259	CoNLL 2008 will be held in Manchester, UK, August 16-17, 2008, in conjunction with Coling 2008.
3:259	We are delighted to report that CoNLLs main session received a large number of submissions.
4:259	A total of 85 papers were under consideration for the main session after several withdrawals, and of them only 20 were accepted.
5:259	This makes this years CoNLL especially competitive and contributes to an interesting program.
6:259	We are grateful to the program committee members for their service in evaluating the submissions.
7:259	Special thanks to the program committee members who joined on a short notice to help with the larger than expected number of submissions.
8:259	This year CoNLL had two special themes of interest, both of which solicited papers on models that explain natural phenomena relating to human language.
9:259	The first concerned the central scientific problemaddressed byCoNLL: thestudyof firstlanguage acquisition.
10:259	The secondthemewas thecentral engineering problem: how to build systems that do something useful, especially complete systems that solve real problems.
11:259	The first theme contributed to an increased number of high-quality submissions in the first language acquisition area.
12:259	Two sessions of the conference will be devoted to papers on this topic.
13:259	The second theme led to submissions in diverse traditional NLP application areas.
14:259	As in previous years, CoNLL 2008 has a shared task.
15:259	This year, the conference shared task proposed to merge the shared task topics from the last four years (2004-2007) into a unique task called Joint Learning of Syntactic and Semantic Dependencies.
16:259	Both syntactic dependencies (extracted from the Penn Treebank ) and semantic dependencies (extracted from PropBank and NomBank) were jointly addressed under a unique unified representation.
17:259	The shared task was organized by Mihai Surdeanu, Richard Johansson, Adam Meyers, Llus M`arquez, and Joakim Nivre.
18:259	The call was very successful attracting the interest of more than 50 teams from all over the world, which represented a wide variety of universities, research institutions, and companies.
19:259	At the end of the evaluation period, 22 teams submitted results (with 19 and 5 contributions to the closed and open challenges, respectively).
20:259	All this work will be presented in the conference in the form of 5 selected oral talks and 14 posters.
21:259	Inouropinion, thecurrentsharedtaskconstitutesaqualitativestepaheadandwehopethattheresources created and the body of work presented will both serve as a benchmark and have a substantial impact on future research on syntactic-semantic parsing.
22:259	We are excited that the invited speakers at CoNLL 2008 will be Regina Barzilay and Nick Chater.
23:259	Finally, we would like to thank the SIGNLL board members for useful discussion, Erik Tjong Kim Sang, who acted as the information officer, and especially Llus M`arquez and Joakim Nivre, who helped us greatly with advice around the conference organization, as well as to the organizers of COLING iii 2008, Harold Somers, Mark Stevenson and Roger Evans.
24:259	Many thanks also to Microsoft Research for sponsoring CoNLL this year and to Priscilla Rasmussen for help with the finances.
25:259	Enjoy this years conference!
26:259	Alex Clark and Kristina Toutanova CoNLL 2008 Conference Chairs iv Conference Chairs: Alex Clark, Royal Holloway, University of London Kristina Toutanova, Microsoft Research Programme Committee: Steve Abney, University of Michigan Eneko Agirre, University of the Basque Country Galen Andrew, Microsoft Research Tim Baldwin, University of Melbourne Regina Barzilay, MIT Roberto Basili, University of Roma, Tor Vergata Phil Blunsom, University of Edinburgh Thorsten Brants, Google Research Paula Buttery, Cambridge University Xavier Carreras, MIT Nick Chater, University College London Ciprian Chelba, Google Research Colin Cherry, Microsoft Research Alexander Clark, Royal Holloway, University of London Stephen Clark, Oxford University James Cussens, University of York Walter Daelemans, CNTS, Antwerp Hal Daume III, University of Utah Jenny Finkel, Stanford University Radu Florian, IBM research Dayne Freitag, HNC Software Michel Galley, Stanford University Jianfeng Gao, Microsoft Research Daniel Gildea, Rochester Sharon Goldwater, University of Edinburgh Jan Hajic, Institute of Formal and Applied Linguistics, Charles University in Prague Marti Hearst, UC Berkeley James Henderson, University of Geneva Liang Huang, University of Pennsylvania Rie Johnson (formerly, Ando), RJ Research Consulting Rohit Kate, University of Texas at Austin Philipp Koehn, University of Edinburgh Rob Koeling, Sussex University Anna Korhonen, Cambridge University Shalom Lappin, Kings College, London v Roger Levy, UC San Diego Percy Liang, UC Berkeley Rob Malouf, San Diego State University Llus M`arquez, Technical University of Catalonia Yuji Matsumoto, Nara Institute of Science and Technology Diana McCarthy, Sussex University Rada Mihalcea, University of North Texas Alessandro Moschitti, DISI, University of Trento John Nerbonne, University of Groningen Hwee Tou Ng, National University of Singapore Vincent Ng, University of Texas at Dallas Joakim Nivre, Uppsala University Franz Och, Google Research Miles Osborne, University of Edinburgh Slav Petrov, UC Berkeley David Powers, Flinders University Chris Quirk, Microsoft Research Ari Rappoport, Hebrew University Ellen Riloff, University of Utah Dan Roth, University of Illinois, Urbana-Champaign William Sakas, City University of New York Anoop Sarkar, Simon Fraser University Richard Sproat, University of Illinois, Urbana-Champaign Suzanne Stevenson, University of Toronto Mihai Surdeanu, Barcelona Media Information Center Charles Sutton, UC Berkeley Kristina Toutanova, Microsoft Research Antal van den Bosch, Tilburg University Charles Yang, University of Pennsylvania Invited Speakers: Regina Barzilay, MIT Computer Science & Artificial Intelligence Lab Nick Chater, Department of Pyschology, University College London vi Table of Contents Semantic Parsing for High-Precision Semantic Role Labelling Paola Merlo and Gabriele Musillo . . .
27:259	1 TAG, Dynamic Programming, and the Perceptron for Efficient, Feature-Rich Parsing Xavier Carreras, Michael Collins and Terry Koo . . .
28:259	9 A Fast Boosting-based Learner for Feature-Rich Tagging and Chunking Tomoya Iwakura and Seishi Okamoto . . .
29:259	17 Linguistic features in data-driven dependency parsing Lilja Ovrelid . . .
30:259	25 Transforming Meaning Representation Grammars to Improve Semantic Parsing Rohit Kate . . .
31:259	33 Using LDA to detect semantically incoherent documents Hemant Misra, Olivier Cappe and Francois Yvon . . .
32:259	41 Picking them up and Figuring them out: Verb-Particle Constructions, Noise and Idiomaticity Carlos Ramisch, Aline Villavicencio, Leonardo Moura and Marco Idiart . . .
33:259	49 Fast Mapping in Word Learning: What Probabilities Tell Us Afra Alishahi, Afsaneh Fazly and Suzanne Stevenson . . .
34:259	57 Improving Word Segmentation by Simultaneously Learning Phonotactics Daniel Blanchard and Jeffrey Heinz . . .
35:259	65 A MDL-based Model of Gender Knowledge Acquisition Harmony Marchal, Benot Lemaire, Maryse Bianco and Philippe Dessus . . .
36:259	73 Baby SRL: Modeling Early Language Acquisition.
37:259	Michael Connor, Yael Gertner, Cynthia Fisher and Dan Roth . . .
38:259	81 An Incremental Bayesian Model for Learning Syntactic Categories Christopher Parisien, Afsaneh Fazly and Suzanne Stevenson . . .
39:259	89 Fully Unsupervised Graph-Based Discovery of General-Specific Noun Relationships from Web Corpora Frequency Counts Gael Dias, Raycho Mukelov and Guillaume Cleuziou . . .
40:259	97 Acquiring Knowledge from the Web to be used as Selectors for Noun Sense Disambiguation Hansen A. Schwartz and Fernando Gomez . . .
41:259	105 Automatic Chinese Catchword Extraction Based on Time Series Analysis Han Ren, Donghong Ji, Jing Wan and Lei Han . . .
42:259	113 Easy as ABC?
43:259	Facilitating Pictorial Communication via Semantically Enhanced Layout Andrew B. Goldberg, Xiaojin Zhu, Charles R. Dyer, Mohamed Eldawy and Lijie Heng . . .
44:259	119 A Nearest-Neighbor Approach to the Automatic Analysis of Ancient Greek Morphology John Lee.
45:259	.127 vii Context-based Arabic Morphological Analysis for Machine Translation Thuy Linh Nguyen and Stephan Vogel . . .
46:259	135 A Tree-to-String Phrase-based Model for Statistical Machine Translation Thai Phuong Nguyen, Akira Shimazu, Tu Bao Ho, Minh Le Nguyen and Vinh Van Nguyen . . .
47:259	143 Trainable Speaker-Based Referring Expression Generation Giuseppe Di Fabbrizio, Amanda Stent and Srinivas Bangalore . . .
48:259	151 The CoNLL 2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies Mihai Surdeanu, Richard Johansson, Adam Meyers, Llus M`arquez and Joakim Nivre . . .
49:259	159 A Latent Variable Model of Synchronous Parsing for Syntactic and Semantic Dependencies James Henderson, Paola Merlo, Gabriele Musillo and Ivan Titov . . .
50:259	178 Dependency-based SyntacticSemantic Analysis with PropBank and NomBank Richard Johansson and Pierre Nugues . . .
51:259	183 A Joint Model for Parsing Syntactic and Semantic Dependencies Xavier Llus and Llus M`arquez . . .
52:259	188 Collective Semantic Role Labelling with Markov Logic Sebastian Riedel and Ivan Meza-Ruiz.
53:259	.193 Hybrid Learning of Dependency Structures from Heterogeneous Linguistic Resources Yi Zhang, Rui Wang and Hans Uszkoreit . . .
54:259	198 Parsing Syntactic and Semantic Dependencies with Two Single-Stage Maximum Entropy Models Hai Zhao and Chunyu Kit . . .
55:259	203 A Combined Memory-Based Semantic Role Labeler of English Roser Morante, Walter Daelemans and Vincent Van Asch . . .
56:259	208 A Puristic Approach for Joint Dependency Parsing and Semantic Role Labeling Alexander Volokh and Gunter Neumann . . .
57:259	213 Discriminative Learning of Syntactic and Semantic Dependencies Lu Li, Shixi Fan, Xuan Wang and Xiaolong Wang . . .
58:259	218 Discriminative vs. Generative Approaches in Semantic Role Labeling Deniz Yuret, Mehmet Ali Yatbaz and Ahmet Engin Ural . . .
59:259	223 A Pipeline Approach for Syntactic and Semantic Dependency Parsing Yotaro Watanabe, Masakazu Iwatate, Masayuki Asahara and Yuji Matsumoto.
60:259	.228 Semantic Dependency Parsing using N-best Semantic Role Sequences and Roleset Information Joo-Young Lee, Han-Cheol Cho and Hae-Chang Rim . . .
61:259	233 A Cascaded Syntactic and Semantic Dependency Parsing System Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li, Bing Qin, Ting Liu and Sheng Li . . .
62:259	238 The Integration of Dependency Relation Classification and Semantic Role Labeling Using Bilayer Maximum Entropy Markov Models Weiwei Sun, Hongzhan Li and Zhifang Sui.
63:259	.243 viii Mixing and Blending Syntactic and Semantic Dependencies Yvonne Samuelsson, Oscar Tackstrom, Sumithra Velupillai, Johan Eklund, Mark Fishel and Markus Saers . . .
64:259	248 Dependency Tree-based SRL with Proper Pruning and Extensive Feature Engineering Hongling Wang, Honglin Wang, Guodong Zhou and Qiaoming Zhu . . .
65:259	253 DeSRL: A Linear-Time Semantic Role Labeling System Massimiliano Ciaramita, Giuseppe Attardi, Felice DellOrletta and Mihai Surdeanu . . .
66:259	258 Probabilistic Model for Syntactic and Semantic Dependency Parsing Enhong Chen, Liu Shi and Dawei Hu . . .
67:259	263 Applying Sentence Simplification to the CoNLL-2008 Shared Task David Vickrey and Daphne Koller . . .
68:259	268 ix  Conference Programme Saturday, August 16, 2008 8:308:50 Opening Remarks Session 1: Parsing 8:509:15 Semantic Parsing for High-Precision Semantic Role Labelling Paola Merlo and Gabriele Musillo 9:159:40 TAG, Dynamic Programming, and the Perceptron for Efficient, Feature-Rich Parsing Xavier Carreras, Michael Collins and Terry Koo 9:4010:05 A Fast Boosting-based Learner for Feature-Rich Tagging and Chunking Tomoya Iwakura and Seishi Okamoto 10:0510:30 Linguistic features in data-driven dependency parsing Lilja vrelid 10:30-11:00 Coffee break Session 2: Semantics 11:0011:25 Transforming Meaning Representation Grammars to Improve Semantic Parsing Rohit Kate 11:2511:50 Using LDA to detect semantically incoherent documents Hemant Misra, Olivier Cappe and Francois Yvon 11:5012:40 Invited talk by Regina Barzilay 12:4014:00 Lunch xi Saturday, August 16, 2008 (continued) Shared Task 14:0014:30 The CoNLL 2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies Mihai Surdeanu, Richard Johansson, Adam Meyers, Llus M`arquez and Joakim Nivre Oral Presentations 14:3014:50 A Latent Variable Model of Synchronous Parsing for Syntactic and Semantic Dependencies James Henderson, Paola Merlo, Gabriele Musillo and Ivan Titov 14:5015:10 Dependency-based SyntacticSemantic Analysis with PropBank and NomBank Richard Johansson and Pierre Nugues 15:1015:30 A Joint Model for Parsing Syntactic and Semantic Dependencies Xavier Llus and Llus M`arquez 15:3015:50 Collective Semantic Role Labelling with Markov Logic Sebastian Riedel and Ivan Meza-Ruiz 15:5016:10 Hybrid Learning of Dependency Structures from Heterogeneous Linguistic Resources Yi Zhang, Rui Wang and Hans Uszkoreit 16:10-16:20 Closing remarks 16:20-16:45 Coffee break xii Saturday, August 16, 2008 (continued) Poster session 16:4518:00 Parsing Syntactic and Semantic Dependencies with Two Single-Stage Maximum Entropy Models Hai Zhao and Chunyu Kit A Combined Memory-Based Semantic Role Labeler of English Roser Morante, Walter Daelemans and Vincent Van Asch A Puristic Approach for Joint Dependency Parsing and Semantic Role Labeling Alexander Volokh and Gunter Neumann Discriminative Learning of Syntactic and Semantic Dependencies Lu Li, Shixi Fan, Xuan Wang and Xiaolong Wang Discriminative vs. Generative Approaches in Semantic Role Labeling Deniz Yuret, Mehmet Ali Yatbaz and Ahmet Engin Ural A Pipeline Approach for Syntactic and Semantic Dependency Parsing Yotaro Watanabe, Masakazu Iwatate, Masayuki Asahara and Yuji Matsumoto Semantic Dependency Parsing using N-best Semantic Role Sequences and Roleset Information Joo-Young Lee, Han-Cheol Cho and Hae-Chang Rim A Cascaded Syntactic and Semantic Dependency Parsing System Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li, Bing Qin, Ting Liu and Sheng Li The Integration of Dependency Relation Classification and Semantic Role Labeling Using Bilayer Maximum Entropy Markov Models Weiwei Sun, Hongzhan Li and Zhifang Sui Mixing and Blending Syntactic and Semantic Dependencies Yvonne Samuelsson, Oscar Tackstrom, Sumithra Velupillai, Johan Eklund, Mark Fishel and Markus Saers Dependency Tree-based SRL with Proper Pruning and Extensive Feature Engineering Hongling Wang, Honglin Wang, Guodong Zhou and Qiaoming Zhu DeSRL: A Linear-Time Semantic Role Labeling System Massimiliano Ciaramita, Giuseppe Attardi, Felice DellOrletta and Mihai Surdeanu xiii Saturday, August 16, 2008 (continued) Probabilistic Model for Syntactic and Semantic Dependency Parsing Enhong Chen, Liu Shi and Dawei Hu Applying Sentence Simplification to the CoNLL-2008 Shared Task David Vickrey and Daphne Koller Sunday, August 17, 2008 Session 1: Language Acquisition I 8:509:15 Picking them up and Figuring them out: Verb-Particle Constructions, Noise and Idiomaticity Carlos Ramisch, Aline Villavicencio, Leonardo Moura and Marco Idiart 9:159:40 Fast Mapping in Word Learning: What Probabilities Tell Us Afra Alishahi, Afsaneh Fazly and Suzanne Stevenson 9:4010:05 Improving Word Segmentation by Simultaneously Learning Phonotactics Daniel Blanchard and Jeffrey Heinz 10:0510:30 A MDL-based Model of Gender Knowledge Acquisition Harmony Marchal, Benot Lemaire, Maryse Bianco and Philippe Dessus 10:3011:00 Coffee break Session 2: Language Acquisition II 11:0011:25 Baby SRL: Modeling Early Language Acquisition.
69:259	Michael Connor, Yael Gertner, Cynthia Fisher and Dan Roth 11:2511:50 An Incremental Bayesian Model for Learning Syntactic Categories Christopher Parisien, Afsaneh Fazly and Suzanne Stevenson 11:5012:40 Invited talk by Nick Chater 12:4014:00 Lunch xiv Sunday, August 17, 2008 (continued) 13:4014:00 CoNLL Business Meeting Session 3: Semantic extraction 14:0014:25 Fully Unsupervised Graph-Based Discovery of General-Specific Noun Relationships from Web Corpora Frequency Counts Gael Dias, Raycho Mukelov and Guillaume Cleuziou 14:2514:50 Acquiring Knowledge from the Web to be used as Selectors for Noun Sense Disambiguation Hansen A. Schwartz and Fernando Gomez 14:5015:15 Automatic Chinese Catchword Extraction Based on Time Series Analysis Han Ren, Donghong Ji, Jing Wan and Lei Han 15:1515:40 Easy as ABC?
70:259	Facilitating Pictorial Communication via Semantically Enhanced Layout Andrew B. Goldberg, Xiaojin Zhu, Charles R. Dyer, Mohamed Eldawy and Lijie Heng 15:4016:00 Coffee break Session 4: Morphology, MT and Generation 16:0016:25 A Nearest-Neighbor Approach to the Automatic Analysis of Ancient Greek Morphology John Lee 16:2516:50 Context-based Arabic Morphological Analysis for Machine Translation Thuy Linh Nguyen and Stephan Vogel 16:5017:15 A Tree-to-String Phrase-based Model for Statistical Machine Translation Thai Phuong Nguyen, Akira Shimazu, Tu Bao Ho, Minh Le Nguyen and Vinh Van Nguyen 17:1517:40 Trainable Speaker-Based Referring Expression Generation Giuseppe Di Fabbrizio, Amanda Stent and Srinivas Bangalore 17:4018:00 Closing remarks and best paper award xv  CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 18 Manchester, August 2008 Semantic Parsing for High-Precision Semantic Role Labelling Paola Merlo Linguistics Department University of Geneva 5 rue de Candolle 1211 Gen`eve 4 Switzerland merlo@lettres.unige.ch Gabriele Musillo Depts of Linguistics and Computer Science University of Geneva 5 Rue de Candolle 1211 Gen`eve 4 Switzerland musillo4@etu.unige.ch Abstract In this paper, we report experiments that explore learning of syntactic and semantic representations.
71:259	First, we extend a state-of-the-art statistical parser to produce a richly annotated tree that identifies and labels nodes with semantic role labels as well as syntactic labels.
72:259	Secondly, we explore rule-based and learning techniques to extract predicate-argument structures from this enriched output.
73:259	The learning method is competitive with previous single-system proposals for semantic role labelling, yields the best reported precision, and produces a rich output.
74:259	In combination with other high recall systems it yields an F-measure of 81%.
75:259	1 Introduction In statistical natural language processing, considerable ingenuity and insight have been devoted to developing models of syntactic information, such as statistical parsers and taggers.
76:259	Successes in these syntactic tasks have recently paved the way to applying novel statistical learning techniques to levels of semantic representation, such as recovering the logical form of a sentence for information extraction and question-answering applications (Miller et al., 2000; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007).
77:259	Inthispaper, wealsofocusourinterestonlearning semantic information.
78:259	Differently from other work that has focussed on logical form, however, we explore the problem of recovering the syntactic structure of the sentence, the propositional c2008.
79:259	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
80:259	Some rights reserved.
81:259	argument-structure of its main predicates, and the substantive labels assigned to the arguments in the propositional structure, the semantic roles.
82:259	This rich output can be useful for information extraction and question-answering, but also for anaphora resolution and other tasks for which the structural information provided by full syntactic parsing is necessary.
83:259	Thetaskofsemanticrolelabelling(SRL),ashas been defined by previous researchers (Gildea and Jurafsky, 2002), requires collecting all the arguments that together with a verb form a predicateargument structure.
84:259	In most previous work, the task has been decomposed into the argument identification and argument labelling subtasks: first the arguments of each specific verb in the sentence are identified by classifying constituents in the sentence as arguments or not arguments.
85:259	The arguments are then labelled in a second step.
86:259	We propose to produce the rich syntacticsemantic output in two steps, which are different from the argument identification and argument labelling subtasks.
87:259	First, we generate trees that bear both syntactic and semantic annotation, such as those in Figure 1.
88:259	The parse tree, however, does not explicitly encode information about predicateargument structure, because it does not explicitly associate each semantic role to the verb that governs it.
89:259	So, our second step consists in recovering the predicate-argument structure of each verb by gleaning this information in an already richly decorated tree.
90:259	There are linguistic and computational reasons to think that we can solve the joint problem of recovering the constituent structure of a sentence and its lexical semantics.
91:259	From a linguistic point of view, the assumption that syntactic distributions will be predictive of semantic role assignments is based on linking theory (Levin, 1986).
92:259	Linking theory assumes the existence of a hierarchy of se1 mantic roles which are mapped by default on a hierarchy of grammatical functions and syntactic positions, and it attempts to predict the mapping of the underlying semantic component of a predicatesmeaningontothesyntacticstructure.
93:259	Forexample, Agents are always mapped in syntactically higher positions than Themes.
94:259	From a computational point of view, if the internal semantics of a predicate determines the syntactic expressions of constituents bearing a semantic role, it is then reasonable to expect that knowledge about semantic roles in a sentence will be informative of its syntactic structure.
95:259	It follows rather naturally that semantic and syntactic parsing can be integrated into a single complex task.
96:259	Our proposal also addresses the problem of semantic role labelling from a slightly different perspective.
97:259	We identify and label argument nodes first, while parsing, and we group them in a predicate-argument structure in a second step.
98:259	Our experiments investigate some of the effects that result from organising the task of semantic role labelling in this way, and the usefulness of some novel features defined on syntactic trees.
99:259	In the remainder of the paper, we first illustrate the data and the graphical model that formalise the architecture used and its extension for semantic parsing.
100:259	We then report on two kinds of experiments: we first evaluate the architecture on the joint task of syntactic and semantic parsing and then evaluate the joint approach on the task of semantic role labelling.
101:259	We conclude with a discussion which highlights the practical and theoretical contribution of this work.
102:259	2 The Data Our experiments on joint syntactic and semantic parsing use data that is produced automatically by merging the Penn Treebank (PTB) with PropBank (PRBK) (Marcus et al., 1993; Palmer et al., 2005), as shown in Figure 1.
103:259	PropBank encodes propositional information by adding a layer of argument structure annotation to the syntactic structures of the Penn Treebank.1 Verbal predicates in the Penn Treebank (PTB) receive a label REL and their arguments are annotated with abstract semantic role labels, such as A0, A1, or AA for those complements of the predicative verb that are considered arguments.
104:259	Those complements of the verb la1We use PRBK data as they appear in the CONLL 2005 shared task.
105:259	S NP-A0 the authority VP VBD-REL dropped PP-TMP IN(TMP) at NP NN midnight PP-DIR TO(DIR) to NP QP $ 2.80 trillion Figure1: Asamplesyntacticstructurewithsemantic role labels.
106:259	belled with a semantic functional label in the original PTB receive the composite semantic role label AM-X, where X stands for labels such as LOC, TMP or ADV, for locative, temporal and adverbial modifiers respectively.
107:259	A tree structure with PropBank labels is shown in Figure 1.
108:259	(The bold labels are not relevant for the moment and they will be explained later.)
109:259	3 The Syntactic and Semantic Parser Architecture To achieve the complex task of joint syntactic and semantic parsing, we extend a current state-of-theart statistical parser (Titov and Henderson, 2007) to learn semantic role annotation as well as syntactic structure.
110:259	The parser uses a form of left-corner parsing strategy to map parse trees to sequences of derivation steps.
111:259	We choose this parser because it exhibits the best performance for a single generative parser, and does not impose hard independence assumptions.
112:259	It is therefore promising for extensions to new tasks.
113:259	Following (Titov and Henderson, 2007), we describe the original parsing architecture and our modifications to it as a Dynamic Bayesian network.
114:259	Our description is brief and limited to the few aspects of interest here.
115:259	For more detail, explanations and experiments see (Titov and Henderson, 2007).
116:259	A Bayesian network is a directed acyclic graph that illustrates the statistical dependencies between the random variables describing a set of events (Jensen, 2001).
117:259	Dynamic networks are Bayesian networks applied to unboundedly long sequences.
118:259	They are an appropriate model for sequences of derivation steps in 2 dtk St1 t sit tcD Dt1 tcS S Dt Figure2: Thepatternonconnectivityandthelatent vectors of variables in an Incremental Bayesian Network.
119:259	parsing (Titov and Henderson, 2007).
120:259	Figure 2 illustrates visually the main properties that are of relevance for us in this parsing architecture.
121:259	Let T be a parse tree and D1,,Dm be the sequence of parsing decisions that has led to the building of this parse tree.
122:259	Let also each parsing decision be composed of smaller parsing decisions d11,,d1k, and let all these decisions be independent.
123:259	Then, P(T) = P(D1,,Dm) = producttextt P(Dt|D1,,Dt1) = producttexttproducttextk P(dtk|h(t,k)) (1) where h(t,k) denotes the parse history for subdecision dtk.
124:259	The figure represents a small portion of the observed sequence of decisions that constitute the recovery of a parse tree, indicated by the observed states Di.
125:259	Specifically, it illustrates the pattern of connectivityfordecisiondtk.
126:259	Ascanbeseentherelationship between different probabilistic parsing decisions are not Markovian, nor do the decisions influenceeachotherdirectly.
127:259	Pastdecisionscaninfluence the current decision through state vectors of independent latent variables, referred to as Si.
128:259	These state vectors encode the probability distributions of features of the history of parsing steps (the features are indicated by sti in Figure 2).
129:259	As can be seen from the picture, the pattern of inter-connectivity allows previous non-adjacent states to influence future states.
130:259	Not all states in the history are relevant, however.
131:259	The interconnectivity is defined dynamically based on the topological structure and the labels of the tree that is being developed.
132:259	This inter-connectivity depends on a notion of structural locality (Henderson, 2003; Musillo and Merlo, 2006).2 2Specifically, the conditioning states are based on the In order to extend this model to learn decisions concerning a joint syntactic-semantic representation, the semantic information needs to be highlighted in the model in several ways.
133:259	We modify the network connectivity, and bias the learner.
134:259	First, we take advantage of the networks dynamic connectivity to highlight the portion of the tree that bears semantic information.
135:259	We augment the nodes that can influence parsing decisions at the current state by explicitly adding the vectors of latent variables related to the most recent child bearing a semantic role label of either type (REL, A0 to A5 or AM-X) to the connectivity of the current decision.
136:259	These additions yield a model that is sensitive to regularities in structurally defined sequences of nodes bearing semantic role labels, within and across constituents.
137:259	These extensions enlarge the locality domain over which dependencies between predicates bearing the REL label, arguments bearing an A0-A5 label, and adjuncts bearing an AM-X role can be specified, and capture both linear and hierarchical constraints between predicates, arguments and adjuncts.
138:259	Enlarging the locality domain this way ensures for instance that the derivation of the role DIR in Figure 1 is not independent of the derivations of the roles TMP, REL (the predicate) and A0.
139:259	Second, this version of the Bayesian network tags its sentences internally.
140:259	Following (Musillo and Merlo, 2005), we split some part-of-speech tags into tags marked with semantic role labels.
141:259	Thesemanticrolelabelsattachedtoanon-terminal directly projected by a preterminal and belonging to a few selected categories (DIR, EXT, LOC, MNR, PRP, CAUS or TMP) are propagated down to the pre-terminal part-of-speech tag of its head.3 This third extension biases the parser to learn the relationship between lexical items, semantic roles and the constituents in which they occur.
142:259	This technique is illustrated by the bold labels in Figure 1.
143:259	We compare this augmented model to a simple baseline parser, that does not present any of the task-specific enhancements described above, stack configuration of the left-corner parser and the derivation tree built so far.
144:259	The nodes in the partially built tree and stack configuration that are selected to determine the relevant states are the following: top, the node on top of the pushdown stack before the current derivation move; the left-corner ancestor of top (that is, the second top-most node on the parser stack); the leftmost child of top; and the most recent child of top, if any.
145:259	3Exploratory data analysis indicates that these tags are the most useful to disambiguate parsing decisions.
146:259	3 PTB/PRBK 24 P R F Baseline 79.6 78.6 79.1 ST 80.5 79.4 79.9 ST+ EC 81.6 80.3 81.0 Table 1: Percentage F-measure (F), recall (R), and precision (P) of our joint syntactic and semantic parser on merged development PTB/PRBK data (section 24).
147:259	Legend of models: ST=Split Tags; EC=enhanced connectivity.
148:259	other than being able to use the complex syntacticsemantic labels.
149:259	Our augmented model has a total of 613 non-terminals to represent both the PTB and PropBank labels of constituents, instead of the 33 of the original syntactic parser.
150:259	The 580 newly introduced labels consist of a standard PTB label followed by a set of one or more PropBank semantic role such as PP-AM-TMP or NP-A0-A1.
151:259	As a result of lowering the six AM-X semantic role labels, 240 new part-of-speech tags were introduced to partition the original tag set which consisted of 45 tags.
152:259	As already mentioned, argumental labels A0-A5 are specific to a given verb or a given verb sense, thus their distribution is highly variable.
153:259	To reduce variability, we add the tag-verb pairs licensing these argumental labels to the vocabulary of our model.
154:259	We reach a total of 4970 tag-word pairs.
155:259	These pairs include, among others, all the tag-verb pairs occuring at least 10 times in the training data.
156:259	In this very limited form of lexicalisation, all other words are considered unknown.
157:259	4 Parsing Experiments Our extended joint syntactic and semantic parser was trained on sections 2-21 and validated on section 24 from the merged PTB/PropBank.
158:259	To evaluate the joint syntactic and semantic parsing task, we compute the standard Parseval measures of labelled recall and precision of constituents, taking into account not only the original PTB labels, but also the newly introduced PropBank labels.
159:259	This evaluation gives us an indication of how accurately and exhaustively we can recover this richer set of syntactic and semantic labels.
160:259	The results, computed on the development data set from section 24 of the PTB with added PropBank annotation, are shown in Table 1.
161:259	As the table indicates, both the enhancementsbasedonsemanticrolesyieldanimprovement on the baseline.
162:259	This task enables us to compare, albeit indirectly, our integrated method to other methods where semantic role labels are learnt separately from syntactic structure.
163:259	(Musillo and Merlo, 2006) report results of a merging technique where the output of the semantic role annotation produced by the best semantic role labellers in the 2005 CONLL shared task is merged with the output of Charniaks parser.
164:259	Results range between between 82.7% and 83.4% F-measure.
165:259	Our integrated method almost reaches this level of performance.
166:259	The performance of the parser on the syntactic labels only(note reported inTable 1) isslightly degraded in comparison to the original syntax-only architecture (Henderson, 2003), which reported an F-measure of 89.1% since we reach 88.4% Fmeasureforthebestsyntactic-semanticmodel(last line of Table 1).
167:259	This level of performance is still comparable to other syntactic parsers often used for extraction of semantic role features (88.2% Fmeasure) (Collins, 1999).
168:259	These results indicate that the extended parser is able to recover both syntactic and semantic labels in a fully connected parse tree.
169:259	While it is true that the full fine-grained interpretation of the semantic label is verb-specific, the PropBank labels (A0,A1, etc) do respect some general trends.
170:259	A0 labels are assigned to the most agentive of the arguments, while A1 labels tend to be assigned to arguments bearing a Theme role, and A2, A3, A4 and A5 labels are assigned to indirect object roles, while all the AM-X labels tend to be assigned to adjuncts.
171:259	The fact that the parser learns these labels without explicit annotation of the link between the arguments and the predicate to which they are assigned, but based on the smoothed representation of the derivation of the parse tree and only very limited lexicalisation, appears to confirm linking theory, which assumes a correlation between the syntactic configuration of a sentence and the lexical semantic labels.
172:259	We need to show now that the quality of the output produced by the joint syntactic and semantic parsing is such that it can be used to perform other tasks where semantic role information is crucial.
173:259	The most directly related task is semantic role labelling (SRL) as defined in the shared task of CoNLL 2005.
174:259	4 5 Extraction of Predicate-Argument Structures Although there is reason to think that the good performance reported in the previous section is due to implicit learning of the relationship of the syntactic representation and the semantic role assignments, the output produced by the parser does notexplicitlyencodethepredicate-argumentstructures.
175:259	Collecting these associations is required to solvethesemanticrolelabellingtaskasusuallydefined.
176:259	We experimented with two methods: a simple rule-based method and a more complex learning method.
177:259	5.1 The rule-based method The rule-based extraction method is the natural second step to solve the complete semantic role labelling task, after we identify and label semantic roles while parsing.
178:259	Since in our proposal, we solve most of the problem in the first step, then we should be able to collect the predicate-argument pairs by simple, deterministic rules.
179:259	The simplicity of the method also provides a useful comparisonformorecomplexlearningmethods,whichcan be justified only if they perform better than simple rule-based predicate-argument extraction.
180:259	Our rule-based method automatically compiles finite-state automatata defining the paths that connect the first node dominating a predicate to its semantic roles from parse trees enriched with semantic role labels.4 Such paths can then be used to traverse parse trees returned by the parsing model and collect argument structures.
181:259	More specifically, a sample of sentences are randomly selected from the training section of the PTB/PRBK.
182:259	For each predicate, then, all the arguments left and right of the predicate and all the adjuncts left and right respectively are collected and filtered by simple global constraints, thereby guaranteeing that only one type of obligatory argument label (A0 to A5) is assigned in each proposition.
183:259	Whenevaluatedongolddata,thisrule-basedextraction method reaches 94.9% precision, 96.9% recall, for an F-measure of 95.9%.
184:259	These results provide an upper bound as well as indicating that, while not perfect, the simple extraction rules reach a very good level of correctness if the input from the first step, syntactic and semantic parsing, is correct.
185:259	The performance is much lower when 4It uses VanNoords finite-state-toolkit http://www.let.rug.nl/ vannoord/Fsa/.
186:259	parses are not entirely correct, and semantic role labels are missing, as indicated by the results of 72.9% precision, 66.7% (F-measure 69.7%), obtained when using the best automatic parse tree.
187:259	The fact that performance depends on the quality of the output of the first step, indicates that the extraction rules are sensitive to errors in the parse trees, as well as errors in the labelling.
188:259	This indicates that a learning method might be more adapted to recover from these mistakes.
189:259	5.2 The SVM learning method In a different approach to extract predicate argument structures from the parsing output, the second step learns to associate the right verb to each semantically annotated node (srn) in the tree produced in the first step.
190:259	Each individual (verb, srn) pairin thetreeiseither apositiveexample (thesrn is a member of the verbs argument structure) or a negative example (the argument either should not have been labelled as an argument or it is not associated to the verb).
191:259	The training examples are produced by parsing section 2-21 of the merged PTB/PRBK data with the joint syntactic-semantic parser and producing the training examples by comparison with the CONLL 2005 gold propositions.
192:259	There are approximately 800000 training examples in total.
193:259	These examples are used by an SVM classifier (Joachims, 1999).5.
194:259	Once the predicate-argument structures are built, they are evaluated with the CONLL 2005 shared task criteria.
195:259	5.3 The learning features The features used for the extraction of the predicate-argument structure reflect the syntactic properties that are useful to identify the arguments of a given verb.
196:259	We use syntactic and semantic node label, the path between the verb and the argument, and the part-of-speech tag of the verb, which provides useful information about the tense of the verb.
197:259	We also use novel features that encode minimality conditions and locality constraints (Rizzi, 1990).
198:259	Minimality is a typical property of natural languages that is attested in several domains.
199:259	In recovering predicate-argument structures, minimality guarantees that the arguments are related to the closest verb in a predicate domain, which is not always the verb to which they are connected by the 5We use a radial basis function kernel, where parameters c and  were determined by a grid search on a small subset of 2000 training examples.
200:259	They are set at c=8 and  = 0.03125.
201:259	5 shortest path.
202:259	For example, the subject of an embedded clause can be closer to the verb of the main clause than to the predicate to which it should be attached.
203:259	Minimalityisencodedasabinaryfeature that indicates whether a verbw intervenes between the verb v and the candidate argument srn.
204:259	Minimalityisdefinedbothintermsoflinearprecedence (indicated below as ) and of dominance within the same VP group.
205:259	A VP group is a stack of VPs covering the same compound verb group, such as [V P should [V P have [V P [V come ]]]].
206:259	Formal definitions are given below: minimal(v, srn, w)=df 8 < : false if (v  w  srn or srn  w  v) and VPG-dominates(v, srn, w) true otherwise VPG-dominates(v, srn, w)=df 8 < : true if VPpath(v, srn) and VPVP-group directly dominating w false otherwise In addition to the minimality conditions, which resolve ambiguity when two predicates compete to govern an argument, we use locality constraints to capture distinct local relationships between a verb and the syntactic position occupied by a candidate argument.
207:259	Inparticular,wedistinguishbetweeninternal arguments occupying a position dominated by a VP node, external arguments occupying a position dominated by an S node, and extracted arguments occupying a position dominated by an SBAR node.
208:259	To approximate such structural distinctions, weintroducetwobinaryfeaturesindicating, respectively, whether there is a a node labelled S or SBAR on the path connecting the verb and the candidate argument.
209:259	6 Results and Discussion Table 2 illustrates our results on semantic role labelling.
210:259	Notice how much more precise the learning method is than the rule-based method, when theminimalityconstraintisadded.
211:259	Thesecondand third line indicate that this contribution is mostly due to the minimality feature.
212:259	The fifth and sixth line however illustrate that these features together are more useful than the widely used feature path.
213:259	Recallhowever,suffersinthelearntmethod.
214:259	Overall, the learnt method is better than a rule-based method only if path and either minimality or locality constraints are added, thus suggesting that Prec Rec F Learning all features 87.4 63.6 73.7 Learning all min 75.4 66.2 70.5 Learning all loc 87.4 63.6 73.6 Rule-based 72.9 66.7 69.7 Learning all path 80.6 60.9 69.4 Learning all min loc 74.3 63.8 68.6 Baseline 57.4 53.9 55.6 Table 2: Results on the development section (24), rule-based, and learning, (with all features, and without path, minimality and locality constraints) compared to a closest verb baseline.
215:259	the choice of features is crucial to reach a level of performance that justifies the added complexity of a learning method.
216:259	Both methods are much better than a baseline that attaches each role to a verb by the shortest path.6 Notice that both these approaches are not lexicalised, they apply to all verbs.
217:259	Learning experiments where the actual verbswereusedshowedalittledegradationaswell as a very considerable increase in training times (precision: 87.0%; recall: 61.0%; F: 71.7%).7 Some comments are in order to compare properly our best results  the learning method with all features  to other methods.
218:259	Most of the best performing SRL systems are ensemble learners or rerankers, or they use external sources of information such as the PropBank frames files.
219:259	While these techniques are effective to improve classificationaccuracy,wemightwanttocomparethesingle systems, thus teasing apart the contribution of the features and the model from the contribution of the ensemble technique.
220:259	Table 3 reports the single systems performance on the test set.
221:259	These resultsseemtoindicatethatmethodslikeours, based on a first step of PropBank parsing, are comparable to other methods when learning regimes are factored out, contrary to pessimistic conclusions in previous work (Yi and Palmer, 2005).
222:259	(Yi and Palmer, 2005) share the motivation of our work.
223:259	They observe that the distributions of semantic la6Incaseoftie, thefollowingverbischosenforanA0label and the preceding verb is chosen for all the other labels.
224:259	7We should notice that all these models encode the feature pathassyntacticpath, becauseinexploratorydataanalysiswe found that this feature performed quite a bit better than path encodedtakingintoaccountthesemanticrolesassignedtothe nodes on the path.
225:259	Concerning the learning model, we notice that a simpler, and much faster to train, linear SVM classifier performs almost as well as the more complex RBF classifier.
226:259	It is therefore preferable if speed is important.
227:259	6 Model CONLL 23 Comments P R F (Surdeanu and Turmo, 2005) 80.3 73.0 76.5 Propbank frames to filter output, boosting (Liu et al., 2005) 80.5 72.8 76.4 Single system + simple post-processing (Moschitti et al., 2005) 76.6 75.2 75.9 Specialised kernels for each kind of role This paper 87.6 65.8 75.1 Single system and model, locality features (Ozgencil and McCracken, 2005) 74.7 74.2 74.4 Simple system, no external knowledge (Johansson and Nugues, 2005) 75.5 73.2 74.3 Uses only 3 sections for training Table 3: Final Semantic Role Labelling results on test section 23 of Propbank as encoded in the CONLL shared task for those CONLL 2005 participants not using ensemble learning or external resources.
228:259	bels could potentially interact with the distributions of syntactic labels and redefine the boundaries of constituents, thus yielding trees that reflect generalisations over both these sources of information.
229:259	They also attempt to assign SRL while parsing, by merging only the first two steps of the standard pipeline architecture, pruning and argument identification.
230:259	Their parser outputs a binary argument-nonargument distinction.
231:259	The actual fine-grained labelling is performed, as in other methods, by an ensemble classifier.
232:259	Results are not among the best and Yi and Palmer conclude that PropBank parsing is too difficult and suffers from differences between chunk annotation and tree structure.
233:259	We think instead that the method is promising, as shown by the results reported here, once the different factors that affect performance are teased apart.
234:259	Some qualitative observations on the errors are useful.
235:259	On the one hand, as can be noticed in Table 3, our learning method yields the best precision, but often the worse recall and it has the most extreme difference between these two scores.8 This is very likely to be a consequence of the method.
236:259	Since the assignment of the semantic role labels proper is performed during parsing, the number of nodes that require a semantic role is only 20% of the total.
237:259	Therefore the parser develops a bias against assigning these roles in general, and recall suffers.9 Ontheotherhand, precisionisverygood, thanks to the rich context in which the roles are assigned.
238:259	This property of our method suggests that combining our results with those of other existing se8This observation applies also in a comparison to the other systems that participated in the CONLL shared task.
239:259	9The SVM classifier, on the other hand, exceeds 94% in accuracy and its F measures are situated around 8788% depending on the feature sets.
240:259	mantic role labellers might be beneficial, since the errors it performs are quite different.
241:259	We tested this hypothesis by combining our outputs, which are the most precise, with the outputs of the system that reported the best recall (Haghighi et al., 2005).
242:259	The combination, performed on sections 24 and 23, gives priority to our system when it outputs a non-null label (because of its high precision) and uses the other systems label when our system outputs a null label.
243:259	This combination produces a result of 79.0% precision, 80.4% recall, and 79.7% F-measure for section 24, and 80.5% precision, 81.4% recall, and 81.0% F-measure for section23.
244:259	Weconcludethatthecombinationisindeed able to exploit the positive aspects of both approaches, as the F-measure of the combined result is better than each individual result.
245:259	It is also the best compared to the other systems of the CoNLL shared task.
246:259	Comparatively, we find that applying the same combination technique to the output of the system by (Haghighi et al., 2005) with the output of the best system in the CoNLL 2005 shared task (Punyakanok et al., 2005) yields combined outputs that are not as good as the better of the two systems (P:76.3%; R:78.6%; F:77.4% for section 24; P:78.5%; R:80.0%; F:79.3% for section 23).
247:259	This result confirms our initial hypothesis, that combination of systems with different performance characteristics yields greater improvement.
248:259	Another direct consequence of assigning roles in a rich context is that in collecting arguments for a given verb we hardly need to verify global constraints.
249:259	Differently from previous work that had found that global coherence constraints considerably improved performance (Punyakanok et al., 2005), using global filtering contraints showed no improvement in our learning model.
250:259	Thus, these results confirm the observations that a verb does 7 not assign its semantic roles independently of each other (Haghighi et al., 2005).
251:259	Our method too can be seen as a way of formulating the SRL problem ina waythat isnotsimply classificationofeach instance independently.
252:259	Because identification of arguments and their labelling is done while parsing, the parsing history, both syntactic and semantic, is taken into account in identifying and labelling an argument.
253:259	Semantic role labelling is integrated in structured sequence prediction.
254:259	Further integration of semantic role labelling in structured probabilistic models related to the one described here has recently been shown to result in accurate synchronous parsers that derive both syntactic and semantic dependency representations (Henderson et al., 2008).
255:259	7 Conclusion Overall our experiments indicate that an integrated approach to identification and labelling followed by predicate-argument recovery can solve the problem of semantic role labelling at a level of performance comparable to other approaches, as well as yielding a richly decorated syntacticsemantic parse tree.
256:259	The high precision of our method yields very good results in combination with other high-recall systems.
257:259	Its shortcomings indicates that future work lies in improving recall.
258:259	Acknowledgments We thank the Swiss NSF for supporting this research under grant number 101411-105286/1, James Henderson for sharing the SSN software, and Xavier Carreras for providing the CoNLL-2005 data.
259:259	Part of this work was completed while the second author was visiting MIT/CSAIL, hosted by Prof. Michael Collins.

