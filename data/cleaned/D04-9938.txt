1:120	Adaptation of Maximum Entropy Capitalizer: Little Data Can Help a Lot Ciprian Chelba and Alex Acero Microsoft Research One Microsoft Way Redmond, WA 98052 {chelba,alexac}@microsoft.com Abstract A novel technique for maximum a posteriori (MAP) adaptation of maximum entropy (MaxEnt) and maximum entropy Markov models (MEMM) is presented.
2:120	The technique is applied to the problem of recovering the correct capitalization of uniformly cased text: a background capitalizer trained on 20Mwds of Wall Street Journal (WSJ) text from 1987 is adapted to two Broadcast News (BN) test sets  one containing ABC Primetime Live text and the other NPR Morning News/CNN Morning Edition text  from 1996.
3:120	The in-domain performance of the WSJ capitalizer is 45% better than that of the 1-gram baseline, when evaluated on a test set drawn from WSJ 1994.
4:120	When evaluating on the mismatched out-ofdomain test data, the 1-gram baseline is outperformed by 60%; the improvement brought by the adaptation technique using a very small amount of matched BN data  25-70kwds  is about 20-25% relative.
5:120	Overall, automatic capitalization error rate of 1.4% is achieved on BN data.
6:120	1 Introduction Automatic capitalization is a practically relevant problem: speech recognition output needs to be capitalized; also, modern word processors perform capitalization among other text proofing algorithms such as spelling correction and grammar checking.
7:120	Capitalization can be also used as a preprocessing step in named entity extraction or machine translation.
8:120	We study the impact of using increasing amounts of training data as well as using a small amount of adaptation data on this simple problem that is well suited to data-driven approaches since vast amounts of training data are easily obtainable by simply wiping the case information in text.
9:120	As in previous approaches, the problem is framed as an instance of the class of sequence labeling problems.
10:120	A case frequently encountered in practice is that of using mismatched  out-of-domain, in this particular case we used Broadcast News  test data.
11:120	For example, one may wish to use a capitalization engine developed on newswire text for email or office documents.
12:120	This typically affects negatively the performance of a given model, and more sophisticated models tend to be more brittle.
13:120	In the capitalization case we have studied, the relative performance improvement of the MEMM capitalizer over the 1-gram baseline drops from in-domain  WSJ  performance of 45% to 35-40% when used on the slightly mismatched BN data.
14:120	In order to take advantage of the adaptation data in our scenario, a maximum a-posteriori (MAP) adaptation technique for maximum entropy (MaxEnt) models is developed.
15:120	The adaptation procedure proves to be quite effective in further reducing the capitalization error of the WSJ MEMM capitalizer on BN test data.
16:120	It is also quite general and could improve performance of MaxEnt models in any scenario where model adaptation is desirable.
17:120	A further relative improvement of about 20% is obtained by adapting the WSJ model to Broadcast News (BN) text.
18:120	Overall, the MEMM capitalizer adapted to BN data achieves 60% relative improvement in accuracy over the 1-gram baseline.
19:120	The paper is organized as follows: the next section frames automatic capitalization as a sequence labeling problem, presents previous approaches as well as the widespread and highly sub-optimal 1gram capitalization technique that is used as a baseline in most experiments in this work and others.
20:120	The MEMM sequence labeling technique is briefly reviewed in Section 3.
21:120	Section 4 describes the MAP adaptation technique used for the capitalization of out-of-domain text.
22:120	The detailed mathematical derivation is presented in Appendix A. The experimental results are presented in Section 5, followed by conclusions and suggestions for future work.
23:120	2 Capitalization as Sequence Tagging Automatic capitalization can be seen as a sequence tagging problem: each lower-case word receives a tag that describes its capitalization form.
24:120	Similar to the work in (Lita et al. , 2003), we tag each word in a sentence with one of the tags:  LOC lowercase  CAP capitalized  MXC mixed case; no further guess is made as to the capitalization of such words.
25:120	A possibility is to use the most frequent one encountered in the training data.
26:120	 AUC all upper case  PNC punctuation; we decided to have a separate tag for punctuation since it is quite frequent and models well the syntactic context in a parsimonious way For training a given capitalizer one needs to convert running text into uniform case text accompanied by the above capitalization tags.
27:120	For example, PrimeTime continues on ABC.PERIOD Now,COMMA from Los Angeles,COMMA Diane Sawyer .PERIOD becomes primetime_MXC continues_LOC on_LOC abc_AUC .period_PNC now_CAP,comma_PNC from_LOC los_CAP angeles_CAP,comma_PNC diane_CAP sawyer_CAP .period_PNC The text is assumed to be already segmented into sentences.
28:120	Any sequence labeling algorithm can then be trained for tagging lowercase word sequences with capitalization tags.
29:120	At test time, the uniform case text to be capitalized is first segmented into sentences 1 after which each sentence is tagged.
30:120	2.1 1-gram capitalizer A widespread algorithm used for capitalization is the 1-gram tagger: for every word in a given vocabulary (usually large, 100kwds or more) use the most frequent tag encountered in a large amount of training data.
31:120	As a special case for automatic capitalization, the most frequent tag for the first word in a sentence is overridden by CAP, thus capitalizing on the fact that the first word in a sentence is most likely capitalized 2 . 1 Unlike the training phase, the sentence segmenter at test time is assumed to operate on uniform case text.
32:120	2 As with everything in natural language, it is not hard to find exceptions to this rule.
33:120	Due to its popularity, both our work and that of (Lita et al. , 2003) uses the 1-gram capitalizer as a baseline.
34:120	The work in (Kim and Woodland, 2004) indicates that the same 1-gram algorithm is used in Microsoft Word 2000 and is consequently used as a baseline for evaluating the performance of their algorithm as well.
35:120	2.2 Previous Work We share the approach to capitalization as sequence tagging with that of (Lita et al. , 2003).
36:120	In their approach, a language model is built on pairs (word, tag) and then used to disambiguate over all possible tag assignments to a sentence using dynamic programming techniques.
37:120	The same idea is explored in (Kim and Woodland, 2004) in the larger context of automatic punctuation generation and capitalization from speech recognition output.
38:120	A second approach they consider for capitalization is the use a rule-based tagger as described by (Brill, 1994), which they show to outperform the case sensitive language modeling approach and be quite robust to speech recognition errors and punctuation generation errors.
39:120	Departing from their work, our approach builds on a standard technique for sequence tagging, namely MEMMs, which has been successfully applied to part-of-speech tagging (Ratnaparkhi, 1996).
40:120	The MEMM approach models the tag sequence T conditionally on the word sequence W, which has a few substantial advantages over the 1-gram tagging approach:  discriminative training of probability model P(T|W) using conditional maximum likelihood is well correlated with tagging accuracy  ability to use a rich set of word-level features in a parsimonious way: sub-word features such as prefixes and suffixes, as well as future words 3 are easily incorporated in the probability model  no concept of out-of-vocabulary word: subword features are very useful in dealing with words not seen in the training data  ability to integrate rich contextual features into the model More recently, certain drawbacks of MEMM models have been addressed by the conditional random field (CRF) approach (Lafferty et al. , 2001) which slightly outperforms MEMMs on a standard partof-speech tagging task.
41:120	In a similar vein, the work 3 Relative to the current word, whose tag is assigned a probability value by the MEMM.
42:120	of (Collins, 2002) explores the use of discriminatively trained HMMs for sequence labeling problems, a fair baseline for such cases that is often overlooked in favor of the inadequate maximum likelihood HMMs.
43:120	The work on adapting the MEMM model parameters using MAP smoothing builds on the Gaussian prior model used for smoothing MaxEnt models, as presented in (Chen and Rosenfeld, 2000).
44:120	We are not aware of any previous work on MAP adaptation of MaxEnt models using a prior, be it Gaussian or a different one, such as the exponential prior of (Goodman, 2004).
45:120	Although we do not have a formal derivation, the adaptation technique should easily extend to the CRF scenario.
46:120	A final remark contrasts rule-based approaches to sequence tagging such as (Brill, 1994) with the probabilistic approach taken in (Ratnaparkhi, 1996): having a weight on each feature in the MaxEnt model and a sound probabilistic model allows for a principled way of adapting the model to a new domain; performing such adaptation in a rule-based model is unclear, if at all possible.
47:120	3 MEMM for Sequence Labeling A simple approach to sequence labeling is the maximum entropy Markov model.
48:120	The model assigns a probability P(T|W) to any possible tag sequence T = t 1 t n = T n 1 for a given word sequence W = w 1 w n . The probability assignment is done according to: P(T|W)= n productdisplay i=1 P(t i |x i (W, T i1 1 )) where t i is the tag corresponding to word i and x i (W, T i1 1 ) is the conditioning information at position i in the word sequence on which the probability model is built.
49:120	The approach we took is the one in (Ratnaparkhi, 1996), which uses x i (W, T i1 1 )= {w i,w i1,w i+1,t i1,t i2 }.
50:120	We note that the probability model is causal in the sequencing of tags (the probability assignment for t i only depends on previous tags t i1,t i2 ) which allows for efficient algorithms that search for the most likely tag sequence T  (W) = arg max T P(T|W) as well as ensures a properly normalized conditional probability model P(T|W).
51:120	The probability P(t i |x i (W, T i1 1 )) is modeled using a maximum entropy model.
52:120	The next section briefly describes the training procedure; for details the reader is referred to (Berger et al. , 1996).
53:120	3.1 Maximum Entropy State Transition Model The sufficient statistics that are extracted from the training data are tuples (y,#,x)=(t i,#,x i (W, T i1 1 )) where t i is the tag assigned in context x i (W, T i1 1 )= {w i,w i1,w i+1,t i1,t i2 } and # denotes the count with which this event has been observed in the training data.
54:120	By way of example, the event associated with the first word in the example in Section 2 is (*bdw* denotes a special boundary type): MXC 1 currentword=primetime previousword=*bdw* nextword=continues t1=*bdw* t1,2=*bdw*,*bdw* prefix1=p prefix2=pr prefix3=pri suffix1=e suffix2=me suffix3=ime The maximum entropy probability model P(y|x) uses features which are indicator functions of the type: f(y,x)={ 1, 0, if y = MXC and x.w i = primetime o/w Assuming a set of features F whose cardinality is F, the probability assignment is made according to: p  (y|x)=Z 1 (x,)  exp bracketleftBigg F summationdisplay i=1  i f i (x,y) bracketrightBigg Z(x,) = summationdisplay y exp bracketleftBigg F summationdisplay i=1  i f i (x,y) bracketrightBigg where ={ 1  F }R F is the set of realvalued model parameters.
55:120	3.1.1 Feature Selection We used a simple count cut-off feature selection algorithm which counts the number of occurrences of all features in a predefined set after which it discards the features whose count is less than a pre-specified threshold.
56:120	The parameter of the feature selection algorithm is the threshold value; a value of 0 will keep all features encountered in the training data.
57:120	3.1.2 Parameter Estimation The model parameters  are estimated such that the model assigns maximum log-likelihood to the training data subject to a Gaussian prior centered at 0,  N(0,diag( 2 i )), that ensures smoothing (Chen and Rosenfeld, 2000): L() = summationdisplay x,y p(x,y)logp  (y|x) (1)  F summationdisplay i=1  2 i 2 2 i + const() As shown in (Chen and Rosenfeld, 2000)  and rederived in Appendix A for the non-zero mean case  the update equations are:  (t+1) i =  (t) i +  i, where  i satisfies: summationdisplay x,y p(x,y)f i (x,y)   i  2 i =  i  2 i + (2) summationdisplay x,y p(x)p  (y|x)f i (x,y)exp( i f # (x,y)) In our experiments the variances are tied to  i =  whose value is determined by line search on development data such that it yields the best tagging accuracy.
58:120	4 MAP Adaptation of Maximum Entropy Models In the adaptation scenario we already have a MaxEnt model trained on the background data and we wish to make best use of the adaptation data by balancing the two.
59:120	A simple way to accomplish this is to use MAP adaptation using a prior distribution on the model parameters.
60:120	A Gaussian prior for the model parameters  has been previously used in (Chen and Rosenfeld, 2000) for smoothing MaxEnt models.
61:120	The prior has 0 mean and diagonal covariance:   N(0,diag( 2 i )).
62:120	In the adaptation scenario, the prior distribution used is centered at the parameter values  0 estimated from the background data instead of 0:  N( 0,diag( 2 i )).
63:120	The regularized log-likelihood of the adaptation training data becomes: L() = summationdisplay x,y p(x,y)logp  (y|x) (3)  F summationdisplay i=1 ( i   0 i ) 2 2 2 i + const() The adaptation is performed in stages:  apply feature selection algorithm on adaptation data and determine set of features F adapt .  build new model by taking the union of the background and the adaptation feature sets: F = F background F adapt ; each of the background features receives the corresponding weight  i determined on the background training data; the new features F adapt \F background 4 introduced in the model receive 0 weight.
64:120	The resulting model is thus equivalent with the background model.
65:120	 train the model such that the regularized loglikelihood of the adaptation training data is maximized.
66:120	The prior mean is set at  0 =  background  0;  denotes concatenation between the parameter vector for the background model and a 0-valued vector of length |F adapt \ F background | corresponding to the weights for the new features.
67:120	As shown in Appendix A, the update equations are very similar to the 0-mean case: summationdisplay x,y p(x,y)f i (x,y)  ( i   0 i )  2 i =  i  2 i + (4) summationdisplay x,y p(x)p  (y|x)f i (x,y)exp( i f # (x,y)) The effect of the prior is to keep the model parameters  i close to the background ones.
68:120	The cost of moving away from the mean for each feature f i is specified by the magnitude of the variance  i :a small variance  i will keep the weight  i close to its mean; a large variance  i will make the regularized log-likelihood (see Eq.
69:120	3) insensitive to the prior on  i, allowing the use of the best value  i for modeling the adaptation data.
70:120	Another observation is that not only the features observed in the adaptation data get updated: even if E p(x,y) [f i ]=0, the weight  i for feature f i will still get updated if the feature f i triggers for a context x encountered in the adaptation data and some predicted value y  not necessarily present in the adaptation data in context x. In our experiments the variances were tied to  i =  whose value was determined by line search on development data drawn from the adaptation data.
71:120	The common variance  will thus balance optimally the log-likelihood of the adaptation data with the  0 mean values obtained from the background data.
72:120	Other tying schemes are possible: separate values could be used for the F adapt \F background and F background feature sets, respectively.
73:120	We did not experiment with various tying schemes although this is a promising research direction.
74:120	4.1 Relationship with Minimum Divergence Training Another possibility to adapt the background model is to do minimum KL divergence (MinDiv) train4 We use A\B to denote set difference.
75:120	ing (Pietra et al. , 1995) between the background exponential model B  assumed fixed  and an exponential model A built using the F background  F adapt feature set.
76:120	It can be shown that, if we smooth the A model with a Gaussian prior on the feature weights that is centered at 0  following the approach in (Chen and Rosenfeld, 2000) for smoothing maximum entropy models  then the MinDiv update equations for estimating A on the adaptation data are identical to the MAP adaptation procedure we proposed 5 . However, we wish to point out that the equivalence holds only if the feature set for the new model A is F background F adapt . The straightforward application of MinDiv training  by using only the F adapt feature set for A  will not result in an equivalent procedure to ours.
77:120	In fact, the difference in performance between this latter approach and ours could be quite large since the cardinality of F background is typically several orders of magnitude larger than that of F adapt and our approach also updates the weights corresponding to features in F background \F adapt . Further experiments are needed to compare the performance of the two approaches.
78:120	5 Experiments The baseline 1-gram and the background MEMM capitalizer were trained on various amounts of WSJ (Paul and Baker, 1992) data from 1987  files WS87_001-126}.
79:120	The in-domain test data used was file WS94_000 (8.7kwds).
80:120	As for the adaptation experiments, two different sets of BN data were used, whose sizes are summarized in Table 1: 1.
81:120	BN CNN/NPR data.
82:120	The training/development/test partition consisted of a 3-way random split of file BN624BTS.
83:120	The resulting sets are denoted CNN-trn/dev/tst, respectively 2.
84:120	BN ABC Primetime data.
85:120	The training set consisted of file BN623ATS whereas the development/test set consisted of a 2-way random split of file BN624ATS 5.1 In-Domain Experiments We have proceeded building both 1-gram and MEMM capitalizers using various amounts of background training data.
86:120	The model sizes for the 1gram and MEMM capitalizer are presented in Table 2.
87:120	Count cut-off feature selection has been used 5 Thanks to one of the anonymous reviewers for pointing out this possible connection.
88:120	Data set Partition train devel test WSJ 2-20M  8.7k CNN 73k 73k 73k ABC 25k 8k 8k Table 1: Background and adaptation training, development, and test data partition sizes for the MEMM capitalizer with the threshold set at 5, so the MEMM model size is a function of the training data.
89:120	The 1-gram capitalizer used a vocabulary of the most likely 100k wds derived from the training data.
90:120	Model No.
91:120	Param.
92:120	(10 3 ) Training Data Size (10 6 ) 2.0 3.5 20.0 1-gram 100 100 100 MEMM 76 102 238 Table 2: Background models size (number of parameters) for various amounts of training data We first evaluated the in-domain and out-ofdomain relative performance of the 1-gram and the MEMM capitalizers as a function of the amount of training data.
93:120	The results are presented in Table 3.
94:120	The MEMM capitalizer performs about 45% better Model Test Data Cap ERR (%) Training Data Size (10 6 ) 2.0 3.5 20.0 1-gram WSJ-tst 5.4 5.2 4.4 MEMM WSJ-tst 2.9 2.5 2.3 1-gram ABC-dev 3.1 2.9 2.6 MEMM ABC-dev 2.2 2.0 1.6 1-gram CNN-dev 4.4 4.2 3.5 MEMM CNN-dev 2.7 2.5 2.1 Table 3: Background models performance on indomain (WSJ-test) and out-of-domain (BN-dev) data for various amounts of training data than the 1-gram one when trained and evaluated on Wall Street Journal text.
95:120	The relative performance improvement of the MEMM capitalizer over the 1gram baseline drops to 35-40% when using out-ofdomain Broadcast News data.
96:120	Both models benefit from using more training data.
97:120	5.2 Adaptation Experiments We have then adapted the best MEMM model built on 20Mwds on the two BN data sets (CNN/ABC) and compared performance against the 1-gram and the unadapted MEMM models.
98:120	There are a number of parameters to be tuned on development data.
99:120	Table 4 presents the variation in model size with different count cut-off values for the feature selection procedure on the adaptation data.
100:120	As can be seen, very few features are added to the background model.
101:120	Table 5 presents the variation in log-likelihood and capitalization accuracy on the CNN adaptation training and development data, respectively.
102:120	The adaptation procedure was found Cut-off 0 5 10 6 No.
103:120	features 243,262 237,745 237,586 Table 4: Adapted model size as a function of count cut-off threshold used for feature selection on CNNtrn adaptation data; the entry corresponding to the cut-off threshold of 10 6 represents the number of features in the background model to be insensitive to the number of reestimation iterations, and, more surprisingly, to the number of features added to the background model from the adaptation data, as shown in 5.
104:120	The most sensitive parameter is the prior variance  2, as shown in Figure 1; its value is chosen to maximize classification accuracy on development data.
105:120	As expected, low values of  2 result in no adaptation at all, whereas high values of  2 fit the training data very well, and result in a dramatic increase of training data loglikelihood and accuracies approaching 100%.
106:120	CutLogL Cap ACC (%) off  2 (nats) CNN-trn CNN-dev 0 0.01 -4258.58 98.00 97.98 0 3.0 -1194.45 99.63 98.62 5 0.01 -4269.72 98.00 97.98 5 3.0 -1369.26 99.55 98.60 10 6 0.01 -4424.58 98.00 97.96 10 6 3.0 -1467.46 99.52 98.57 Table 5: Adapted model performance for various count cut-off and  2 variance values; log-likelihood and accuracy on adaptation data CNN-trn as well as accuracy on held-out data CNN-dev; the background model results (no new features added) are the entries corresponding to the cut-off threshold of 10 6 Finally, Table 6 presents the results on test data for 1-gram, background and adapted MEMM.
107:120	As can be seen, the background MEMM outperforms the 1-gram model on both BN test sets by about 35-40% relative.
108:120	Adaptation improves performance even further by another 20-25% relative.
109:120	Overall, the adapted models achieve 60% relative reduction in capitalization error over the 1-gram baseline on both BN test sets.
110:120	An intuitively satisfying result is the fact that the cross-test set performance (CNN 0 1 2 3 4 5 6 4500 4000 3500 3000 2500 2000 1500 1000  2 variance LogL(train) Adaptation: Training LogL (nats) with  2 0 1 2 3 4 5 6 97.5 98 98.5 99 99.5 100  2 variance Accuracy Adaptation: Training and Development Capitalization Accuracy with  2 Figure 1: Variation of training data log-likelihood, and training/development data (-/ line) capitalization accuracy as a function of the prior variance  2 Cap ERR (%) Model Adapt Data ABC-tst CNN-tst 1-gram  2.7 3.7 MEMM  1.8 2.2 MEMM ABC-trn 1.4 1.7 MEMM CNN-trn 2.4 1.4 Table 6: Background and adapted models performance on BN test data; two adaptation/test sets are used: ABC and CNN adapted model evaluated on ABC data and the other way around) is worse than the adapted one.
111:120	6 Conclusions and Future Work The MEMM tagger is very effective in reducing both in-domain and out-of-domain capitalization error by 35%-45% relative over a 1-gram capitalization model.
112:120	We have also presented a general technique for adapting MaxEnt probability models.
113:120	It was shown to be very effective in adapting a background MEMM capitalization model, improving the accuracy by 20-25% relative.
114:120	An overall 50-60% reduction in capitalization error over the standard 1-gram baseline is achieved.
115:120	A surprising result is that the adaptation performance gain is not due to adding more, domain-specific features but rather making better use of the background features for modeling the in-domain data.
116:120	As expected, adding more background training data improves performance but a very small amount of domain specific data also helps significantly if one can make use of it in an effective way.
117:120	The Theres no data like more data rule-of-thumb could be amended by , especially if its the right data!.
118:120	As future work we plan to investigate the best way to blend increasing amounts of less-specific background training data with specific, in-domain data for this and other problems.
119:120	Another interesting research direction is to explore the usefulness of the MAP adaptation of MaxEnt models for other problems among which we wish to include language modeling, part-of-speech tagging, parsing, machine translation, information extraction, text routing.
120:120	Acknowledgments Special thanks to Adwait Ratnaparkhi for making available the code for his MEMM tagger and MaxEnt trainer.

