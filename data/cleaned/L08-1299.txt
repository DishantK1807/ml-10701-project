<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>J Allen</author>
<author>L Schubert</author>
<author>G Ferguson</author>
<author>P Heeman</author>
<author>C Hwang</author>
<author>T Kato</author>
<author>M Light</author>
<author>N Martin</author>
<author>B Miller</author>
<author>M Posesio</author>
<author>D Traum</author>
</authors>
<title>The TRAINS project: a case study in building a conversational planning agent</title>
<date>1995</date>
<journal>Journal of Experimental and Theoretical Artificial Intelligence</journal>
<volume>7</volume>
<pages>48</pages>
<contexts>
<context>, 2001; Peckham, 1991)). There are two broad categories of computational model used to interpret these acts. The first, including the work of Cohen and Perrault (1979) and the TRAINS dialogue system (Allen et al., 1995), relies on processing belief logics, centring on the impact each utterance has on the hearer what the hearer believes the speaker intended to communicate. These models can be very accurate, partic</context>
</contexts>
<marker>Allen, Schubert, Ferguson, Heeman, Hwang, Kato, Light, Martin, Miller, Posesio, Traum, 1995</marker>
<rawString>J. Allen, L. Schubert, G. Ferguson, P. Heeman, C. Hwang, T. Kato, M. Light, N. Martin, B. Miller, M. Posesio, and D. Traum. 1995. The TRAINS project: a case study in building a conversational planning agent. Journal of Experimental and Theoretical Artificial Intelligence, 7:7– 48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Austin</author>
</authors>
<title>How to Do Things with Words</title>
<date>1962</date>
<publisher>Oxford University Press</publisher>
<location>Oxford</location>
<marker>Austin, 1962</marker>
<rawString>J. L. Austin. 1962. How to Do Things with Words. Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harry Bunt</author>
</authors>
<title>Context and dialogue control</title>
<date>1994</date>
<journal>THINK</journal>
<pages>3--19</pages>
<contexts>
<context>based dialogue act classifier tagging new, out-of-domain data are given in Section 5. Finally we end with some discussion and an outline of intended further work. 2. Related Work Dialogue Acts (DAs) (Bunt, 1994), also known as speech acts or dialogue moves (Power, 1979), represent the functional performance of a speaker’s utterance. Searle (1969) is probably most often associated with speech acts, building </context>
</contexts>
<marker>Bunt, 1994</marker>
<rawString>Harry Bunt. 1994. Context and dialogue control. THINK, 3:19–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: the kappa statistic</title>
<date>1996</date>
<journal>Computational Linguistics</journal>
<volume>22</volume>
<contexts>
<context>used both in our experiments and those of Stolcke et al. (2000). In measuring the agreement between annotators in labelling this data, Jurafsky et al. (1998) report an average pair-wise kappa of .80 (Carletta, 1996). An excerpt of dialogue from the SWITCHBOARD corpus can be seen in Figure 3. 3.2. AMITI ´ES The AMITI ´ES project (Hardy et al., 2005) collected 1000 English human-human dialogues from European GE c</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>J. C. Carletta. 1996. Assessing agreement on classification tasks: the kappa statistic. Computational Linguistics, 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Cohen</author>
<author>C R Perrault</author>
</authors>
<title>Elements of a plan based theory of speech acts</title>
<date>1979</date>
<journal>Cognitive Science</journal>
<volume>3</volume>
<marker>Cohen, Perrault, 1979</marker>
<rawString>P. R. Cohen and C. R. Perrault. 1979. Elements of a plan based theory of speech acts. Cognitive Science, 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark G Core</author>
<author>James Allen</author>
</authors>
<title>Coding dialogs with the DAMSL annotation scheme</title>
<date>1997</date>
<booktitle>In AAAI Fall Symposium on Communicative Action in Humans and Machines</booktitle>
<location>MIT, Cambridge, MA</location>
<contexts>
<context>nalised a task-independent set of DAs, called DAMSL (Dialogue Act Markup in Several Layers), for use across different domains. DAMSL has been used to mark-up several dialogue corpora, such as TRAINS (Core and Allen, 1997), and the SWITCHBOARD corpus (Jurafsky et al., 1998). DAMSL draws both on the need Corpus Availability Utterancecount Dialoguecount Wordcount Distinctwords Dialoguetype SWITCHBOARD public 223606 1155</context>
</contexts>
<marker>Core, Allen, 1997</marker>
<rawString>Mark G. Core and James Allen. 1997. Coding dialogs with the DAMSL annotation scheme. In AAAI Fall Symposium on Communicative Action in Humans and Machines, MIT, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Fernndez</author>
<author>J Ginzburg</author>
<author>S Lappin</author>
</authors>
<title>Clarifying ellipsis in dialogue: a machine learning approach</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics</booktitle>
<location>Geneva, Switzerland</location>
<contexts>
<context>led examples of speech acts. In recent years, a range of state-of-the-art machine learning algorithms have been applied to the leading annotated corpora for this task, including Memory-Based Learning(Fernndez et al., 2004), Graph models (Ji and Bilmes, 2006) and Support Vector Machines (Liu, 2006), with little significant performance difference between these models. As an example of these probabilistic methods, Reithi</context>
</contexts>
<marker>Fernndez, Ginzburg, Lappin, 2004</marker>
<rawString>R. Fernndez, J. Ginzburg, and S. Lappin. 2004. Clarifying ellipsis in dialogue: a machine learning approach. In Proceedings of the 20th International Conference on Computational Linguistics, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Grosz</author>
<author>Candace Sidner</author>
</authors>
<title>Attention, Intentions, and the Structure of Discourse</title>
<date>1986</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<contexts>
<context>rforming an utterance. For example, the utterance “Hello” is a form of greeting, whereas the utterance “What’s your name?” is a type of question. A number of researchers (Hirschberg and Litman, 1993; Grosz and Sidner, 1986) speak of cue phrases in utterances that can serve as useful indicators of discourse structure. We have investigated the use of cue phrases to predict dialogue acts (functional tags which represents </context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara Grosz and Candace Sidner. 1986. Attention, Intentions, and the Structure of Discourse. Computational Linguistics, 19(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hilda Hardy</author>
<author>Kirk Baker</author>
<author>Laurence Devillers</author>
<author>Lori Lamel</author>
<author>Sophie Rosset</author>
<author>Tomek Strzalkowski</author>
<author>Cristian Ursu</author>
<author>Nick Webb</author>
</authors>
<title>Multi-layered dialogue annotation for automated multilingual customer service</title>
<date>2002</date>
<booktitle>In Proceedings of the ISLE workshop on Dialogue Tagging for Multimodal Human Computer Interaction</booktitle>
<location>Edinburgh</location>
<contexts>
<context>d using these methods. 3. Experimental Corpora Our work as described here applies to two corpora the DAtagged portion of the SWITCHBOARD corpus (Jurafsky et al., 1998), and the AMITI ´ES GE corpus (Hardy et al., 2002; Hardy et al., 2003), created as part of the AMITI ´ES European 5th Framework program project (Hardy et al., 2005). A summary of the two corpora can be seen in Figure 2. 3.1. Switchboard The annotate</context>
<context> identifying features such as names, addresses and account numbers with generic information (”John Doe”, ”1 The Street”) and the corpus is annotated with DAs using XDML, combining a variant of DAMSL (Hardy et al., 2002) with domain specific semantic information such as account numbers and credit card details (Hardy et al., 2003). The most frequent tag in the AMITI ´ES corpus is Influenceon-listener=“Information-req</context>
</contexts>
<marker>Hardy, Baker, Devillers, Lamel, Rosset, Strzalkowski, Ursu, Webb, 2002</marker>
<rawString>Hilda Hardy, Kirk Baker, Laurence Devillers, Lori Lamel, Sophie Rosset, Tomek Strzalkowski, Cristian Ursu, and Nick Webb. 2002. Multi-layered dialogue annotation for automated multilingual customer service. In Proceedings of the ISLE workshop on Dialogue Tagging for Multimodal Human Computer Interaction, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hardy</author>
<author>K Baker</author>
<author>H Bonneau-Maynard</author>
<author>L Devillers</author>
<author>S Rosset</author>
<author>T Strzalkowski</author>
</authors>
<title>Semantic and dialogic annotation for automated multilingual customer service</title>
<date>2003</date>
<booktitle>In Eurospeech</booktitle>
<location>Geneva, Switzerland</location>
<contexts>
<context>s. 3. Experimental Corpora Our work as described here applies to two corpora the DAtagged portion of the SWITCHBOARD corpus (Jurafsky et al., 1998), and the AMITI ´ES GE corpus (Hardy et al., 2002; Hardy et al., 2003), created as part of the AMITI ´ES European 5th Framework program project (Hardy et al., 2005). A summary of the two corpora can be seen in Figure 2. 3.1. Switchboard The annotated portion of the SWI</context>
<context> Street”) and the corpus is annotated with DAs using XDML, combining a variant of DAMSL (Hardy et al., 2002) with domain specific semantic information such as account numbers and credit card details (Hardy et al., 2003). The most frequent tag in the AMITI ´ES corpus is Influenceon-listener=“Information-request”, which occurs 20% of the time. For this corpus, the average pair-wise kappa score of .59 was significantl</context>
</contexts>
<marker>Hardy, Baker, Bonneau-Maynard, Devillers, Rosset, Strzalkowski, 2003</marker>
<rawString>H. Hardy, K. Baker, H. Bonneau-Maynard, L. Devillers, S. Rosset, and T. Strzalkowski. 2003. Semantic and dialogic annotation for automated multilingual customer service. In Eurospeech, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hardy</author>
<author>A Biermann</author>
<author>R Bryce Inouye</author>
<author>A McKenzie</author>
<author>T Strzalkowski</author>
<author>C Ursu</author>
<author>N Webb</author>
<author>M Wu</author>
</authors>
<title>The AMITI ´ES System: Data-Driven Techniques for Automated Dialogue. Speech Communication</title>
<date>2005</date>
<pages>48--354</pages>
<contexts>
<context>gue systems, has focused on the some of the more conversational roles such acts can perform, and their use in the automatic interpretation of user utterances in spoken language dialogue systems (cf. (Hardy et al., 2005; Pellom et al., 2001; Peckham, 1991)). There are two broad categories of computational model used to interpret these acts. The first, including the work of Cohen and Perrault (1979) and the TRAINS di</context>
<context>rtion of the SWITCHBOARD corpus (Jurafsky et al., 1998), and the AMITI ´ES GE corpus (Hardy et al., 2002; Hardy et al., 2003), created as part of the AMITI ´ES European 5th Framework program project (Hardy et al., 2005). A summary of the two corpora can be seen in Figure 2. 3.1. Switchboard The annotated portion of the SWITCHBOARD corpus comprises 1155 annotated conversations between two human participants, where t</context>
<context>ta, Jurafsky et al. (1998) report an average pair-wise kappa of .80 (Carletta, 1996). An excerpt of dialogue from the SWITCHBOARD corpus can be seen in Figure 3. 3.2. AMITI ´ES The AMITI ´ES project (Hardy et al., 2005) collected 1000 English human-human dialogues from European GE call centres. These calls are of an information seeking or transactional type, in which customers interact with their financial accounts</context>
</contexts>
<marker>Hardy, Biermann, Inouye, McKenzie, Strzalkowski, Ursu, Webb, Wu, 2005</marker>
<rawString>H. Hardy, A. Biermann, R. Bryce Inouye, A. McKenzie, T. Strzalkowski, C. Ursu, N. Webb, and M. Wu. 2005. The AMITI ´ES System: Data-Driven Techniques for Automated Dialogue. Speech Communication, 48:354–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hirschberg</author>
<author>Diane Litman</author>
</authors>
<date>1993</date>
<booktitle>Empirical Studies on the Disambiguation of Cue Phrases. Computational Linguistics</booktitle>
<contexts>
<context>intention of the user when performing an utterance. For example, the utterance “Hello” is a form of greeting, whereas the utterance “What’s your name?” is a type of question. A number of researchers (Hirschberg and Litman, 1993; Grosz and Sidner, 1986) speak of cue phrases in utterances that can serve as useful indicators of discourse structure. We have investigated the use of cue phrases to predict dialogue acts (functiona</context>
<context>al SWITCHBOARD corpus. The ability to apply cues extracted from one corpus to new data is an interesting challenge. It could confirm work which indicates the prominence of such word cues in language (Hirschberg and Litman, 1993). The fact that such cues can be general across domains and applications is of obvious interest. A tag mechanism that can operate across domains presents a range of benefits for example it can be u</context>
</contexts>
<marker>Hirschberg, Litman, 1993</marker>
<rawString>Julia Hirschberg and Diane Litman. 1993. Empirical Studies on the Disambiguation of Cue Phrases. Computational Linguistics, 19(3):501–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gang Ji</author>
<author>Jeff Bilmes</author>
</authors>
<title>Backoff Model Training using Partially Observed Data: Application to Dialog Act Tagging</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology/ American chapter of the Association for Computational Linguistics(HLT/NAACL’06</booktitle>
<contexts>
<context> years, a range of state-of-the-art machine learning algorithms have been applied to the leading annotated corpora for this task, including Memory-Based Learning(Fernndez et al., 2004), Graph models (Ji and Bilmes, 2006) and Support Vector Machines (Liu, 2006), with little significant performance difference between these models. As an example of these probabilistic methods, Reithinger and Klesen (1997) applied a HMM</context>
</contexts>
<marker>Ji, Bilmes, 2006</marker>
<rawString>Gang Ji and Jeff Bilmes. 2006. Backoff Model Training using Partially Observed Data: Application to Dialog Act Tagging. In Proceedings of the Human Language Technology/ American chapter of the Association for Computational Linguistics(HLT/NAACL’06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>Rebecca Bates</author>
<author>Noah Coccaro</author>
<author>Rachel Martin</author>
<author>Marie Meteer</author>
<author>Klaus Ries</author>
<author>Elizabeth Shriberg</author>
<author>Andreas Stolcke</author>
<author>Paul Taylor</author>
<author>Carol Van EssDykema</author>
</authors>
<title>Switchboad discourse language modeling project final report. Research Note 30, Center for Language and Speech Processing</title>
<date>1998</date>
<institution>Johns Hopkins University</institution>
<location>Baltimore</location>
<marker>Jurafsky, Bates, Coccaro, Martin, Meteer, Ries, Shriberg, Stolcke, Taylor, Van EssDykema, 1998</marker>
<rawString>Daniel Jurafsky, Rebecca Bates, Noah Coccaro, Rachel Martin, Marie Meteer, Klaus Ries, Elizabeth Shriberg, Andreas Stolcke, Paul Taylor, and Carol Van EssDykema. 1998. Switchboad discourse language modeling project final report. Research Note 30, Center for Language and Speech Processing, Johns Hopkins University, Baltimore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
</authors>
<title>Using SVM and Error-correcting Codes for Multiclass Dialog Act Classification in Meeting Corpus</title>
<date>2006</date>
<booktitle>In Proceedings of Interspeech (ICSLP</booktitle>
<contexts>
<context>ng algorithms have been applied to the leading annotated corpora for this task, including Memory-Based Learning(Fernndez et al., 2004), Graph models (Ji and Bilmes, 2006) and Support Vector Machines (Liu, 2006), with little significant performance difference between these models. As an example of these probabilistic methods, Reithinger and Klesen (1997) applied a HMM approach to the DA sequences of the VER</context>
</contexts>
<marker>Liu, 2006</marker>
<rawString>Yang Liu. 2006. Using SVM and Error-correcting Codes for Multiclass Dialog Act Classification in Meeting Corpus. In Proceedings of Interspeech (ICSLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Peckham</author>
</authors>
<title>Speech understanding and dialogue over the phone: an overview of progress in the sundial project</title>
<date>1991</date>
<booktitle>In Proceedings of the 2nd European Conference on Speech Communication and Technology</booktitle>
<pages>1469--1472</pages>
<contexts>
<context>he more conversational roles such acts can perform, and their use in the automatic interpretation of user utterances in spoken language dialogue systems (cf. (Hardy et al., 2005; Pellom et al., 2001; Peckham, 1991)). There are two broad categories of computational model used to interpret these acts. The first, including the work of Cohen and Perrault (1979) and the TRAINS dialogue system (Allen et al., 1995), </context>
</contexts>
<marker>Peckham, 1991</marker>
<rawString>J. Peckham. 1991. Speech understanding and dialogue over the phone: an overview of progress in the sundial project. In Proceedings of the 2nd European Conference on Speech Communication and Technology, pages 1469 – 1472.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pellom</author>
<author>W Ward</author>
<author>J Hansen</author>
<author>K Hacioglu</author>
<author>J Zhang</author>
<author>X Yu</author>
<author>S Pradhan</author>
</authors>
<title>Dialog Systems for Travel and Navigation</title>
<date>2001</date>
<booktitle>In Human Language Technology Conference (HLT-2001</booktitle>
<institution>University of Colorado</institution>
<location>San Diego, USA</location>
<contexts>
<context>used on the some of the more conversational roles such acts can perform, and their use in the automatic interpretation of user utterances in spoken language dialogue systems (cf. (Hardy et al., 2005; Pellom et al., 2001; Peckham, 1991)). There are two broad categories of computational model used to interpret these acts. The first, including the work of Cohen and Perrault (1979) and the TRAINS dialogue system (Allen </context>
</contexts>
<marker>Pellom, Ward, Hansen, Hacioglu, Zhang, Yu, Pradhan, 2001</marker>
<rawString>B. Pellom, W. Ward, J. Hansen, K. Hacioglu, J. Zhang, X. Yu, and S. Pradhan. 2001. University of Colorado Dialog Systems for Travel and Navigation. In Human Language Technology Conference (HLT-2001), San Diego, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard J D Power</author>
</authors>
<title>The organisation of purposeful dialogues</title>
<date>1979</date>
<journal>Linguistics</journal>
<pages>17--107</pages>
<contexts>
<context>ata are given in Section 5. Finally we end with some discussion and an outline of intended further work. 2. Related Work Dialogue Acts (DAs) (Bunt, 1994), also known as speech acts or dialogue moves (Power, 1979), represent the functional performance of a speaker’s utterance. Searle (1969) is probably most often associated with speech acts, building on prior work on illocutionary acts by Austin (1962), as a </context>
</contexts>
<marker>Power, 1979</marker>
<rawString>Richard J. D. Power. 1979. The organisation of purposeful dialogues. Linguistics, 17:107–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norbert Reithinger</author>
<author>Martin Klesen</author>
</authors>
<title>Dialogue act classification using language models</title>
<date>1997</date>
<booktitle>In Proceedings of EuroSpeech-97</booktitle>
<marker>Reithinger, Klesen, 1997</marker>
<rawString>Norbert Reithinger and Martin Klesen. 1997. Dialogue act classification using language models. In Proceedings of EuroSpeech-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Samuel</author>
<author>Sandra Carberry</author>
<author>K Vijay-Shanker</author>
</authors>
<title>Dialogue act tagging with transformation-based learning</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics</booktitle>
<location>Montreal</location>
<marker>Samuel, Carberry, Vijay-Shanker, 1998</marker>
<rawString>Ken Samuel, Sandra Carberry, and K. Vijay-Shanker. 1998. Dialogue act tagging with transformation-based learning. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Samuel</author>
<author>Sandra Carberry</author>
<author>K Vijay-Shanker</author>
</authors>
<title>Automatically selecting useful phrases for dialogue act tagging</title>
<date>1999</date>
<booktitle>In Proceedings of the Fourth Conference of the Pacific Association for Computational Linguistics</booktitle>
<location>Waterloo, Ontario, Canada</location>
<contexts>
<context>racy over the VERBMOBIL corpus. A significant aspect of this work, of particular relevance to our work, is the automatic identification of word sequences that might serve as useful dialogue act cues (Samuel et al., 1999). A number of statistical criteria are applied to identify potentially useful word n-grams that are then supplied to the transformation-based learning method as ‘features’. What has not been explored</context>
</contexts>
<marker>Samuel, Carberry, Vijay-Shanker, 1999</marker>
<rawString>Ken Samuel, Sandra Carberry, and K. Vijay-Shanker. 1999. Automatically selecting useful phrases for dialogue act tagging. In Proceedings of the Fourth Conference of the Pacific Association for Computational Linguistics, Waterloo, Ontario, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Searle</author>
</authors>
<title>Speech Acts: An Essay in the Philosophy of Language</title>
<date>1969</date>
<publisher>Cambridge University Press</publisher>
<location>Cambridge</location>
<marker>Searle, 1969</marker>
<rawString>J. R. Searle. 1969. Speech Acts: An Essay in the Philosophy of Language. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
<author>R Dhillon</author>
<author>S Bhagat</author>
<author>J Ang</author>
<author>H Carvey</author>
</authors>
<title>The ICSI meeting recorder dialog act (MRDA) corpus</title>
<date>2004</date>
<booktitle>In Special Interest Group on Discourse and Dialogue (SIGdial</booktitle>
<location>Boston, USA</location>
<marker>Shriberg, Dhillon, Bhagat, Ang, Carvey, 2004</marker>
<rawString>E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey. 2004. The ICSI meeting recorder dialog act (MRDA) corpus. In Special Interest Group on Discourse and Dialogue (SIGdial), Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>K Ries</author>
<author>N Coccaro</author>
<author>E Shriberg</author>
<author>R Bates</author>
<author>D Jurafsky</author>
<author>P Taylor</author>
<author>R Martin</author>
<author>C Van Ess-Dykema</author>
<author>M Meteer</author>
</authors>
<title>Dialogue act modeling for automatic tagging and recognition of conversational speech</title>
<date>2000</date>
<journal>In Computational Linguistics</journal>
<volume>26</volume>
<pages>339--373</pages>
<marker>Stolcke, Ries, Coccaro, Shriberg, Bates, Jurafsky, Taylor, Martin, Van Ess-Dykema, Meteer, 2000</marker>
<rawString>A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates, D. Jurafsky, P. Taylor, R. Martin, C. Van Ess-Dykema, and M. Meteer. 2000. Dialogue act modeling for automatic tagging and recognition of conversational speech. In Computational Linguistics 26(3), 339–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick Webb</author>
<author>Mark Hepple</author>
<author>Yorick Wilks</author>
</authors>
<title>Dialogue Act Classification Based on Intra-Utterance Features</title>
<date>2005</date>
<booktitle>In Proceedings of the AAAI Workshop on Spoken Language Understanding</booktitle>
<contexts>
<context>ful indicators of discourse structure. We have investigated the use of cue phrases to predict dialogue acts (functional tags which represents the communicative intentions behind each user utterance) (Webb et al., 2005a). We developed an approach, in common with the work of Samuel et al. (1999), where word n-grams that might serve as potentially useful cue phrases are automatically detected in a corpus. The novelty</context>
<context>rom the SWITCHBOARD corpus Speaker A: DA=&amp;quot;statement-non-opinion&amp;quot;: but I also believe that the earth is a kind of a self-regulating system Figure 4: Example SWITCHBOARD utterance incorrectly labelled (Webb et al., 2005a). Our best reported figures on the 202k utterance SWITCHBOARD corpus are a cross-validated score of 69.09%, with a single high score of 71.29%, which compares well with the (non-cross-validated) 71%</context>
<context>ct categories seemed to be most easily confused where utterances of one category are consistently incorrectly tagged as being of a second category a view confirmed in a subsequent error analysis (Webb et al., 2005c). For the most part, these errors fall into two categories poor annotation of the data, where two categories have been inconsistently assigned (as with &lt;statement-opinion&gt; vs. &lt;statement-non-opini</context>
<context>his intra-utterace technique cannot resolve. We also presented information that shows that adding a sequence model of DA progressions an n-gram model of DAs results in no increase in performance (Webb et al., 2005a). This is surprising considering that Stolcke et al. (2000) report their best figures when combining a HMM model of the words inside utterances with a tri-gram model of the Dialogue Act sequence, as</context>
</contexts>
<marker>Webb, Hepple, Wilks, 2005</marker>
<rawString>Nick Webb, Mark Hepple, and Yorick Wilks. 2005a. Dialogue Act Classification Based on Intra-Utterance Features. In Proceedings of the AAAI Workshop on Spoken Language Understanding.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick Webb</author>
<author>Mark Hepple</author>
<author>Yorick Wilks</author>
</authors>
<title>Empirical determination of thresholds for optimal dialogue act classification</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Workshop on the Semantics and Pragmatics of Dialogue</booktitle>
<contexts>
<context>ful indicators of discourse structure. We have investigated the use of cue phrases to predict dialogue acts (functional tags which represents the communicative intentions behind each user utterance) (Webb et al., 2005a). We developed an approach, in common with the work of Samuel et al. (1999), where word n-grams that might serve as potentially useful cue phrases are automatically detected in a corpus. The novelty</context>
<context>rom the SWITCHBOARD corpus Speaker A: DA=&amp;quot;statement-non-opinion&amp;quot;: but I also believe that the earth is a kind of a self-regulating system Figure 4: Example SWITCHBOARD utterance incorrectly labelled (Webb et al., 2005a). Our best reported figures on the 202k utterance SWITCHBOARD corpus are a cross-validated score of 69.09%, with a single high score of 71.29%, which compares well with the (non-cross-validated) 71%</context>
<context>ct categories seemed to be most easily confused where utterances of one category are consistently incorrectly tagged as being of a second category a view confirmed in a subsequent error analysis (Webb et al., 2005c). For the most part, these errors fall into two categories poor annotation of the data, where two categories have been inconsistently assigned (as with &lt;statement-opinion&gt; vs. &lt;statement-non-opini</context>
<context>his intra-utterace technique cannot resolve. We also presented information that shows that adding a sequence model of DA progressions an n-gram model of DAs results in no increase in performance (Webb et al., 2005a). This is surprising considering that Stolcke et al. (2000) report their best figures when combining a HMM model of the words inside utterances with a tri-gram model of the Dialogue Act sequence, as</context>
</contexts>
<marker>Webb, Hepple, Wilks, 2005</marker>
<rawString>Nick Webb, Mark Hepple, and Yorick Wilks. 2005b. Empirical determination of thresholds for optimal dialogue act classification. In Proceedings of the Ninth Workshop on the Semantics and Pragmatics of Dialogue.</rawString>
</citation>
</citationList>
</algorithm>

