1:144	Dependencies vs. Constituents for Tree-Based Alignment Daniel Gildea Computer Science Department University of Rochester Rochester, NY 14627 Abstract Given a parallel parsed corpus, statistical treeto-tree alignment attempts to match nodes in the syntactic trees for a given sentence in two languages.
2:144	We train a probabilistic tree transduction model on a large automatically parsed Chinese-English corpus, and evaluate results against human-annotated word level alignments.
3:144	We find that a constituent-based model performs better than a similar probability model trained on the same trees converted to a dependency representation.
4:144	1 Introduction Statistical approaches to machine translation, pioneered by Brown et al.5:144	(1990), estimate parameters for a probabilistic model of word-to-word correspondences and word re-orderings directly from large corpora of parallel bilingual text.
6:144	In recent years, a number of syntactically motivated approaches to statistical machine translation have been proposed.
7:144	These approaches assign a parallel tree structure to the two sides of each sentence pair, and model the translation process with reordering operations defined on the tree structure.
8:144	The tree-based approach allows us to represent the fact that syntactic constituents tend to move as unit, as well as systematic differences in word order in the grammars of the two languages.
9:144	Furthermore, the tree structure allows us to make probabilistic independence assumptions that result in polynomial time algorithms for estimating a translation model from parallel training data, and for finding the highest probability translation given a new sentence.
10:144	Wu (1997) modeled the reordering process with binary branching trees, where each production could be either in the same or in reverse order going from source to target language.
11:144	The trees of Wus Inversion Transduction Grammar were derived by synchronously parsing a parallel corpus, using a grammar with lexical translation probabilities at the leaves and a simple grammar with a single nonterminal providing the tree structure.
12:144	While this grammar did not represent traditional syntactic categories such as verb phrases and noun phrases, it served to restrict the word-level alignments considered by the system to those allowable by reordering operations on binary trees.
13:144	Yamada and Knight (2001) present an algorithm for estimating probabilistic parameters for a similar model which represents translation as a sequence of re-ordering operations over children of nodes in a syntactic tree, using automatic parser output for the initial tree structures.
14:144	This gives the translation model more information about the structure of the source language, and further constrains the reorderings to match not just a possible bracketing as in Wu (1997), but the specific bracketing of the parse tree provided.
15:144	Recent models of alignment have attempted to exploit syntactic information from both languages by aligning a pair of parse trees for the same sentence in either language node by node.
16:144	Eisner (2003) presented such a system for transforming semantic-level dependecy trees into syntactic-level dependency trees for text generation.
17:144	Gildea (2003) trained a system on parallel constituent trees from the Korean-English Treebank, evaluating agreement with hand-annotated word alignments.
18:144	Ding and Palmer (2004) align parallel dependency trees with a divide and conquer strategy, choosing a highly likely word-pair as a splitting point in each tree.
19:144	In addition to providing a deeper level of representation for the transformations of the translation model to work with, tree-to-tree models have the advantage that they are much less computationally costly to train than models which must induce tree structure on one or both sides of the translation pair.
20:144	Because Expectation Maximization for tree-to-tree models iterates over pairs of nodes in the two trees, it is O(n2) in the sentence length, rather than O(n6) for Wus Inversion Transduction Grammar or O(n4) for the Yamada and Knight tree-to-string model.
21:144	In this paper, we make a comparison of two treeto-tree models, one trained on the trees produced by automatic parsers for both our English and Chinese corpora, and one trained on the same parser output converted to a dependency representation.
22:144	The trees are converted using a set of deterministic head rules for each language.
23:144	The dependency representation equalizes some differences in the annotation style between the English and Chinese treebanks.
24:144	However, the dependency representation makes the assumption that not only the bracketing structure, but also the head word choices, will correspond in the two trees.
25:144	Our evaluation is in terms of agreement with word-level alignments created by bilingual human annotators.
26:144	Our model of alignment is that of Gildea (2003), reviewed in Section 2 and extended to dependency trees in Section 3.
27:144	We describe our data and experiments in Section 4, and discuss results in Section 5.
28:144	2 The Tree-to-Tree Model A tree-to-tree alignment model has tree transformation operations for reordering a nodes children, inserting and deleting nodes, and translating individual words at the leaves of the parse trees.
29:144	The transformed tree must not only match the surface string of the target language, but also the tree structure assigned to the string by the parser.
30:144	In order to provide enough flexibility to make this possible, tree transformation operations allow a single node in the source tree to produce two nodes in the target tree, or two nodes in the source tree to be grouped together and produce a single node in the target tree.
31:144	The model can be thought of as a synchronous tree substitution grammar, with probabilities parameterized to generate the target tree conditioned on the structure of the source tree.
32:144	The probability P(TbjTa) of transforming the source tree Ta into target tree Tb is modeled in a sequence of steps proceeding from the root of the target tree down.
33:144	At each level of the tree: 1.
34:144	At most one of the current nodes children is grouped with the current node in a single elementary tree, with probability Pelem(taj"a ) children("a)), conditioned on the current node "a and its children (ie the CFG production expanding "a).
35:144	2.
36:144	An alignment of the children of the current elementary tree is chosen, with probability Palign( j"a ) children(ta)).
37:144	This alignment operation is similar to the re-order operation in the tree-to-string model, with the extension that 1) the alignment can include insertions and deletions of individual children, as nodes in either the source or target may not correspond to anything on the other side, and 2) in the case where two nodes have been grouped into ta, their children are re-ordered together in one step.
38:144	In the final step of the process, as in the tree-tostring model, lexical items at the leaves of the tree are translated into the target language according to a distribution Pt(fje).
39:144	Allowing non-1-to-1 correspondences between nodes in the two trees is necessary to handle the fact that the depth of corresponding words in the two trees often differs.
40:144	A further consequence of allowing elementary trees of size one or two is that some reorderings not allowed when reordering the children of each individual node separately are now possible.
41:144	For example, with our simple tree A B X Y Z if nodes A and B are considered as one elementary tree, with probability Pelem(tajA ) BZ), their collective children will be reordered with probability Palign(f(1; 1)(2; 3)(3; 2)gjA ) XYZ) A X Z Y giving the desired word ordering XZY.
42:144	However, computational complexity as well as data sparsity prevent us from considering arbitrarily large elementary trees, and the number of nodes considered at once still limits the possible alignments.
43:144	For example, with our maximum of two nodes, no transformation of the tree A B W X C Y Z is capable of generating the alignment WYXZ.
44:144	In order to generate the complete target tree, one more step is necessary to choose the structure on the target side, specifically whether the elementary tree has one or two nodes, what labels the nodes have, and, if there are two nodes, whether each child attaches to the first or the second.
45:144	Because we are Operation Parameterization elementary tree grouping Pelem(taj"a ) children("a)) re-order Palign( j"a ) children(ta)) insertion can include insertion symbol lexical translation Pt(fje) cloning Pmakeclone(") can include clone symbol Table 1: The probabilistic tree-to-tree model ultimately interested in predicting the correct target string, regardless of its structure, we do not assign probabilities to these steps.
46:144	The nonterminals on the target side are ignored entirely, and while the alignment algorithm considers possible pairs of nodes as elementary trees on the target side during training, the generative probability model should be thought of as only generating single nodes on the target side.
47:144	Thus, the alignment algorithm is constrained by the bracketing on the target side, but does not generate the entire target tree structure.
48:144	While the probability model for tree transformation operates from the top of the tree down, probability estimation for aligning two trees takes place by iterating through pairs of nodes from each tree in bottom-up order, as sketched below: for all nodes "a in source tree Ta in bottom-up order do for all elementary trees ta rooted in "a do for all nodes "b in target tree Tb in bottom-up order do for all elementary trees tb rooted in "b do for all alignments of the children of ta and tb do ("a;"b) += Pelem(taj"a)Palign( j"i)Q(i;j)2 ("i;"j) end for end for end for end for end for The outer two loops, iterating over nodes in each tree, require O(jTj2).
49:144	Because we restrict our elementary trees to include at most one child of the root node on either side, choosing elementary trees for a node pair is O(m2), where m refers to the maximum number of children of a node.
50:144	Computing the alignment between the 2m children of the elementary tree on either side requires choosing which subset of source nodes to delete, O(22m), which subset of target nodes to insert (or clone), O(22m), and how to reorder the remaining nodes from source to target tree, O((2m)!).
51:144	Thus overall complexity of the algorithm is O(jTj2m242m(2m)!), quadratic in the size of the input sentences, but exponential in the fan-out of the grammar.
52:144	2.1 Clone Operation Both our constituent and dependency models make use of the clone operation introduced by Gildea (2003), which allows words to be aligned even in cases of radically mismatched trees, at a cost in the probability of the alignment.
53:144	Allowing mto-n matching of up to two nodes on either side of the parallel treebank allows for limited nonisomorphism between the trees.
54:144	However, even given this flexibility, requiring alignments to match two input trees rather than one often makes tree-totree alignment more constrained than tree-to-string alignment.
55:144	For example, even alignments with no change in word order may not be possible if the structures of the two trees are radically mismatched.
56:144	Thus, it is helpful to allow departures from the constraints of the parallel bracketing, if it can be done in without dramatically increasing computational complexity.
57:144	The clone operation allows a copy of a node from the source tree to be made anywhere in the target tree.
58:144	After the clone operation takes place, the transformation of source into target tree takes place using the tree decomposition and subtree alignment operations as before.
59:144	The basic algorithm of the previous section remains unchanged, with the exception that the alignments between children of two elementary trees can now include cloned, as well as inserted, nodes on the target side.
60:144	Given that specifies a new cloned node as a child of "j, the choice of which node to clone is made as in the tree-to-string model: Pclone("ijclone 2 ) = Pmakeclone("i)P k Pmakeclone("k) Because a node from the source tree is cloned with equal probability regardless of whether it has already been used or not, the probability of a clone operation can be computed under the same dynamic programming assumptions as the basic tree-to-tree model.
61:144	As with the tree-to-string cloning operation, this independence assumption is essential to keep the complexity polynomial in the size of the input sentences.
62:144	3 Dependency Tree-to-Tree Alignments Dependencies were found to be more consistent than constituent structure between French and English by Fox (2002), though this study used a tree representation on the English side only.
63:144	We wish to investigate whether dependency trees are also more suited to tree-to-tree alignment.
64:144	Figure 1 shows a typical Xinhua newswire sentence with the Chinese parser output, and the sentences English translation with its parse tree.
65:144	The conversion to dependency representation is shown below the original parse trees.
66:144	Examination of the trees shows both cases where the dependency representation is more similar across the two languages, as well as its potential pitfalls.
67:144	The initial noun phrase, 14 Chinese open border cities has two subphrases with a level of constituent structure (the QP and the lower NP) not found in the English parse.
68:144	In this case, the difference in constituent structure derives primarily from differences in the annotation style between the original English and Chinese treebanks (Marcus et al. , 1993; Xue and Xia, 2000; Levy and Manning, 2003).
69:144	These differences disappear in the constituent representation.
70:144	In general, the number of levels of constituent structure in a tree can be relatively arbitrary, while it is easier for people (whether professional syntacticians or not) to agree on the word-to-word dependencies.
71:144	In some cases, differences in the number of level may be handled by the tree-to-tree model, for example by grouping the subject NP and its base NP child together as a single elementary tree.
72:144	However, this introduces unnecessary variability into the alignment process.
73:144	In cases with large difference in the depths of the two trees, the aligner may not be able to align the corresponding terminal nodes because only one merge is possible at each level.
74:144	In this case the aligner will clone the subtree, at an even greater cost in probability.
75:144	The rest of our example sentence, however, shows cases where the conversion to dependency structure can in some cases exacerbate differences in constituent structure.
76:144	For example, jingji and jianshe are sisters in the original constituent structure, as are their English translations, economic and construction.
77:144	In the conversion to Chinese dependency structure, they remain sisters both dependent on the noun chengjiu (achievements) while in English, economic is a child of construction.
78:144	The correspondence of a three-noun compound in Chinese to a noun modified by prepositional phrase and an adjective-noun relation in English means that the conversion rules select different heads even for pieces of tree that are locally similar.
79:144	3.1 The Dependency Alignment Model While the basic tree-to-tree alignment algorithm is the same for dependency trees, a few modifications to the probability model are necessary.
80:144	First, the lexical translation operation takes place at each node in the tree, rather than only at the leaves.
81:144	Lexical translation probabilities are maintained for each word pair as before, and the lexical translation probabilities are included in the alignment cost for each elementary tree.
82:144	When both elementary trees contain two words, either alignment is possible between the two.
83:144	The direct alignment between nodes within the elementary tree has probability 1 Pswap.
84:144	A new parameter Pswap gives the probability of the upper node in the elementary tree in English corresponding to the lower node in Chinese, and vice versa.
85:144	Thus, the probability for the following transformation: A B X Y ) B A X Y is factored as Pelem(ABjA)B) Pswap Pt(A0jA) Pt(B0jB) Palign(f(1; 1)(2; 2)gjA ) XY ).
86:144	Our model does not represent the position of the head among its children.
87:144	While this choice would have to be made in generating MT output, for the purposes of alignment we simply score how many tree nodes are correctly aligned, without flattening our trees into a string.
88:144	We further extended the tree-to-tree alignment algorithm by conditioning the reordering of a nodes children on the nodes lexical item as well as its syntactic category at the categories of its children.
89:144	The lexicalized reordering probabilities were smoothed with the nonlexicalized probabilities (which are themselves smoothed with a uniform distribution).
90:144	We smooth using a linear interpolation of lexicalized and unlexicalized probabilities, with weights proportional to the number of observations for each type of event.
91:144	4 Experiments We trained our translation models on a parallel corpus of Chinese-English newswire text.
92:144	We reIP NP NP NR Zhongguo QP CD shisi CLP M ge NP NN bianjing NN kaifang NN chengshi NP NN jingji NN jianshe NN chengjiu VP VV xianzhu S NP CD 14 NNP Chinese JJ open NN border NNS cities VP VBP make NP NP JJ significant NNS achievements PP IN in NP JJ economic NN construction VV:xianzhu NN:chengshi NR:Zhongguo CD:shisi M:ge NN:bianjing NN:kaifang NN:chengjiu NN:jingji NN:jianshe VV:make NNS:cities CD:14 NNP:Chinese JJ:open NN:border NNS:achievements JJ:significant IN:in NN:construction JJ:economic Figure 1: Constituent and dependency trees for a sample sentence Alignment Precision Recall Error Rate IBM Model 1.56 .42 .52 IBM Model 4 .67 .43 .47 Constituent Tree-to-Tree .51 .48 .50 Dependency Tree-to-Tree .44 .38 .60 Dependency, lexicalized reordering .41 .37 .61 Table 2: Alignment results on Chinese-English corpus.
93:144	Higher precision and recall correspond to lower alignment error rate.
94:144	stricted ourselves to sentences of no more than 25 words in either language, resulting in a training corpus of 18,773 sentence pairs with a total of 276,113 Chinese words and 315,415 English words.
95:144	The Chinese data were automatically segmented into tokens, and English capitalization was retained.
96:144	We replace words occurring only once with an unknown word token, resulting in a Chinese vocabulary of 23,783 words and an English vocabulary of 27,075 words.
97:144	Chinese data was parsed using the parser of Bikel (2002), and English data was parsed using Collins (1999).
98:144	Our hand-aligned test data were those used in Hwa et al.99:144	(2002), and consisted of 48 sentence pairs also with less than 25 words in either language, for a total of 788 English words and 580 Chinese words.
100:144	The hand aligned data consisted of 745 individual aligned word pairs.
101:144	Words could be aligned one-to-many in either direction.
102:144	This limits the performance achievable by our models; the IBM models allow one-to-many alignments in one direction only, while the tree-based models allow only one-to-one alignment unless the cloning operation is used.
103:144	A separate set of 49 hand-aligned sentence pairs was used to control overfitting in training our models.
104:144	We evaluate our translation models in terms of agreement with human-annotated word-level alignments between the sentence pairs.
105:144	For scoring the viterbi alignments of each system against goldstandard annotated alignments, we use the alignment error rate (AER) of Och and Ney (2000), which measures agreement at the level of pairs of words:1 AER = 1 2jA \ GjjAj + jGj where A is the set of word pairs aligned by the automatic system, and G the set aligned in the gold standard.
106:144	For a better understanding of how the models 1While Och and Ney (2000) differentiate between sure and possible hand-annotated alignments, our gold standard alignments come in only one variety.
107:144	differ, we break this figure down into precision: P = jA \ GjjAj and recall: R = jA \ GjjGj Since none of the systems presented in this comparison make use of hand-aligned data, they may differ in the overall proportion of words that are aligned, rather than inserted or deleted.
108:144	This affects the precision/recall tradeoff; better results with respect to human alignments may be possible by adjusting an overall insertion probability in order to optimize AER.
109:144	Table 2 provides a comparison of results using the tree-based models with the word-level IBM models.
110:144	IBM Models 1 and 4 refer to Brown et al.111:144	(1993).
112:144	We used the GIZA++ package, including the HMM model of Och and Ney (2000).
113:144	We trained each model until AER began to increase on our held-out cross validation data, resulting in running Model 1 for three iterations, then the HMM model for three iterations, and finally Model 4 for two iterations (the optimal number of iterations for Models 2 and 3 was zero).
114:144	Constituent Tree-to-Tree indicates the model of Section 2 trained and tested directly on the trees output by the parser, while Dependency Tree-to-Tree makes the modifications to the model described in Section 3.
115:144	For reasons of computational efficiency, our constituent-based training procedure skipped sentences for which either tree had a node with more than five children, and the dependency-based training skipped trees with more than six children.
116:144	Thus, the tree-based models were effectively trained on less data than IBM Model 4: 11,422 out of 18,773 sentence pairs for the constituent model and 10,662 sentence pairs for the dependency model.
117:144	Our tree-based models were initialized with lexical translation probabilities trained using IBM Model 1, and uniform probabilities for the tree reordering operations.
118:144	The models were trained until AER began to rise on our held-out cross-validation data, though in practice AER was nearly constant for both tree-based models after the first iteration.
119:144	5 Discussion The constituent-based version of the alignment model significantly outperforms the dependencybased model.
120:144	The IBM models outperform the constituent tree-to-tree model to a lesser degree, with tree-to-tree achieving higher recall, and IBM higher precision.
121:144	It is particularly significant that the treebased model gets higher recall than the other models, since it is limited to one-to-one alignments unless the clone operation is used, bounding the recall it can achieve.
122:144	In order to better understand the differences between the constituent and dependency representations of our data, we analyzed how well the two representations match our hand annotated alignment data.
123:144	We looked for consistently aligned pairs of constituents in the two parse trees.
124:144	By consistently aligned, we mean that all words within the English constituent are aligned to words inside the Chinese constituent (if they are aligned to anything), and vice versa.
125:144	In our example in Figure 1, the NP 14 Chinese border cities and the Chinese subject NP Zhongguo shisi ge bianjing kaifang chengshi are consistenly aligned, but the PP in economic construction has no consistently aligned constituent in the Chinese sentence.
126:144	We found that of the 2623 constituents in our English parse trees (not counting unary consituents, which have the same boundaries as their children), for 1044, or 40%, there exists some constituent in the Chinese parse tree that is consistently aligned.
127:144	This confirms the results of Fox (2002) and Galley et al.128:144	(2004) that many translation operations must span more than one parse tree node.
129:144	For each of our consistently aligned pairs, we then found the head word of both the Chinese and English constituents according to our head rules.
130:144	The two head words correspond in the annotated alignments 67% of the time (700 out of 1044 consistently aligned constituent pairs).
131:144	While the headswapping operation of our translation model will be able to handle some cases of differing heads, it can only do so if the two heads are adjacent in both tree structures.
132:144	Our system is trained and test on automatically generated parse trees, which may contribute to the mismatches in the tree structures.
133:144	As our test data was taken from the Chinese Treebank, handannotated parse trees were available for the Chinese, but not the English, sentences.
134:144	Running the analysis on hand-annotated Chinese trees found slightly better English/Chinese agreement overall, but there were still disagreements in the head words choices for a third of all consistently aligned constuent pairs.
135:144	Running our alignment system on gold standard trees did not improve results.
136:144	The comparison between parser output and gold standard trees is summarized in Table 3.
137:144	We used head rules developed for statistical parsers in both languages, but other rules may be better suited to the alignment task.
138:144	For example, the tensed auxiliary verb is considered the head of English progressive and perfect verb phrases, rather than the present or past particple of the main verb.
139:144	Such auxiliaries carry agreement information relevant to parsing, but generally have no counterpart in Chinese.
140:144	A semantically oriented dependency structure, such as Tree Adjoining Grammar derivation trees, may be more appropriate for alignment.
141:144	6 Conclusion We present a comparison of constituent and dependency models for tree-to-tree alignment.
142:144	Despite equalizing some mismatches in tree structure, the dependency representation does not perform as well, likely because it is less robust to large differences between the tree structures.
143:144	Acknowledgments We are very grateful to Rebecca Hwa, Hao Zhang, everyone at the 2003 John Hopkins speech and language summer research workshop, and EMNLPs reviewers for their assistance, criticism, and data.
144:144	This work was partially supported by NSF ITR IIS-09325646, NSF research infrastructure grant EIA-0080124, and NSF grant 0121285 to the summer workshop.

