Evaluation Framework Question series in TREC represent an attempt at modeling information-seeking dialogues between a user and a system (Kato et al., 2004).
W04-2509
The “goodness” of answers can only be quantified with respect to a task— examples range from winning a game show (Clarke et al., 2001) to intelligence gathering (Small et al., 2004).
C04-1189
We employed POURPRE (Lin and Demner-Fushman, 2005), a recently developed method for automatically evaluating answers to complex questions.
H05-1117
To assess the quality of system responses, we adopt the nugget-based methodology used previously for many types of complex questions (Voorhees, 2003), which shares similarities with the pyramid evaluation scheme used in summarization (Nenkova and Passonneau, 2004).
N04-1019
Since 2004, the main task at the TREC QA tracks has consisted of question series organized around topics (called “targets”)—which can be people, organizations, entities, or events (Voorhees, 2004; Voorhees, 2005).
H05-1038
As a result, an IR-based sentence extraction approach performs quite well—this explanation is consistent with the observations of Lin and Demner-Fushman (2006).
N06-1049
Nevertheless, one might imagine that such output forms the basis for generating coherent query-focused summaries with sentencerewrite techniques, e.g., (Barzilay et al., 1999).
P99-1071
Voorhees. 2005.
H05-1038
