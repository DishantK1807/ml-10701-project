<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<title>Speech translation by confusion network decoding</title>
<date>2007</date>
<booktitle>In Proceedings of the 32nd IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP</booktitle>
<location>Honolulu, Hawaii, USA</location>
<contexts>
<context>sentences of a given list will be very similar, so translating each of them independently is more computationally expensive than translating a lattice where common parts are factored. Bertoldi et al. (2007) present an algorithm translating confusion networks. Confusion networks are lattices where each node has at most one predecessor and one successor. Their algorithm deals with non-monotone efficiently</context>
</contexts>
<marker>2007</marker>
<rawString>2007. Speech translation by confusion network decoding. In Proceedings of the 32nd IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Honolulu, Hawaii, USA, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep M Crego</author>
<author>Jos´e B Mari˜no</author>
</authors>
<title>Extending MARIE: an N-gram-based SMT decoder</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions</booktitle>
<pages>213--216</pages>
<location>Prague, Czech Republic</location>
<marker>Crego, Mari˜no, 2007</marker>
<rawString>Josep M. Crego and Jos´e B. Mari˜no. 2007. Extending MARIE: an N-gram-based SMT decoder. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 213–216, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cameron Shaw Fordyce</author>
</authors>
<title>Overview of the iwslt 2007 evaluation campaign</title>
<date>2007</date>
<booktitle>In Proceedings of IWSLT 2007</booktitle>
<location>Trento, Italy</location>
<contexts>
<context>e created and tuned to convert word lattices into confusion networks. The decoder presented in this work was initially developed for the International Workshop on Spoken Language Translation (IWSLT) (Fordyce, 2007; Patry et al., 2007). It is a phrase-based system (Koehn et al., 2003) translating lattices in two passes. The first pass simultaneously recognizes and translates the source sentence and the second p</context>
</contexts>
<marker>Fordyce, 2007</marker>
<rawString>Cameron Shaw Fordyce. 2007. Overview of the iwslt 2007 evaluation campaign. In Proceedings of IWSLT 2007, Trento, Italy, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<date>2003</date>
<contexts>
<context>rks. The decoder presented in this work was initially developed for the International Workshop on Spoken Language Translation (IWSLT) (Fordyce, 2007; Patry et al., 2007). It is a phrase-based system (Koehn et al., 2003) translating lattices in two passes. The first pass simultaneously recognizes and translates the source sentence and the second pass rescores a list of top-ranked translations obtained from the first</context>
<context>ated with path p. The decoder described in this work estimates Eq. (2) under the so-called maximum approximation: ^t = argmax t2Lt max p2Po Pr(t;sp;pjo) (3) 3. Decoder A typical phrase-based decoder (Koehn et al., 2003) translates a source sentence one phrase at a time using a translation table (a bilingual dictionary of phrases). A partial target sentence can be extended by the translation of any untranslated phra</context>
<context>l the material and we removed the punctuation marks, since our lattices do not contain any. We trained one translation table on IN-DOMAIN and another on EUROPARL using the grow-diag-final heuristics (Koehn et al., 2003), which extracts phrase pairs from a word alignment that was produced by GIZA++ (Och and Ney, 2000). Knowing that the corpora contain many dates and numbers, we manually created a third translation t</context>
<context>directions estimated by relative frequencies. The lexical weighting of the phrases in both translation directions. Lexical weighting estimates the probability of a phrase at the word alignment level (Koehn et al., 2003). Three binary functions associating a pair of phrases with its origin (IN-DOMAIN, EUROPARL or manually created). and the following feature functions are added for the second pass: Two English and tw</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003.</rawString>
</citation>
<citation valid="true">
<title>Statistical phrase-based translation</title>
<booktitle>In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics</institution>
<location>Morristown, NJ, USA</location>
<marker></marker>
<rawString>Statistical phrase-based translation. In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 48–54, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic</location>
<contexts>
<context>ces have been developed. While MISTRAL uses lattices for spoken lan1http://www.gnu.org/copyleft/gpl.html guage translation, MARIE uses them to encode word reordering (Crego and Mari˜no, 2007). MOSES (Koehn et al., 2007) translates confusion networks and lattices, but at the time of this writing, its lattice translation algorithm is not documented. We are thus not able to compare our work with it. This paper is orga</context>
<context>lation, which can distort a little bit the source sentences to make them easier to translate. To validate our implementation, we translated the ASR output and the reference transcriptions with MOSES (Koehn et al., 2007). When given the same parameters, both MISTRAL and MOSES produce exactly the same translations. 5. Conclusion We presented and evaluated MISTRAL, a decoder for spoken language translation. To the bes</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation</title>
<date>2005</date>
<booktitle>In 2nd Workshop on EBMT of MT-Summit X</booktitle>
<contexts>
<context>use the IN-DOMAIN corpus is small, we also trained our models on the Italian-English section of the proceedings of the European Parliament (EUROPARL), which contains more than 928,000 sentence pairs (Koehn, 2005). Before training, we lowercased all the material and we removed the punctuation marks, since our lattices do not contain any. We trained one translation table on IN-DOMAIN and another on EUROPARL us</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In 2nd Workshop on EBMT of MT-Summit X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Mathias</author>
<author>W Byrne</author>
</authors>
<title>Statistical phrase-based speech translation</title>
<date>2006</date>
<booktitle>In IEEE Conference on Acoustics, Speech and Signal Processing</booktitle>
<contexts>
<context>m to reduce its recognition errors (Ney, 1999). Word lattices have already been translated by systems based on finite state transducers (Saleem et al., 2004; Matusov et al., 2005; Zhang et al., 2005; Mathias and Byrne, 2006). Our decoder manipulates the same kind of lattices as finite state transducers, but it allows a finer control on pruning policies and it eases the recovery of aggregated informations like individual</context>
</contexts>
<marker>Mathias, Byrne, 2006</marker>
<rawString>L. Mathias and W. Byrne. 2006. Statistical phrase-based speech translation. In IEEE Conference on Acoustics, Speech and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Matusov</author>
<author>S Kanthak</author>
<author>H Ney</author>
</authors>
<title>On the integration of speech recognition and statistical machine translation</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th European Conference on Speech Communication and Technology (Interspeech</booktitle>
<contexts>
<context>d in the MT system will help the ASR system to reduce its recognition errors (Ney, 1999). Word lattices have already been translated by systems based on finite state transducers (Saleem et al., 2004; Matusov et al., 2005; Zhang et al., 2005; Mathias and Byrne, 2006). Our decoder manipulates the same kind of lattices as finite state transducers, but it allows a finer control on pruning policies and it eases the recove</context>
</contexts>
<marker>Matusov, Kanthak, Ney, 2005</marker>
<rawString>E. Matusov, S. Kanthak, and H. Ney. 2005. On the integration of speech recognition and statistical machine translation. In Proceedings of the 9th European Conference on Speech Communication and Technology (Interspeech), September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermann Ney</author>
</authors>
<title>Speech translation: coupling of recognition and translation</title>
<date>1999</date>
<booktitle>In Proceedings of ICASSP</booktitle>
<contexts>
<context> translate lattices produced by ASR systems. The motivation for lattice translation is the hope that the knowledge embedded in the MT system will help the ASR system to reduce its recognition errors (Ney, 1999). Word lattices have already been translated by systems based on finite state transducers (Saleem et al., 2004; Matusov et al., 2005; Zhang et al., 2005; Mathias and Byrne, 2006). Our decoder manipul</context>
</contexts>
<marker>Ney, 1999</marker>
<rawString>Hermann Ney. 1999. Speech translation: coupling of recognition and translation. In Proceedings of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Improved statistical alignment models</title>
<date>2000</date>
<booktitle>In Conference of the Association for Computational Linguistic (ACL</booktitle>
<pages>440--447</pages>
<location>Hong Kong, China</location>
<contexts>
<context>ed one translation table on IN-DOMAIN and another on EUROPARL using the grow-diag-final heuristics (Koehn et al., 2003), which extracts phrase pairs from a word alignment that was produced by GIZA++ (Och and Ney, 2000). Knowing that the corpora contain many dates and numbers, we manually created a third translation table made of translations for days, months and numbers. For all the experiments, the first pass is </context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F. J. Och and H. Ney. 2000. Improved statistical alignment models. In Conference of the Association for Computational Linguistic (ACL), pages 440–447, Hong Kong, China, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Daniel Gildea</author>
</authors>
<title>Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar</title>
<date>2004</date>
<location>Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen</location>
<marker>Och, Gildea, 2004</marker>
<rawString>Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A smorgasbord of features for statistical machine translation.</rawString>
</citation>
<citation valid="true">
<date></date>
<booktitle>HLT-NAACL 2004: Main Proceedings</booktitle>
<pages>161--168</pages>
<editor>In Daniel Marcu Susan Dumais and Salim Roukos, editors</editor>
<publisher>Association for Computational Linguistics</publisher>
<location>Boston, Massachusetts, USA</location>
<marker></marker>
<rawString>In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 161– 168, Boston, Massachusetts, USA, May. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation</title>
<date>2001</date>
<booktitle>In ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<location>Morristown, NJ, USA</location>
<contexts>
<context>he target sentence. We compare the source sentences extracted from the lattices with the original transcriptions using word-error-rate (WER) and the target with the reference translations using BLEU (Papineni et al., 2001). Both scores take a value between zero and one, but reported figures are multiplied by 100 to enhance readability. 4.3. Feature Functions The following feature functions are used in the first pass: </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2001. Bleu: a method for automatic evaluation of machine translation. In ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311–318, Morristown, NJ, USA. Association for Computational Linguistics. Alexandre Patry, Fabrizo Gotti, and Philippe Langlais.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MOOD</author>
</authors>
<title>A modular object-oriented decoder for statistical machine translation</title>
<date></date>
<booktitle>In 5th LREC</booktitle>
<pages>709--714</pages>
<location>Genoa, Italy</location>
<marker>MOOD, </marker>
<rawString>2006. MOOD: A modular object-oriented decoder for statistical machine translation. In 5th LREC, pages 709– 714, Genoa, Italy, May. Alexandre Patry, Philippe Langlais, and Fr´ed´eric B´echet.</rawString>
</citation>
<citation valid="true">
<title>MISTRAL: A lattice translation system for IWSLT 2007</title>
<date>2007</date>
<booktitle>In Proceedings of IWSLT 2007</booktitle>
<location>Trento, Italy</location>
<contexts>
<context>sentences of a given list will be very similar, so translating each of them independently is more computationally expensive than translating a lattice where common parts are factored. Bertoldi et al. (2007) present an algorithm translating confusion networks. Confusion networks are lattices where each node has at most one predecessor and one successor. Their algorithm deals with non-monotone efficiently</context>
</contexts>
<marker>2007</marker>
<rawString>2007. MISTRAL: A lattice translation system for IWSLT 2007. In Proceedings of IWSLT 2007, Trento, Italy, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William H Press</author>
<author>Saul A Teukolsky</author>
<author>William T Vetterling</author>
<author>Brian P Flannery</author>
</authors>
<title>Numerical Recipes in C: The Art of Scientific Computing</title>
<date>1992</date>
<publisher>Cambridge University Press</publisher>
<location>New York, NY, USA</location>
<contexts>
<context>scores from models trained on IN-DOMAIN and EUROPARL corpora in both translation directions. The weights of the different feature functions are optimized on BLEU using the downhill simplex algorithm (Press et al., 1992). All the weights are initialized to 0.1 except the weights of ASR features, which get a higher values in order to start with good source sentences. Running the decoder whenever weights are updated i</context>
</contexts>
<marker>Press, Teukolsky, Vetterling, Flannery, 1992</marker>
<rawString>William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. 1992. Numerical Recipes in C: The Art of Scientific Computing. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H V Quan</author>
<author>M Federico</author>
<author>M Cettolo</author>
</authors>
<title>Integrated n-best re-ranking for spoken language translation</title>
<date>2005</date>
<booktitle>In Interspeech 2005 Eurospeech 9th European Conference on Speech Communication and Technology</booktitle>
<contexts>
<context> an n-best list of sentences extracted from the lattice and then select the best Figure 1: A lattice of words where nodes are labelled with time and edges with words. translation (Zhang et al., 2004; Quan et al., 2005). Most of the sentences of a given list will be very similar, so translating each of them independently is more computationally expensive than translating a lattice where common parts are factored. B</context>
</contexts>
<marker>Quan, Federico, Cettolo, 2005</marker>
<rawString>H.V. Quan, M. Federico, and Cettolo M. 2005. Integrated n-best re-ranking for spoken language translation. In Interspeech 2005 Eurospeech 9th European Conference on Speech Communication and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Saleem</author>
<author>S-C Jou</author>
<author>S Vogel</author>
<author>T Schultz</author>
</authors>
<title>Using word lattice information for a tighter coupling in speech translation systems</title>
<date>2004</date>
<booktitle>In Proc. ICSLP, Jeju Island</booktitle>
<location>Korea</location>
<contexts>
<context>the knowledge embedded in the MT system will help the ASR system to reduce its recognition errors (Ney, 1999). Word lattices have already been translated by systems based on finite state transducers (Saleem et al., 2004; Matusov et al., 2005; Zhang et al., 2005; Mathias and Byrne, 2006). Our decoder manipulates the same kind of lattices as finite state transducers, but it allows a finer control on pruning policies a</context>
</contexts>
<marker>Saleem, Jou, Vogel, Schultz, 2004</marker>
<rawString>S. Saleem, S.-C. Jou, S. Vogel, and T. Schultz. 2004. Using word lattice information for a tighter coupling in speech translation systems. In Proc. ICSLP, Jeju Island, Korea, Oct.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM an extensible language modeling toolkit</title>
<date>2002</date>
<booktitle>In Proceedings of ICSLP</booktitle>
<location>Denver, Colorado</location>
<contexts>
<context>ort of shadows all the other scores. In order to reduce the variance of ASR scores, we augmented each edge with its posterior probability using the lattice-tool utility packaged in the SRILM toolkit (Stolcke, 2002). The posterior probability of an edge is computed by summing the scores of all the paths containing the edge normalized over the sum of the scores of all the paths in the lattice (Wessel et al., 200</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM an extensible language modeling toolkit. In Proceedings of ICSLP, Denver, Colorado, Sept.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Wessel</author>
<author>Ralf Schl¨uter</author>
<author>Hermann Ney</author>
</authors>
<title>Using posterior word probabilities for improved speech recognition</title>
<date>2000</date>
<booktitle>In IEEE International Conference on Acoustics, Speech, and Signal Processing</booktitle>
<pages>1587--1590</pages>
<location>Istanbul, Turkey</location>
<marker>Wessel, Schl¨uter, Ney, 2000</marker>
<rawString>Frank Wessel, Ralf Schl¨uter, and Hermann Ney. 2000. Using posterior word probabilities for improved speech recognition. In IEEE International Conference on Acoustics, Speech, and Signal Processing, pages 1587– 1590, Istanbul, Turkey, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruiqiang Zhang</author>
<author>Genichiro Kikui</author>
<author>Hirofumi Yamamoto</author>
<author>Taro Watanabe</author>
<author>Frank Soong</author>
<author>Wai Kit Lo</author>
</authors>
<title>A unified approach in speech-to-speech translation: integrating features of speech recognition and machine translation</title>
<date>2004</date>
<booktitle>In COLING ’04: Proceedings of the 20th international conference on Computational Linguistics</booktitle>
<pages>1168</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics</institution>
<location>Morristown, NJ, USA</location>
<contexts>
<context>mise is to translate an n-best list of sentences extracted from the lattice and then select the best Figure 1: A lattice of words where nodes are labelled with time and edges with words. translation (Zhang et al., 2004; Quan et al., 2005). Most of the sentences of a given list will be very similar, so translating each of them independently is more computationally expensive than translating a lattice where common pa</context>
</contexts>
<marker>Zhang, Kikui, Yamamoto, Watanabe, Soong, Lo, 2004</marker>
<rawString>Ruiqiang Zhang, Genichiro Kikui, Hirofumi Yamamoto, Taro Watanabe, Frank Soong, and Wai Kit Lo. 2004. A unified approach in speech-to-speech translation: integrating features of speech recognition and machine translation. In COLING ’04: Proceedings of the 20th international conference on Computational Linguistics, page 1168, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>

