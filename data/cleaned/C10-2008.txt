Coling 2010: Poster Volume, pages 63–71,
Beijing, August 2010
A Formal Scheme for Multimodal Grammars
Philippe Blache & Laurent Prévot
LPL-CNRS, Université de Provence
blache@lpl-aix.fr
Abstract
We present in this paper a formal approach
for the representation of multimodal in-
formation. This approach, thanks to the
to use of typed feature structures and hy-
pergraphs, generalizes existing ones (typ-
ically annotation graphs) in several ways.
It first proposes an homogenous represen-
tation of different types of information
(nodes and relations) coming from differ-
ent domains (speech, gestures). Second,
it makes it possible to specify constraints
representing the interaction between the
different modalities, in the perspective of
developing multimodal grammars.
1 Introduction
Multimodality became in the last decade an im-
portant challenge for natural language processing.
Among the problems we are faced with in this do-
main, one important is the understanding of how
does the different modalities interact in order to
produce meaning. Addressing this question re-
quires to collect data (building corpora), to de-
scribe them (enriching corpora with annotations)
and to organize systematically this information
into a homogeneous framework in order to pro-
duce, ideally, multimodal grammars.
Many international projects address this ques-
tion from different perspectives: data represen-
tation and coding schemes (cf. ISLE (Dybk-
jaer, 2001), MUMIN (Allwood, 2005), etc.), cor-
pus annotation (cf. LUNA (Rodriguez, 2007) or
DIME (Pineda, 2000), etc.), annotation and edit-
ing tools (such as NITE NXT (Carletta, 2003),
Anvil (Kipp, 2001), Elan (Wittenburg, 2006),
Praat (Boersma, 2009), etc.).
We propose in this paper a generic approach
addressing both formal representation and con-
crete annotation of multimodal data, that relies on
typed-feature structure (TFS), used as a descrip-
tion language on graphs. This approach is generic
in the sense that it answers to different needs: it
provides at the same time a formalism directly us-
able for corpus annotation and a description lan-
guage making it possible to specify constraints
that constitute the core of a multimodal grammar.
In the first section, we motivate the use of TFS
and present how to concretely implement them for
multimodal annotation. We address in the second
section one of the most problematic question for
multimodal studies: how to represent and imple-
ment the relations between the different domains
and modalities (a simple answer in terms of time
alignment being not powerful enough). In the last
section, we describe how to make use of this rep-
resentation in order to specify multimodal gram-
mars.
2 Typed-feature structures modeling
Information representation is organized in two di-
mensions: type hierarchies and constituency re-
lations (typically, a prosodic unit is a set of syl-
lables, which in turn are sets of phonemes). The
former corresponds to an is-a relation, the latter to
a part-of one. For example intonational phrase is
a subtype of prosodic phrase, and phonemes are
constituents of syllables.
Such an organization is directly represented by
means of typed feature structures. They can be
considered as a formal annotation schema, used as
63
a preliminary step before the definition of the con-
crete coding scheme1. This step is necessary when
bringing together information (and experts) from
different fields: it constitutes a common represen-
tation framework, homogenizing information rep-
resentation. Moreover, it allows to clearly distin-
guish between knowledge representation and an-
notation. The coding scheme, at the annotation
level (labels, features, values), is deduced from
this formal level.
The remaining of the section illustrates how
to represent objects from different domains by
means of TFS. The Figure 1 presents the type hi-
erarchy and the constituency structure of objects
taken here as example.
2.1 Phonetics
The phoneme is used as primary data: this object
is at the lowest level of the constituent hierarchy
(most of the objects are set of phonemes). The fol-
lowing feature structure proposes a precise encod-
ing of the main properties describing a phoneme,
including articulatory gestures.
phon











SAMPA_LABEL sampa_unit
CAT
braceleftbig
vowel, consonant
bracerightbig
TYPE
braceleftbig
occlusive, fricative, nasal, etc.
bracerightbig
ARTICULATION






LIP
bracketleftBig
PROTUSION string
APERTURE aperture
bracketrightBig
TONGUE

TIP
bracketleftBig
LOCATION string
DEGREE string
bracketrightBig
BODY
bracketleftBig
LOCATION string
DEGREE string
bracketrightBig


VELUM aperture
GLOTTIS aperture






ROLE
bracketleftBig
EPENTHETIC boolean
LIAISON boolean
bracketrightBig











Phonemes being at the lowest level, they do not
have any constituents. They are not organized
into precise subtypes. The feature structure rep-
resent then the total information associated with
this type.
2.2 Prosody
As seen above, prosodic phrases are of two differ-
ent subtypes: ap (accentual phrases) and ip (into-
national phrases). The prosodic type hierarchy is
represented as follows:
1This approach has been first defined and experimented
in the XXXX project, not cited for anonymity reasons.
pros_phr
a8a8
a8a8
a72a72
a72a72
apbracketleftBig
LABEL AP
CONSTS list(syl)
bracketrightBig ip


LABEL IP
CONSTS list(ap)
CONTOUR
bracketleftBigg
DIRECTION string
POSITION string
FUNCTION string
bracketrightBigg



Accentual phrases have two appropriate fea-
tures: the label which is simply the name of the
corresponding type, and the list of constituents, in
this case a list of syllables. The objects of type ip
contain the list of its constituents (a set of aps) as
well as the description of its contour. A contour is
a prosodic event, situated at the end of the ip and
is usually associated to an ap.
The prosodic phrases are defined as set of syl-
lables. They are described by several appropriate
features: the syllable structure, its position in the
word, its possibility to be accented or prominent:
syl




STRUCT syl_struct
POSITION
bracketleftBigg
RANK
braceleftbig
integer
bracerightbig
SYL_NUMBER
braceleftbig
integer
bracerightbig
bracketrightBigg
ACCENTUABLE boolean
PROMINENCE boolean
CONSTITUENTS list(const_syl)




Syllable constituents (objects of type const_syl)
are described by two different features: the set of
phonemes (syllable constituents), and the type of
the constituent (onset, nucleus and coda). Note
that each syllable constituent can contain a set of
phonemes.
const_syl
bracketleftbigg
PHON list(phon)
CONST_TYPE
braceleftbig
onset, nucleus, coda
bracerightbig
bracketrightbigg
2.3 Disfluencies
We can distinguish two kinds of disfluencies: non
lexicalized (without any lexical material, such as
lengthening, silent pauses or filled pauses) and
lexicalized (non-voluntary break in the phrasal
flow, generating a word or a phrase fragment).
Lexicalized disfluencies have a particular organi-
zation with three subparts (or constituents):
• Reparandum: the word or phrase fragment,
in which the break occurs
• Break: a point or an interval that can eventu-
ally be filled by a fragment repetition, paren-
thetical elements, etc.
64
object
a16a16a16
a16a16a16
a16a16a16a16
a0
a0
a0a0
a64
a64
a64a64
a80a80a80
a80a80a80
a80a80a80a80
pros_phr
a8a8a72a72ip ap
phono
a8a8 a72a72syllable phoneme
disfluence
a8a8a72a72lex non-lex
gest
a8a8
a8
a72a72
a72
hand head ...
IP ::= AP∗
AP ::= SYL+
SYL ::= CONST_SYL+
CONST_SYL ::= PHON+
DISF ::= REPRANDUM BREAK REPRANS
Figure 1: Type and constituent hierarchies
• Reparans: all that follow the break and
recovers the reparandum (in modifying or
completing it) or simply left it uncompleted.
The general disfluency type hierarchy, with the
appropriate features at each level is given in the
following figure:
disfluency
a8a8
a8a8a8
a72a72
a72a72a72
lexbracketleftBig
REPRANDUM frag
BREAK_INT break
bracketrightBig
a8a8
a8a8
a72a72
a72a72
repairedbracketleftBig
TYPE rep
REPRANS change
bracketrightBig incompletebracketleftbig
DIS_TYPE inc
bracketrightbig
non_lex
a8a8 a72a72filled
bracketleftbig
TYPE fill
bracketrightbig silentbracketleftbig
TYPE sil
bracketrightbig
2.4 Gestures
Besides verbal communication, gestures consti-
tute the main aspect of multimodality. In multi-
modal annotation, this is probably the most dif-
ficult and time-consuming task. Moreover, only
few works really focus on a precise description of
all the different domains of verbal and non verbal
modalities. The TFS-based approach proposed
here answers to the first need in such a perspec-
tive: a common representation framework.
We give in this section a brief illustration of
the representation of one gesture (hands). It re-
lies on adaptation of different proposals, espe-
cially (Kipp03) or MUMIN (Allwood, 2005), both
integrating McNeill’s gesture description (Mc-
Neill05).
The following structure encodes the description
of gesture phases, phrases (representing different
semiotic types), the hand shape as well as its ori-
entation, the gesture space, and the possible con-
tact with bodies or objects. A last feature also
describes the movement itself: trajectory, qual-
ity (fast, normal or slow) and amplitude (small,
medium and large).
hands_type














SYMMETRY boolean
PHASE Phase_Type
PHRASE




SEMIOTIC Type Semiotic_Type
EMBLEM Emblem_Type
DEICTIC Deictic_Type
METAPHORIC Metaphoric_Type
PASSIVE_HAND boolean
ACTIVE_HAND boolean
ICONIC Iconic_Type




HANDSHAPE
bracketleftBigS
HAPE HandShape_Type
LAX boolean
bracketrightBig
GESTURESPACE Space_Type
ORIENTATION Orientation_Type
CONTACT
bracketleftBigA
DAPTOR Adaptor_Type
CONTACT PART Contact_Type
bracketrightBig
MOVEMENT
bracketleftBigg
TRAJECTORY Trajectory_Type
AMPLITUDE Amplitude_Type
QUALITY quality_Type
bracketrightBigg














2.5 Application
We have experimented this modeling in the com-
plete annotation of a multimodal corpus (see
(Blache, 2010)). In this project, a complete TFS
model has been first designed, covering all the
different domains (prosody, syntax, gestures, dis-
course, etc.). From this model, the annotations
have been created, leading to a 3-hours corpus of
narrative dialogs, fully transcribed. The corpus
is fully annotated for some domains (phonetics,
prosody and syntax) and partly for others (ges-
tures, discourse, disfluencies, specific phenom-
ena). The result is one of the first large annotated
multimodal corpus.
3 Graphs
for Multimodal Annotation
Graphs are frequently used in the representation
of complex information, which is the case with
multimodality. As for linguistic annotation, one
of the most popular representations is Annotation
Graphs (Bird, 2001). They have been proposed
in particular in the perspective of anchoring dif-
ferent kinds of information in the same reference,
65
making it possible to align them2. In AGs, nodes
represent positions in the signal while edges bear
linguistic information. Two edges connecting the
same nodes are aligned: they specify different in-
formation on the same part of the input. Implic-
itly, this means that these edges bear different fea-
tures of the same object.
Such a representation constitutes the basis of
different approaches aiming at elaborating generic
annotation formats, for example LAF (and its ex-
tension GrAF (Ide, 2007)). In this proposal, edge
labels can be considered as nodes in order to build
higher level information. One can consider the re-
sult as an hypergraph, in which nodes can be sub-
graphs.
We propose in this section a more generalized
representation in which nodes are not positions in
the signal, but represent directly objects (or set of
objects). All nodes have here the same structure,
being them nodes or hypernodes. The main inter-
est of this proposal, on top of having an homoge-
neous representation, is the possibility to anchor
information in different references (temporal, spa-
tial or semantic).
3.1 Nodes
As seen above, multimodal annotation requires
the representation of different kinds of informa-
tion (speech signal, video input, word strings, im-
ages, etc.). The objects3 that will be used in the
description (or the annotation) of the input are of
different nature: temporal or spatial, concrete or
abstract, visual or acoustic, etc. A generic de-
scription requires first a unique way of locating
(or indexing) all objects, whatever their domain.
In this perspective, an index (in the HPSG sense)
can be specified, relying on different information:
• LOCATION: objects can in most of the cases
be localized in reference to a temporal or
a spatial situation. For example, phonemes
have a temporal reference into the speech
2Another important interest of AGs is that they can
constitute the basis for an exchange format, when think-
ing on annotation tools interoperability (a proposal is cur-
rently elaborated under auspices of the MITRE program, see
http://www.mitre.org/).
3We call object any annotation that participates to the de-
scription: phoneme, words, gestures, but also phrases, emo-
tions, etc.
signal, physical objects have spatial local-
ization that can be absolute (spatial coordi-
nates), or relative (with respect to other ob-
jects).
• REALIZATION: data can either refer to con-
crete or physical objects (phonemes, ges-
tures, referential elements, etc.) as well as
abstract ones (concepts, emotions, etc.).
• MEDIUM: specification of the different
modalities: acoustic, tactile and visual.4
• ACCESSIBILITY: some data are directly ac-
cessible from the signal or the discourse, they
have a physical existence or have already
been mentioned. In this case, they are said
to be “given” (e.g. gestures, sounds, physical
objects). Some other kinds of data are de-
duced from the context, typically the abstract
ones. They are considered as “accessible".
A generic structure node can be given, gather-
ing the index and the some other object properties.
node








ID
DOMAIN
braceleftbig
prosody, syntax, pragmatics, ...
bracerightbig
INDEX





LOCATION
braceleftBigg
TEMPORAL
bracketleftBig
START value
END value
bracketrightBig
SPATIAL coord
bracerightBigg
REALIZATION
braceleftbig
concrete, abstract
bracerightbig
MEDIUM
braceleftbig
acoustic, tactile, visual
bracerightbig
ACCESSIBILITY
braceleftbig
given, accessible
bracerightbig





FEATURES object_type








This structure relies on the different informa-
tion. Besides INDEX, some other features com-
plete the description:
• ID: using an absolute ID is useful in the per-
spective of graph representation, in which
nodes can encode any kind of information
(atomic or complex, including subgraphs).
• DOMAIN: specification of the domain to
which the information belongs. This feature
is useful in the specification of generic inter-
action constraints between domains.
• FEATURES: nodes have to bear specific lin-
guistic indications, describing object proper-
ties. This field encodes the type of informa-
tion presented in the first section.
4See the W3C EMMA recommenda-
tion (Extensible Multi-Modal Annotations,
http://www.w3.org/2002/mmi/.
66
The following examples illustrate the represen-
tation of atomic nodes from different domains: a
phoneme (node n1) and a gesture (node n2), that
are temporally anchored, and a physical object
(node n3) which is spatially situated. This last ob-
ject can be used as a referent, for example by a
deictic gesture.






ID n1
DOMAIN phonetics
INDEX



TEMP
bracketleftBig
START 285
END 312
bracketrightBig
REALIZATION concrete
MEDIUM acoustic
ACCESSIBILITY given



FEATURES
phoneme
bracketleftBigg
LABEL /u/
CAT vowel
...
bracketrightBigg













ID n2
DOMAIN gesture
INDEX
bracketleftBigg
TEMP
bracketleftBig
START 200
END 422
bracketrightBig
...
bracketrightBigg
FEAT
hand
bracketleftBigg
PHRASE deictic
ORIENTATION front
...
bracketrightBigg










ID n3
DOMAIN context
INDEX
bracketleftbig
LOC|SPATIAL <x=242, y=422, z=312 >
bracketrightbig
FEATURES
discourse_referent
bracketleftBigg
SEM book’
COLOR red
...
bracketrightBigg




3.2 Relations
Linguistic information is usually defined in terms
of relations between (sets of) objects, which can
be atomic or complex. For example, a phrase is
defined by syntactic relations (government, agree-
ment, linearity, etc.) between its constituents. In
some cases, these relations can concern objects
from the same domain (e.g. syntax in the previous
example). In other cases, different domains can
be involved. For example, a long break (greater
than 200ms) usually precedes a left corner of a
new phrase.
The nature of the relation can also be differ-
ent according to the kind of information to be en-
coded. Many relations are binary and oriented
(precedence, dependency, etc.). Some others only
consists in gathering different objects. A con-
struction (in the sense of Construction Grammars,
see (Fillmore96)) is precisely that: a set of ob-
ject or properties that, put together, form a spe-
cific phenomenon. It is then useful in our rep-
resentation to distinguish between oriented rela-
tions and set relations. Oriented relations (for ex-
ample precedence) connect a source and a target,
that can be eventually formed with set of objects.
Set relations are used to gather a set of objects,
without orientation or order (e.g. the constituency
relation).
On top of this distinction, it is also necessary
to give an index to the relations, in order to make
their reference possible by other objects. As for
nodes, an index is used, even though its form is
simple and does not need a complex anchor. Fi-
nally, for the same reasons as for nodes, the speci-
fication of the domain is necessary. The following
feature structure gives a first view of this organi-
zation:
relation




INDEX
DOMAIN
braceleftbig
prosody, syntax, pragmatics, ...
bracerightbig
REL_TYPE




ORIENTED_REL
bracketleftBig
SOURCE index
TARGET index
bracketrightBig
SET_REL
angbracketleftbig
node list
angbracketrightbig








Besides these information, a relation descrip-
tion has to be completed with other information:
• TYPE: different types of relations can be
implemented in such representation, such
as dependency, precedence, constituency,
anaphore, etc.
• SCOPE: a relation can be specific to a con-
struction or at the opposite valid whatever
the context. For example, the precedence
relation [V ≺ Clit[nom]] is only valid
in the context of interrogative constructions
whereas the relation exluding the realization
of a backchannel5 after a connective is valid
whatever the context. We distinguish then
between local and global scopes.
• POLARITY: a relation can be negated, imple-
menting the impossibility of a relation in a
given context.
• CONSTRUCTION: in the case of a local rela-
tion, it is necessary to specify the construc-
tion to which it belongs.
• STRENGTH: some relation are mandatory,
some other optional. As for constraints, we
distinguish then between hard and soft rela-
tions, depending on their status.
Finally, a last property has to be precisely de-
fined: the synchronization between two objects
5A backchannel is a reaction, verbal or gestual, of the
adressee during a conversation.
67
coming from different domains (for example ges-
tures and words). In some cases, both objects
have to be strictly aligned, with same boundaries.
For example, a syllable has to be strictly aligned
with its set of phonemes: the left syllable bound-
ary (resp. the right) has to be the same as that
of the first syllable phoneme (resp. the last). In
other cases, the synchronization must not be strict.
For example, a deictic gesture is not necessarily
strictly aligned with a referential pronoun. In this
case, boundaries of both objects only have to be
roughly in the same part of the signal.
We propose the definition of alignment opera-
tors adapted from (Allen, 1985) as follows:
= same boundaries have to be equal
<∆ before b1 <∆ b2 means b1 value is lowerthan b
2, with b2−b1 ≤∆
>∆ after b1 >∆ b2 means that the boundaryb
1 follows b2, with b1−b2 ≤∆
≈∆ almost boundaries are neighbors, withoutorder relation, with|b1−b2 |≤∆
This set of operators allow to specify alignment
equations between different objects. The advan-
tage of this mechanism is that an equation system
can describe complex cases of synchronization.
For example, a construction can involve several
objects from different domains. Some of these ob-
jects can be strictly aligned, some others not.
The final TFS representation is as follows:
relation










INDEX
DOMAIN
braceleftbig
prosody, syntax, pragmatics, ...
bracerightbig
REL_TYPE




ORIENTED_REL
bracketleftBig
SOURCE index
TARGET index
bracketrightBig
SET_REL
angbracketleftbig
node list
angbracketrightbig




TYPE
braceleftbig
dependency, precedence, etc.
bracerightbig
SCOPE
braceleftbig
global, local
bracerightbig
POLARITY
braceleftbig
plus, minus
bracerightbig
CONSTRUCTION contruction_type
STRENGTH
braceleftbig
hard, soft
bracerightbig
ALIGNMENT
angbracketleftbig
alignment_equations
angbracketrightbig










The following feature structure shows an exam-
ple of a global relation indicating that a verbal nu-
cleus usually comes with a minor raising of the
intonation (only main features are indicated here).
This information is represented by an implica-
tion relation, which is oriented from the syntac-
tic category to the prosodic phenomenon. Align-
ment equations stipulate a strict synchronization
between object.
relation





INDEX
REL_TYPE|ORIENTED_REL
bracketleftBig
SOURCE VN1
TARGET mr2
bracketrightBig
TYPE
braceleftbig
implication
bracerightbig
STRENGTH
braceleftbig
soft
bracerightbig
ALIGNMENT
angbracketleftbig
lb1=lb2; rb1=rb2
angbracketrightbig





4 Representation
with Hypergraphs
Nodes and relations can be combined and form
higher level nodes, representing constructions
which are a set of objects (the constituents) plus
a set of relations between them. Such nodes are
in fact hypernodes and bear two kinds of informa-
tion: the properties characterizing the object plus
a set of relations between the constituents (repre-
senting a subgraph). In the syntactic domain, for
example, they represent phrases, as follows:











DOMAIN syntax
INDEX|LOCATION|TEMPORAL
bracketleftBig
START 122
END 584
bracketrightBig
FEATURES
bracketleftbig
CAT VP
bracketrightbig
RELATIONS









INDEX r1
REL_TYPE|SET_REL
angbracketleftbig
V, NP, Adv
angbracketrightbig
TYPE constituency
STRENGTH hard

;



INDEX r2
REL_TYPE|ORIENTED_REL
bracketleftBig
SOURCE NP
TARGET V
bracketrightBig
TYPE dependency
STRENGTH hard





















In the same way, the interaction between dif-
ferent objects from different domains can involve
several relations. For example, a deictic con-
struction can be made of the conjunction of an
anaphoric pronoun, a deictic gesture and a physi-
cal object (for example a book on a shelf). Such
a construction can be described by the following
structure:









INDEX|LOCATION|TEMPORAL
bracketleftBig
START 841
END 1520
bracketrightBig
FEATURES
bracketleftbig
SEM book’
bracketrightbig
RELATIONS










INDEX r3
SET_REL
angbracketleftbig
Pro1, Dx_gest2, Ph_object3
angbracketrightbig
TYPE constituency
ALIGNMENT
angbracketleftbig
lb1 ≈∆lb2; rb1 ≈∆rb2
angbracketrightbig


;


INDEX r4
ORIENTED_REL
bracketleftBig
SOURCE Pro1
TARGET Ph_object3
bracketrightBig
TYPE reference


















This construction indicates some properties
(limited here to the semantic value) and two re-
68
lations between the different objects: one con-
stituency, indicating the different objects involved
in the construction and their (fuzzy) alignment
and a reference relation between the pronoun and
a physical object (here, a book).
This structure represents an hypergraph: it is
a graph connecting different nodes, each of them
being to its turn described by another graph, as
shown above. The main interest of such a repre-
sentation is its flexibility: all kinds of information
can be described, at any level. Graphs being less
constrained than trees, and edges (or relations) be-
ing typed, we can gather different levels, different
domains and different granularities. For example,
an agreement relation can be specified thanks to
the deictic construction, besides the constituency
one, making it possible to instanciate the agree-
ment value of the pronoun.
Note that hypergraphs are also investigated in
other knowledge representation, their properties
are well known (Hayes, 2004) and the implemen-
tation of specific hypergraphs as the one presented
here could be done in RDF graphs for example as
suggested in (Cassidy, 2010).
5 Constraints
for Multimodal
Grammars
In the same way as typed feature structures can
implement constraints and constitute a description
language on linguistic structures (cf. HPSG, ),
the same approach can be generalized to multi-
modal information. SOme recent works have been
done in this direction (see (Alahverdzhieva, 2010;
?)). The representation we propose can implement
generic information about multimodal construc-
tions. We illustrate in the following this aspect
with two phenomena: backchannels and disloca-
tion.
Several studies on conversational data (see for
example (Bertrand09)) have described backchan-
nels (that can be vocal or gestual) and their con-
text. They have in particular underline some reg-
ularities on the left context:
• backchannels usually follow: major intona-
tive phrases (IP), flat contours, end of conver-
sational turn (i.e. saturated from a semantic,
syntactic and pragmatic point of view)
• backchannels never appear after connectives
These constraints can be implemented by
means of a feature structure (representing an hy-
pernode) with a set of precedence relations. The
different objects involved in the description of the
phenomenon (IP, flat contour, conversational turn,
connective) are indicated with an indexed ID, re-
ferring to their complete feature structure, not pre-
sented here.



















ID 1
DOMAIN pragmatics
FEATURES
bracketleftbig
TYPE 2
bracketrightbig
RELATIONS




















INDEX r5
SET_REL
angbracketleftbigg
IP 3 , FLAT_CONTOUR 4 ,
CONV_TURN 5 , CONNECTIVE 6
angbracketrightbigg
TYPE constituency

;


INDEX r6
ORIENTED_REL
bracketleftbigg
SOURCE
angbracketleftbig
3 , 4 , 5
angbracketrightbig
TARGET 1
bracketrightbigg
TYPE precedence

;



INDEX r7
ORIENTED_REL
bracketleftbigg
SOURCE 6
TARGET 1
bracketrightbigg
TYPE precedence
POLARITY minus






INDEX r8
ORIENTED_REL
bracketleftbigg
SOURCE 3
TARGET vocal_ 2
bracketrightbigg
TYPE precedence
STRENGTH hard








































Figure 2: Backchannel Constraint
This structure (cf. Figure 2) represents a con-
straint that backchannels have to satisfy. The
first relation specifies the constituents and their
indexes, with which the different precedence con-
straints are represented. The relation r6 indicates
all kinds of object that should precede a backchan-
nel. This constraint subsumes the most specific
relation r8 stipulating that a vocal backchannel is
always preceded with an IP (this is a hard con-
straint). The relation r7 excludes the possibility
for a backchannel to be preceded with a connec-
tive.
The second example (cf. Figure 3) proposes a
constraint system describing dislocated structures.
We propose in this description to distinguish two
syntactic constituents that form the two parts of
the dislocation: the dislocated phrase (called S1)
and the sentence from which the phrase has been
69
extracted (called S2). Usually (even if not al-
ways), S2 contains a clitic referring to S1. We
note in the following this clitic with the notation
S2//Clit. For readability reasons, we only present
in this structure the relations.
This structure describes the case of a left dislo-
cation (with S1 preceding S2, the constraint being
hard). In such cases, S1 is usually realized with
a minor raising contour. The constraint r13 im-
plements the anaphoric relation between the clitic
and the dislocated element. Finally, the relation
r14 indicates an agreement relation between the
clitic and S1 and in particular the fact that the case
has to be the same for both objects.















DOMAIN syntax
RELATIONS


















INDEX r11
SET_REL
angbracketleftbigg
S1 1 , S2 2 , MINOR_RAISING 3 ,
S2//CLIT 4
angbracketrightbigg
TYPE constituency

;


INDEX r12
ORIENTED_REL
bracketleftbigg
SOURCE 1
TARGET 2
bracketrightbigg
TYPE precedence

;


INDEX r13
ORIENTED_REL
bracketleftbigg
SOURCE 1
TARGET 4
bracketrightbigg
TYPE anaphor




INDEX r14
ORIENTED_REL
bracketleftbigg
SOURCE 1 [CASE 3 ]
TARGET 4 [CASE 3 ]
bracketrightbigg
TYPE agreement

































Figure 3: Dislocation Constraint
6 Conclusion
Linguistic annotation in general, and multimodal-
ity in particular, requires high level annotation
schemes making it possible to represent in an ho-
mogeneous way information coming from the dif-
ferent domains and modalities involved in human
communication.
The approach presented in this paper general-
izes previous methods (in particular annotation
graphs) thanks to two proposals: first in providing
a way to index objects without strict order relation
between nodes and second in specifying a precise
and homogeneous representation of the objects
and their relations. This approach has been devel-
oped into a formal scheme, typed feature struc-
tures, in which all the different domains can be
represented, and making it possible to implement
directly hypergraphs. TFS and hypergraphs are
particularly well adapted for the specification of
interaction constraints, describing interaction re-
lations between modalities. Such constraints con-
stitute the core of the definition of future multi-
modal grammars.
From a practical point of view, the proposal
described in this paper is currently under exper-
imentation within the OTIM project (see (Blache,
2010)). An XML scheme has been automatically
generated starting from TFS formal scheme. The
existing multimodal annotations, created with ad
hoc annotation schemes, are to their turn automat-
ically translated following this format. We obtain
then, for the first time, a large annotated multi-
modal corpus, using an XML schema based on a
formal specification.
References
Alahverdzhieva, K. and A. Lascarides (2010)
“Analysing Language and Co-verbal Gesture and
Constraint-based Grammars”, in Proceedings of
the 17th International Conference on Head-Driven
Phase Structure Grammar.
Allen F. and P. J. Hayes (1985) “A common-sense the-
ory of time”, in 9th International Joint Conference
on Artificial Intelligence.
Allwood J., L. Cerrato, L. Dybkjaer and al. (2005)
The MUMIN Multimodal Coding Scheme, NorFA
yearbook 2005
Bertrand R., M. Ader, P. Blache, G. Ferré, R. Es-
pesser, S. Rauzy (2009) “Représentation, édition et
exploitation de données multimodales : le cas des
backchannels du corpus CID”, in Cahiers de lin-
guistique française, 33:2.
Blache P., R. Bertrand, and G. Ferré (2009) “Creat-
ing and Exploiting Multimodal Annotated Corpora:
The ToMA Project”. in Kipp, Martin, Paggio and
Heylen (eds.) Multimodal Corpora: From Models
of Natural Interaction to Systems and Applications,
LNAI 5509, Springer.
Blache P. et al. (2010) “Multimodal Annotation of
Conversational Data”, in proceedings of LAW-IV -
The Linguistic Annotation Workshop
Bird S., Day D., Garofolo J., Henderson J., Laprun C.
& Liberman M. (2000) “ATLAS : A Flexible and
Extensible Architecture for Linguistic Annotation",
in procs of LREC00
70
Bird S., M. Liberman (2001) “A formal framework
for linguistic annotation" Speech Communication,
Elsevier
Boersma P. & D. Weenink (2009) Praat: doing pho-
netics by computer, http://www.praat.org/
Carletta, J., J. Kilgour, and T. O’Donnell (2003) “The
NITE Object Model Library for Handling Struc-
tured Linguistic Annotation on Multimodal Data
Sets" in procs of the EACL Workshop on Language
Technology and the Semantic Web
Carpenter B. (1992) The Logic of Typed Feature
Structures. Cambridge University Press.
Cassidy S. (2010) An RDF Realisation of LAF in the
DADA Annotation Server. Proceedings of ISA-5,
Hong Kong, January 2010.
Dipper S., M. Goetze and S. Skopeteas (eds.) (2007)
Information Structure in Cross-Linguistic Corpora:
Annotation Guidelines for Phonology, Morphol-
ogy, Syntax, Semantics and Information Structure,
Working Papers of the SFB 632, 7:07
Dybkjaer L., S. Berman, M. Kipp, M. Wegener Olsen,
V. Pirrelli, N .Reithinger, C. Soria (2001) “Sur-
vey of Existing Tools, Standards and User Needs for
Annotation of Natural Interaction and Multimodal
Data", ISLE Natural Interactivity and Multimodal-
ity Working Group Deliverable D11.1
Fillmore C. & P. Kay (1996) Construction Grammar,
Manuscript, University of California at Berkeley
Department of linguistics.
Gruenstein A., J. Niekrasz, and M. Purver. (2008)
“Meeting structure annotation: Annotations col-
lected with a general purpose toolkit”. In L. Dybk-
jaer and W. Minker, editors, Recent Trends in Dis-
course and Dialogue, Springer-Verlag.
Hayes J. and Gutierrez C. (2004) Bipartite graphs as
intermediate model for RDF. Proceedings of ISWC
2004, 3rd International Semantic Web Conference
(ISWC2004), Japan.
Ide N. and K. Suderman (2007) “GrAF: A Graph-
based Format for Linguistic Annotations” in pro-
ceedings of the Linguistic Annotation Workshop
(LAW-07)
Ide N. and Suderman K. (2009) Bridging the Gaps:
Interoperability for GrAF, GATE, and UIMA. Pro-
ceedings of the Third Linguistic Annotation Work-
shop, held in conjunction with ACL 2009, Singa-
pore.
Kipp M. (2001) “Anvil-a generic annotation tool for
multimodal dialogue" in procs of 7th European
Conference on Speech Communication and Tech-
nology
Kipp, M. (2003) Gesture Generation by Immitation:
From Human Behavior to Computer Character An-
imation, PhD Thesis, Saarland University.
Lascarides, A. and M. Stone (2009) “A Formal Se-
mantic Analysis of Gesture”, in Journal of Seman-
tics, 26(4).
McNeill, D. (2005) Gesture and Thought, The Univer-
sity of Chicago Press.
Pineda, L., and G. Garza (2000) “A Model for Mul-
timodal Reference Resolution", in Computational
Linguistics, Vol. 26 no. 2
Rodriguez K., Stefan, K. J., Dipper, S., Goetze,
M., Poesio, M., Riccardi, G., Raymond, C., Wis-
niewska, J. (2007) “Standoff Coordination for
Multi-Tool Annotation in a Dialogue Corpus", in
procs of the Linguistic Annotation Workshop at the
ACL’07 (LAW-07)
Wegener Knudsen M.and al. (2002) Survey of Multi-
modal Coding Schemes and Best Practice, ISLE
Wittenburg, P.; Brugman, H.; Russel, A.; Klassmann,
A. and Sloetjes, H. (2006) “ELAN: a Professional
Framework for Multimodality Research”. In pro-
ceedings of LREC 2006
71

