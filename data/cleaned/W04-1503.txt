A Simple String-Rewriting Formalism for Dependency Grammar Alexis NASR Lattice, UFRL Universit´e Paris 7 F-75005 Paris France alexis.nasr@linguist.jussieu.fr Owen RAMBOW Columbia University Department of Computer Science 1214 Amsterdam Avenue New York, NY 10027-7003, USA rambow@cs.columbia.edu Abstract Recently, dependency grammar has gained renewed attention as empirical methods in parsing have emphasized the importance of relations between words, which is what dependency grammars model explicitly, but context-free phrase-structure grammars do not.
While there has been much work on formalizing dependency grammar and on parsing algorithms for dependency grammars in the past, there is not a complete generative formalization of dependency grammar based on string-rewriting in which the derivation structure is the desired dependency structure.
Such a system allows for the definition of a compact parse forest in a straightforward manner.
In this paper, we present a simple generative formalism for dependency grammars based on Extended Context-Free Grammar, along with a parser; the formalism captures the intuitions of previous formalizations while deviating minimally from the much-used Context-Free Grammar.
1 Introduction
Dependency grammar has a long tradition in syntactic theory, dating back to at least Tesni`ere’s work from the thirties.
Recently, it has gained renewed attention as empirical methods in parsing have emphasized the importance of relations between words (see, e.g., (Collins, 1997)), which is what dependency grammars model explicitly, but context-free phrase-structure grammars do not.
In this paper, we address an important issue in using grammar formalisms: the compact representation of the parse forest.
Why is this an important issue?
It is well known that for non-toy grammars and non-toy examples, a sentence can have a staggeringly large number of analyses (for example, using a contextfree grammar (CFG) extracted from the Penn Treebank, a sentence of 25 words may easily have 1,000,000 or more analyses).
By way of an example of an ambiguous sentence (though with only two readings), the two dependency representations for the ambiguous sentence (1) are given in Figure 1.
(1) Pilar saw a man with a telescope It is clear that if we want to evaluate each possible analysis (be it using a probabilistic model or a different method, for example a semantic checker), we cannot efficiently do so if we enumerate all cases.1 We have two options: we can either use a greedy heuristic method for checking which does not examine all possible solutions, which entails we may miss the optimal solution, or we perform our checking operation on a representation which encodes all options in a compact representation.
This is possible because the exponential number of possible analyses (exponential in the length of the input sentence) share subanalyses, thus making a polynomial-size representation possible.
This representation is called the shared parse forest and it has been extensively studied for CFGs (see, for example, (Lang, 1991)).
To our knowledge, there has been no description of the notion of shared parse forest for dependency trees to date.
In this paper, we propose a formalization which is very closely based on the shared parse forest for CFG.
We achieve this by defining a generative string-rewriting formalism whose derivation trees are dependency trees.
The formalism, and the corresponding shared parse forests, are used in a probabilistic chart parser for dependency grammar, which is described in (Nasr and Rambow, 2004b).
While there has been much work on formalizing dependency grammar and on parsing algorithms for dependency grammars in the past, we observe that there is not, to our knowledge, a complete generative formalization2 of dependency grammar based on string-rewriting in which the derivation structure is exactly the desired dependency structure.
The most salient reason for the lack of such a generative dependency grammar is the absence of non1We would like to thank two anonymous reviewers for useful comments.
2While the notions are related historically and conceptually, we refer to a type of mathematical formalization, not to the school of linguistics known as “Generative Grammar”.
terminal symbols in a dependency tree, which prevents us from interpreting it as a derivation structure in a system that distinguishes between terminal and nonterminal symbols.
The standard solution to this problem, proposed by Gaifman (1965), is to introduce nonterminal symbols denoting lexical categories, as depicted in figure 2 (called the “labelled phrase-structure trees induced by a dependency tree” by Gaifman (1965)).
Clearly, the “pure” dependency tree can be derived in a straightforward manner.
The string rewriting system described in (Gaifman, 1965) generates as derivation structures this kind of trees.
There is however a deeper problem when considering dependency trees as derivation structures, following from the fact that in a dependency tree, modifiers3 are direct dependents of the head they modify, and (in certain syntactic contexts) the number of modifiers is unbounded.
Thus, if we wish to obtain a tree as shown in Figure 2, we need to have productions whose right-hand side is of unbounded size, which is not possible in a context-free grammar.
Indeed, the formalization of dependency grammar proposed by Gaifman (1965) is unsatisfactory in that it does not allow for an unbounded number of modifiers!
In this paper, we follow a suggestion made by Abney (1996) and worked out in some detail in (Lombardo, 1996)4 to extend Gaifman’s notation with regular expressions, similar to the approach used in extended context-free grammars.
The result is a simple generative formalism which has the property that the derivation structures are dependency trees, except for the introduction of preterminal nodes as shown in Figure 2.
We do not mean to imply that our formalism is substantially different from previous formalizations of dependency grammar; the goal of this paper is to present a clean and easy-to-use generative formalism with a straightforward notion of parse forest.
In particular, our formalism, Generative Dependency Grammar, allows for an unbounded number of daughter nodes in the derivation tree through the use of regular expressions in its rules.
The parser uses the 3We use the term modifier in its linguistic sense as a type of syntactic dependency (another type being argument).
We use head (or mother) and dependent (or daughter) to refer to nodes in a tree.
Sometimes, in the formal and parsing literature, modifier is used to designate any dependent node, but we consider that usage confusing because of the related but different meaning of the term modifier that is well-established in the linguistic literature.
4In fact, much of our formalism is very similar to (Lombardo, 1996), who however does not discuss parsing (only recognition), nor the representation of the parse forest.
corresponding finite-state machines which straightforwardly allows for a binary-branching representation of the derivation structure for the purpose of parsing, and thus for a compact (polynomial and not exponential) representation of the parse forest.
This formalism is based on previous work presented in (Kahane et al., 1998), which has been substantially reformulated in order to simplify it.5 In particular, we do not address non-projectivity here, but acknowledge that for certain languages it is a crucial issue.
We will extend our basic approach in the spirit of (Kahane et al., 1998) in future work.
The paper is structured as follows.
We start out by surveying previous formalizations of dependency grammar in Section 2.
In Section 3, we introduce several formalisms, including Generative Dependency Grammar.
We present a parsing algorithm in Section 4, and mention empirical results in Section 5.
We then conclude.
2 Previous
Formalizations of Dependency Grammar We start out by observing that “dependency grammar” should be contrasted with “phrase structure grammar”, not “CFG”, which is a particular formalization of phrase structure grammar.
Thus, just as it only makes sense to study the formal properties of a particular formalization of phrase structure grammar, the question about the formal properties of dependency grammar in general is not well defined, nor the question of a comparison of a dependency formalism with dependency grammar.
There have been (at least) four types of formalizations of dependency grammars in the past.6 None of these approaches, to our knowledge, discuss the notion of shared parse forest.
The first approach (for example, (Lombardo and Lesmo, 2000)) follows Gaifman (1965) in proposing traditional string rewriting rules, which however do not allow for an unbounded number of adjuncts.
In the second approach, the dependency structure is constructed in reference to a parallel (“deeper”) structure (Sgall et al., 1986; Mel’ˇcuk, 1988).
Because the rules make reference to other struc5Kahane et al.(1998) present three different types of rules, for subcategorization, modification, and linear precedence.
In the formalism presented in this paper, they have been collapsed into one.
6We leave aside here work on tree rewriting systems such as Tree Adjoining Grammar, which, when lexicalized, have derivation structures which are very similar to dependency trees.
See (Rambow and Joshi, 1997) for a discussion related to TAG, and see (Rambow et al., 2001) for the definition of a tree-rewriting system which can be used to develop grammars whose derivations faithfully mirror syntactic dependency.
saw HHPilar man HHa with telescope a saw HH HH Pilar man a with telescope a Figure 1: Two dependency trees Pilar N a D man a VV saw D man a sawN P with Pilar N N a D telesope N P with N telesopeD Figure 2: Two dependency trees with lexical categories tures, these approaches cannot be formalized in a straightforward manner as context-free rewriting formalisms.
In the third approach, which includes formalizations of dependency structure such as Dependency Tree Grammar of Modina (see (Dikovsky and Modina, 2000) for an overview), Link Grammar (Sleator and Temperley, 1993) or the tree-composition approach of Nasr (1996), rules construct the dependency tree incrementally; in these approaches, the grammar licenses dependency relations which, in a derivation, are added to the tree one by one, or in groups.
In contrast, we are interested in a stringrewriting system; in such a system, we cannot add dependency relations incrementally: all daughters of a node must be added at once to represent a single rewrite step.
In the fourth approach, the dependency grammar is converted into a headed context-free grammar (Abney, 1996; Holan et al., 1998), also the Basic Dependency Grammar of Beletskij (1967) as cited in (Dikovsky and Modina, 2000).
This approach allows for the recovery of the dependency structure both from the derivation tree and from a parse forest represented in polynomial space.
(In fact, our parsing algorithm draws on this work).
However, the approach of course requires the introduction of additional nonterminal nodes.
Finally, we observe that Recursive Transition Networks (Woods, 1970) can be used to encode a grammar whose derivation trees are dependency trees.
However, they are more a general framework for encoding grammars than a specific type of grammar (for example, we can also use them to encode CFGs).
In a somewhat related manner, Alshawi et al.(2000) use cascaded head automata to derive dependency trees, but leave the nature of the cascading under-formalized.
Eisner (2000) provides a formalization of a system that uses two different automata to generate left and right children of a head.
His formalism is very close to the one we present, but it is not a string-rewriting formalism (and not really generative at all).
We are looking for a precise formulation of a generative dependency grammar, and the question has remained open whether there is an alternate formalism which allows for an unbounded number of adjuncts, introduces all daughter nodes at once in a string-rewriting step, and avoids the introduction of additional nonterminal nodes.
3 Formalism
In this section we first review the definition of Extended Context-Free Grammar and then show how we use it to model dependency derivations.
An Extended Context-Free Grammar (or ECFG for short) is like a context-free grammar (CFG), except that the right-hand side is a regular expression over the terminal and nonterminal symbols of the grammar.
At each step in a derivation, we first choose a rewrite rule (as we do in CFG), and then we choose a string which is in the language denoted by the regular expression associated with the rule.
This string is then treated like the right-hand side of a CFG rewrite rule.
In the following, if G is a grammar and R a regular expression, then L(G) denotes the language generated by the grammar and L(R) the language denoted by the regular expression.
If F is a class of grammars (such as CFG), then L(F) denote the class of languages generated by the grammars in F.
We now give a formal definition, which closely follows that given by Albert et al.(1999).7 A Extended Context-Free Grammar is a 4tuple (VN;VT;P;S), where: VN is a finite set of nonterminal symbols, VT is a finite set of terminal symbols (disjoint from VN), P is a finite set of rules, which are ordered pairs consisting of an element of VN and a regular expression over VN [VT, S, a subset of VN, contains the possible start symbols.
We will use the traditional arrow notation ( !) to write rules.
For A 2 VN and u;v 2 (VN [ VT) we say that uAv yields uwv (written uAv =) uwv) if A ! R is in P and w is in L(R).
The transitive closure of the yield relation (denoted =) ) is defined in the usual manner.
The language generated by a Extended ContextFree Grammar is the set fw 2 V T j A =)w;A 2 Sg.
We now define a restricted version of ECFG which we will use for defining dependency grammars.
The only new formal requirement is that the rules be lexicalized in the sense of (Joshi and Schabes, 1991).
For our formalism, this means that the regular expression in a production is such that each string in its denoted language contains at least one terminal symbol.
Linguistically speaking, this means that each rule is associated with exactly 7ECFG has been around informally since the sixties (e.g., the Backus-Naur form); for a slightly different formalization, see (Madsen and Kristensen, 1976), whose definition allows for an infinite rule set.
one lexical item (which may be multi-word).
We will call this particular type of Extended ContextFree Grammar a lexicalized Extended ContextFree Grammar or, for obvious reasons, a Generative Dependency Grammar (GDG for short).
When we use a GDG for linguistic description, its lefthand side nonterminal will be interpreted as the lexical category of the lexical item and will represent its maximal projection.8 A Generative Dependency Grammar is a lexicalized ECFG.
It is sometimes useful to have dependency representations with labeled arcs (typically labeled with syntactic functions such as SUBJ for subject or ADJ for adjunct).
There are different ways of achieving this goal; here, we discuss the use of feature structures in conjunction with the nonterminal symbols, for example N[gf:subj] instead of just N.
Feature structures are of course useful for other reasons as well, such as expressing morphological features.
In terms of our formalism, the use of bounded feature structures can be seen as a shorthand notation for an increased set of nonterminal symbols.
The use of feature structures (rather than simple nonterminal symbols) allows for a more perspicuous representation of linguistic relations through the use of underspecification.
Note that the use of underspecified feature structures in rules can potentially lead to an exponential increase (exponential in the size of the set of feature values) of the size of the set of rules if rules contain underspecified feature structures on the right-hand side.
However, we note that the feature representing, grammatical function will presumably always be fully specified on the righthand side of a rule (the head determines the function of its dependents).
Underspecification in the left-hand side of a rule only leads to linear compactification of the rule set.
We now give a toy linguistic example.
We let GLing be (VN;VT;P;S) as follows: VN = fV;N;D;A;Advg VT = f Pilar, saw, man, a, telescope, with, tall, very g P consists of the following rules: p1 : V ! N saw N P p2 : N ! (Pilar j D (A) (man j telescope) P ) 8For practical purposes, we can separate the lexicon (which assigns lexical categories to lexemes) from the syntactic rules (which hold for all lexemes of a class), as does Gaifman (1965), resulting in a straightforward notational extension to our formalism.
Vp 1=)N saw N P p2p2p3=) Pilar saw D man with N p6p2=)Pilar saw a man with D telescope p6=)Pilar saw a man with a telescope Figure 3: A sample GDG derivation p3 : P ! with N p4 : A ! Adv tall p5 : Adv ! very p6 : D ! a S = fV g A derivation is shown in Figure 3; the corresponding derivation tree is shown in the right part of Figure 2.
As can be seen, the derivation structure is a dependency tree, except for the use of preterminals, as we desired.
The first part of the following theorem follows from the existence of a Greibach Normal Form for ECFG (Albert et al., 1999).
The second part follows immediately from the closure of CFG under regular substitution.
L(GDG) = L(ECFG) = L(CFG).
Of course, ECFG, GDG and CFG are not strongly equivalent in the standard sense for string rewriting systems of having the same sets of derivation trees.
Clearly, ECFG can generate all sets of derivation trees that GDG can, while CFG cannot (because of the unbounded branching factor of ECFG and of GDG); ECFG can also generate all sets of derivation trees that CFG can, while GDG cannot (because of the lexicalization requirement).
ECFG thus has a greater strong generative capacity than CFG and GDG, while those of GDG and CFG are incomparable.
It is interesting to notice the difference between the rewriting operation of a nonterminal symbol as defined for a ECFG or a GDG and the equivalent rewriting steps with a weakly equivalent CFG.
A GDG rewriting operation of a symbol X using a rule r is decomposed in two stages, the first stage consists in choosing a string w which belongs to the set denoted by the right-hand side of r.
During the second stage, X is replaced by w.
These two stages are of a different nature, the first concerns the generation of CFG rules (and hence a CFG) using a GDG while the second concerns the generation of a string using the generated CFG.
The equivalent rewriting operation (X ) w) with a CFG does not distinguish the same two stages, both the selection of w and the rewriting of X as w are done at the same time.
man (m2,4) (m2,3) (m3,3) (m3,2) (m2,4) (m2,3) telescopesaw (m2,4) (m2,4) Pilar a (D,2) with a (D,2) (m1,4) (m1,4) (m1,4) (m1,3) (m1,2) Figure 4: A packed parse forest 4 Parsing Algorithm The parsing algorithm given here is a simple extension of the CKY algorithm.
The difference is in the use of finite state machines in the items in the chart to represent the right-hand sides of the rules of the ECFG.9 A rule with category C as its left-hand side will give rise to a finite state machine which we call a C-rule FSM; its final states mark the completed recognition of a constituent of label C.
CKY-Style parsing algorithm for Extended Context-Free Grammars.
Input. A ECFG G and an input string W = w1 wn.
Output. The parse table T for W such that ti;j contains (M;q) iff M is a C-rule-FSM, q is one of the final states of M, and we have a derivation C +=)wi wj.
If i = j, ti;j also contains the input symbol wi.
Method. Initialization: For each i, 1 i n, add wi to ti;i.
Completion: If ti;j contains either the input symbol w or an item (M;q) such that q is a final state of M, and M is a C-rule-FSM, then add to ti;j all (M0;q0) such that M0 is a rule-FSM which transitions from a start state to state q0 on input w or C.
Add a single backpointer from (M0;q0) in ti;j to (M;q) or w in ti;j.
9Recent work in the context of using ECFG for parsing SGML and XML proposes an LL-type parser for ECFG (Br¨uggemann-Klein and Wood, 2003); their approach also exploits the automaton representation of the right-hand side of rules, as is natural for an algorithm dealing with ECFG.
1 2 3 N saw N 4 Pm1 1 3 4 Pilar A manD 2 Ptelescope telescope man m2 with N2 31m3 Figure 5: Three rules FSM m1, m2 and m3.
m1 is a V-rule-FSM corresponding to rule p1, m2 is an N-rule-FSM which corresponds to rule p2 and m3 is a P-rule-FSM which corresponds to rule p3 P(3) P(2) N(4) N(3) telescopewith a D(2) V(5) V(5) V(4) sawV(3) N(4) Pilar man N(4) N(3) a D(2) V(5) P(3) P(2) N(4) N(3) telescopewith a D(2) N(4) man N(4) N(3) a D(2) V(4) sawV(3) N(4) Pilar Figure 6: A parse forest Scanning: If (M1;q1) is in ti;k, and tk+1;j contains either the input symbol w or the item (M2;q2) where q2 is a final state and M2 is a C-rule-FSM, then add (M1;q) to ti;j (if not already present) if M1 transitions from q1 to q on either w or C.
Add a double backpointer from (M1;q) in ti;j to (M1;q1) in ti;k (left backpointer) and to either w or (M2;q2) in tk+1;j (right backpointer).
At the end of the parsing process, a packed parse forest has been built.
The packed forest corresponding to the parse of sentence Pilar saw a man with a telescope, using the grammar of Section 3 is represented in Figure 4.
The nonterminal nodes are labeled with pairs (M;q) where M is an rule-FSM and q a state of this FSM.
Three rule-FSMs corresponding to rules p1, p2 and p3 have been represented in Figure 5.
Obtaining the dependency trees from the packed parse forest is performed in two stages.
In a first stage, a forest of binary syntagmatic trees is obtained from the packed forest and in a second stage, each syntagmatic tree is transformed into a dependency tree.
We shall not give the details of these processes.
The two trees resulting from de-packing of Figure 4 are represented in Figure 6.
The different nodes of the syntagmatic tree that will be grouped in a single node of the dependency trees have been circled.
5 Empirical
Results While the presentation of empirical results is not the object of this paper, we give an overview of some empirical work using ECFG for natural language processing in this section.
For full details, we refer to (Nasr and Rambow, 2004a; Nasr and Rambow, 2004b; Nasr, 2004).
The parser presented in Section 4 above has been implemented.
We have investigated the use the parser in a two-step probabilistic framework.
In a first step, we determine which rules of the ECFG should be used for each word in the input sentence.
(Recall that a grammar rule encodes the active and passive valency, as well as how any arguments are realized, for example, fronted or in canonical position).
This step is called supertagging and has been suggested and studied in the context of Tree Adjoining Grammar by Bangalore and Joshi (1999).
In a second step, we use a probabilistic ECFG where the probabilities are non-lexical and are based entirely on the grammar rules.
We extract the most probable derivation from the compact parse forest using dynamic programming in the usual manner.
This non-lexical probability model is used because the supertagging step already takes the words in the sentence into account.
The probabilities can be encoded directly as weights on the transitions in the rule-FSMs used by the parser.
The ECFG grammar we use has been automatically extracted from the Penn Treebank (PTB).
In fact, we first extract a Tree Insertion Grammar following the work of (Xia et al., 2000; Chen, 2001; Chiang, 2000), and then directly convert the trees of the obtained TAG into automata for the parser.
It is clear that one could also derive an explicit ECFG in the same manner.
The extracted grammar has about 4.800 rules.
The probabilities are estimated from the corpus during extraction.
Note that there are many different ways of extracting an ECFG from the PTB, corresponding to different theories of syntactic dependency.
We have chosen to directly model predicate-argument relations rather than more surface-oriented syntactic relations such as agreement, so that all function words (determiners, auxiliaries, and so on) depend on the lexical word.
Strongly governed prepositions are treated as part of a lexeme rather than as full prepositions.
We have investigated several different ways of modeling the probability of attaching a sequence of modifiers at a certain point in the derivation (conditioning on the position of the modifier in the sequence or conditioning on the previous modifier used).
We found that using position or context improves on using neither.
We have performed two types of experiments: using the correct ECFG rule for each word, and assigning ECFG rules automatically using supertagging.
In the case of using the correct supertag, we obtain unlabeled dependency accuracies of about 98% (i.e., in about 2% of cases a word is assigned a wrong governor).
Automatic supertagging (using standard n-gram tagging methods) for a grammar our size has an accuracy of about 80%.
This is also approximately the dependency accuracy obtained when parsing the output of a supertagger.
We conclude from this performance that if we can increase the performance of the supertagger, we can also directly increase the performance of the parser.
Current work includes examining which grammatical distinctions the grammar should make in order to optimize both supertagging and parsing (Toussenel, 2004).
6 Conclusion
We have presented a generative string-rewriting system, Extended Context-Free Grammar, whose derivation trees are dependency trees with unbounded branching factor.
We have shown how we can reuse the representation of shared parse forests well-known from CFGs for Extended Context-Free Grammar.
The question arises whether we can represent the shared parse forest in a manner more directly in the spirit of dependency.
This question was investigated by (Nasr, 2003).
He shows that the factoring realized in the shared forest presented here and which is the key to the polynomial representation of a potentially exponential set, can be done directly on the dependency trees by introducing the notion of dependency sets.
References Abney, Steven (1996).
A grammar of projections.
Unpublished manuscript, Universit¨at T¨ubingen.
Albert, J¨urgen; Giammarresi, Dora; and Wood, Derick (1999).
Extended context-free grammars and normal form algorithms.
In Champarnaud, JeanMarc; Maurel, Denis; and Ziadi, Djelloul, editors, Automata Implementations: Third International Workshop on Implementing Automata (WIA’98), volume 1660 of LNCS, pages 1–12.
Springer Verlag.
Alshawi, Hiyan; Bangalore, Srinivas; and Douglas, Shona (2000).
Learning dependency translation models as collections of finite-state head transducers.
cl, 26(1):45–60.
Bangalore, Srinivas and Joshi, Aravind (1999).
Supertagging: An approach to almost parsing.
Computational Linguistics, 25(2):237–266.
Br¨uggemann-Klein, Anne and Wood, Derick (2003).
The parsing of extended context-free grammars.
Unpublished manuscript, Technische Universit¨at M¨unchen and Hong Kong University of Science & Technology.
Chen, John (2001).
Towards Efficient Statistical Parsing Using Lexicalized Grammatical Information.
PhD thesis, University of Delaware.
Chiang, David (2000).
Statistical parsing with an automatically-extracted tree adjoining grammar.
In 38th Meeting of the Association for Computational Linguistics (ACL’00), pages 456–463, Hong Kong, China.
Collins, Michael (1997).
Three generative, lexicalised models for statistical parsing.
In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, Madrid, Spain.
Dikovsky, Alexander and Modina, Larissa (2000).
Dependencies on the other side of the curtain.
Traitement Automatique des Langues, 41(1):79– 111.
Eisner, Jason (2000).
Bilexical grammars and their cubic-time parsing algorithms.
In Bunt, Harry C.
and Nijholt, Anton, editors, New Developments in Natural Language Parsing.
Kluwer Academic Publishers.
Gaifman, Haim (1965).
Dependency systems and phrase-structure systems.
Information and Control, 8:304–337.
Holan, Tom´aˇs; Kuboˇn, Vladislav; Oliva, Karel; and Pl´atek, Martin (1998).
Two useful measures of word order complexity.
In Kahane, Sylvain and Polgu`ere, Alain, editors, Processing of Dependency Grammars: Proceeding of the Workshop, pages 21–28, Montr´eal, Canada.
ACL/COLING. Joshi, Aravind K.
and Schabes, Yves (1991).
Treeadjoining grammars and lexicalized grammars.
In Nivat, Maurice and Podelski, Andreas, editors, Definability and Recognizability of Sets of Trees.
Elsevier. Kahane, Sylvain; Nasr, Alexis; and Rambow, Owen (1998).
Pseudo-projectivity: A polynomially parsable non-projective dependency grammar.
In 36th Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics (COLINGACL’98), pages 646–652, Montr´eal, Canada.
Lang, Bernard (1991).
Towards a uniform formal framework for parsing.
In Tomita, M., editor, Current Issues in Parsing technology, chapter 11, pages 153–171.
Kluwer Academic Publishers.
Lombardo, Vincenzo (1996).
An Earley-style parser for dependency grammars.
In Proceedings of the 16th International Conference on Computational Linguistics (COLING’96), Copenhagen.
Lombardo, Vincenzo and Lesmo, Leonardo (2000).
A formal theory of dependency syntax with empty units.
Traitement automatque des langues, 41(1):179–209.
Madsen, O.L. and Kristensen, B.B.
(1976). LRparsing of extended context-free grammars.
Acta Informatica, 7:61–73.
Mel’ˇcuk, Igor A.
(1988). Dependency Syntax: Theory and Practice.
State University of New York Press, New York.
Nasr, Alexis (1996).
Un syst`eme de reformulation automatique de phrases fond´e sur la Th´eorie Sens-Texte : application aux langues contrˆol´ees.
PhD thesis, Universit´e Paris 7.
Nasr, Alexis (2003).
Factoring sufrace syntactic structures.
In First International Conference on Meaning-Text Theory, pages 249–258, Paris, France.
Nasr, Alexis (2004).
Grammaires de d´ependances g´en´eratives: exp´eriences sur le Corpus Paris 7.
Unpublished manuscript, Universit´e Paris 7.
Nasr, Alexis and Rambow, Owen (2004a).
Dependency parsing based on n-best-path supertagging.
Unpublished manuscript, Universit´e Paris 7 and COlumbia University.
Nasr, Alexis and Rambow, Owen (2004b).
Supertagging and full parsing.
In Proceedings of the Workshop on Tree Adjoining Grammar and Related Formalisms (TAG+7), Vancouver, BC, Canada.
Rambow, Owen and Joshi, Aravind (1997).
A formal look at dependency grammars and phrasestructure grammars, with special consideration of word-order phenomena.
In Wanner, Leo, editor, Recent Trends in Meaning-Text Theory, pages 167–190.
John Benjamins, Amsterdam and Philadelphia.
Rambow, Owen; Vijay-Shanker, K.
and Weir, David (2001).
D-Tree Substitution Grammars.
Computational Linguistics, 27(1).
Sgall, P.
Hajiˇcov´a, E.
and Panevov´a, J.
(1986). The meaning of the sentence and its semantic and pragmatic aspects.
Reidel, Dordrecht.
Sleator, Daniel and Temperley, Davy (1993).
Parsing english with a link grammar,.
In Proceedings of the Third International Workshop on Parsing Technologies IIWPT’93).
Toussenel, Franc¸ois (2004).
Why supertagging is hard.
In Proceedings of the Workshop on Tree Adjoining Grammar and Related Formalisms (TAG+7), Vancouver, BC, Canada.
Woods, William A.
(1970). Transition network grammars for natural language analysis.
Commun.
ACM, 3(10):591–606.
Xia, Fei; Palmer, Martha; and Joshi, Aravind (2000).
A uniform method of grammar extraction and its applications.
In Proc.
of the EMNLP 2000, Hong Kong.

