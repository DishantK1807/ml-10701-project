Proceedings of ACL-08: HLT, pages 914–922,
Columbus, Ohio, USA, June 2008. c©2008 Association for Computational Linguistics
A Probabilistic Model for Fine-Grained Expert Search 
 
 
Shenghua Bao
1, Huizhong Duan
1, Qi Zhou
1, Miao Xiong
1, Yunbo Cao
1,2, Yong Yu
1
 
1
Shanghai Jiao Tong University, 
2
Microsoft Research Asia 
Shanghai, China, 20240 Beijing, China, 10080 
{shhbao,summer,jackson,xiongmiao,yyu} 
@apex.sjtu.edu.cn 
yunbo.cao@microsoft.com 
 
 
 
Abstract 
Expert search, in which given a query a 
ranked list of experts instead of documents is 
returned, has ben intensively studied recently 
due to its importance in facilitating the neds 
of both information aces and knowledge 
discovery. Many aproaches have ben pro-
posed, including metadata extraction, expert 
profile building, and formal model generation. 
However, al of them conduct expert search 
with a coarse-grained aproach. With these, 
further improvements on expert search are 
hard to achieve. In this paper, we propose 
conducting expert search with a fine-grained 
aproach. Specificaly, we utilize more spe-
cific evidences existing in the documents. An 
evidence-oriented probabilistic model for ex-
pert search and a method for the implementa-
tion are proposed. Experimental results show 
that the proposed model and the implementa-
tion are highly efective. 
1 Introduction

Nowadays, team work plays a more important role 
than ever in problem solving. For instance, within 
an enterprise, people handle new problems usually 
by leveraging the knowledge of experienced col-
leagues. Similarly, within research communities, 
novices step into a new research area often by 
learning from well-established researchers in the 
research area. Al these scenarios involve asking 
the questions like “who is an expert on X?” or 
“who knows about X?” Such questions, which 
cannot be answered easily through traditional 
document search, raise a new requirement of 
searching people with certain expertise. 
To meet that requirement, a new task, called ex-
pert search, has been proposed and studied inten-
sively. For example, TREC 205, 206, and 207 
provide the task of expert search within the enter-
prise track. In the TREC setting, expert search is 
defined as: given a query, a ranked list of experts is 
returned. In this paper, we engage our study in the 
same seting. 
Many approaches to expert search have been 
proposed by the participants of TREC and other 
researchers. These approaches include metadata 
extraction (Cao et al., 205), expert profile build-
ing (Craswell, 201, Fu et al., 207), data fusion 
(Maconald and Ounis, 206), query expansion 
(Macdonald and Ounis, 207), hierarchical lan-
guage model (Petkova and Croft, 206), and for-
mal model generation (Balog et al., 206; Fang et 
al., 206). However, all of them conduct expert 
search with what we call a coarse-grained ap-
proach. The discovering and use of evidence for 
expert locating is carried out under a grain of 
document. With it, further improvements on expert 
search are hard to achieve. This is because differ-
ent blocks (or segments) of electronic documents 
usually present different functions and qualities 
and thus different impacts for expert locating. 
In contrast, this paper is concerned with propos-
ing a probabilistic model for fine-grained expert 
search. In fine-grained expert search, we are to 
extract and use evidence of expert search (usually 
blocks of documents) directly. Thus, the proposed 
probabilistic model incorporates evidence of expert 
search explicitly as a part of it. A piece of fine-
grained evidence is formally defined as a quadru-
ple, <topic, person, relation, document>, which 
denotes the fact that a topic and a person, with a 
certain relation between them, are found in a spe-
cific document. The intuition behind the quadruple 
is that a query may be matched with phrases in 
various forms (denoted as topic here) and an expert 
candidate may appear with various name masks 
(denoted as person here), e.g., ful name, email, or 
abbreviated names. Given a topic and person, rela-
tion type is used to measure their closeness and 
914
document serves as a context indicating whether it 
is god evidence. 
Our proposed model for fine-grained expert 
search results in an implementation of two stages. 
 1) Evidence Extraction: document segments in 
various granularities are identified and evidences 
are extracted from them. For example, we can have 
segments in which an expert candidate and a que-
ried topic co-occur within a same section of docu-
ment-01:  “…later, Berners-Lee describes a 
semantic web search engine experience…” As the 
result, we can extract an evidence by using same-
section relation, i.e., <semantic web search engine, 
Berners-Lee, same-section, document-01>.  
2) Evidence Quality Evaluation: the quality (or 
reliability) of evidence is evaluated. The quality of 
a quadruple of evidence consists of four aspects, 
namely topic-matching quality, person-name-
matching quality, relation quality, and document 
quality. If we regard evidence as link of expert 
candidate and queried topic, the four aspects wil 
correspond to the strength of the link to query, the 
strength of the link to expert candidate, the type of 
the link, and the document context of the link re-
spectively. 
Al the evidences with their scores of quality are 
merged together to generate a single score for each 
expert candidate with regard to a given query. We 
empirically evaluate our proposed model and im-
plementation on the W3C corpus which is used in 
the expert search task at TREC 205 and 206. 
Experimental results show that both explored evi-
dences and evaluation of evidence quality can im-
prove the expert search significantly. Compared 
with existing state-of-the-art expert search methods, 
the probabilistic model for fine-grained expert 
search shows promising improvement. 
The rest of the paper is organized as folows. 
Section 2 surveys existing studies on expert search. 
Section 3 and Section 4 present the proposed prob-
abilistic model and its implementation, respec-
tively. Section 5 gives the empirical evaluation. 
Finally, Section 6 concludes the work. 
2 Related
Work 
2.1 Expert
Search Systems 
One setting for automatic expert search is to as-
sume that data from specific resources are avail-
able. For example, Expertise Recommender (Kautz 
et al., 196), Expertise Browser (Mockus and 
Herbsleb, 202) and the system in (McDonald and 
Ackerman, 198) make use of log data in software 
development systems to find experts. Yet another 
approach is to mine expert and expertise from 
email communications (Campbel et al., 203; 
Dom et al. 203; Sihn and Heren, 201). 
Searching expert from general documents has 
also been studied (Davenport and Prusak, 198; 
Mattox et al., 199; Hertzum and Pejtersen, 200). 
P@NOPTIC employs what is referred to as the 
‘profile-based’ approach in searching for experts 
(Craswell et al., 201). Expert/Expert-Locating 
(EEL) system (Steer and Lochbaum, 198) uses 
the same approach in searching for expert groups. 
DEMOIR (Yimam, 196) enhances the profile-
based approach by separating co-occurrences into 
different types. In essence, the profile-based ap-
proach utilizes the co-occurrences between query 
words and people within documents. 
2.2 Expert
Search at TREC 
A task on expert search was organized within the 
enterprise track at TREC 205, 206 and 207 
(Craswell et al., 205; Soboroff  et al., 206; Bai-
ley et al., 207). 
Many approaches have been proposed for tack-
ling the expert search task within the TREC track. 
Cao et al. (205) propose a two-stage model with a 
set of extracted metadata. Balog et al. (206) com-
pare two generative models for expert search. Fang 
et al. (206) further extend their generative model 
by introducing the prior of expert distribution and 
relevance feedback. Petkova and Croft (206) fur-
ther extend the profile based method by using a 
hierarchical language model. Macdonald and 
Ounis (206) investigate the effectiveness of the 
voting approach and the associated data fusion 
techniques. However, such models are conducted 
in a coarse-grain scope of document as discused 
before. In contrast, our study focuses on proposing 
a model for conducting expert search in a fine-
grain scope of evidence (local context). 
3 Fine-grained Expert Search 
Our research is to investigate a direct use of the 
local contexts for expert search. We call each local 
context of such kind as fine-grained evidence. 
In this work, a fine-grained evidence is formally 
defined as a quadruple, <topic, person, relation, 
915
document>. Such a quadruple denotes that a topic 
and a person occurrence, with a certain relation 
between them, are found in a specific document. 
Recall that topic is different from query. For ex-
ample, given a query “semantic web coordination”, 
the corresponding topic may be either “semantic 
web” or “web coordination”. Similarly, person 
here is different from expert candidate. E.g, given 
an expert candidate “Ritu Raj Tiwari”, the matched 
person may be “Ritu Raj Tiwari”, “Tiwari”, or 
“RRT” etc. Although both the topics and persons 
may not match the query and expert candidate ex-
actly, they do have certain indication on the con-
nection of query “semantic web coordination” and 
expert “Ritu Raj Tiwari”. 
3.1 Evidence-Oriented Expert Search Model 
We conduct fine-grained expert search by incorpo-
rating evidence of local context explicitly in a 
probabilistic model which we call an evidence-
oriented expert search model. Given a query q, the 
probability of a candidate c being an expert (or 
knowing something about q) is estimated as 
(|)(,|)
e
Pcq
q
=
!, 
(1) 
where e denotes a quadruple of evidence. 
Using the relaxation that the probability of c is 
independent of a query q given an evidence e, we 
can reduce Equation (1) as, 
(|)(|)|PcP=
!
. 
(2) 
Compared to previous work, our model conducts 
expert search with a new way in which local con-
texts of evidence are used to bridge a query q and 
an expert candidate c. The new way enables the 
expert search system to explore various local con-
texts in a precise manner. 
In the folowing sub-sections, we wil detail two 
sub-models: the expert matching model P(c|e) and 
the evidence matching model P(e|q). 
3.2 Expert
Matching Model 
We expand the evidence e as quadruple <topic, 
people, relation, document> (<t, p, r, d> for short) 
for expert matching. Given a set of related evi-
dences, we assume that the generation of an expert 
candidate c is independent with topic t and omit it 
in expert matching. Therefore, we simplify the ex-
pert matching formula as below: 
),|(|),|()| drpPcdrpcPe=, 
(3) 
where P(c|p) depends on how an expert candidate c 
matches to a person occurrence p (e.g. ful name or 
email of a person). The different ways of matching 
an expert candidate c with a person occurrence p 
results in varied qualities. P(c|p) represents the 
quality. P(p|r,d) expresses the probability of an 
occurrence p given a relation r and a document d. 
P(p|r,d) is estimated in MLE as, 
),(
),|(
drL
pfreq
drpP=, 
(4) 
where freq(p,r,d) is the frequency of person p 
matched by relation r in document d, and L(r, d) is 
the frequency of all the persons matched by rela-
tion r in d. This estimation can further be smoothed 
by using the evidence colection as folows: 
!
"
#+=
D
S
'
|
',
1|µ, 
(5) 
where D denotes the whole document colection. 
|D| is the total number of documents. 
We use Dirichlet prior in smoothing of parame-
ter µ: 
KL+
=
),(
µ, 
(6) 
where K is the average frequency of all the experts 
in the colection. 
3.3 Evidence
Matching Model 
By expanding the evidence e and employing inde-
pendence assumption, we have the folowing for-
mula for evidence matching: 
)|||
),(|
q
r
. (7) 
In the folowing, we are to explain what these 
four terms represent and how they can be estimated. 
The first term P(t|q) represents the probability 
that a query q matches to a topic t in evidence. Re-
call that a query q may match a topic t in various 
ways, not necessarily being identical to t. For ex-
ample, both topic “semantic web” and “semantic 
web search engine” can match the query “semantic 
web search engine”. The probability is defined as 
916
(),)|(qttypePqt!, (8) 
where type(t, q) represents the way that q matches 
to t, e.g., phrase matching. Different matching 
methods are associated with different probabilities. 
The second term P(p|q) represents the probabil-
ity that a person p is generated from a query q. The 
probability is further approximated by the prior 
probability of p, 
)(|(pPq!
. (9) 
The prior probability can be estimated by MLE, 
i.e., the ratio of total occurrences of person p in the 
colection. 
The third term represents the probability that a 
relation r is generated from a query q. Here, we 
approximate the probability as 
)()|(rtypePqr!, (10) 
where type(r) represents the way r conecting 
query and expert. P(type(r)) represents the reliabil-
ity of relation type of r. 
Folowing the Bayes rule, the last term can be 
transformed as 
)(|
)
|
)| dPq
dq
!=, 
(1) 
where priority distribution P(d) can be estimated 
based on static rank, e.g., PageRank (Brin and 
Page, 198). P(q|d) can be estimated by using a 
standard language model for IR (Ponte and Croft, 
198). 
In summary, Equation (7) is converted to 
())(|)((,)|( dPqrtypePpqttypePqe!
. (12) 
3.4 Evidence
Merging 
We assume that the ranking score of an expert can 
be acquired by summing up together all scores of 
the suporting evidences. Thus we calculate ex-
perts’ scores by aggregating the scores from all 
evidences as in Equation (1). 
4 Implementation

Th implementation of the proposed moel con-
sists of two stages, namely evidence extraction and 
evidence quality evaluation. 
4.1 Evidence
Extraction 
Recall that we define an evidence for expert search 
as a quadruple <topic, person, relation, document>. 
The evidence extraction covers the extraction of 
the first three elements, namely person identifica-
tion, topic discovering and relation extraction. 
4.1.1 Person
Identification 
The occurrences of an expert can be in various 
forms, such as name and email adress. We call 
each type of form an expert mask. Table 1 provides 
a statistic on various masks on the basis of W3C 
corpus. In Table 1, rate is the proportion of the 
person occurrences with relevant masks to the per-
son occurrences with any of the masks, and ambi-
guity is defined as the probability that a mask is 
shared by more than one expert. 
 
Mask Rate/Ambiguity Sample 
Ful Name(N
F
) 48.2% / 0.00 Ritu Raj Tiwari 
Email Name(N
E
) 20.1% / 0.00 rtiwari@nuance.com 
Combined Name 
(N
C
) 
4.2% /0.392 Tiwari, Ritu R;      
R R Tiwari 
Abr. Name(N
A
) 21.2% / 0.4890 Ritu Raj ; Ritu 
Short Name(N
S
) 0.7% / 0.6396 RT 
Alias, new email 
(N
AE
) 
7% / 0.460 Ritiwari rti-
wari@hotmail.com 
Table 1. Various masks and their ambiguity 
1) Every ocurence of a candidate’s email adres 
is normalized to the apropriate candidate_id. 
2) Every ocurence of a candidate’s ful_name is 
normalized to the apropriate candidate_id if 
there is no ambiguity; otherwise, the ocurence 
is normalized to the candidate_id of the most 
frequent candidate with that ful_name. 
3) Every ocurence of combined name, abrevi-
ated name, and email alias is normalized to the 
apropriate candidate_id if there is no ambigu-
ity; otherwise, the ocurence may be normal-
ized to the candidate_id of a candidate whose 
ful name also apears in the document. 
4) Al the personal ocurences other than those 
covered by Heuristic 1) ~ 3) are ignored. 
Table 2. Heuristic rules for expert extraction 
As Table 1 demonstrates, it is not an easy task to 
identify all the masks with regards to an expert. On 
one hand, the extraction of ful name and email 
adress is straightforward but suffers from low 
coverage. On the other hand, the extraction of 
917
combined name and abreviated name can com-
plement the coverage, while needs handling of am-
biguity. 
Table 2 provides the heuristic rules that we use 
for expert identification. In the step 2) and 3), the 
rules use frequency and context discourse for re-
solving ambiguities respectively. With frequency, 
each expert candidate actually is assigned a prior 
probability. With context discourse, we utilize the 
intuition that person names appearing similar in a 
document usually refers to the same person. 
4.1.2 Topic
Discovering 
A queried topic can occur within documents in 
various forms, to. We use a set of query process-
ing techniques to handle the isue. After the proc-
essing, a set of topics transformed from an original 
query wil be obtained and then be used in the 
search for experts. Table 3 shows five forms of 
topic discovering from a given query. 
 
Forms Description Sample 
Phrase 
Match(Q
P
) 
The exact match with origi-
nal query given by users 
“semantic web 
search engine” 
Bi-gram 
Match(Q
B
) 
A set of matches formed by 
extracting bi-gram of words 
in the original query 
“semantic web” 
“search en-
gine” 
Proximity 
Match(Q
PR
) 
Each query term apears as 
a neighborhod within a 
window of specified size 
“semantic web 
enhanced 
search engine” 
Fuzy 
Match(Q
F
) 
A set of matches, each of 
which resembles the origi-
nal query in apearance. 
“sementic web 
serch engine” 
Stemed 
Match(Q
S
) 
A match formed by stem-
ming the original query. 
“sementic web 
serch engin” 
Table 3. Discovered topics from query “semantic web 
search engine” 
4.1.3 Relation
Extraction 
We focus on extracting relations between topics 
and expert candidates within a span of a document. 
To make the extraction easier, we partition a 
document into a pre-defined layout. Figure 1 pro-
vides a template in Backus–Naur form. Figure 2 
provides a practical use of the template. 
Note that we are not restricting the use of the 
template only for certain corpus. Actually the tem-
plate can be applied to many kinds of documents. 
For example, for web pages, we can construct the 
<Title> from either the ‘title’ metadata or the con-
tent of web pages (Hu et al., 206). As for e-mail, 
we can use the ‘subject’ field as the <Title>. 
 
Figure. 1. A template of document layout 
.
.
RDF Prime
Editos:ank Mola, fnla@cm.org
 ricrW3Cew3
2. aking Stnts About Rsrce
RDF isted toprvide aimple wayt k ta
Thcapbils(hntscifondescrib)
2.1 Bs
Ct
Imaginetryio sae tsoen ae Jh Smit
Th fo fpl uchs:
<Title>
Auhr
Boy
<SectiTtle>
toy
 
Figure 2. An example use of the layout template 
With the layout of partitioned documents, we 
can then explore many types of relations among 
different blocks. In this paper, we demonstrate the 
use of five types of relations by extending the 
study in (Cao et al., 205). 
Section Relation (R
S
): The queried topic and 
the expert candidate occur in the same <Section>. 
Windowed Section Relation (R
WS
): The que-
ried topic and the expert candidate occur within a 
fixed window of a <Section>. In our experiment, 
we used a window of 20 words. 
Reference Section Relation (R
RS
): Some <Sec-
tion>s should be treated specially. For example, 
the <Section> consisting of reference information 
like a list of <bok, author> can serve as a reliable 
source conecting a topic and an expert candidate. 
We call the relation appearing in a special type of 
<Section> a special reference section relation. It 
might be argued whether the use of special sections 
can be generalized. According to our survey, the 
special <Section>s can be found in various sites 
such as Wikipedia as well as W3C. 
Title-Author Relation (R
TA
): The queried topic 
appears in the <Title> and the expert candidate 
appears in the <Author>. 
918
Section Title-Body Relation (R
STB
): The que-
ried topic and the expert candidate appear in the 
<Section Title> and <Section Body> of the same 
<Section>, respectively. Reversely, the queried 
topic and the expert candidate can appear in the 
<Section Body> and <Section Title> of a <Section>. 
The latter case is used to characterize the docu-
ments introducing certain expert or the expert in-
troducing certain document. 
Note that our model is not restricted to use these 
five relations. We use them only for the aim of 
demonstrating the flexibility and effectiveness of 
fine-grained expert search. 
4.2 Evidence
Quality Evaluation 
In this section, we elaborate the mechanism used 
for evaluating the quality of evidence. 
4.2.1 Topic-Matching Quality 
In Section 4.1.2, we use five techniques in process-
ing query matches, which yield five sets of match 
types for a given query. Obviously, the different 
query matches should be associated with diferent 
weights because they represent different qualities. 
We further note that different bi-grams gener-
ated from the same query with the bi-gram match-
ing method might also present different qualities. 
For example, both topic “css test” and “test suite” 
are the bi-gram matching for query “css test suite”; 
however, the former might be more informative. 
To model that, we use the number of returned 
documents to refine the query weight. The intuition 
behind that is similar to the thought of IDF popu-
larly used in IR as we prefer to the distinctive bi-
grams. 
Taking into consideration the above two factors, 
we calculate the topic-matching quality Q
t 
(corre-
sponding to P(type(t,q) in Equation (12) ) for the 
given query q as 
df
MIN
qtypeQ
)(
),(
''
=, 
(13) 
where t means the discovered topic from a docu-
ment and type(t,q) is the matching type between 
topic t and query q. W(type(t,q) is the weight for a 
certain query type, df
t
 is the number of returned 
documents matched by topic t. In our experiment, 
we use the 10 training topics of TREC205 as our 
training data, and the best quality scores for phrase 
match, bi-gram match, proximity match, fuzy 
match, and stemed match are 1, 0.01, 0.05, 10
-8, 
and 10
-4, respectively. 
4.2.2 Person-Matching Quality 
An expert candidate can occur in the documents in 
various ways. The most confident occurrence 
should be the ones in ful name or email address. 
Others can include last name only, last name plus 
initial of first name, etc. Thus, the action of reject-
ing or accepting a person from his/her mask (the 
surface expression of a person in the text) is not 
simply a Bolean decision, but a probabilistic one 
with a reliability weight Q
p 
(corresponding to P(c|p) 
in Equation (3) ). Similarly, the best trained 
weights for ful name, email name, combined name, 
abreviated name, short name, and alias email are 
set to 1, 1, 0.8, 0.2, 0.2, and 0.1, respectively. 
4.2.3 Relation
Type Quality 
The relation quality consists of two factors. One 
factor is about the type of the relation. Different 
types of relations indicate different strength of the 
conection between expert candidates and queried 
topics. In our system, the section title-body rela-
tion is given the highest confidence. The other fac-
tor is about the degree of proximity between a 
query and an expert candidate. The intuition is that, 
the more distant are a query and an expert candi-
date within a relation, the loser the conection 
between them is. To include these two factors, the 
quality score Q
r 
(corresponding to P(type(r) in 
Equation (12) )of a relation r is defined as: 
1)+
=
dis
C
W
r, 
(14) 
where W
r
 is the weight of relation type r, dis(p, t) 
is the distance from the person occurrence p to the 
queried topic t and C
r
 is a constant for normaliza-
tion. Again, we optimize the W
r
 based on the train-
ing topics, the best weights for section relation, 
windowed section relation, reference section rela-
tion, title-author relation, and section title-body 
relation are 1, 4, 10, 45, and 100 respectively. 
4.2.4 Document
Quality 
The quality of evidence also depends on the quality 
of the document, the context in which it is found. 
The document context can affect the credibility of 
the evidence in two ways: 
919
Static quality: indicating the authority of a 
document. In our experiment, the static quality Q
d
 
(corresponding to P(d) in Equation (12) ) is esti-
mated by the PageRank, which is calculated using 
a standard iterative algorithm with a damping fac-
tor of 0.85 (Brin and Page, 198). 
Dynamic quality: by “dynamic”, we mean the 
quality score varies for different queries q. We de-
note the dynamic quality as Q
DY
(d,q) (correspond-
ing to P(q|d) in Equation (12) ), which is actually 
the document relevance score returned by a stan-
dard language model for IR(Ponte and Croft, 198). 
5 Experimental
Results 
5.1  The Evaluation Data 
In our experiment, we used the data set in the ex-
pert search task of enterprise search track at TREC 
205 and 206. The document colection is a crawl 
of the public W3C sites in June 204. The crawl 
comprises in total 31,307 web pages. In the fol-
lowing experiments, we used the training set of 10 
topics of TREC 205 for tuning the parameters 
aforementioned in Section 4.2, and used the test set 
of 50 topics of TREC 205 and 49 topics of TREC 
206 as the evaluation data sets. 
5.2 Evaluation
Metrics 
We used three measures in evaluation: Mean aver-
age precision (MAP), R-precision (R-P), and Top 
N precision (P@N). They are also the standard 
measures used in the expert search task of TREC. 
5.3 Evidence
Extraction 
In the folowing experiments, we constructed the 
baseline by using the query matching methods of 
phrase matching, the expert matching methods of 
ful name matching and email matching, and the 
relation of section relation. To show the contribu-
tion of each individual method for evidence extrac-
tion, we incrementally add the methods to the 
baseline method. In the folowing description, we 
wil use ‘+’ to denote applying new method on the 
previous seting. 
5.3.1 Query
Matching 
Table 4 shows the results of expert search achieved 
by applying different methods of query matching. 
Q
B, Q
PR, Q
F, and Q
S
 denote bi-gram match, prox-
imity match, fuzzy match, and stemmed match, 
respectively. The performance of the proposed 
model increases stably on MAP when new query 
matches are added incrementally. We also find that 
the introduction of Q
F
 and Q
S
 bring some drop on 
R-Precision and P@10. It is reasonable because 
both Q
F
 and Q
S 
bring high recall while affect the 
precision a bit. The overall relative improvement 
of using query matching compared to the baseline 
is presented in the row “Improv.”. We performed t-
tests on MAP. The p-values (< 0.05) are presented 
in the “T-test” row, which shows that the im-
provement is statistically significant. 
 
 TREC 205 TREC 206 
 MAP R-P P@10 MAP R-P P@10 
Baseline 0.1840 0.2136 0.3060 0.3752 0.4585 0.5604 
+Q
B
 0.1957 0.2438 0.320 0.4140 0.4910 0.579 
+Q
PR
 0.2024 0.2501 0.360 0.4530 0.5137 0.592 
+Q
F
 ,Q
S
 0.2030 0.2501 0.360 0.4580 0.512 0.5901 
Improv. 10.3% 17.09% 9.80% 2.07% 1.49% 5.30% 
T-test 0.084   0.00   
Table 4. The efects of query matching 
5.3.2 Person
Matching 
For person matching, we considered four types of 
masks, namely combined name (N
C
), abreviated 
name (N
A
), short name (N
S
) and alias and new 
email (N
AE
). Table 5 provides the results on person 
matching at TREC 205 and 206. The baseline is 
the best model achieved in previous section. It 
seems that there is litle improvement on P@10 
while an improvement of 6.21% and 14.0% is 
observed on MAP. This might be due to the fact 
that the matching method such as N
C
 has a higher 
recall but lower precision. 
 
 TREC 205 TREC 206 
 MAP R-P P@10 MAP R-P P@10 
Baseline 0.2030 0.2501 0.360 0.4580 0.512 0.5901 
+N
C
 0.2056 0.2539 0.3463 0.4709 0.5152 0.5931 
+N
A
 0.2106 0.2545 0.340 0.5010 0.5181 0.600 
+N
S
 0.211 0.2578 0.340 0.5121 0.5192 0.600 
+N
AE
 0.2156 0.2591 0.340 0.521 0.5212 0.600 
Improv. 6.21% 3.60% 1.19% 14.0% 1.96% 1.68% 
T-test 0.064   0.057   
Table 5. The efects of person matching 
920
5.3.3 Multiple
Relations 
For relation extraction, we experimentally demon-
strated the use of each of the five relations pro-
posed in Section 4.1.3, i.e., section relation (R
S
), 
windowed section relation (R
WS
), reference section 
relation (R
RS
), title-author relation (R
TA
), and sec-
tion title-body relation (R
STB
). We used the best 
model achieved in previous section as the baseline. 
From Table 6, we can see that the section title-
body relation contributes the most to the improve-
ment of the performance. By using all the discov-
ered relations, a significant improvement of 
19.94% and 8.35% is achieved. 
 
 TREC 205 TREC 206 
 MAP R-P P@10 MAP R-P P@10 
Baseline 0.2156 0.2591 0.340 0.521 0.5212 0.600 
+R
WS
 0.2158 0.263 0.380 0.525 0.531 0.6082 
+R
RS
 0.2160 0.2630 0.380 0.5272 0.5314 0.6061 
+R
TA
 0.234 0.2634 0.3580 0.5354 0.535 0.6245 
+R
STB
 0.2586 0.3107 0.3740 0.5657 0.569 0.6510 
Improv. 19.94% 19.91% 10.0% 8.35% 8.7% 8.50% 
T-test 0.013   0.043   
Table 6. The efects of relation extraction 
5.4 Evidence
Quality 
The performance of expert search can be further 
improved by considering the evidence quality. Ta-
ble 7 shows the results by considering the differ-
ences in quality. 
We evaluated two kinds of evidence quality: 
context static quality (Q
d
) and context dynamic 
quality (Q
DY
). Each of the evidence quality con-
tributes about 1%-2% improvement for MAP. The 
improvement from the PageRank that we calcu-
lated from the corpus implies that the web scaled 
rank technique is also effective in the corpus of 
documents. Finally, we find a significant relative 
improvement of 6.13% and 2.86% on MAP by us-
ing evidence qualities. 
 
 TREC 205 TREC 206 
 MAP R-P P@10 MAP R-P P@10 
Baseline 0.2586 0.3107 0.3740 0.5657 0.569 0.6510 
+Q
d
 0.271 0.318 0.3720 0.590 0.5813 0.6796 
+Q
DY
 0.275 0.3252 0.380 0.5943 0.587 0.7061 
Improv. 6.13% 4.67% 3.74% 2.86% 3.67% 8.61% 
T-test 0.0360   0.0252   
Table 7. The efects of using evidence quality 
5.5 Comparison
with Other Systems 
In Table 8, we juxtapose the results of our prob-
abilistic model for fine-grained expert search with 
automatic expert search systems from the TREC 
evaluation. The performance of our proposed 
model is rather encouraging, which achieved com-
parable results to the best automatic systems on the 
TREC 205 and 206. 
 
 MAP R-prec Prec@10 
TREC205 0.2749 0.330 0.4520 Rank-1 
System TREC206
1
 0.5947 0.5783 0.7041 
TREC205 0.275 0.3252 0.380 Our 
System TREC206 0.5943 0.587 0.7061 
Table 8. Comparison with other systems 
6 Conclusions

This paper proposed to conduct expert search using 
a fine-grained level of evidence. Specifically, 
quadruple evidence was formally defined and 
served as the basis of the proposed model. Differ-
ent implementations of evidence extraction and 
evidence quality evaluation were also comprehen-
sively studied. The main contributions are: 
 
1. The proposal of fine-grained expert search, 
which we believe to be a promising direc-
tion for exploring subtle aspects of evidence. 
2. The proposal of probabilistic model for fine-
grained expert search. The model facilitates 
investigating the subtle aspects of evidence. 
3. The extensive evaluation of the proposed 
probabilistic model and its implementation 
on the TREC data set. The evaluation shows 
promising expert search results. 
 
In future, we are to explore more domain inde-
pendent evidences and evaluate the proposed 
model on the basis of the data from other domains. 
Acknowledgments 
The authors would like to thank the three anony-
mous reviewers for their elaborate and helpful 
comments. The authors also appreciate the valu-
able sugestions of Hang Li, Nick Craswell, 
Yangbo Zhu and Linyun Fu. 
                                                             
1
 This system, where cluster-based re-ranking is used, is a 
variation of the fine-grained model proposed in this paper. 
921

References 

Bailey, P., Soborof , I., Craswel, N., and Vries A.P.,  Overview of the TREC 207 Enterprise Track. In:  Proc. of TREC 207. 

Balog, K., Azopardi, L., and Rijke, M. D., 206   Formal models for expert finding in enterprise corpora. In: Proc. of SIGIR’06,p.43-50. 

Brin, S. and Page, L., 198. The anatomy of a  large-scale hypertextual Web search engine, Computer  Networks and ISDN Systems (30), p.107-17. 

Campbel, C.S., Maglio, P., Cozi, A. and Dom, B.,  203. Expertise identification using email comunications. In: Proc. of CIKM ’03 p.528–531. 

Cao, Y., Liu, J., and Bao, S., and Li, H., 205. Research on expert search at enterprise track of TREC 205. In: Proc. of TREC 205. 

Craswel, N., Hawking, D., Vercoustre, A. M. and Wilkins, P., 201. P@NOPTIC Expert: searching for experts not just for documents. In: Proc. of Ausweb’01. 

Craswel, N., Vries, A.P., and Soborof, I., 205. Overview of the TREC 205 Enterprise Track. In: Proc. of TREC 205. 

Davenport, T. H. and Prusak, L., 198. Working  Knowledge: how organizations manage what they  know. Howard Busines, Schol Pres, Boston, MA. 

Dom, B., Eiron, I., Cozi A. and Yi, Z., 203. Graph-based ranking algorithms for e-mail expertise analysis, In: Proc. of SIGMOD’03 workshop on Researchisues in data mining and knowledge discovery. 

Fang, H., Zhou, L., Zhai, C., 206. Language models for expert finding-UIUC TREC 206 Enterprise  Track Experiments, In: Proc. of TREC206. 

Fu, Y., Xiang, R., Liu, Y., Zhang, M., Ma, S., 207. A  CD-based Formal Model for Expert Finding. In  Proc. of CIKM 207. 

Hertzum, M. and Pejtersen, A. M., 200. The information-seking practices of enginers: searching for documents as wel as for people. Information Proc-esing and Management, 36(5), p.761–78. 

Hu, Y., Li, H., Cao, Y., Meyerzon, D. Teng, L., and Zheng, Q., 206. Automatic extraction of titles from  general documents using machine learning, IPM. 

Kautz, H., Selman, B. and Milewski, A., 196. Agent  amplified comunication. In: Proc. of AAI‘96, p.3–9. 

Matox, D., Maybury, M. and Morey, D., 199. Enterprise expert and knowledge discovery. Technical Report. 

McDonald, D. W. and Ackerman, M. S., 198. Just Talk  to Me: a field study of expertise location. In: Proc. of  CSCW’98, p.315-324. 

Mockus, A. and Herbsleb, J.D., 202. Expertise  Browser: a quantitative aproach to identifying expertise, In: Proc. of ICSE’02. 

Maconald, C. and Ounis, I., 206. Voting for candidates: adapting data fusion techniques for an expert search task. In: Proc. of CIKM'06, p.387-396. 

Macdonald, C. and Ounis, I., 207. Expertise Drift and Query Expansion in Expert Search. In Proc. of CIKM  207. 

Petkova, D., and Croft, W. B., 206. Hierarchical language models for expert finding in enterprise corpora, In: Proc. of ICTAI’06, p.59-608. 

Ponte, J. and Croft, W., 198. A language modeling  aproach to information retrieval, In: Proc. of  SIGIR’98, p.275-281. 

Sihn, W. and Heren F., 201. Xpertfinder-expert finding within specified subject areas through analysis of e-mail comunication. In: Proc. of the 6th Anual Scientific conference on Web Technology. 

Soborof, I., Vries, A.P., and Craswel, N., 206. Overview of the TREC 206 Enterprise Track. In: Proc. of TREC 206. 

Ster, L.A. and Lochbaum, K.E., 198. An expert/expert locating system based on automatic representation of semantic structure, In: Proc. of the 4th IEE Conference on Artificial Inteligence Aplications. 

Yimam, D., 196. Expert finding systems for organizations: domain analysis and the DEMOIR aproach. In: ECSCW’9 workshop of beyond knowledge maagement: managing expertise, p. 276–283. 

