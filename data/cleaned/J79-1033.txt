1:860	American Journal of Computational Linguistics PROCEEDINGS 13TH ANNUAL fl'EETING ASSOCIATION FOR COMPUTATIONAL LINGUI STI cs Timothy C. Diller, Editor Sperry-Univac St. Paul, Minnesata 55101 Microfiche 33 Copyright @ 1975 by the Association for Computational Linguistics PREFACE The papers comprising this microfiche (the second of five) present in expanded form (as submitted by their authors) the six talks given in Session 2: Language Generation Systems.
2:860	Various aspects of generation are considered, among them: relationsHips between parsing and generation (Knaus), planning modules and data structures basic to story development (Meehan), semantic networks and linguistic generatorq (Shapiro and Slocum), message structures and translation strategies (McDonald), and lexical processes in compound noun formation (Rhyne).
3:860	Thanks to Martin Kay for chairing this session.
4:860	Timothy C. Diller Program Committee Chairman TABLE OF CONTENTS A Rxtsbework for Writing Generation Grammars for Interactive Computer Programs mvid Mcmnald 4  Incremental Sentence Processing Rodger mus 18 A Lexical Process Model of Nominal Compounding in  English James R. Rhyne 33 Generation as Parsing from a Network into a Linear String Stuart C. shepiro  45 Speech Generation from Semantic Nets J~Adthdn S~OCW . . 63 Using Planning Structures to Generate Stories ~ams R.  Heehan 78 American Jourdal of Computational tinguktics Microfiche 33 4 Artificial Intelligence Ldbora tory Massachusetts Ins ti tute of Technology Cambridge, Massachusetts 02139 ABSTRACT This paper outlines the stucture and operation of the 1 ingui stic component from a language generation system in an interactive program.
5:860	The component receives messages describing what is to be said formulated in the representation of the main prograr and produces fluent English utterances appropriate to the current discourse situation.
6:860	The component is data-directed and uses a procedural grammar, organized as a set of strategies.
7:860	Interactive, speclalist prograas presently under developwent will heed to produce fluent, intentional English utterances in responce to particular, complex titutlons.
8:860	This creates a requlroaont for language generating facilities that Is not faced in transformational grarapar, rochanical translation programs, or paraphrase generating programs.
9:860	As a component of rn interactive, specialist program, the production of the English must be driven directlr by the communicative intentions of the program and by the discourse situation, We can imagine that the overall program consist, of a number of cooperating modules for parsing and interpreting what is said to it, ior solving ptoblens in its domain, for managing its renary, and, in particular, for generating utterances to com~unicate wl th Its users* This generation component can be profitably viewed as having three aspects or msub-corponentsw.
10:860	1) Situation/doaain specie1 ists that are activated when the program recognizes what situation it is in+ They then decide what message will be produced.
11:860	They will decide what effect on the listener is desired, and exactly what objects and relations are to be nentioned.
12:860	Fot example, an appoint~ent scheduling program might be told to *scm~ule a group meeting for Fridayw and then find that a critical aentber of the group is uncxvailable, The situation specialists in the scheduling prograr are the ones to decide whether it is more appropriate to simply say "1 can'tR, OF whether to voluntcp information wI can't; Mitch won't be back uWlt Mondayn.
13:860	2) Models of the audience and the discourse situation to use in construct2ng utterances.
14:860	There must be a record of the past conversation to gulcfe in the selection af pronouns, Also, the program must have nodels of, and heuristics about what the audience already knows and therefore doesn't have to be told.
15:860	This Informtion lay be very specific and domain dependent.
16:860	Fot exarple, In chess, one can say "the white queen could take e knightn.
17:860	There is no need to say "a black knlphtw, because this information is supplied by inferences from what one knows about chess inferences that the spcarer assures the listener shares.
18:860	3) Llnpu!rtic know1 edge about how to construct understandable utterances in the English Isnpuagc.
19:860	Obviously, this lnformatlon vill Include a lexicon assoclatlng objects and relations from the min program with rtrate&i@~ for realizing them in English (particular words, phrases, syntactic constructions, etc.).
20:860	There is also a tremendous amount of informatian which describes the characteristics of the English language and the conditions of its use.
21:860	It specifies rhe allowable arrangements of strategies and what niodlfications or alternatives to them nay be appropriate in particular circumstances.
22:860	Of the three aspects just described, my work has concentrated on the third.
23:860	What follows is drawn from sy thesis McDonald '75) and from ongoing research.
24:860	The Lingufetio Component The 1 inguistic knowledge required for generating utterances is put into one component whose job is to take a message from the sltuatlon specialists and cpnstruct a translatlw of that message In English.
25:860	The messages are in the representation used by the main program and the s1 tuation specialists.
26:860	Tho translation is done by a data-directed process wherein the elenents and structure of the message itself provide the control.
27:860	The design of the 1 ingui stics component was arrived at independent of any particular main program, for the simple reason that no programs of adequate complexity were available at the time.
28:860	However, at the present time a grammar and fcxlcon is being developed to use with at least two prograRs being developed by other people at MIT.
29:860	They are an appointment scheduling program (Goldstein '75) and an advisor to aid users of MACSYMA (Genesereth '75).
30:860	The short dialog below is an example of the degree of fluency we arc hoping to eventually achieve.
31:860	The dlalog ts between a scheduling prograa acting as an appolntaent secretary (P), and a student (5).
32:860	(5) I want to see Professor Winston sometime in the next few days.
33:860	(PI Hers pretty busy all week.
34:860	Can it wait?
35:860	(S) No, it can't, All I need is his signature on a forb (PI Well, maybe he can squeeze you in tommorrow ~ornlng.
36:860	Give me your name and check back in an hour.
37:860	Messages Using the current message foraat and Ignoring the details of the schedulerts representation, the phrase "maybe he can squeeze you in to~morrow~ could have cow from a Pessage like this one, put together by one of the situation specialists.
38:860	Message1 features.
39:860	( prediction ) event (event actor (Winston) action (fit person-into dull schedule> the (31-10-75, gar-12am) hedge <fs possible) aim-at-audience hedge Messages have features describing the program's communicative intentions what sort of utterance is this to be; what effect is it to have.
40:860	Messages list the objects to be desert-bed (the right hand column) along with annotations for each object (left hand co1u.n) to show how they relate to the rest of the message.
41:860	The phrases on the right in angle brackets represent actual structures from the scheduler wf th those The Lexicon Translatl~n ftor tho internal reprcscntai ton of s coaputcr program to natural language has the same sort of problew as translating betw~en two natural languages.
42:860	Tk same concepts nay not be available as prim1 tivos in both rcpresents.tions, and the conventions of the target Isnguape my require additional information that was not in the source.
43:860	Generally speaking translation cannot be one for one.
44:860	What English phrase is best for a particular element in a program's message will depend on what is in the rest of the message and of what the external context is. In such circunstances, translation by tablelookup is inadequate.
45:860	In this component, in order to allow all factors to be considered, the translation of each element ias done by individualized procedures called wcorposersH.
46:860	For each main program that the linguistic component becomes associated with, a lexicon must be created which will list the elements of the rain program's representation that could appear in a message (1.
47:860	C. wprCdl~tl~nH, "eventw, w<WinstonP, etc. ).
48:860	With each element is recorded the composer that will be run when the time comes to produce en English description for it (examples will be given shortly).
49:860	Some conposers nay be applicable for a whole class of elements, such as "eventsw.
50:860	They would know the structure that all events have in common (e.g. actor, actlon, tine) and would know how to interpret the idiosyncratic details of each event by using data in the lexicon associated with them.
51:860	The Grammar strategies The bulk of the grauar con&ists of "strategiesw.
52:860	Strategies arc associated with particular languages rather than with particular main prograas as composers are.
53:860	A given strategy may be used for several different purposes.
54:860	A typical case is the strategy use-s,?mp~resenttense: a clause in the simple present ("prices risew) my be understood as future, cond'itlonal, or timeless, according to what other phrases arc present, Each composer ray know of several strategies, or Coabinatlons of strategies which it could use in dcscrlbing an ele.cnt from the message, It wiT1 choose between the& according to the context usually details of the element or syntactic constraints iaposed by previously selected strategies.
55:860	The strategies themselves do no reasoning; they are implemented as functions which the corposers call to do all the actual corsst'ruction of the utterance.
56:860	The Tr~nslation Prooesa At this point.
57:860	the out1 ine of the data-dr Iven translation process can be su~rnriztd.
58:860	A message is glven for translation.
59:860	The ele~ents of the aessage are associated in a lexicon with procedures to describe the The procedures are run; they call grs~latica1 strategies; and the strategies construct the English utterance.
60:860	Of course, if this were all there was to it, the process would never run, betause all of the subprocesses aust be throughly coordinated if they are not to "trip over their own feet", or, for that ~atter, if ordinary human beings are to bo able to design the In a system where the knowledge of what to do is distributed over a large number of separate procedures, control structure assumes central.
61:860	iaportance.
62:860	Plans Before dt~cribing the control structure, I must lay out some additional aspects of the design of the ilngulstlcs coaponent.
63:860	There is no interlingua or intermediate level of structure co~parablc to the dkep structures of Transformttlonal Gra~sar, or the sewntlc nets of Sl~aons (73) or Goldban (74).
64:860	Detorminln~ the appropriate sutface structure, however, requires plannfnp, if for no other reason than that the Message can only be examined one piece ax a the.
65:860	The entire utterance must be organized before a detailed analysis and translation can get underway.
66:860	As this is done, the wproto-utterancew is represented in terns of a sort of scaffolding a representatiqp of the ultimate surface structure tree insofar as its details are known with extensive annotation, explicit and implicit, to point out where elements that are not yet described may be positioned, and to implement the graamatical restrictions on possible future details as dictated by what has already been done.
67:860	The scaffolding that is constructed in the translation of each message is called its wp4a0w.
68:860	Plans are made up of syntactic nodes of the usual sort clauses, noun groups, etc, and nodes may have features in the Banner of tysteni; grammar Winograd '72~ Nodes have subplans consisting of a list of named slots marking the possible potftlons for sub-constituents, given in the order ~f tho eventual surface structure.
69:860	Possible slots would be wsubJectw, *.%in verbw, "noun headw, wpre-verb-adverbw, and so on.
70:860	The syntactic node types will each have a nuaber of -possible plans, corresponding" to the different possible arrangements or sub-consti tuents that may occur wl th tho different combinations of features that tho Mdc may have, Depending on tho rtage of the translation process, a slot may bc "fl1lodw with a pointer to an lnternal object fro.
71:860	the message, a syntactic node, a word or idior, ar nothing.
72:860	The translation proaaea Tho translation is done in two phases.
73:860	The second phase does not begin until the first is coapletely finished.
74:860	During the first phase, a plan is selected and the eleaents of the message are transferred, largely untouched; to the slots of the plan and features added to its nodes.
75:860	During the second phase, the plan is wwalktdR topdown and from left to right.
76:860	Conpostrs for mssage ele~ehts in the plan's slots are activated to produce English descriptions for the elcments as they are reached in turn.
77:860	Both processes are data-directed, the first by the particular contents of the message and the second by the structure of the plan and the contents of its slots.
78:860	There are sound linguistic reasons for this two stage processing.
79:860	Most parts of a lessage Bay be translated in terms of very modular sysltactic and lexical units.
80:860	But other parts are translated in terms of relations between such units, expressed usually by ordering or clauselevel syntactic aechanislps.
81:860	The exact for@ of the s~aller units cannot be deternlned until their larger scale relations have been fixed.
82:860	Accordingly, the objective of the first phase is to determine what global relationships are required and to choose the plan, features, and positions of message elemnts within the plan's slots that will realfzt those relationships.
83:860	Once this has been done, Engl tsh descriptions for the elements can be made Independent of each other and will not need to be changed after they arc initially created.
84:860	One of the lost iaportant features of natural language is the ability to omit, prono~inal ize, or otherwise abbreviate elements in certain contexts.
85:860	The only known rules and hllristlcs for using this feature rro phrased in terms of Surface structure configurations and temporal ordering.
86:860	Because the second ~hase works directly in thcse terms, stating and using the available heuristics becoaos a straight orwsrd, tractable problem.
87:860	"Maybe he can eg.ueeze you in tommoww morning" The rest of this paper will try to put solae flesh on your picture of how this linguistics conponent works by following the translation of the message given in the beginning to the sentence above.
88:860	The message was this.
89:860	Massage1 features* ( prediction ) event (event actor <Winston> action.
90:860	tfit person into full schedule) time <31-10-75,9aar-l2rm>) h-@biie <is possible, aim-at-audience hedge The intentional features of a message tend to require the nost global representation in the f Inal utterance, because that is where indicators for questions, special emphasis, speclal formats lee n. conpari son), and the like will be found.
91:860	By convention then.
92:860	the composers associated wlth the intentions are given the job of arranging for the disposition of all of the messake elements.
93:860	The total aperatlon of phase one consists of executing the composer associated with each feature, one af $er the other.
94:860	Thls aessage has only one feature, so its composer will assume al) tho work.
95:860	Thc linguistics component is implemented in MACLISP, features (and annotations and slots and nodes) are atoms, and coar>Qsers ass functions on their property lists.
96:860	Prediction composer-with (lambda  ) Making a prediction is a speech act, an& wo nay expect there to be particular forms in a language for expressing thee, for example, the use af the explicit "willw for the future tense.
97:860	Knowledge of these would De part of the composer.
98:860	Inside the aain program, or the situation special lst, the concept of a prediction may always include certain parts: what is predicted, the time, any hedges, and so on.
99:860	These part are directly reflected in thc makeup of the elements present in the message, and their annotations mark what internal roles the7 have.
100:860	There does not need to be a direct correspondence between these and the parts in the linguistic forms used, the actual correspondence is part of the knowledge of the prediction cosposer.
101:860	Typically, for any feature, one particular annotated ele~ent will be of greatest l~portance in seting the character of the whole utterance.
102:860	For predictions, this is the "eventw.
103:860	The prediction composer chooses a plan for the utterance to fit the requireaents of the event-element.
104:860	The realization of any other elements will be restricted to be compatible with it.
105:860	The prediction composer docs not need to know the element's linguistic correlates itself, it can delegate the work to the composer for the element itself.
106:860	The element look like this, (event actor <Winston> action <fit person into full schedule> tine t31-10-75,9am-lZaa>) The first word points to the name of the composer, and the pairs give particular details.
107:860	There is nothing special about the words used here (actor, action, tlme), Just a$ long as the composer is designed to expect the inforwatton in those places that the message-asseabler wants to put it.
108:860	The event composerfs strategy is to use a clause, and the choice of plan is determined by the character of the event's "actionw.
109:860	The action is "<fit person into full schedulerw, and it will have two relevant properties in the lexicon: "plan*, and ".appingW.
110:860	klan is either the nane of a standard plan to be used; or an actual plan, partially filled with words (1.
111:860	e. it can be a phrase).
112:860	"Mappingw is an association list showing how the subelements of the message are to be transferred to the plan.
113:860	<fit person into full schedule) PLAN node-i (clause trans1 particle) slots frontings nil subject nil vg node-j (verb-group particle) slots modal nil pre-vb-adv nil mvb "squeezew prt "inw object1 <person being talked about) post-modifiers nil MAPP I NG (( actor subject ) ( time post-modifiers)) The event composer proceeds to instanticte the nodes in the phrase and make the transfers; the prediction composer then takes the resulting plan, and makes it the plan of the whole utterance.
114:860	Two message elements remain, but actually there is only one, because waim-at-audiancew is supplying additional informati~n about the hedge.
115:860	The annotation means that the contents of the hedge (<is possible>) ere pore something that we want to tell the audience than a detail of the prediction.
116:860	This will affect how the element is positioned in the plan.
117:860	The prediction composer looks in the lexicon to see what grammatical unit will be used to realize <is possible,, and sees, let us say, two possibilities involving different configurations of the adverb wn&ybcn and the modal "can be able ton, with the differences hinging on the placement of the adverb.
118:860	Theoretically, adverbs can be positioned in a nunber of places in a clause, depending on their characteristics.
119:860	In this instance, the choice is forced because of a heuristic written into the grammar of adverbs and accessible to the composer, that says that when the intent of an adverb is directed to the audience, it should be in the first position (the "frontiogsW slot).
120:860	This choice implies putting "canw In the modal slot directly.
121:860	The alternative with w~ybe* in the pre-vb-adv slot would have necessitated a different form of the .o8al, yielding "ray be able ton, These details would have been taken care of by syntactic routines associated with the verb group node.
122:860	A11 the message ererents have been placed and the first phase is over.
123:860	The plan is now as below.
124:860	n4e-l (clause trans1 particle) slats fmntfngs "8aybeW subject twinston> vg node-2 (verb-group par tic1 el slots modal "canw pre-vb-adv nil mvb "squeezen prt " inw object1 cperson being talked about) post-modifiers nil The second phase controller is a simple dispaching function that mves from slot to slot.
125:860	"Fronttngs* contains a word, so the word is printed .directly (there is a trap for morphological adjustnents when necessary).
126:860	wSttbjectw contains an internal object, so the controller should go to the lexicon for its composer and then come back to handle whatever the composer replaced the clement with, However, there is always an lhtervening step to check for the possibility of pronominalizing.
127:860	This check is made with the elelpent still in its internal foro.
128:860	The record of the discourse is given directly in term of the internal representation and test for prior uccurence can be as simple as identity checks against a reference list, svoiding potentially intricate string matching operations with words.
129:860	In the dialog that this message came from, there is clear reference to twin9ton>, so it can be prononinallzed and "hew is printed.
130:860	Any slot, or any node type may have procedures associated with it that are executed when the slot or node is reached during the second phase.
131:860	These procedures will handle syntactic processes like agreement, rearangelaent of slots to realize features, add function words, watch scope relationships, and in particular, position the particle in verbparticle pairs.
132:860	Generally, particle position ("squeeze John inn vs. n~q~me in Johnw) is not specifled by the grammar except when the object is a pronoun and the particle must be displaced.
133:860	This, of course, will not be known untlll after the verb group has been passed.
134:860	To deal with this, a subroutine in the "when-ent$redn procedure of the verb group is activated by the "particlen procedure.
135:860	First, it records the particle and relaoves it from the VG plan so it will not be generated automatically.
136:860	A "hookw is available on any slot for a,procedure which can be run after prononinalization is checked and before the composer is called (if it is to be called).
137:860	The subroutine incorporates the particle into a standard procedure and places it on that hook for the object1 slot.
138:860	The procedure will check if the object has been prlnted as a pronoun, and if so, prints out the particle (which is now In the proper displaced pori tion).
139:860	If the object wasn' t pronominal ized, then it does nothing, nothing has yet been printed beyond the verb group, and other heuristics will be free to apply to choose the proper position.
140:860	Since (person being talked about, is here equal to the student, the person the prograa is talking with, it is realized as the pronoun "youw and the particle is dlsplaccd.
141:860	Going irom <31-10-75,9a~-12arn> to wtom~rr~~ ~orning* my be little more than table lookup by a wtfme" coBposer that hat been designed to know the formats of the time expressions inside the scheduler.
142:860	This presentation has had to be unfortunately short for the amount of new naterial involved.
143:860	A large nu~ber of interesting detail s and questions about the processing have had to be oritted.
144:860	At the moaent <September, 19751, the data and control structures ~cntioned have been fully iep1e~ented and tests are underway on gedanken data.
145:860	Hopefully, by the end of 1975 the component will have a reasonable gramar and will be working with messages and lexicons form the two programs mentioned before.
146:860	A MI7 A. I. lab technical report describing this work in depth should be ready in the spring of next year.
147:860	David McDonald Cambridge, Mass. American Journal of Computational Linguistics Microfiche 33 : 28 RODGER KNAUS Systems of tware Division Social and Economic Statistics Administration Bureau of the Census Washington, D. C. 20233 A human who learns a language can both parse and generate sentences in the language.
148:860	In contrast most artificial language processors operate in one direction only or requtre separate grammars for parsing and generation.
149:860	This paper describes a model for human language processing which uses a single language description for parsing and generation.
150:860	1.
151:860	Choice of Parsing Strategy A number of constraints limit the processors suitable as models of human language processing.
152:860	Because short term memory is limited.
153:860	the listener must absorb incoming words into larger chunks as the sentence is heard.
154:860	Also because he is expected to reply within a couple seconds after the speaker finishes, regardless of length of the speaker's utterance, the listener must do much of the semantic processfng of a sentence as he hears it.
155:860	19 Bever and Hatt point out that the difficulty in understanding a sentence S is not predicted by the number of transformations used to generate S. Furthermore the process of detransforrnation appears too time-consuming (petrick) for the approximately two seconds before a listeaer is expected to reply.
156:860	A depth first transition network parser (Woods, ~aplan), in which parsing difficulty is measured by the number of arcs traversed, correctly predicts the relative di fficul ty of active and passive sentences progressive and adjectival present participle sentences and the extreme difficulty of multiple center embeddings.
157:860	However a syntactically directed depth first parser does not explain why syntactically similar sentences such as (5A) The horse sold at the fair escaped.
158:860	(5%) The horse raced past the barn fell.
159:860	vary in difficulty, nor does it explain experiments on the completion and veriftcation of ambiguous sentences (MacKay, Olsen and MacKay) which suggest that a pruned breadth first strategy is used to par ce sentences.
160:860	Sentences with two equal ly plausible a1 ternatives took longer to process than sentences with only one likely interpretation.
161:860	This extra processing time may be attributed to the construction of two alternate interpretations over a 1onge~ portion of the sentence when more than one interpretation is plausible.
162:860	In addition subjects somet~mes become confused by the two interpretations of an ambiguous sentence.
163:860	Finally in experiments in which subjects hear an ambiguous sentence in ode ear and a disfrnbiguating sentence simultaneously in the other ear (Garrett) the interpretation af the ambiguity actually percelved by the subject may be switched between the possibilities by changing the disambiguating sentences.
164:860	Step 3 (a): (S NP (N mail) (N Boxes)) [V like) (.NP) (PP*)) (b): (S (NP (NP (N mall) (N Boxes)) (PP (PREP like) NP') (PP*)) V(NP) fpp*)) (c): (S (NP (N mail)) (V Boxes) I PP (PREP like) NP) (PP*)) (d): (S V mail) (NP (N Boxes)) (PP (PREP like) NP) (PP*)) (e): (S (V mail) (NP (NP (N Boxes)) (PP (PREP like) NP) (PP*)) (pp* 1) After completing the sentence after Step 4, the parser produces phrase markers from a, c, d and e by adding the last word and deleting unfilled optional nodes.
165:860	The phrase marker obtained from 48 is rejected because it contains an unfilled obligatory V node.
166:860	The incremental parser adds each successive sentence word to the partially completed phrase markers built from the earlier part of the sentence.
167:860	The new word is added at the leftmost oblig unfilled node of each partial phrase marker and at all optional nodes to the left of this node.
168:860	Three different operations are used to add a new word to a partial parse.
169:860	The word may be directly added to an unexpanded node, as in Step 3a above.
170:860	A1 ternatively, a new word may be attached to an unfilled node with a left branching acyclic tree built from the grammar such as (PP PREP NP) or (S (NP N, IN*)) V (NP) (PP*)).
171:860	Attaching occurs in steps 1 and 3c.
172:860	Finally a subtrbe of an existing partial phrase marker may be 4eft embedded in a larger structure of the same grammatjcal category, as in steps 3b and 3e above.
173:860	The embedding operation uses at most two left branching trees bui1 t from the gr'ammar: a tree TI with a single cycle on the left branch is used to replace the existing subtree E being embedded.
174:860	In step 3e, for example, the structure (S (V mail) (NP NP (PP*)) (PP*)) would be obtained.
175:860	The E is used to expand the leftmost unexpanded node of TI: for 3 b this results in: 3e.
176:860	(S (V mail) (NP (NP (N Boxes) (N*)) PP*) (PP*)).
177:860	Finally to the resulting structure the new sentence word is added through direct node expansion or attaching with an acyclic left branching tree; in the example above this produces 3e from 3el' Using direct expanston attaching and embedding, the incremental parser finds a1 1 the phrase markers of sentences fn context free or regular expression language; a formal definition of the parser and a proof of its correctness appear in [lo].
178:860	Sometlmes, as at steps 3b and 3e, the same structure (a prepositiobnal phrase in step 2) is used in more than one partial parse.
179:860	Following Earley's Algo'rithm, the incremental parser builds a single copy of the shared substructure Sf!!
180:860	and maintains pofnters lfnking Sb to nodes in larger structures which $9 expands.
181:860	Far all its tree building operations the incremental parser uses a flnlte set of trees.
182:860	e., the trees with only left subnodes expanded and at most onelcycle on the leftmost branch.
183:860	These trees may be computed djrectly from the grammar and referenced by root and leftmost unexpanded node during the parse.
184:860	Using these preconstructed trees, the incremental parser requires only a fixed number of operations to add a new word to a partial parse: a retrieval on a doubly indexed set, copying the left branching tree, and at most four structure changing operations +o paste words and trees together.
185:860	Like Earley's Algorithm, IP processes each word proportionally to sentence length.
186:860	However on sentences satisfying a depth difference bound, the parsing time per word is constant.
187:860	Because humans can't remember large numbers of sentence words but must, process speech at an approximately constant rate, a constant parsing time per ward is a necessary property of any algorithm model i ng human language processing.
188:860	Let the depth of constituent C in phrase marker P be defined as the length of the path from the root of C to the root of P. If TI and T2 are tuo adjacent terminals with TI preceding T2.
189:860	the depth difference from 11 to T2 is defined as the difference in depth between 11 and the root ~f the smallest tree containing TI and T2.
190:860	For exarn~le in the phrase marker (9) (S (NP NP (DET the) (N telephone)) [PP (PREP IN) (F(P (DET the) (N room))) (V rang) (ADV loudly)) the depth difference between "the" and "telephone"' is 1 and between "roam" and "rang" is 3.
191:860	The depth difference between 11 and T2 is the number of nodes from TI to the node expanded when adding T2 on a postorder traversal from TI in the partial phrase marker containing TI but not T2.
192:860	The depth difference between TI and T2 also represents the number of constituents of which TI is the rightmost ward.
193:860	A proof (requiring a forma.1 def ini tion of the i ncremental parse) that parsing time per word is constant in depth difference bounded sentences appears in [lo].
194:860	Informally the depth difference bound places a bound both on the number of next nodes to expand which may follow a given terminal and on the amount of tree traversal which the parser must perform to ffnd each next unexpanded node.
195:860	Sfnce each modification requires only a fi'xed number of operati-ons,, each of which is bounded on the finite set of at most once cyclic left branching trees, the computation adding a new word to existing partial parses is bounded inde pendently of sentence length.
196:860	Natural language sentences tend to have small depth differences.
197:860	Both right branching sentences and left branching sentences (found in Japanese for example) have an average depth difference over each three or four word segment of two or less.
198:860	On the other hand sentences are difficult to understand when they have two consecutive large depth differences, such as the mu1 ti ple center embedding (10) The rat the cat the dog bit chased diedb or the complex noun phrase in The pad on a clarfnet in the last row whicn 1 fixed earlier far Eb fe7l out.
199:860	Furthermore in amhlguous sentences such as (11) Joe figured that it was time to take the cat out.
200:860	Kimball observes that subjects prifer the reading with the smaller depth difference.
201:860	Flnally, Blumenthal found that subjects tended to understand a multfple center embedded sentence as a conjunct1 ve sentence.
202:860	The conjunctive sentence ~ontains a rearrangement.
203:860	with lower depth differences of the constituents of the center embedded sentence.
204:860	3.
205:860	Sentence Generation The syntactic form given to a sentence depends on the information being communicated in a sentence and on the cultural context in which the sentence appears.
206:860	Clark and Haviland show that a speaker uses various syntactic devices sentences to place the "given" informatian known to the listener before the information "new" to the listener.
207:860	Particular syntactic structures are also used to emphasize or suppress particular kinds of information; for example newspaper traffic accident reports usually begin with a passive sentence such as (12) An elderly Lakewood man was injured when ., presumably to emphasize the result of the accident.
208:860	To capture the dependence of syntax on semantic content and bocjal context, the sentence generator uses function-like grammar rules of the form (Rul erame Cat Variables Predicate Forms ) . Rulename is the name of the rule and cat is the grammatieal category of the constituent generated by the rul em Variables is a list of formal parameters.
209:860	Usually the variable list contains a varfable bound during rule execution to a node in a semantic network and another variable bound to a control association list containing information about the context in which the generated constituent will appear and possibly the syntactic form the constituent should have.
210:860	Predicate is a Boolean-valued form on the parameters in Variables.
211:860	A rule is used only when Predicate is true.
212:860	Forms is a list of forms depending on Variables which generate terminals or calls to the grammar for subconstituerits of CAT.
213:860	An example of a generation rule is (SPI SI(X Y) (Equal (Voice Y) (Quote Passive)) (NP (Object X) Y) t Beverb X) Pap (Action X)) (M* X Y)) which generates simple passive sentences.
214:860	The variable X is bound to a node in a semantic network and Y to a control association list.
215:860	The rule is applied only if the control alist contains a passfve flag and if the semantic node has an object and action; in general a rule is applied only if the semantic subnodes called in the rule body appear in the semantic net.
216:860	The form (NP (Obj X) Y) generates a form (NP XI fl), where XB is the semantic node on the object indicator from X. and Yj3 fs the value of Y. Beverb and Pap are procedures which generate respectively a form ~f the verb "to be" and a past particfple form of the verb Action(X).
217:860	M* is a procedure which generates a list depending on X and Y such as (PP~Value of Tlme(X)> <Value of Y>) for generating optional prepositi~nal phrases or relatf vc clauses.
218:860	AS each rule is applied, the list of terminals and calls to grammar rules generated by the rule is added to a phrase marker representing the structure of the sentence being generated.
219:860	Grammar calls in the phrase marker are expanded top down-and left to right, in a preorder traversal of the growing phrase marker.
220:860	As terminals are generated they are printed out.
221:860	As an example, illustrating the effect of semantic and social contest on sentence generation, an initial sentence OF a traffic accident report, (13).
222:860	A man was killed when a car hit him in Irvine.
223:860	was generated from the semantic nodes Al: Agent Afl A2: Agent A@: Classman Object v0 Action hit Action Kill Object VJI) Place Irvine Instrument Car Cause AZ and the control alist.
224:860	Purpose: 1ntroduction;cases: object, cause, place using a grammar built for generating traffic accident report sentences.
225:860	To summarize a trace of the generation, a call to the sentence rule with purpose = introduction generates a sentence call with voice = passive.
226:860	The passive rule applies and a noun phrase on Afl is called for.
227:860	Because Purpose Introduction a NP rule applies which calls for a NP to be generated on the semantic class to which A8 belongs.
228:860	Because CASES contains TIME and CAUSE, the passive rule generated calls far modifying structures of these CASES.
229:860	Because the cause semantic node A2 has an action, the modifier rule M => Relative conjunction S generates the cause while the time is described by a prepositlonal phrase.
230:860	The pronoun "him" is generated by a noun phrase rule.
231:860	NP-1 which generates a pronoun when the first semantic argument to the left of the NP-1 call in the generation phrase marker which is described by the same pronoun as the semantic argument A of NF-1 is in fact equal to A. 4.
232:860	Finding Semantic Preimages While the generator described in section 3 produces sentences from semantic and contextual information, the incremental parser described in section 2 recovers merely the syntactic structure of a sentence.
233:860	To obtain the semantic arguments from which a sentence might have been generated a procedure to Invert the generation rule forms must be added to the incremented parser.
234:860	While the incremental parser begins the construction of constituents top down, it completes them syntactically in a bottom up direction.
235:860	In fact IP executes postorder traversals on all the syntactic parse trees it builds; of course if a particular partial phrase marker can not be finished, the traversal is not completed.
236:860	However each node not a tree terminal of a syntactic phrase marker visited by the incremental parser is a syntactically complete constituent.
237:860	When the parser visits a syntactically complete constituent C, it applies a function INVERT to find the semantic preimages of C. In finding the semantic structure of C, INVERT has available not only the syntactic structure of C, but also the semantic preimages which it found for subconstituents of C. 'INVERT finds the set of generation rules which might proruce a constituent having the same syntactic form as C. For each such rule R. , INVERT constructs all the possible parings between each outputgenerating form F of R and the constituents of C which F might produce.
238:860	For example if C is (S (NP Man) (Beverb is) (PAP Injured)) the pairing established for the passive sentence rule would be (NP (Object X) Y) (NP the man) (Beverb X) (Beverb is) (pap (Action X)) (Pap Injured) (M* X Y) NIL The pair ((Equal (Voice Y) PASSIVE) 1) is also created, since the rule predicate is true whenever a rule applies.
239:860	Each indicidual pair P in s-uch a pairing of a rule form and rule form outputs is processed by a function FIND which returns an association list containing possible values of the rule parameters (X and Y in the example above) which would produce the output appearing in P. For the example above FIND would produce ( X ((Object Man) Y NIL)) (1 x ((rime past) Y NIL)) (( X NIL) (Y (( Cases Nil)))).
240:860	(( X NIL) (Y (( Voice Passive)))) Using an extension to association lists of the computational logic Unification Algorithm, these association lists are unified into a single association list, which for the example is (( X ((Agent man) (Time Past) (Action Injure)) (( Y ((Cases Nil) (Voice Passive)))) Finally INVERT creates a grammar rule call, (S ((Agent man)(Time Past)(Action Injure)) ((Cases Nil)(Voice Passive)))) from the association list and stores the result in the inverse image of C. In finding a semantic preimaqe, the INVERT function must know which grammar rules might produce a parti cul ar grammati cal constf tuent.
241:860	This information is computed by symbol ical ly evaluating the grammar rules to produce the strings of regular expression grammar ncnterminals (as opposed to grammar calls) representing the possible output of each rule.
242:860	The resulting relation from rules to strings is inverted into a table giving possible rules generating each string.
243:860	The heart of this symbolic evaluator is a function ETERM on the output generating forms of a rule which returns a list all lists of regular expression nonterminals representing the output of a form.
244:860	ETERM takes advantage of the similar syntax of most grammar rule forms, and is defined in simplified form (with comments in angle brackets) as Eterm (form) = if atom (form) then NIL <terminates recurs ion> else if car (form) is a grammatical category then list (Ifst (car (form))) <these forms generate a single grammar call> elsa if car (form) = FUNCTION ar LAMBDA then ETERM (cadr (form)) else if car (form) = LAMBDA then ETERM (caddr (form)) else if car (form) = LlST if form is not properly contained in a LIST expression then Mapcar((Function Concatenate) (Cartesian ((Mapcar (Function ETERM) cdr (form)) ) ) <outer LISTS are used to create lists of grammar calls> else if farm Is inside e LIST expression ETERM (cadr (form)) <inner lists are used to create grammatically> else if car (form) = MAPCONC then make optional and repeatable all the nonterrninals returned in ETERM ([function argument of MAPCONC]) else if car (form) = COND then MAPCONC((LAMBDA(X) ETERH ([last form in XI) (cdr form) <returns alternatives from each branch of the COND> else if car (form) is a user-defined function then ETERM ([definition of function]) else if there is a stored value for ETERM (form) then that value else ask the the user for help The function FIND which returns possible bindings for rule variables when given a rule form and its output is defined below.
245:860	The variable AL1,ST holds the value of the association list being hypothesized by FIND; this variable is NIL when FIND is called from INVERT.
246:860	Like ETERM, the definition of FIND is based on the rules for evaluating recursive functions.
247:860	FIND (Alist form value)= if eval (form a1 fst)=val ue then l i st (A1 ist) else if recursion depth exceeded, then NIL else if atom (form) then list (Merge (list (cons (form Value)) Alist) else if car (form)= COND let L = clauses which might be entered by evaluating form then Mapconc (FM 1) where FM (clause) = list (Merge Find (Alist Car (c1ause)T) Find (Alist last (clause))) else if car (form) = Quote then if cadr (form) = value then Alist else NIL else if car (form) is a defined function then FIND (Alist (Substitute cdr (form) for formal parameters in definition of car (form)) Value) else if car (form) = MAPCONC (fn 1st) then Me'rge (Find (Alist 1st value) For each X in 1st.
248:860	Merge (Alist for X)) <this clause makes the assumption, which works in practice, that fn generates either one-element or empty lists, else NIL With a definition of FIND similar to the one above, the parser found the preimage (3 (((place ((class (park)))) (agent ([class (man)))) (action (walked] [the extra parentheses den3te lists of alternatives] for the sentence (13) The man walked in the park.
249:860	generated by the grammar [Sp S (X) T (NP (Agent X)) (V(Action X)) (Optional (PP (Place X) ((Case Place] [NPfl NP (X) T (Det X) t N (Class XI [ppa PP (XY) T (Prep XY) (NPX] and the preposition function Prep (XY) = Selectq (Assoc CASE Y) (Place IN) (Instrument WITH) (Source FROM] 5.
250:860	Imp1 ementation The processors described in this paper have been programmed in University of California, Irvine, LISP and run in about 45K on a PDP-10 computer.
251:860	American Journal of Computational Linguistics Microfiche 33 : 33 Department of Computer Science University of Houston Houston, Texas 77004 ABSTRACT A theoretical model for nominal compound formation in English is presented in which the rul-es are representations of lexical processes.
252:860	It is argued that such rules can be generalized to account for many nominal compounds with similar structure and to enable new compounds to be produced and understood.
253:860	It is shown that nominal compounding depends crucially on the existence of a llcharacteristic'' relationship between a nominal and the vexb which occurs in a relative clause paraphrase of a compound which contains the nominal.
254:860	A computer implementation of the model is presented and the problems of binding and rule selection are discussed.
255:860	Linguistic Issues.
256:860	Nominal compounds are sequences of two or more nominals which have the semantic effect of noun phrases with attached relative clauses.
257:860	The rightmost nominal is generally i he primary referent of the compound the other nominals restrict the reference of the rightmost nominal in much the same fashion that a relative clause does.
258:860	Tbeae are, of course, exceptions in which the rightmost nominal is figurative or euphemistic (e.g. family jewels).
259:860	Compounds occur frequently in English and Germanic languages, but infrequently in the Romance languages where their function is largely performed by nominal-prepositionnominal sequences (e. g. chemin de fer, agent de change) . __C The syntact kc structure nominal compounds is quite simple --the three variants are NAN, N-participle-N, and N-gerund-N.
260:860	In the N-N form, either 0% the two nominals may in fact be yet another nominal compound, giving a structure like (N-N)-N or N-(N-N); the first of these forms seems to occur much more often than the second (examples of each type are: typewriter mechanic, liquid roach poison).
261:860	I assume that the process of nominal compounding is syntactically a process in which a relative clause is reduced by deleting all elements of the relative clause but one and preposing the single remaining element in front of the antecedent nominal.
262:860	In addition, the clause verb may be nominalized 4nd preposed.
263:860	Other linguists have proposed different derivations for nominal compounds; Lees [3], for example, derives nominal compounds from nominal-preposition-nominal sequences.
264:860	There are two reasons why I feel that Lees approach is wrong: (1) there are English compounds for which no reasonable equivalent nominal-prepos it ionnominal paraphrase can be given (e.g. windmill), and (2) there are subtle meaning differences between the nominal compounds and their nominal-preposition-nominal counterparts (county clerk vs. clerk for the county).
265:860	If nominal compounds and nominal-preposition-nominal sequences are derived from forms like relative clauses, then the differences in meaning can be accounted for by deriving each form from a distinct relative clause; the relative clauses may, of course, be quite closely related to each other.
266:860	I have spoken rather loosely about deriving nominal compounds from relative clauses; I am not proposing a derivation system which operates on surface forms of the language, and what I intend that the reader should understand is that an underlying form for a nominal compound is derived from an underlying form for a relative clause by a language process which I term a lexical rule because, as we shall see, the operation of such rules depends crucially on the specific lexical items which are present in the underlying structures.
267:860	Linguists have identified a number of lexical processes in English; some examples of such processes may be found in [I] and [2].
268:860	The underlying forms associated with relative clauves and nominal compounds in the model of nominal c om pounding being presented here are networks (trees for the most part) defined in terms of a case grammar which is closely related to that used by Simon8 [ 51.
269:860	The cases which appear in this system fall into two general categories: (1) cases of the clause verb, which are the following -Performer, Object, Goal, Source, Location, Means, Cause, and Enabler -and (2) structural cases, which are REEL (relative clause) and COMP (compound).
270:860	I will not explain these cases in detail, as that is the subject of a forthcoming paper.
271:860	But the following observations will illuminate the case system for verb cases.
272:860	The case system distinguishes the immediate performer of an act from a remote cause or agent of the act.
273:860	The reason for this distinction lies in an intimate connection between verbs and the assumed or habitual performer of the act which is the reference of the verb.
274:860	The case system also distinguishes an active causative agent of an act from an agent which merely permits thk act to occur; this distinction in the case system permits two classes of verbs to be distinguished according Po whether the surface subject commonly causes the act or permits the act to occur.
275:860	The case system used in the present model of nominal campounding is not a deep case system; on the contrary, it Seems that nominal compounding is a lexical process which occurs rather near the surface in a derivatidnal grammar model.
276:860	An example which can be given to support this is the compound ignition key; this is a key Vhich turns a switch which enables a complex sequence of events to take place that ultimately result in the ignition of a fuel/air mixture in an engine, ar one may describe it equivaaently as a key which causes ignition.
277:860	The first aescription corresponds to a deep case level of description while the second corresponds to the level at which the compound ignition key is formed.
278:860	I would argue tHat if one takes the deep case approach, then one is forced to include a great deal of structure in the rules for nominal compounding; in particular, the rule for ignition key must remove all af the links in the causal chain leading to the ignition act.
279:860	The deletion of this intermediate information mast be done to obtain the description given in the second case, and to include the deletion procedure in both a compounding rule and in the rule process which leads to the shorter description means unnecessarily duplicating the procedure.
280:860	Moreover, if one derives compounds from paradigm relative clauses of the second sort, e.g. key which causes an action to occur, then it is possible to generalize compound forming rules so that a siqgle rule may produce several compounds.
281:860	It will not be possible to do this if deep cases are used as the deep case structure of firing key will be quite different from that of ignition key.
282:860	In order tb understand the model of compounding whfch is being presented here, it is essential to consider the function of wmpounding in language.
283:860	In my view, compounding is a process which allows a speaker to systematic8lly deLete information from an utterance just when the speaker has reason to expect that the hearer can reconstruct that information.
284:860	In effect, I consider compounding (and a great many other linguistic procesbes) ro be examples of linguistic encoding which are used to speed up communication, and the grammar shared by the speaker and hearer must include the encodihg and decoding functions.
285:860	Consider the nominal compound steam distillation, which refers to the distillation of same substance with steam; the hearer of the compound steam distillation knows that distillation is the derived nominal form of distill.
286:860	The hearep also knows what the common or characteristic cases of the verb distill are: the agent is invariably a person or machine (this would be the occupant of the Cause case slot in my system), the instrument (or Means) may be an apparatus or a heated medium such as steam and the Goal is a liquid which is missing some of the constituents that it entered the distillation process with.
287:860	It happens that in English, whenever a, derived nominal of an act is the right element in a compound, then the left element is almbst always an occupant of one of the case slots of the verb.
288:860	In order to recreate the underlying relative clause structure, it is only necessary for the hearer to properly choose the case for the nominal steam.
289:860	A great deal of lexical information can be brought to bear on this question; for example, steam is not a liquid, is water vapor and thus cannot substance or the end product of a distillation process.
290:860	Steam might be the Cause of the act of distillation except that there do not seem to be any compounds in English which have distillation as the right element and a Cause as the left element.
291:860	Thus the hearer can assign steam to the Means case with some assurance.
292:860	In another example, shrimp boat, the hearer can ascertain by lexical relations involving the word boat, that boats are characteristically used to catch marine life.
293:860	One choice eor the main verb in a synonymous relative clause is catch, which will have boat as an element of the Means case.
294:860	The Cause for catch is commonly a person or perhaps a sophisticated machine designed to oatch things (i.e. a trap).
295:860	The Object-is characteristically an animal.
296:860	There is a strong characteristic relation between the animal being caught and the means used to catch it, for example mink is trapped, calves are roped, birds are netted, and fish are caught with a boat.
297:860	This relation exists as a rule in the lbxfcon of both the speaker and the hearer and it enables the speaker to produce the nominal compound and the hearer to understand it.
298:860	Furthermore, shrimp boat is one member of a class of closely related nominal compounds whioh includes lobster boat, whale boat tuna boat and many others.
299:860	It would be most -' interesting if a single rule could be formulated which would generate all of these cqpounds.
300:860	A lobster boat is a boat which is used to catch lobster, a tuna boat is a boat which is used to catch tuna, and so forth.
301:860	All of these examples are identical except for the particular marine animal being caught.
302:860	The logical next step is the creation of a rule which generalizes the individual marine anfmals to the cbmmon category of marine animal.
303:860	This rule fill state that a marine animal boat is a boat which is used to catch marine animals, In making this generalization, I have given the rule the power to help interpret novel compounds and to generate them.
304:860	With this power comes a difficulty, Which is constraining the rule so that it does not generate bad compounds or produce incorrect interpretations.
305:860	The key to this constra'int lies in what I will term the characteristic or habitual aspect of nominal compomds.
306:860	In the case of the boat compounds, a boat will only be a shrimp boat if it is characteristically, usually, habitually or invarfably used to catch shrimp.
307:860	So the operation of a compounding rule is enabled only if a characteristic aspect is associated with the verb; in English, this is usually indicated by an adverb or an adverbial phrase.
308:860	If the speaker is willing to assert that a boat is characteristically used to catch turtles, then the nominal compound turtle boat may be used.
309:860	The hearer sill use the general rule to place turtle and boat in the proper case slots, and because a compound was used by the speaker, the hearer will infer Qhat the boat is one which is characteristically used to catch turtles, There are other problems which arise with the generalization of rules; for example, compounding never produces a compound in which the lert element is a proper noun, unless the proper noun ie the name of a process (e.g. Harkov process) or is a Source, Performer, or Goal of an act of giving.
310:860	It also seems to be true that compounds are not generally formed when a lexical item is several levels below the general term which appears in the rule (e.g. repaimidget) or when a cross-classificatory term is used (e.g. automobile Indian as an Indian who repairs automobiles).
311:860	With all of the preceding discussion in mind, I would now like to turn to the model of nominal compounding which I have presently implemented and running.
312:860	The Computer Model The computer model of compounding accepts relative clause structures as input and produces nominal compound structures as output when the input is appropriate.
313:860	It is written in a language with many parentheses the language was chosen for its program development facilities, i.e. built-in editor, rather than for its interpretive capabilities.
314:860	The program which produces nominal compounds is a pattern matching interpreter; it applies a rule of compound formation by matching one side of the rule with the input structure, and if certain criteria are satisfied by the match, items from the input structure are bound into the rule, transferred to the other side of the rule, and a copy is then maae af the other side of the rule.
315:860	The result is a nominal compound structure.
316:860	The model has two components: a rule interpreter and a lexicon of rules for compounding.
317:860	There is nothing tricky about rule application.
318:860	Consider the nominal compound flower market and its associated relative clause paraphrase market where flowers are characteristically sold.
319:860	These phrases have in my system the underlying structures shown in Figure 1.
320:860	The notation in square braces means that the verb sell has the characteristic aspect in this instance.
321:860	market I RELCL sell [+char] m / \Jmarket flowers Figure 1.
322:860	market flower These two structures can be made into a rule by linking them together.
323:860	Whenever a relative clause structure identical to that in Figure 1 is received, the rule applies and a copy is created of the nominal compound flower market.
324:860	The matching procedure is a relatively straightforward, top down, recursive process which has backtracking capability in the event that a structure or case occurs more than once at any given level of the structure.
325:860	There are two problems which arise; however: if e rule is generalized to account for compounds other than flower market, then the lexical items in the rule will behave as variables and some provisions must be made for binding of values to these variables; also, the rule interpreter must have some heuristics for selecting appropriate rules if the time required to produce a compound is not to increase exponentially with the size of the lexicon.
326:860	The present version of the model only partly solves the binding problem.
327:860	Consider the rule given in Figure 2 which is a generalization of that given in Figure 1.
328:860	market I sell [+char] LOC market goods market I COW goods Figure 2.
329:860	If this rule is to apply to the relative clause structure glven in Figure 1 and generate the compound flower market, then the rule interpreter must recognize that the relative clause in Figure 1 is an instance of that given in Figure 2.
330:860	The matching procedure does this by determining that the reference set of the nominal flowers is a subset of the reference set of the nominal goods.
331:860	In addition, the nominal flowers must be carried across to the other side of the rule and substituted there for goods before the other side of the rule is copied.
332:860	Thus market and goods must be bound across the rule so that whatever lexical item matches either of these nominals becomes the value associated with these nominals on the other side of the rule.
333:860	In the initial version of the model, this binding was established explicitly when the rule was entered into the lexicon, but this seemed unsatisfactorily ad hoc.
334:860	In a subsequent version, -the identity of the lexical items on both sides of the rule was the relation used to establish binding relationships.
335:860	Consider, however, the structure shown in F:Lgure 3.
336:860	person I RELCL steal [+char ] PERF/ \ OBJ person valuables Figure 3 thief Here person should be bound to thief but the previous technique is not able to establish this binding.
337:860	The reason that we know that person and thief should be bound is because we know that a thief is a person who steals characteristically.
338:860	In the most recent version pf the model, this information is used to find the binding relationships when the rule of identity doe$ not work.
339:860	The lexicon is searched for a rule which can be used to establish this bindiag.
340:860	The rule which is used in the example shown in Figure 3 is displayed below in Figure 4.
341:860	person I RELCL thief steal [+char ] I PERF person Figure 4 From the structures given in Figure 4, one can see that person shduld be bound to thief because the rule states that the reference set of thief is the same as the reference set of person as restricted by the relative clause.
342:860	The technique of using lexical rules to establish bindings works in virtually every instance, but it has the defect of requiring that the information that a thief is a person who steals things be represented in the lexicon twice at least.
343:860	A new model is under construction which attempts to reduce this redundancy by allowing the rules to have multiple left and right parts.
344:860	The problem of selecting appr~priate rules is rather easier to solve.
345:860	In most compounds in English, there is a characteristic association between the right element of the nominal compound and the main verb of the associated relative clause paraphrase.
346:860	These two elements which occur on opposite sides of the compounding rule supply a great deal of information about the possibilities for application of the rule.
347:860	So, in the model, each rule in the lexicon is indexed by the main verb of the relative clause and by the right element of the nominal compaund.
348:860	This index actually contains some environmental information as well; for the clause verb, this environmental information is the case frame of the verb and the fact that it is the main verb of the relative clause -for the compound nominal, the environmental information is just the fact that the nominal is the rightmost one in a nominal compound.
349:860	The basic model has been tested with a set of several hundred nominal compounds and is very successful in coping with a wide vqriety of compound types.
350:860	The productivity of the rules varies greatly; some rules may produce hundreds of compounds while other rules may only result in one or two compounds.
351:860	Frozen forms such as keel boat are handled by a rule which generates only one compound; there is a rule for each frozen form.
352:860	The rule structures contain exclusion lists associated with each lexical item in the rule, and these exclusion lists prevent the rule from operating whenever a lexical item matches one Of the items on an exclhsion list if the items occur at corresponding locations in the structures.
353:860	The model is quite quick in operation; on a high speed dibplay console, it will generally produce compounds much faster, than a person sitting at the console can conveniently read them.
354:860	vi~ is mainly due to the rule selection heuristic, but the match procedure has been carefully optimized as well.
355:860	Conclusions The model program is an excellent demonstration of the appropriateness of the basic theory; moreover, the rules themselves can be generalized to deal with syntactic processes, so there is no discontinuity in the grammar model between the lexical processes and the syntactic processes.
356:860	It seems clear that the rules could also be used to represent other lexical processes in language and this is currently being pursugd.
357:860	There is no reason why the rules could not be used for recognition as well as for the production of nominal compounds.
358:860	The bindings are not one-way, and the matching procedure will work equally well for compound structures.
359:860	The reasons why the computer model is a production model are: (1) that the computer model assumes the semantic correctness of the input relative clause structures, and (2) that compounds are often ambiguous and may be paraphrased by two or more relative clauses, while the converse of this is almost never true.
360:860	A recognition model would have to generate underlying relative clause structures for each ambiguity and a semantic component would have to~screen the relative clauses for semantic errors.
361:860	I hope that the reader has noticed the avoidance of rule procedures in this model.
362:860	When I began working on the design of the computer programs, I had in mind the creation of a model which once implemented in LISP could be extended merely by adding new ~ules without having to construct any additional LISP programs.
363:860	I ultimately wanted to have a model which could lllearntl new rules by systematic generalization and restriction of existing rules.
364:860	I feel that this would be relatively easy with rule structures and extremely difficult with rule procedures written in a programming language.
365:860	Furthermore, I subscribe to Karl Popper's icfeas of scientific endeavour, and rule structures appealed because it would be more difficult to bury flaws or ill understood aspects of compounding and rule processes in structures than in procedures where the computational power of the programming language permits and even encourages -ad hoc solutions to be found to problems.
366:860	Acknowledgements I would like to here acknowledge the suggestions made by Robert F. Simmons, Carlota Smith, Mary Boss T. Rhyne, Uurent Siklossy, and Stanley Peters which have helped tsprbve my understanding of nominal compounding.
367:860	American Journal of Computational Linguistics Microfiche 33 : 45 Computer Science Department Indiana University Bloonlington 47401 ABSTRACT Generation of English surface strings from a semantic network is viewed as the creation of a linear surface string that describes a node of the semantic network.
368:860	The form of the surface string is controlled by a recursive augmented transition network grammar, which is capable of examining the form and content of the semantic network connected to the semantic node being described.
369:860	A single node of the grammar network may result in different forms of surface strings depending on the semantic node it is given, and a single semantic node may be described by different surface strings depending on the grammar node it is given to.
370:860	Since generation from a semantPc network rather than from disconnected phrase markers,, the surface string may be generated directly, left to right.
371:860	Introduction In this paper, we discuss the approach being taken in the Engllsh generation subsystem of a natural language understanding system presently under development at Indiana University.
372:860	The core of the understander is a semantic network processing system, SNePS (Shapiro, 1975), which is a descendant of the MENTAL semantio subsystem (Shapiro, 1971a, 1971b) of the MIND system (Kay, 1973).
373:860	The role of the generator 13 to describe, in English, any oP the nodes in the sernantjc network, all of which represent concepts of the understanding aystem.
374:860	46 and other computations are ~equired in the process of pasting these trees tog ther in appropriate places until a 'single phrase marker Is attained which will lead to the surface string.
375:860	Since we are generating from a semantic network, aL1 the pasting together is already done.
376:860	Grabbing the network by the node of interest and letting the network dangl-e from it gives a structure mich may be searched apppogriately in order to generate the surface strfng directly in left to right fashion.
377:860	Our system bears a superficial resemblance to that described fn Simmons and Slocum, 1972 and in Simmons, 1973.
378:860	That system, however, stores surface information such as tense and voice in its semantic rietwork and its ATN takes as input a linear list containing the semantic node and a generation pattern consisting of a "series of constraints on the moclalltyfl (Simmons et al. , 1973, p. 92 The generator d.escribed in Schank et al. , 1973, translates from a "conceptual structuref1 into a network of the form of Simmons ' network which is then given to a version of Simmons generation program.
379:860	The two stages use different mechanisms.
380:860	Our system amounts to a unificatio of these two stages.
381:860	The generator, as described in this pager, as well as SNePS, a parser and an inference mechanism have been written in LISP 1.6 and are running Interactively on a DEC system-10 on the Indiana University Computing Network.
382:860	Representation in the Semantic_Network Conceptual information derived from parsed sentences or deduced from other information (or input directly via the SNePS user's language) is stored in a semantic network.
383:860	The nodes in the network represent concepts which may be discussed and reasoned abaat.
384:860	The edges represent semantic but non-conceptual binarx relations between nodes.
385:860	There are also auxiliary nodes which SNePS can use or which the user can use as SNePS variables.
386:860	(For a more complete diecussion of SNePS and the network nee Zhapiro, 1975).
387:860	The semantic network representation being used does not in47 olude information considered .t.6 be features of' the surface string such as tense, voice or main vs. relative clause.
388:860	Ihstead of tense, temporal information is stored PeIative to a growing time line in a manner similar to that of Bruce, 1972.
389:860	From this information a tense can be generated for an output sentence, but it may be a different tense than that of the original input sentence if time has progressed iri %he 5nterim.
390:860	The voice of a generated sentence is usually determined by the top level call to the generator function.
391:860	However, sometimes it is determined by the generator grammar. For example, when generating a relative clause, voice is determined by whether the nodn being modified-is the agent or object of the action described by the relative clause.
392:860	The Main clawe of a generated sentence depends on which semantic node is given to the generator in the top level call.
393:860	Other nodes connected to it may result in relative clauses being generated.
394:860	These roles may be reversed in other top level calls to the generator.
395:860	The generator is driven by two sets of data: the semantic network and a grammar in the form of a recursive augmented transition network (ATN) similar to that of Woods, 1973.
396:860	The edges on our ATN are somewhat different from those of Woods since our view is that the generator is a tranducer from a network into a linear string, whereas a parser Is a transducer from a linear string into a tree or network.
397:860	The changes this entails are discussed below.
398:860	During any point in generation, the generator is working on some particular semantic node.
399:860	Functions on the edges of the ATN can examine the network connecteb to this node and fail or succeed accordingly.
400:860	In this way, nodes of the ATN can "decide" what surface form is most appropriate for describing a semantic node, while different ATN nodes may generate different surface foI?ms to describe the same semantic node.
401:860	A common assumption among linguists is that generation begin3 with a set of disconnected deep phrase markers.
402:860	Trnnuf o~-matlon~ LEX Fizure 1: Semantic Network RepresentatLon for "Charlie believes that a dog kissed sweet young Lucy," "Charlie is a person," and "Lucy is a pperson.
403:860	,.Af=rmation considered to be features of surface strings are not stored in the semantic network, but are used by the parser in constructing the network rrom the input sentence and by the generator for generating a surface string from the network.
404:860	For example, tense is mapped into and from temporal relations between a node representing that some action has, is, or will occur and a growing time line.
405:860	Restrictive relative clauses are used by the parser to Identify a node being discussed, while non-restrictive relative clauses may result in new information being added to the network.
406:860	The example used in this paper is designed to illustrate the generation issues being discussed.
407:860	Although it also illustrates our general approach to representational issues, some details will *(SNEG MOOLb) (CHARLIE IS BELIEVING THAT A DOG KISSED SWEET YOUNG LUCY), * (SNEG M0023) (A DOG KISSED SWEET YOUNG LUCY) *(SNEG M0007) (CHARLIE WHO IS BELIEVING THAT A DOG KISSED SWEET YOUNG LUCY) "(SNEG i4OOd;j (CHARLIE IS A PERSON WXO IS BELIEVING THAT A DOG KISSED SWEET YOUNG LUCY) * (SNEG M0006) (CHRRLIE~WHO IS BELIEVING THAT A DOG KISSED SWEET YOUNG LUCY IS A PERSON) (SNEG M0008) (THE BELJEVING THAT A DOG KISSED SWEET YOUNG LUCY BY CHARLIE) * (SNEG M0011) (A DOG WHICH KISSED SWEET YOUNG LUCY) *(SNEG ~0OlOj (THAT WHICH KISSED SWEET YOYNG LUCY IS A DOG) * (SNEG M0012) (THE KISSING OF SWEET YOUNG LUCY BY k DOG) @(SNEG M0020) (SWEET YOUNG LUCY WHO WAS KISSED BY A DOG) *(SNEG M0014) (LUCY IS A SWEET YOUNG PERSON WHO NAS KISSED BY A DOG) *(SWG M0015) (SWEET YOUNG LUCY HH3 WAS KISSED BY A DOG IS A PERSON) *(SNEG M0017) (SWEET LUCY WHO WAS KLSSED BY A DOG IS YOUNG) *(SNEG M0019) (YOUNG LUCY WHO WAS KISSED BY A DOG IS SWEET) Figure 2: Results of calls to the generator with nodes from Figure I* User input is on lines beginning with *.
408:860	certainly change as work progresses.
409:860	Figure 1 shows the semantic network representation for the information in the sefitencesb, "Charlie believes that a dog kissed sweet young Lucy," "Charlie is a person," and "Lucy is a person".
410:860	Converse edges are not shown, but in all cases the label of a converse edge is the label of the forward edge with '*' appended exoept for BEFORE, whose converse edge is labelled AFTER.
411:860	LEX pointers point to nodes containing ;lexical entries.
412:860	STIME points to the starting time of an action and ETIME to its ending time.
413:860	Nodes representing instants of time are related to each other by the BEFORE/AFTER edges.
414:860	The auxiliary node NOW has a :VAL pointer to the current instant of time.
415:860	Figure 2 shows the generator's output for many of the nadcs of Figure 1.
416:860	Figure 3 shows the Lcxicon uncd in the example.
417:860	(BELIEVE((CTGY.V)(I~BELIEVE) (PRES.BELIEVES)(PAST.BELIEVED)(PASTP.BELIEVED)(PRESP.BELIE~JING))) (CHARLIE ( (CTGY . NPR) (PI . CHARLIE) 1') (DO~'((CTQY.N)(SING.DOG)(PLUR.DOGS))) (K~SS((CTGY .v) CINF.KISS) ~PRES.KISSES)~PP.ST.KISSED~~PASTP.KISSED)(PRESP.KISSING~~) (LUCY-((CTGY.NPR)(PI.LUCY))) (PERSON((CTGY.N!(sING.PERSON)(PLUR.PEOPLE)~~ (SWEET(~CTGY.ADJ)(PI.SUEET) 1) (YO~NG((CTGY.ADJ)(PI.YOUNG))) Figure 3: The lexicon used in the example of Figures 1 and 2.
418:860	Generation as Parsing Normal pa~sing involves taking input from a linear string and producing a tree or network structure as output.
419:860	Viewing this in terns of an ATN grammar as described in Woods, 1973, there is a well-defined next input function which simply places successive word6 into the** register.
420:860	The output function, however, is more complicated, uslng BUILDQ to build pieces of trees, or, as in our parser, a BUILD function to build pieces of network.
421:860	If we now consider generating in these terms, we see that there is no simple next input function.
422:860	The generator will focus on some semantic node for a while, recursively shifting its attention to adjacent nodes and back.
423:860	Since there are several adjacent nodes, connected by variously labelled edges, the grammar author must specify which edge to follow when the generator is to move to another semantic node.
424:860	For these reasons, the same focal semantic node is used when traversing edges of the grammar network and a new semantic node isspecified by gl,ving a path from the current semantic node when pushing to a new grammar node.
425:860	The register SNODE is usedto hold the current semantic node.
426:860	The output function of generation is straightforward, simply being concatenation onto a growing string.
427:860	Since the output string is analogous to the parser's Input string, we store it in the reggarc : := (TEST test [action]* (TO gnode) ) (JUMP [action]*(TO gnode)) (mM wform (wqrd*) test [action]*(TO gnode)) (NOTMEM wform (word#) test [action]*(T~ gnode)) (TRANSR ([regname] regname regname) test [action]* (TO gnode) ) (GEN gnode sform [action]*regnarne [action]*(TO gnode)) sform ::= wform SNODE wform : := (CONCAT f~~m form*) (GETF sarc [aform]) ( GETR re gname ) (LEXLOOK lf e at [ sf orm] ) sexp form -: := wform sform act ion : := (SETR regname f om (ADDTO regname form* ) (ADDON regname f om* ) sexp test :: (MEMS form form) (PAT23 sform sarc* sform) form sexp gnode : := <any LISP atom which represents a grammar node> word : := <any LISP atom> regname ::= <any non-numeric LISP atom used as a register name> sarc ::= <any LISP atom used as a semantic arc label> lfeat : := <any LISP atom used as a lexical feature> sexp : := <any LISP s-expression> Figure 4: Syntax of edged of generator ATN grammars ister *.
428:860	When a pop occurs, it is always the current value of * that is retu~ned.
429:860	Figure 4 shows the syntax of the generator ATN grammar.
430:860	Object language symbols are ), (, and elements in capital letters.
431:860	Metalanguage symbols are in lower case, Square brackets enclose optional elements.
432:860	Elements followed by * may be repeated one or more tjmes.
433:860	Angle brackets enclose informal English descr~ptions.
434:860	Semantics of Eage Functions In this section, the semantics of the grammar arcs, forms and tests ape preaehted and compared to those of Woods1 ATNs.?
435:860	The --t All comparlsona arc with Woodo, 1973.
436:860	NCL ') JUMP(SETR * @(//// NO GRAMMAR NODE FOUND)) MEND ) Figure 5: The default entry into the grammar netwo~k. essential differences are those required by the differences between generating and parsfng as discussed in the previous section.
437:860	(TEST test [action]*(TO gnode)) If the test is successful (evaluates to non-NIL), the actions are performed and generation continues at gnode.
438:860	If the test fails, this edge is not taken.
439:860	TESTis the same as WoodsT TST, while TEST(GETF sarc) is analogous to Woods' CAT.
440:860	(JUMP action]*(^^ gnode)) Equivalent to (TEST T [actlon]*(TO gnode)).
441:860	JUMP is simllar In use to Woods JUMP, but the difference from TEST T disappears since no edge r'consumesl' anything.
442:860	(MEM wform (word*) test [action]* (TO gnode) ) If the value of wform has a non-null intersection with the list of words, the test is performed.
443:860	If the test is also successful the actions are performed and generation continues at gnode.
444:860	if either the Intersection is null or the test fails, the edge MEM(GETR VC)(PASS)T G-EN NCLNP(~ETF OBJECT) (ADDTO DONE SNODEI* ( SREG PRED) OEN NCLNP(GETF AGENT)(ADDTO DONE SNODE)* Figure 6: Generation of subject of sub ject-verb-ob ject sentence.
445:860	is not taken.
446:860	This is similar in form to Woods1 MEM, but mainly used for testing registers.
447:860	(NOTMEM wform (word*) test [action]*(TO gnode)) This is exactly like MEM except the intersection must be null.
448:860	(TRANSR ([regnamel] regname2 regname3) test [action]* gnode) ) If regnamel is present, the contents of regname2 are added on the end of regnamelo If regname is empty, the edge is not 3 taken.
449:860	Otherwise, the first element in regname is removed and 3 placed in regname2 arid the test is performed.
450:860	If the test fails, the edge is not taken, but if it succeeds, the actions are performed and generation continues at gnode.
451:860	TRANSR is used to iterate through several nodes all in the same semantic relation with the main semantic node.
452:860	(GEN gnodel sform [action]*regname [action]*(TO gnode*)) The first set of actions are performed and the generation is called recursively with the semantic node that is the value of sform and at the grammar node gnodel.
453:860	If this generation is successful (returns non-NIL), the result is placed in the register regname, the second set of actions are performed and generation continues at gnode*.
454:860	If the generation fails, the edge is not taken.
455:860	This Is the same as Woods1 PUSH but requires a semantic node to be npecified and allows any register to be used to hold the result.
456:860	Instead of having a POP edge, a return automatically occurs when (ADDON * @WILL @HAVE) (ADDON * QWQULD) UI@(ADDON * @(///CANNOT COMPUTE TENSE)) TEST( MENS (GETR REF) (* @NOW) ) (ADDON PIS') \a MEM(GETR VC) (PASS)T(ADDON * @BEEN) CVPF MEM(GETR VC)(PASS)T(ADDON * @BE) (tp~ JUMP @FAST~ JUMP(ADD0N * (LExLOOK PASTP(GETF VERB))) JuMP(ADDON * (LEXLOOK INF(GETF VERB))) CTIINF ) Figure 7: Tense generation network.
457:860	transfer is made to the node END.
458:860	At that point, the contents of the register named * are returned.
459:860	(CONCAT form form*) The forms are evaluated and concatenated in the order given.
460:860	Performs a role analogous to that of Woodst BUILDQ.
461:860	(GETF sarc [sform]) Returns a list of all semantic nodes at the end of the semantic arcs labelled sarc from the aemantic node whl ch is the value Figure 8: The tenses of "breakn which the network of Figure 7 can generate.
462:860	of sfomn.
463:860	If sform is missing, SNODE is assumed.
464:860	Returns NIL if I there are no such semantic nodes.
465:860	It is similar in the semantic Tense past futil~e present prog~essive past progressive future progr6ssive past In future future fq past Active Passive domain to Woods' GETF in the lexical domatn.
466:860	broke will break is break-lng was breaking will be breaking' will have broken would break (GETR regname) Returns the contents of register regname.
467:860	It is essentially the same as Woodst GETR.
468:860	was broken will be broken is being broken was being broken will be being broken will have been broken, wquld be broken .
469:860	(LEXLOOK lfeat [sform]) Returns the value of the lexical feature, lfeat, of the lexical entry associated with the semantic node which is the value of sform.
470:860	If sform is missing, SNODE is assumed.
471:860	If no lexical entry is associated with the semantic node, NIL is returned.
472:860	LEXLOOK is sirnilar to Woods1 GETR and as also in the lexical domain.
473:860	(SETR regnarne form) The value of form is placed in the register regname.
474:860	It is the same as Woods1 SETR.
475:860	(ADDTO Pegname form*) Equivalent to (SETR regname (CONCAT (GETR regname) form*)).
476:860	Equivalent to (SETR regname (CONCAT form* (GETR regname))).
477:860	(MEW form form) Returns T If the values of the two forms have a non-null intersection, NIL otherwise.
478:860	TEST( GETF OBJECT ~PREDOBJ) UMP cpmDAm GEN KLNP (GETF AGENT) REG (ADDON * (GETR 'REG) ) CpRGDOB3 GEN NCLNP (GETF OBJECT) REG (ADDON * (GETR REG ) ) I Figure 9: Generating the surface object.
479:860	(PATH sfoml sarc* sformp) Returns T if a path descr3bedl By the sequence of semantic arcs exists between the value of sfoml and sformp.
480:860	If %he sequence is sarcl sarc2  sarc,, the path described is the same as that indicated by sarcl* sarc2*  sarcni.
481:860	If no such path exists, NIL is returned.
482:860	(Remember, * means repeat one or more times.
483:860	) Discussion of an Example Orammar Network, The top level generator function, SNEG, Ls given as arguments a semantic node and, optionally, a grammar node.
484:860	If the grammar node is not given, generation begins at the node G1 which should be a small disccirnination net to choose the preferred description for the given semantic node, This part of the example grammar is shown in Figure 5.
485:860	Jn it we see that the preferred description for any semantic node is a sentence.
486:860	If no sentence can be formed a noun phrase will be tried.
487:860	Those are the only presently available options.
488:860	Semadtic nodes with an outgoing VERB edge can be described by a normal SUBJECT-VERB-OBJECT sentence.
489:860	(For this example, we have not used additional cases).
490:860	First the subject is generated, Figure 10: Generating the three "non-regular" sentences.
491:860	which depends on whether the sentence is to be in active or passive voice.
492:860	Alternatively, the choice could be expressed in terms of whether the agent or object is to be the topic as suggested by Kay, 1975.
493:860	Figure 6 shows Lhe network that generates the subject.
494:860	The register DONE halds semantic noaes for which sentences are being generated for later checking too prevent infinite recursion.
495:860	WPthout it, node MOO23 of Figure I would be described as, "A dog which kissed young sweet Lucy who was kissed by a dog which kissed.
496:860	" The initla1 part of the PRED network is concerned with generating the tense.
497:860	This depends on the BEFORE/AFTER path between the starting and/or endinb time of the action and the current value of NOW, which,is given by the form (# @NOW).
498:860	Figure 7 shows the tense generation network.
499:860	Figure 8 shows the tenses this network is able to generate.
500:860	After the verb group is generated, the surface object is generated by describing either the semmtic agent or object.
501:860	Figure 9 skuws this part of the network The other three kinds of sentences are lor describing nodes representing: (1) that something has a particular adjective attribuable to it, (2) that something has a name, (3) that something is a member of some class.
502:860	The networks for these are shpwn in Figure 10.
503:860	Again, the DONE register is used to prevent such sentences as "Sweet young Lucy Is sweet," "Charlie is Charlie".
504:860	and "A dog is a dog".
505:860	GEN 3 (GETF OBJECT(GETF VERB*))REG(ADDON * @THAT(GETR REG)) cICL>GEN SREG SNODE *(ADDTO * @THAT) Figure 11: Generating norninalized verbs and sentences.
506:860	Pugure 5 showed three basic kinds of noun phrases that can be generated: the noun clause or nominalized sentence, such as "that a dof~ kissed sweet young Lucyt'; the nominalized verb, such as "the kisslng of sweet young Lucy by a dogn; the regular noun phrase.
507:860	The first two of these are generated by the network shown in Figure 11.
508:860	Bere DONE is ased to prevent, for example, "the kissing of sweet young Lucy who was kiesed by a dog by a dog".
509:860	The regular noun phrase network begins wlth another descrimination net which has the following priorities: use a name of the object; use a class the obJect belongs to; use something else known about *he obJect.
510:860	A lower priority description will be used if all higher priority descriptions are already in DONE.
511:860	Figure 12 shows the beglnnlrrg of the noun phrase network.
512:860	Adjectives are added before the mame or before tkre class name and a relative clause is added after.
513:860	GEN ADJS SNODE *(ADDTO * @A) Cm&uMp (sETR * BA) NEM~ (LEXLOOK SING(GETE CLASS(GETF MEMBER*)))) Figure 12: The beginning of the noun phrase network.
514:860	Figure 13 shows the adjective string generator and Figure 14 shows the relative clause generator.
515:860	Notice the use of the TRANSR edges for iterating.
516:860	At this time, we have no theory for determining the number or which adjectives and relative clauses to generate, so arbitrarily we generate all adjectives not already on DONE but only one relative clause.
517:860	We have not yet implemented any ordering of adjectives.
518:860	It is merely fortuitous that "sweet young Lucyt is eenerated rather than "young sweet Lucyft.
519:860	The network is written so that a relative clause for which the noun is the deep agent is preferred over one in which the noun is the deep object.
520:860	Notice that this choice determines the voice of the embedded clause.
521:860	The fomn (STRIP(FIND MEMBER (1.
522:860	SNODE) CLASS (FIND LEX PERSON))) is a call to a SNePS function that determines if the object is known to be a person, in ~hich case "WHO" is used rather than "'WHICHft.
523:860	This determination is made by referring to the semantic network rather than by including a HUMAN feature on the lexical entries for LUCY and CHARLIE.
524:860	nDJs7JUMP(SETR ADJS(GETF WEICH*)) Figure 13,: The network for generating a string of adjectives.
525:860	Notice that any information about the object being described by a noun phrase may be used to construct a relatfve clause even if that Lnfomnation derived from some main clause.
526:860	Also, while the generator is examining a semantic node all the information about that node is reachable from it and may be used directly.
527:860	There is no need to examine disjoint deep phrase markers to discover where they can be attached to each other so that a complex sentence can be derived.
528:860	Future Work Additional work needs ts be done in developing the style of generation described in this paper.
529:860	Experience with larger and richer networks will lead to the following issues: describing a node by a pronoun when that node has been described earlier in the string; regulating verbosity and complexity, possibly by the use of resource bound8 simulating the limitations of short term memory* keeping sub-, ordinate clauses and descriptions to the point of the conversation posslbly by the use of a TO-DO register holding thenodes that are to be included in the string.
530:860	In thla paper, only indefinite descriptions were generated.
531:860	We are working on a routine that wlll Identify the proper subnet of the semantic network to justify a definite description.
532:860	This must be such that it uniquely identiflee the node being described.
533:860	(SETR VC @ACT) JUMP \JUMP F JUMP TEST(STRI?
534:860	!*I!JD :.lEMEEFl(f SI\l; DE)CLASS (YT?ID EX PERSON) ) ) Figure 14: The relative clause generator.
535:860	Acknowledgements The author is indebted to John Lowrance, who lfiplemnted the generator, Stan Kwasny, who implemented the parser, Bob Bechtel, who worked but the temporal representation, Nich Vitulli and Nick Eastridge, who implemented versions of SNePS, and Jim McKew for general software ?upport.
536:860	Computer service was provided by the IUPUI Computing Facilities.
537:860	Typing and graphics were d~ns by Christopher Charles.
538:860	American Journal of Computational Linguistics ~icrofiche 33 : 63 Artificial Intelligence Center Stanford Research Institute Men10 Park, California 94025 ABSTRACT 8atur.l languag8 output can b* generatrd fram remantic nets by ptoc*rsing ternplats8 asroeiated with concept# in the netr A #qt at verb teaplater is belng derived from a Study of the surfrc9 syntax of ran@ 3000 Englirh Verb88 fhe actlve forms of the verb$ have been tZ~rriflsd rtcorb&n$ to subjectr objeetCm1~ and compl~aent(r1) there Syntactic Patterns, augmented with case nams, ara used as a grammat to Cantrol th4 generation Ot text, Thlr text in turn is pasrad through a speech syntheri8 program and output by 4 VOTRAX speech rynth4o&zax, ThLr analyrar rhould ultimately benefit systems attsaptlng to understand Erl~li~h input by praviding surface etructurs to deep cul itrueturd maps using the rare trmpirter rr emp1Qytd by the generator.
539:860	Thfr reararch w4r rupportrd by the Detenra Advaneed Reuearch Proleeta Agancy af th* Dcprrtmmnt @t D~fenre and manit6rrd by the U, 8.
540:860	&rmY Rs#~rrch Offica under Cantract No, DAHC04~75-C-0006.
541:860	TNTRQDUCTXON It computer6 rtr to canmonicrtr eftretlvcly with people, they nust spark, or at irart write, the urcr*r nrtur61 Language, The bulk ot the work in co~pytatlonrl Llnguilticr has been drvotrd to computer undcrstandlnq of natutal language input# but relatively littie rtfo<rt ha8 been e%pend@d in developin$ natural Language output.
542:860	Mort Enplirh output systems hrve been along the line of nfill In the blanku with Perhap& soma semantic cenrtrrint6 imporad!
543:860	thera have bean few attempts at language generation from what one could crLl wsamrntic netR structures [8g@aon8 and 8locua, 1972; Sloeuar 1973; Goldman, 19741, PeFh&p@ generation is canrldered a much aasisr problem, The ruccrrr of understanding efforts ir generally bclisvtd to drprnd on rome warkrble theory of Rdlrcaurrr 6rganlzatlonc which would rccbunt gar affect8 of context and would ahow how iniphoric ~~pren8l~ns (pronoun# and noun phrases) are resolved and how #rnt*nCeS rro order84 tn the bUtPUtr As tt hrppdnot there m@chanlr@r are pr~L#*lY those that r wrr8ponre g~nerrtor~ must Incorporate it it 1s to appear Lntrlligent.
544:860	Tha lrtudy of qrnrtrtion rill play an important rots in rolving the problem of undrrttandino if it ern draonrtrrte & mapping t~om derp ramantic rtrUctura6 to rutface rtrtngr, bet ua briefly outlina gome relevant preccrrrs in the #peach understanding system bring drYcL0~1d by 8RI and SDC (Walker at a1.t 1915~ and Rite.
545:860	, 1915).
546:860	The urar inltlatrs rrrrton by rrtrblirhtng conmunLc~tion with the ryrtrmf all subarqurnt dialog (input and output) is manlter@d by a Wdircaurae madula8 (Dautacb, 11975) to maintain rn accurate ca~ntrerskition41 context, An exeeuttvc eserdinata# variaur Knarladge sauxcos 4cWrti~r prosodic, syntactic, r~mrntie, pzagmrtlc, and di~eeurrc to Bunderotrnbw cuccersivt utt@rane@rr The analyzed UttaFlnge iQ then Pas8rB la ZRa "resPandarw PBItll another eamponrnt of the dileaurtc module.
547:860	The responder may call the pucrtlonransrcrrr it the lnput ir a question!
548:860	it may call 4 data bead update Program It ths input is r otatcmant of fact1 or it may drcldr an romc other rppropriatc reply.
549:860	Tha content ab the r@DpDnD@ Is prrora to the gsnaratar~ perhapa #i$h Some tndicrtian of hsa it is to be tarmulat@ds TRa reply may bs a rtsrcatypsd rsrpsnss (wy@8Bp "noM, "1 @e@C)I noun phrars (nodel, r o@nt+nes (verb node)r Orr e~~ntu111y~ a paragraph, The qen~PaQ~r outputs stettotyped raspanreg Pmmed$atslyt if!
550:860	the rcrpanrr ir nor@ complicated [a "nounB nab@, *verbR nads, ar rvcnturlly r nrtwork), r mbrr detallcd proptam ir ragulrcd.
551:860	This program nbLl dctrrmina exactly how the responge is ts bs bormulrtad ma &g NBI 61 OH teqUIRCQ 8f St! it may bb ~d~~lr6d to Chooag Verb@ and noun@ with which to @Xgs@@@ the deep ClSc nae ctructurrr, rr wall r8 a ryntaetlc from.
552:860	for the genaration.
553:860	The g~narator P~O~UCII tha rrrponrr in wfrxtw form1 thir in turn ir p&gsad to a B~OIC~ tynthrllr program tar trrnrtarmatlon and cutput by r comn@rCial VOTRAX rprach rynthe~izar.
554:860	Curr~ntly no IsntQner intenstion sr rtr~as con%sWing i~ being p@rt~rmed, 8tnea the major Bntmsost at this papsr it in "textH ganaratlane further reference to the synfhcrlg rtrp rill be meden CQNSTRAXNTS Or?
555:860	RESPONDING There are several censLdcr8tianr involved in responding appropriateby to an utterance.
556:860	First, therc are *conver:ctlanal Po~tUl&te#" (Gordon and Lakoff, 19751 8harsd by the u6ers a e Lanuurprt there pottu2ater serve to canstrain the content and form of ca~~unicationr from the speaker to the heerare For instance, the rperker should not tall the hearer re~sthing the hearer all@&dy knows, lcrt he be bore61 yet the speaker cannot tell the hcarer samething the hearer knows ab~olutely nathlnq about, or the hearer rlll not comprehend.
557:860	The ggenkee chould relate knt ncwa in his marsaga to the prtor knowledgs of the hearer; this requires the rperkcr te have ti model of!
558:860	the haarcf There heuristic6 murt operate in canjunctbon w~th c w~~c~~nc~ producerw to constrain what may be output by a q~entcncen generatore We are only beginning to understanG how to lncorparrtt there partuiater in a languagc groce6~bng kygtcm, Then there Is the matter of con8tructlng the batic sentence Normal English ryntax rcquirrr at ltaat ant verb In the santcncaf chooring a mrln verb constrains the rurfdct Rtructure, For inltrncer in thr rblcncr of campounds any verbs other than the main vwb will have to appear in another Fornr nominal, infinitive, gerundr p~rtLcIpL@~ or subordinate Cfauce, How daec the relevant fnfarmatlon centaintd In & gementic net Indicate tha rpproprlate farm?
559:860	The traditlondl answer Is "by meann of tht laxiC0nrU We will QXplorQ the relationship between net @r~d lexicon and rdvsnce a methodology far raprelcntlng a map tram deep case structura ta surface structure, Thlr prgrr ~OCULII an a philosophy ot sLnglam6cnfencr farmattinqr cmeaaing a main vatbr choosing tna grass structure st the output scntencer and deciding how to generate spgropriata noun phraras, Qur exarnpl~a will anB1oY limPlifia4 semantic net rtrueturrr, remcrhat like those in the actual 5RI ~aartitianad BlarntP~ neta 8ystam lHandrlxr 19751, MMes In the net may raprerant phylicrl objects, rcllttonshipr, evantr, sbtr, rulas, ar uttsrencesr a0 in the Qxempla balaw, Directad lab@Pled areg conncct nsdaa and rsprasant certain wprimi%iran tlmc-invariant ralat tsnrhtpa, In the n@t Lrrqm~nt above, the U.8.
560:860	an4 thr U,K. rra elcmrntl (a) of the gat of CQMW~~~@@, as EXP@tisnt@rr they baeh participate in OWNLng rlfurtionl lnvolvlnp as OBJlctr particular rubmariner) reeh rubmarlnt 11 an rlcmsnt of same class ot subm&r1ntsr rnd te~plafes fat Engllah rtntcner8, Bc choa~e a simple verb for dernmstrrtfin a= OWN, Wa not@ that our verb h $@v&t&1 VynanymrRr WAVE, POSSESS, and BELONG C10L Slmc clfh of thcsc verb8 (including OWN1 ha8 other Ssnsr meanings, re Darlt a node tense they have in Comment this node will be &ha m~rototypZcrlE OWHI in thst It w112 incarpareta the umsrnin~R of the altuatlsn and Ln that all inaerncrs af owning rituatians will b4 rerat~d PQ POSSESSt HIVEI BELONG] and trmplatrr, Wsts that one template rill hat rufftea Ear all dour verbs; For inatanesr the subject 0% the subject ir the FXPeriencQsr EXP own& OBJ 1 0BJ is owned by EXP EXP poroass~a OBJ OBJ 1s Pabl@lldd by EXB EXP hrr aBJ OBJ Gdronv to EXP tOWK (EXP V&ct OBJ) (OBJ Vprr BY EXD]) [PO3SES8 (EX$ Yact OBJ) (063 Ypar BY EXPI] fHAVE (EXP Vact OBJ)] IBEbON6 (084 Vact TO EXP)] New# in Order tb rpcilc about a particular Owning sltuatbonr nt t%dWr BELONG) and an ag~oe1ated template (OBJ VaeE TO EXPI, and But we have 4 problem!
561:860	there is no indication of how tho EXP and OBd Qrgumcntt &re to be generated, NP will not always rofticrr note for inrtanec that the predicate argument of in %?shn hapad to go homen must ba an Infinltlva PhP48e (rather than the QetUfid phrree that NP might producal, Even a cursory 8tUdY of a few hundred verbs in th@ language shawl that they have very defintts Cand regular1 Consttaint& on the syntactic form 0% tnrrr canststucntr, Thaas constraint8 appear ts bc matters %or the lexicon rrthar than the grammar, we associate net) rather than imwemsnt them via QP4MmBF TUI~II and wg cxpllcltly incorporate ths conrtltutnt types in the tamplatcst COWM ((NP EXP) Va~t (NP OBJ)) (CNP OBJ) Ypar BY (NP EXP))I (POSSES6 ((NP EXP) Vact (NP OBJ)) ((NP OBJ) Vpe8 BY (NP EXP))] [HAVE ((HP EXPI Vact (NP OBJ))] [BELONG ((NP OBJ] Yaet TO (NP EXP))I A act of patterns liLe thtra is associated *Ith rvsry "PIO~~YP.
562:860	verbn nods in the knswZ~dg@ bra@* $t would seam that mll ra need is an Pntnrgsebr tkab giv~n any w~e~b instanceR node fn the knowb@dg@ b~ger l0bko up the psttbrns PQT that type of!
563:860	nodh C~OQGQ~ Q VQtbr 1 eOrraapdnding tsmglate for the verb@ and then Proeaedr ts *evrluatan fha patterno verb [OWN, S-38,DWNI sms bQl~n$ tamp [(MP OBJ) Vact TQ (NP EXpll (NP Odd) 913 the S~awoLf YIC~ -13 Bclonqs TO --3 to (NP GXP) the u,s, But us ttlil run into trouble with our rimplc cchema, Consldsr the rent@ncc, *John burned the taarf black," \ ACT \~BJ colorl By uging the simple pattern UNP AGT) Vact (HP OBJI) we could easily generate the *Lncorrrctw sentence, rJahn burned the black tmr;trW since (NP QBJ) might include the color of the te8ltt8 Me need a pattern more llke ((NP AG,T) Vact [NP OW) (nod R~SIII Zn which the RESult of the action wflk bc directly related to the verb.
564:860	Ha*rvcrr thlr 1% not quite enough a= rt laart, not Wtthout a very c0mpfic.tad Lntcrpretcr -* because the interpreter must RnoW that (NF OBJ) cannot inelude ha vrabPs RE8 ergurnawl [black).
565:860	Thus# by convention, nay indicate an extra argument to bs Passed to r eon6tttuant qansrdor (such as the Ounetisn NP) te denote the item(@) not te appear in the resultant canrtiturnta (INP ACT) Vact (HP OBJ RES) (Nod RES)) *he pattern (NP OBJ RES) mrrnr Vgrnaratr an NP using the OBJsct of the VQrbr but do n~t inelude tha RESult of ths verb in the pattarno (Leer a pattern copy far every porr/blr *mmlrrLngfl constituent), This level of detail would be unrdasonable id f@w other verbs could we this template; however, there are rnt?t@ khan rstrtlvaly few tern~lrtarr baCh shw~d by several tang BB hundred@ of verbs, the urt of templates proves to ba quit* hefpfull There?
566:860	rrr other roureqr ot potantla1 pattern proltfarati~n~ an impattant one being the cambindtorial arrangamants of CQea arguments of time, manner, and athar such ~B~Wbi~llr &it well aa other (pot8iblY non*advrrbiall ease arguments such as sourer, $041, inttrumrnt, etc. Some at $ha@@ arguma~td are rath@r censtralnsd In their paaitiona in the rantencar but others may "esterday the ship satled from the lighthouse to the dock," "The #hip sailrd from ths lighthau8a to the dock yesterday".
567:860	wYast@rday the ship railed to the dock from the llghthour~.~ gt is af cauara unrsrranabl@ ta try to maintain all the ~as8ibXe prftrrnr!
568:860	Lnefcad rs leave Lnrsrtlon of fhrte rdvcrbiaf rrguncntr to r ring11 heuristic routine (dcreribcd below), There are rrvcral jurtitlcations for thjl, amanp them8 ti) the particular farm 09 the verb cannot be grneobatsd until the subject bbject(a5) @r~rt@L possible PL&CI~~ and (31 theta rta some heuristic one may question whether passive tenplatao ahould be atore41 c@tt@inlyr they could be derivedr On the other hand, neglecting t~ rtare thorn woWd farce ur to Ind%bat@ wAth each Verb (84n03)~ whether Lt can (or, ronrtimar, must) be parrivizcd, Indicating "tr.n~ltlv@Vir naf enough since there are tranritlvc vrrbr (i.a,, vcrbr that taka an object) that cannot bc pssslvizad.
569:860	8incs we hava to rtora the information anywayp we Can rave dame cads and Coaput$np tiar by storing thr parslvr ternplat@.
570:860	There arr rtvsrrl rraronr for gcnrrafinq the verb after the mafor @rgumentr.
571:860	Ftr~t the lubjret auat be panerrfad ro that the verb can br mrdc to aprrr in nuabnr.
572:860	Second, certain rare trnrrr 8tr true of vrrb-prrticlr conblnitionc rkiir not cL the icolatrd verb, Btncr, in addition, prrticl@c must appear rfttr object8 that F Short (like pranownrl but bsf~rs abjaete that are long (like noun phraras), the particle nurt be positionad after the object jS grncrrtcd.
573:860	FfnaL1~1 inocrtian af soma advarbirbr tr.~, annot" rrqulrrr an ruxllirry verb Ithug verb grnaratlon aurt fotlor rdvrrbirl grncratlon, VERB PATTERNS This #tudy started with the 25 Verb pattcrnr" prcrcntcd by Hornby (1954).
574:860	The~e Ln turn crwr from a dlctgonary by Hornby ct el.
575:860	, (1948).
576:860	VctM in the dict$onary &re cZ&irltird &c~ording ta their gross syntactic prttrrn~ af lubjrctp abjrct(s1, end com~~raentCs)~ most ot the ~aftqrnr rra rub-dlvgdade Thc rutkorr cl@La that there patfrrnr WXXNnt for all conttruetlanr involving &I& the verbs In theLr 4ietianary dndr by Nttnefon, In the IangUagr, I tl&SrifiertLon is not Zmmcdi&telY useful to C~thpUtdhti~nal lingulrtr rlnca it doas not addrags underlying ~emaatics~ Havatkhele~~~ it $r clarr that it can ssrve ae ths brrlr for r &artvatIan ol Und@rlYlng carr rtructuhar prrtlculatly, &r a bar16 for Vplnrrrtion tcngl&tro,* There pattern8 &re b@lng canvartad inta tamplet@@ much llkt tho#* derived arrlfetr tha tnrlyris is Baing perfarmed with ralD*Ct t4 300b V6tbl drawn fr~~ the dtctionary (S~OCUW~ to appear), There templatea serve ro tho major portion of a modular *gan@ratLon grammarr* with the r@aarlnder in thr farm of hsutia;tic tunctianr tor constructing syntactic constituentr, NOUN PHRASES What to IncluBb Ln a noun phrase should be another mrttar far the dircaurte module to judge, There are no w~1l~Earm~laCad fulrl eccountlng far anaphorl In EngliShj indead, there arc trw wsflee8trbliehad parameters ather than that %ha hearer must ba able to r~aalva tho (pto)nouns to their reforants, The &D@ak@a?p Should tmploy anaphoxa In order to avola t@petitlonr but only Pt his B0deX of!
577:860	the hearer indicrto~ that the ~~QPCP can tdsolvd the ambiguity, Thcrc arc same lawcpowrr ptaneminalization ruler that could be directly incarp0ratsd in a generator rafg@x$v%zalPanr for s~11p10.
578:860	NcV~P~~B~~SS~ ft IS impastant LO P~BZIZB that When r generator ir unaware of the eanvcrtafl~nal context, it lhauld not indepsndantLy decide haw to ganaratr noun phpa1481 It CQR only decide when to d6 roe This rbtuatlon har not been univ+roalLy r@caqnfiedr but It 1s becoming hnercarlngZY clear thrt a QZ~csurte msduh must be canrubtsd during the ~snarrtlan phrrs.
579:860	The direourre modal.
580:860	will not know ahead of timr what NPI are %O be pr~due@d un~@sg $t p4rforrn\@ many of ths gem@ ~Poratiana that the paneratar would do anyr4y.
581:860	Yat the cantcxt-8cnritivo decirion strrt~g~ may h.ve to resorb ts rush maarutbr ar dlaewb$quating the proporrb output uoLwg khs model of &he hrsrer in order ta d~termbnr what rnaphdra if4 teseavabla, t% Ir unr@rsanrble ts ancarporrte thlg strrt~gy in ths gancratsr~ rinca for rany rrrsonr it nurt be part of the dlrcourrr modulc, Therefate thr genrrrtor Should pa6S any RncunUconstitucnt to th@ dJscourre module (plrhspt rith its rrcommcndation about how to PIgdUce the constituent,; th~ nodule must dctcrminr if a pronoun or b&ga noun it ambiguour to the hearer, and, if so, what to add to the noun ln order to makt the desired referent elear, Ln the current SRI system, noun prttcrnr [Slocum, ta rpPcar) ara USI~ to control noun phrase grnrrrtian, Much like verb patterns, noun prttcrnr order the ~on~tlfu@nt8 in the phrarr and indicate how each eon8tLturnf Is to be generated by naming a functicn to be Called with the nQtwerK ~anstftuentf ((DET] [Ad3 QUA&) (Adj SIZE) (Adj SHRFE) (Ad1 COLOR) (N)] Batttrhr like thir era dirtributrd about the network hi@r@rehyj in the future@ th@ QL~c~urrc aadulc rill dccLde tor each pqttrrn constituent whather ft i8 to appear in the phrrcs, MEURZSTXC RULES R~rnby d+rcrlb@r three bsrlc porltlanr far advcrbr in the clrussr "rentff pa#b$tionp R~tdn pssltlonr and @andu ~eritlon~ Front poritgen rdvrrbr occur batare the subjrctt wVerta~day h@ want hornat from these ha took a taxi,* The intorroqltiwa rdvarbr (a,g, now, when?
582:860	rrr typically constrdkn~d to front Pooitionl other8 miy rpprrt thrrr tat purporrr of rmpharla cr eantralt.
583:860	Mid perleion rdvarba occur virh the verb (atringlt Sf there ar* medal sr ru#ilLrrv varhrs the dbvrrls occurs aftor the first one, Oth@twi8r the rd~~rb w~LL appear btk~r@ the Verbl cxcapt fbr *un11tre8#aQ~ fhlte~ of bar and 401 '~4 O~~QR $36 thararl @oh@ if8 typi~llly buw8] *he It 1t11I waiting,@ End position adverbs occur rttrr the verb and after any dbteet or indirect object pteacnt, While talativ~ly fen clrurss have mare thrh one adVQrb in tront parttian or mara thrn sns In @id porltianr it ir common tat rrvcrrl .dV@rbS to appear in and ~arltlon in tha rams clausal *thaV play the Piana PeerlY tagcthrrw, hdvarblrlr of time (rnrrrring the 4uartiont wrh@n?w) usually deeut in and poaltion, but may appear In grsnt position far aaphrrtr or contrast.
584:860	kdverbialr of frequency (anrwsrlng tka quettbon, wRaw ~ftan?~l can be split inQ two groups, The first group is comgo@@d ~f ringla-nerd adverbs that tyalcally seeui fn &id porition but aIro may br in ind patLtianj th@ sacand ir eosparsd a$ multipls=word phrrrrr that dgp&ar Pn end parition er~ &err Lrr9UrntlYr in trent Position.
585:860	Adverbs of duration [Vlf~rl hew lonqtw) udu~lly have end parltlan, wbth %song p6sBtPon fbr empha8fs or csntxartr Adverb8 of place and diraetlen narrnall~ hivQ and porition.
586:860	Advcrbr of 4rgrca and manner have mid or and ~OSI~LQRI depbnding on thr advarb, Along With ruah ruler con~~rnlng the BarLtionr OL Vsriout types of advrrbr, there murt br a mcchanirm to 0rd.f the rdvcrbr that arr La s~cur In tha "maQn petitLon, Thrrr rra eome h@ualrticl~ among rdverbi.11 of time Car pla~11 the smallat unit 81 ururlly pL4ccb tlrlt, unlrrl it tr 44604 48 &R sftarthauphtl @the army attaeksd thr village in farce an a hot Augutt @ftrrnoon, just attar riertaH, Adv@rbialr of placa end diroetton usually precede thore of frequency, which in turn praeode thoo~ at time, There rule8 re iaplrmontsd in th@ raRr rautLn~ Mat produce@ the verb1 when @ template is firat fnearprcked =NWh eo B 8equencc of function erllr the RVa~tw or "Vpao kaya arc lonored.
587:860	once the lUbjCCt, ~bjCCt(S1 and compltmcnt(a) lndicrtrd by the tcnplrtr rrr Oanrrrfrdr this *clean upm routine $6 called.
588:860	Et employg the hauxittiek d~rcrfbed abava La add the adverbial C~rt&tftU@nt& and VhFbr then concatsnatss the canstbtutnts ta produce r complete @IruseI DXSCUSSTON fn theory, the set of porsiblc EnpLlrh rantcncrs i a inf fnZtsr Tha obvlaor qusatlbn man ~t@@@r Olf sne tries to account ror them with tarnplitrrr won t there bc an infinite number eL taaplatt~?~ The simple antxcr irr "Nor tor romc of the Sam@ rell~nl that allow a finite grammar to gcnrrata an intlnite number of itr2ngr.R One can preduca rantrncer of arbitrary Length by (1) ~T'bltt'at~ anb@ddlngr and (21 arbitrary conjuncttan@ Caw@ Boto not do so by includdng arbitrary numbers sf distinct car@ erpuacntl.
589:860	Evrn ra the numbcr ot basic patterns could be tnl~amelY Iaf gc, Evidrnca, horevrrr il tc the contrary!
590:860	the lVentUII nUnbqr of t@Bpl.t@# would appear to br reveral time8 the number OI pltternr~ awing to thr SUbltitUtion of parttcular pra~aritlone Ear npr~pw In the r~~tictlc attaro or, and tha arrlgnaent ot dltfetent crle nancr to a particular canstitusnt depending on the prrticular vrgb ur@d. American Journal of Computational Linguistic8 Microfiche 33 : 78 Yale University New Haven, Connectiuct 06511 ABSTRACT TALE-SPIN is s program which makes up stories by using planning structures as part of its world knowledge.
591:860	Planning structures represent goals and the methods of achieving those goals.
592:860	Requirements for aparticular method depend on static and dynamic facts about the world.
593:860	TALE-SPIN changes the state of the world by creating new characters and presenting obstacles to goals.
594:860	The reader / listener makes certain plot decisions during the telling of the story.
595:860	The story is generated using the notation of Conceptual Dependency and is fed to another program which translates it into English.
596:860	INTRODUCTION TALE-SPIN is a computer program which makes up stories about characters who plan how to solve certain problems This work was supported in part by the Adudnced Research Projects Agency of the Department of Defense and monitored by the Office of Naval Research under contract N00014-75-C-1111.
597:860	and then carry out their plans.
598:860	The planning procedures interact with a data base of knowledge about other characters and objects in the world, memory, and the personal relationships which exist between characters.
599:860	The stories are represented in Conceptual Dependency and are passed to a program which expresses them in English.
600:860	The reader is asked to make certain decisions about the story during the process of generation.
601:860	Here is an example.
602:860	JOE BEAR WAS FAMISHED, HE DECIDED HE WOULD BE FULL IF HE ATE SOME HONEY, HE WANTED TO FIND OUT WHERE THE HONEY WAS, HE THOUGHT TRAT IRVING BIRD WOULD TELL HIM WHERE THE HONEY WAS.
603:860	JOE BEAR WALKED TO THE TREE WHERE IRVING BIRD WAS.
604:860	HE ASKED IRVING BIRD IF HE WOULD TELL HIM WHERE THE HONEY WAS.
605:860	>> DECIDE: DOES *IRVINGBIRD* AGREE?
606:860	*NO IRVING BIRD REFUSED TO TELL JOE BEAR WHERE THE HONEY WAS.
607:860	JOE BEAR DECIDED IRVING BIRD WOULD LIKE HIM TO GIVE THE WORM TO HIM.
608:860	JOE BEAR ASKED IRVING BIRD IF HE WOULD TELL HIM WHERE THE HONEY WAS IF HE GAVE THE WORM TO HIM.
609:860	>> DECIDE: DOES *IRVINGBIRD* AGREE?
610:860	*YES HE THOUGHT THAT HE WOULD LIKE JOE BEAR TO GIVE IT TO HIM, HE WANTED TO FIND OUT WHERE THE WORM WAS, HE THOUGHT THAT IRVING BIRD WOULD TELL HIM WHERE THE WORM WAS, JOE BEAR ASKED IRVING BIRD IF HE WOULD TELL HIM WHERE THE WORM WAS.
611:860	>.> DECIDE: DOES *IRVINGBIRD* AGREE?
612:860	*YES IRVING BIRD DECIDED HE WOULD TELL JOE BEAR WHERE THE WORM WASIRVING BIRD TOLD HIM IT WAS AT A PATCH OF GROUND, HE WALKED TO THE PATCH OF GROUND WHERE WORM WAS, HE TOOK THE WORM HE WALKED TO THE TREE WHERE IRVING BIRD WAS.
613:860	HE GAVE THE WORM TO IRVING BIRD.
614:860	>> DECIDE: DOES *IRVING%IRDk KEEP HIS PROMISE?
615:860	*NO HE REFUSED TO TELL JOB BEAR WHERE -THE HONEY WAS.
616:860	JOE BEAR TOLD IRVING BIRD HE XS GOING TO STRIKE HIM IF HE DOES NOT TELL HIM WHERE THE HONEY WAS.
617:860	>> DECIDE: DOE$ *IRVINGBIRD* IGNORE THE THREAT?
618:860	*NO IRVING BIRD DECIDED HE WOULD TELL JOE BEAR WHERE THE HONEY WAS, IRVING BIRD TOLD HIM IT WAS AT THE BEEHIVE.
619:860	80 JOE BEAR THOUGHT THAT HENRY BEE WOULD GIVE THE HONEY TO WIM.
620:860	JOE BEAR WALKED TO THE BEEHIVE WHERE HENRY BEE WAS.
621:860	HE ASKED HENRY BEE IF HE WOULD GIVE THE HOMEY TO HIM.
622:860	>> DECIDE: DOES "HENRYBEE* AGREE?
623:860	*YES HENRY BEE DECIDED HE WOULD GIVE IT TO JOE BEAR.
624:860	HENRY BEE GAVE IT TO JOE BEAR.
625:860	BE ATE IT.
626:860	HE WAS FULL.
627:860	THE END.
628:860	Here is a story which TALE-SPIN generates which the translator is not yet capable of producing in ~nglish: JOE BEAR W4S HUNGRY.
629:860	HE THOUGHT TEWJ!
630:860	IRVING BIRD WUQLD TELL HIM WHERE SOME HONEY WAS, HE WALKED TO TlWE TREE WHERE IRVING BIRD WAS.
631:860	HE ASKED IRVING BIRD TO TELL HIM WHERE THE HONEY WAS.
632:860	IRVING BIRD TOLD HIM THE HONEY WAS IN A [certain] BEEHIVE JOE BEAR WALKED TO THE BEEHIVE WHERE THE HONEY WAS.
633:860	HE ASKED HENRY BEE TQ GIVE HIM THE HONEY.
634:860	HENRY BEE REFUSED.
635:860	JOE BEAR TOLD HIM WHERE SOME FLOWERS WERE.
636:860	HENRY BEE FLEW FROM THE BEEHIVE TO THE FLOWERBED WHERE THEFLOWERS WERE.
637:860	JOE BEAR ATE THE HONEY, HE WAS VERY TIRED.
638:860	HE WALKED TO HIS CAVE.
639:860	HE SLEPT.
640:860	THE END.
641:860	TALE-SPIN starts with a small set of characters and various facts about them, It also has a set of problem-solving procedures which generate the events in the story.
642:860	Many decisions have to be made as the story is being told.
643:860	Some are made at random (names of characters, for example) ; others deperld on the relationships between characters (whom one asks for information, fox example); others are made by the reader (whether a character keeps a promise, for example) . TALE-SPIN generates sentences using the representation system of Conceptual Dependency (Schank 1975).
644:860	Some of the Conceptual Dependency (CD) structures are passed on to a program which expresses them in English.
645:860	(The original version of that program was written by Neil Goldman for the MARGIE system.
646:860	The present version has been modified by Walter Stutzman and Gerald 8 1 De Jong ).
647:860	The sentences which are not passed to the translator are those which represent easily inferred ideas.
648:860	Neither program yet worries about khe style of expression; that is, we worry about whether to say a newly generated piece of the story, but not much about how to say it.
649:860	A TALE-SPIN story involves a single main character who solves some problem.
650:860	To make the process interesting, obstacles are introduced, some by the reader if he chooses, and some at random.
651:860	For instance, the reader's decision Chat Irving Bird is not going to tell Joe Bear what he wants to know produces an obstacle to Joe ear's plan to find something out.
652:860	Some obstacles are c~aated when certain scenes are included in the story.
653:860	For instance, the initial world state has no bees in it, but when it comes time in the story to conjure up some actual honey, we do so by creating a whole scene which includes some honey in a beehive in a tree and a bee who owns that honey.
654:860	The bee may or may not be at home.
655:860	If he is, Joe Bear is going to have another obstacle in his plan when he gets to the beehive.
656:860	The story is the narration of some of the events which occur during the solution (or non-solution) of the problem.
657:860	(That is, more things happen in the solution of a problem than a storyteller says or needs to say).
658:860	TALE-SPIN differs from other problem-solving systems in several ways : (1) the problems it solves are those requiring interaction with other, unpredictable characters rather than with a data base of theorems or blocks or circuits; (2) the world inside TALE-SPIN grows: new characters are created with unpredictable effects on the story; (3) obstacles are deliberately introduced; (4) an unsuccessful" story, one in which the problem is not solved, can be just as interesting as a Usuccessful~ one.
659:860	PLANNING STRUCTURES Planning structures are what we use to organize knowledge about planf ul activity, which is represented in CD by a chain of causes and effects.
660:860	The planning structures include delta-acts, planboxes, packages, scripts, sigma-states, r ho-states, and pi-states.
661:860	A delta-act is used to achieve a particular goalstate.
662:860	Delta-prox (written here as dPROXl is the procedure for becoming proximate to some location.
663:860	A delta-act is defined as a goal, a set of planboxes, and a decision algorithm for choosing between planboxes.
664:860	A planbox is a particular method for achieving a goalstate.
665:860	All the planboxes under a delta-act achieve the same goalstate.
666:860	Each planbox has a set of preconditions (some of which may be delta-acts), and a set of actions to perform.
667:860	"UnconsciousN preconditions are attached to planboxes which would never occur to you to use.
668:860	If you're trying to become proximate to X, you don't even think about persuading X to come to you when X is an inanimate object.
669:860	"~ncontrollab~e" preconditions cannot be made true if they're not already true.
670:860	(The assumption is that they are sometimes true).
671:860	"~lantime' preconditions are the things you worry about when you're making up the plan.
672:860	You don't worry about "runtimen preconditions until you're executing the plan.
673:860	("Planning" is a mental activity.
674:860	PLAN is, in fact, one of the primitive ACTS of CD.
675:860	"Executing a plan" is performing a logically structured sequence of actions to achieve the goal of the plan).
676:860	If 1'm planning to get a Coke out of the machine upstairs, I worry about having enough money, but I don't worry about walking up the stairs until 1'm at the stairs.
677:860	That the machine actually has some Coke is an uncontrollable runtime pr econd it ion : I don't worry about it until I get there, and there's nothing I can do if it is empty when I get there.
678:860	A package is a set of planboxes which lead to a goal act I' ather than state.
679:860	The PERSUADE package, for instance, contains planboxes for X to persuade Y to do some act 2.
680:860	The planboxes include asking, giving legitimate reasons, offering favors in return, threatening, and so on.
681:860	Goalstates come in various flavors.
682:860	There are the goals which are associated with the delta-acts: the goal of dPROX is to be somewhere, the goal of dKNOW is to find out the answer to some question, the goal of dCONTROL is to possess something.
683:860	But there are also goals of satiation, called sigma-states.
684:860	For example, sHUNGER organizes the knowledge about satisfying hunger (invoking dCONTROL of some food, eating).
685:860	TALE-SPIN also uses sigma-state knowledge in the bargaining process; offering someone some food in return for a favor is legitimate since it will satisfy a precondition for sHUNGER.
686:860	There are also goals of preservation, called pi-states, which are most interesting when they are in danger of being violated.
687:860	The logic of the THREATEN planbox in the PERSUADE package, for example, derives from the fact that physical violence conflicts with pHEALTN.
688:860	A SAMPLE DELTA-ACT: dPROX TALE-SPIN does not include all nine delta-acts described by Abelson (1975).
689:860	It contains the three which closely correspond to pr im itive acts: dPROX (PTRANS), dCONTROL (ATRANS), dKNOW (MTRANS).
690:860	Here is an outline of dPROXt ~PROX(X,Y) -X wishes to be near Y Planbox 0: if x is already near Y, succeed.
691:860	Planbox 1: X qoes to Y uncontrollable precondition: can X move himself?
692:860	plantime precondition: P KNOW (location of Y) runtime precondition: dLINK (location of Y) action: PTRANS to location of Y runtime precondition: is Y really there?
693:860	(We may have gotten false information dur ing the KNOW.
694:860	) Planbox 2: Y comes to X unconscidus precondition: is Y animate?
695:860	uncontrollable precondition: is Y rno~ able?
696:860	action: PERSUADE Y to PTRANS himself to X (PERSUADE package) Planbox 3: Agent A brings X to Y uncontrollable precondition: is X movable?
697:860	action: X gets AGENT to bring X to Y (AGENCY package) Planbox 4: Agent A brings Y to X unconscious precondition: is Y animate?
698:860	uncontrollable precondition: is Y movable?
699:860	action: X gets AGENT to bring Y to X (AGENCY package ) Planbox 5: X and Y meet at location Z unconscious precondition: is Y animate?
700:860	uneontrollabls precondition: is Y movable?
701:860	actions: PERSUADE Y to PTRANS himself to Z and dPROX(X,Z) THE DATA BASE Planning structures are essentially procedural.
702:860	-The non-procedural data base used by the planning structures is divided into five classes.
703:860	1.
704:860	Data about individual PPs (Picture Producers, nouns) where applicable : height; weight; where their home is; who their acquaintances are.
705:860	2.
706:860	Data common to classes of PPs (e.g. , data common to all birds) where applicable: what they eat; what their goals (sigma-states) are; whether they are animate (capable of MBUILDing), movable, self -movable ; how they move around.
707:860	3 Sigma-state knowledge indicating how to achieve a sigma-state and what the plantime preconditions are that someone other than the planner can achieve.
708:860	This is used in the bargaining process.
709:860	Joe Bear offers to bring Irving Bird a worm because dCONTROL (FOOD) is a plantime precond it ion for sHUNGER which Joe Bear can achieve for Irving Bird.
710:860	There are no plantime preconditibns for sREST that he can achieve for Irving Bird (except maybe to leave him alone) . 4.
711:860	Memory: what everybody knows (thinks, believes) ; what Joe Bear knows; what Joe Bear thinks Irving Bi~d knows; etc. Planbox 0 of BKNOW, for example, accesses Memory to test whether Joe Bear already knows the answer to the question being asked, or whether it is public knowledge.
712:860	Since both the question and the facts in Memory are represented in CD, the pattern match is very simple, taking advantage of CD's canonical representation of meaning.
713:860	5.
714:860	Personal relationships.
715:860	The relationship of one character to another is descibed by a point on each of three scales: COMPETITION, DOMINANCE, and FAMILIARITY.
716:860	Scale values range from -10 to +I@.
717:860	The relation "is a friend of" is represented by a certain range on each of the three scales.
718:860	The relation "would act as an agent for" is represented by a different range.
719:860	The sentence "Joe Bear thought that Irving Bird would tell him where the honey was" comes from the "Ask a Friend" planbox of dRNOW.
720:860	There is a procedure which goes through a list of Joe Bear's acquaintances and produces a list of those who qualify as "friends", i.e., those who fit somewhere within the "friendn range.
721:860	Relations are not symmetric: Joe Bear may think of Irving Bird as his friend, so he might ask him where the honey is, but Irving Bird may not think of Joe Bear as his friend at all, in which case he might refuse to answer Joe Bear.
722:860	Relationships can change.
723:860	If Gee Bear becomes sufficiently aggravated at his "friend" Irving Bird and has to threaten to bash him in the beak in order to get him to tell him where the honey is, then the relationship between them deter iorate s. We plan to extend this feature to describe a character's "default" relationship: how he relates to total strangers.
724:860	This would not necessarily be the point (0,8,0) but rather some point which would be used to give a rough indication of the character's "persoaalityn.
725:860	Big bad Joe Bear might rate at (+6,+9,+4), where small meek Bill Worm might rate at (-6,-la,-4) . Changing a relationship is a type of goal we haven't yet considered in much detail, a1 though goals of relationships (rho-states) clearly exist.
726:860	The procedure for getting someone to like you (rLIKE) might contain planboxes for ATRANSing gifts, MTRANSing sweet nothings, etc. , in addition to changing your own feelings toward that person so that if he (she) asks you to do something, you don't refuse.
727:860	Information gets into the data base in several ways.
728:860	Memory data gets produced directly by the planning structures.
729:860	Changes in relations are side-effects of the present set of planning structures.
730:860	But things have fo start somewhere.
731:860	There is a function CREATE (X) which invents a new item of type X (e.g. , bear, flower, berry).
732:860	Associated with each type of item is a small procedure called a picture which invents the desired item and others as required.
733:860	For example, when we create some honey, we also create a beehive, a tree, and a bee.
734:860	The honey is "owned" by the bee and is inside the beehive which is in the tree.
735:860	The bee may or not be at home.
736:860	Randomly chosen names, heights, weights, etc. , are attached.
737:860	All this data is then added to Memory.
738:860	The CREATE function is called when needed; remembet that TALE-SPIN models the process of making ue a story as you go along.
739:860	We will now follow, in detail, the production of the second sample story.
740:860	CREATE a bear, which invokes a picture procedure which invents a beat.
741:860	Assume the bear is named Joe; although since the name is chosen at random from a list of first names, it is just as oiten Irving.
742:860	A cave is also invented, and has Joe in if.
743:860	~oe's location becomes public knowledge.
744:860	CREATE a bird, named Irving, and a tree which is his home.
745:860	~rving's location is also now public knowledge.
746:860	Assert that Joe is hungry.
747:860	This fact enters ~oe's Memory.
748:860	We also "say" this: that is, we pass it to the English translator which then produces the sentence "JOE BEAR WAS HUNGRY N. Invoke sHUNGER.
749:860	Choose at random a food that bears eat: honey.
750:860	Assert that Joe is now planning to achieve the goal (sigma-state) of satisfying his hunger.
751:860	Assert that he has decided that eating the food can lead to the achievement of his goal.
752:860	sHUNGER calls dCONTROL (honey) . This forms a new goal, namely, that Joe have some honey.
753:860	CONTROL'S "Planbox 0" asks Memory if the goal is alseady true: does Joe already have some honey?
754:860	The answer comes back: no.
755:860	A plantime precondition is to know the location of some honey, so dCONTROL calls dKNOW(where is honey?).
756:860	(The question is represented in CD, not English).
757:860	dKNOW forms the new goal.
758:860	KNOW'S "Planbox 0" asks Memory whether Joe knows the location of any honey.
759:860	Memory says no.
760:860	Planbox 1 tests whether the question can be answered by consulting a standard reference (e.g. , "What time is it")?
761:860	That fails.
762:860	Planbox 2 tests whether the question requires expertise: no.
763:860	Planbox 3 tests whether this is a "general information" question.
764:860	It is, so we assert that Joe is planning to answer this question using Planbox 3 ("Ask a Friend").
765:860	Planbox 3 starts.
766:860	Choose a friend: Irving.
767:860	dKNOW calls the PERSUADE package to try to get Irving to answer ~oe's question.
768:860	PERSUADE asks Memory whether Joe thinksthat Irving cannot answer the question.
769:860	~nswer: no.
770:860	Irving is a "friend", so we try the ASK planbox.
771:860	Assert that Joe thinks that Irviag will tell him where the honey is. PERSUADE calls dPROX(,Irving), since Joe needs to speak to Irving.
772:860	dPROX asks Memory whether Joe is already near Irving.
773:860	Memory says no.
774:860	Planbox 1: is Joe self-movable?
775:860	Yes.
776:860	Assert that Joe is planning to be near Irving by going there himself.
777:860	dPROX calls dKNOW (where is Irving)?
778:860	~KNOW'S "Planbox 0" asks Memory whether Joe already knows where Irving is. The answer comes back: yes, Irving is in a certain tree.
779:860	dKNOW returns this to ~PROX.
780:860	(We will omit future references to "Planbox On.
781:860	) dPROX asserts that Joe walks to the tree where Irving is. We ask Memory whether Irving is actually there.
782:860	He is, so dPROX has achieved its desired goal; his change in location is added to Memory.
783:860	dPROX returns to PERSUADE.
784:860	30e asks Irving where some honey is. The reader now gets to decide whether Irving agrees to do 39.
785:860	Assume the reader says yes.
786:860	We ask Memory whether Irving actually knows where any honey is. If he did, we would have Irving tell him, but he doesn't, so we CREATE some honey: a storyteller can create s6lutions to problems as well as obstacles!
787:860	Some honey is invented, along with a beehive, a tree, and a bee (Henry) who is at home.
788:860	Irving tells Joe that the honey is in the beehive.
789:860	ASK succeeds, so 90 PERSUADE succeeds, so dKNOW succeeds: Joe knows where some honey is. Back in dCONTROL, we ask Memory whether [Joe thinks that] anyone owns the honey.
790:860	Memory says that Henry does, so CONTROL'S Planbox 1 ("Free for the taking") fails.
791:860	Planbox 2 is to PERSUADE Henry to give the honey to Joe.
792:860	Given no relation between Joe and Henry (they don't know each other), the only planboxes in PERSUADE which can be used are ASK and INFORM REASON.
793:860	We try ASK first.
794:860	This calls dPROX<Henry) which succeeds sirfce Joe knows where Henry is; we omit the details here.
795:860	Joe asks Henry to give him the honey, and the reader decides that Henry refuses.
796:860	We try INFORM REASON next.
797:860	We choose a goal of ~enry's and build a causal chain backwards from the goal,.
798:860	For example, one of Henry s goals is to "eat" flowers.
799:860	(TALE-SP'IN thinks that what beesa do to flowers is equivalent to eating).
800:860	In order to eat a flower, you have to "control" a flower, which results from someone (possibly you yourself) ATRANSing the flower to you.
801:860	We test whether what Joe is trying to PERSUADE Henry to do matches ATRANSing a flower.
802:860	It doesn't, (Joe is trying to PERSUADE Henry to A-TFWNS the honey to him).
803:860	We then consider that in order to ATRANS a flower, you.
804:860	have to be near the flowes, which results from someone PTRANSing you to the flower.
805:860	Does this match?
806:860	No . We repeat this process a few times, trying to construct a short inference chain which -connects= what Joe is trying to persuade Henry to do with one of Henry's goals.
807:860	INFORM REASON fails, and we return to dCONTROL.
808:860	The next Planbox is called "Steal".
809:860	We ask Memory ~hether Henry is home: if he weren't, Joe would simply take the honey.
810:860	But Memory tells us that Henry is home, so STEAL calls PERSUADE to get Henry to leave home; that is, Joe is now going to try to persuade Henry to PTRANS himself from the hive.
811:860	In the context of STEAL, the ASK planbox is not used.
812:860	Joe tries INFORM REASON again and succeeds in producing the following chain: we get to the idea of someone PTRANSing himself to a flower again as we did before, but we notice that this does match what we are trying to persuade Henry to do: the connection is 4 that Henry will PTRANS himself from the beehive to the flowtx.
813:860	Joe now considers the precondition for ~enry 's PTRANSing himself to the flower, namely, that Henry has to know where the flower is. Memory does not indicate that Joe thinks that Henry knows where a flower is, nor does Joe know where a flower is, but rather than invoke dKNOW(where is a flower?), we CREATE a flower: this is legitimate in a plan to steal something.
814:860	Joe now tells Henry that there is a flower in a certain flowerbed, and then asks Hengy if he would like to fly to that flower.
815:860	Henry aqrees and flies away.
816:860	PERSUADE succeeds, and returns to dCONTROL.
817:860	Joe now takes the honey from the hive, so dCONTROL succeeds and returns to sHUNGER.
818:860	Memory is modified to indicate that Joe knows that he has the honey, but that Henry does not.
819:860	Joe now oats the honey, and has achieved the sigma-state of not being hungry.
820:860	But, when bears eat, they become tired, so sREST is invoked.
821:860	sREST is very short.
822:860	It requires a dPROX (cave), which is easily achieved, and then Joe goes to sleep.
823:860	Since the main goal has been achieved, and the goal produced as a consequence of that goal has also been achieved, the story ends.
824:860	What distinguishes stories from simple sequences of events?
825:860	Coherency is important: there has to be a logical flow from one event to the next.
826:860	This is represented in CD as a chain of acts which re,sult in states which enable further acts and so on.
827:860	Interest is important: something interesting or unusual has to happen or else the reader will begin to wonder what the point of the story is. TALE-SPIN creates impediments to goals, on the assumption that the overcoming of obstacles can make an interesting skory.
828:860	"One day Joe Bear was hungry.
829:860	There was a jar of honey right next to him.
830:860	He ate it.
831:860	The end" is not a story.
832:860	It shouldn't be that easy.
833:860	On the other hand, it shouldn't be too hard either.
834:860	In theory at least, there is a cost-effectiveness calculus which people employ when deciding how much energy to expend on a subgoal, based an how much the goal is worth to them.
835:860	This process prevents the plans from being too complicated.
836:860	As the story is generated, various plot decisions have to be made.
837:860	Some decisions are made at random, others are made by the reader.
838:860	When Joe Bear threatens Irving Bird because Irving Bird won't tell him where the honey is, the reader gets to decide whether Irving Bird is going to ignore the threat.
839:860	We use planning structures because any program which reads or writes a story, whether of the folktale variety or the New York Times variety, must have a model of the logic of human activity.
840:860	It might be easier to simulate the generation of a highly stylized form of story, as Klein (1974) has done using ~ropp's analysis of a class of Russian fairy tales, but there is little generality there.
841:860	One could use any of the wel.1-known problem-solving systems like MICRO-PLANNER, but the story is the proof procedure, and the procedure used there does not correspond to my conception of how people solve problems.
842:860	hat's not a criticism of MICRO-PLANNER as a problem-solver, but only as a model human problem-solving . user interaction was included for two reasons.
843:860	First, the interactive feature now serves as a heuristic for placing bounds on the complexity of the story.
844:860	Beyond, some number of obstacles to the goal, a story becomes a kind of joke.
845:860	Second and more important, ex tensions to TALE-SPIN will include more sophisticated responses than the present yes/no variety.
846:860	THE FUTURE OF TALE-SPIN.
847:860	-There are a lot of things that TALE-SPIN doesn't do yet that would improve it as a storyteller.
848:860	Here are some of the theoretical problems we will be working on in the immediate future.
849:860	(1) Bargaining, as it exists now in TALE-SPIN, is a pretty one-sided affair, with the main character making all the proposals.
850:860	Irving Bird is just as likely to suggest that Joe Bear go get him a worm as Joe is to offer to do SO Counter-proposals are certainly common enough.
851:860	(2) Future stories should include planning on the part of more than one character.
852:860	The present stories are all "about" the bear, and only incidentally involve the bird and other characters.
853:860	The stories are more concerned with reaction than interaction.
854:860	(3) For every plan, there may be a counter-plan, a plan to block the achievement of a goal: a plan for keeping away from somethinq or someone; a plan not to find out something, or to be c~nvinced that it isn't true; a plan to get rid of something you own.
855:860	(4) How much of a plan do people consider in advance?
856:860	We have made some efforts in this area by making the distinctions between kinds of preconditions.
857:860	Certainly the most important improvement here will be the cost-effectiveness reasoning.
858:860	(5) The theory of telling stories (what to say) now implemented in TALE-SPIN is to express violations of sigma-states ("Joe Bear was hungryn), physical acts, and those mental acts which provide motivation or justification for later events.
859:860	The reader is assumed to be able to infer the rest.
860:860	This seems to work reasonably we11 for the present simple stories, but may have to be modified to suit longer, more complicated storie s.

