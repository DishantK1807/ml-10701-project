An Efficient Parallel Substrate for Typed Feature Structures on 
Shared Memory Parallel Machines 
NINOMIYA Takashi?, TORISAWA Kentaro ? and TSUJII Jun'ichi t* 
?Department of Information Science 
Graduate School of Science, University of Tokyo* 
*CCL, UMIST, U.K. 
Abstract 
This paper describes an efficient parallel system 
for processing Typed Feature Structures (TFSs) 
on shared-memory parallel machines. We call 
the system Parallel Substrate for TFS (PSTFS). 
PSTFS is designed for parallel computing envi
ronments where a large number of agents are 
working and communicating with each other. 
Such agents use PSTFS as their low-level mod
ule for solving constraints on TFSs and send
ing/receiving TFSs to/fi'om other agents in an 
efficient manner. From a programmers point 
of view. PSTFS provides a simple and unified 
mechanism for building high-level parallel NLP 
svstems. The performance and the flexibility of 
our PSTFS are shown through the experiments 
on two different types of parallel HPSG parsers. 
The speed-up was more than 10 times on both 
parsers. 
1 Introduction

The need for real-time NLP systems has been 
discussed for the last decade. The difficulty in 
implementing such a system is that people can 
not use sophisticated but computationally ex
pensive methodologies. However, if we could 
provide an efficient tool/environment for de
veloping parallel NLP systems, programmers 
would have to be less concerned about the issues 
related to efficiency of the system. This became 
possible due to recent developments of parallel 
machines with shared-memory architecture. 
We propose an efficient programming envi
ronment for developing parallel NLP systems 
on shared-memory parallel machines, called the 
Parallel Substrate for Typed Feature Structures 
(PSTFS). The environment is based on agent
based/object-oriented architecture. In other 
words, a system based on PSTFS has many 
computational agents running on different pro
cessors in parallel; those agents communicate 
with each other by using messages including 
TFSs. Tasks of the whole system, such as pars
* This research is partially founded by the project of 
JSPS(JSPS-RFTF96P00502). 
\] / 
Figure 1: Agent-based System with the PSTFS 
ing or semantic processing, are divided into sev
eral pieces which can be simultaneously com
puted by several agents. 
Several parallel NLP systems have been de
veloped previously. But most of them have been 
neither efficient nor practical enough (Adriaens 
and Hahn, 1994). On the other hand, our 
PSTFS provides the following features. 
• An efficient communication scheme for 
messages including Typed Feature Struc
tures (TFSs)(Carpenter, 1992). 
• Efficient treatment of TFSs by an abstract 
machine (Makino et al., 1998). 
Another possible way to develop parallel NLP 
systems with TFSs is to use a full concurrent 
logic programming language (Clark and Gre
gory, 1986; Ueda, 1985). However, we have ob
served that it is necessary to control parallelism 
in a flexible way to achieve high-performance. 
(Fixed concurrency in a logic programming lan
guage does not provide sufficient flexibility.) 
Our agent-based architecture is suitable for ac
complishing such flexibility in parallelism. 
The next section discusses PSTFS from a pro
grammers' point of view. Section 3 describes 
the PSTFS architecture in detail. Section 4 de
scribes the performance of PSTFS on our HPSG 
parsers. 
968 
Constraint Solver Agent begin-definitions , fFIRS'P Franz 1, nttme~ \[LAST Schubert J )" ,WIRST Johann 1. nameL \[LAST Bach J )" 
concatenate_name(X, Y) 
\[LAST 2L2 j J' 
\[FULL ~\]~ N} 1 Y = I FIRST ~ /" 
/LAST 2L\] u J 
end-deflnltlonu 
(A) Description of CSAs 
AST Sch~be*'t \] ' 
kRsSTT Johann Bach \] ' 
 AST L2:L,, j' 
\]FULL 
3oh~n~ l , T B.hU°h .... Be,h> Jl 
:::} 
(C) VMues of F ~nd R 
define {Sontrol Agent name-co~catenatov-f~b 
When a message sslv,~(x) arrives, do the followings, 
S := CSA ~ lelv*-conltraint(concatenate_name(x, ?)); return ,5'; 
define (Jontrol Agent name-concatenalor When a message s01v* arrives, do the followings, 
/~:= 0; --.3::=0less ¢ ,,~, ...... tin,t( 
..... (:))); 
forall x E F do create ~avt~e. concafe~ator-s~b age~tf N'I; 
H, ~ tolv,(x);i := i+ 1; forallend 
for j := 0 to i do 
R := R U (Wait-Jot ..... lt(Afj)); 
forend return "R; 
(B) Description of CAs 
Figure 2: Examt)le concatenate_name 
2 Progranamers' View 
l:roln a progranmlers' point of view, tile PS'I'I?S 
n~e('hanism is quile simI)le and natural, which 
is d lte Io careful design for accomplishing high
l)erformance and ease of t)rogramlning. 
Systenls Io t)e constructed on our PSTFS will 
include two different tyt)es of agents: 
• Control Agents ((.:As) 
• Constraint Solver Agents (CSAs) 
As ilhlstrated in Figure 1, CAs have overall 
control of a system, including control of par
allelism, and they behave as masters of CSAs. 
CSAs modify TFSs according to the orders from 
CAs. Note that CAs can neither modify nor 
generate TFSs by themselves. 
PSTFS has been implemented by combin
ing two existing programming languages: the 
concurrent object-oriented programming lan
guage ABCL/f (Taura, 1997) and the sequential 
programming language LiLFeS (Makino et al., 
1998). CAs can be written in ABCL/f, while 
description of CSAs can be mainly written in 
LiLFeS. 
Figure 2 shows an example of a part of the 
PSTFS code. The task of this code is to con
catenate the first and the second name in a 
given list. One of the CAs is called name
concatenator. This specific CA gathers pairs of 
the first and last name by asking a CSA with the 
message so\]_ve-consgra:\].rt4:;(tna'nze(.'?)'). When 
the CSA receives this message, the argument 
'name(?)' is treated as a Prolog query in 
LiLFeS 1, according to the program of a CSA 
((A) of Figure 2). There are several facts with 
the predicate 'name'. When the goal 'name(?)' 
is processed by a CSA, all the possible answers 
defined by these facts are returned. The ob
tained pairs are stored in the variable F in the 
name-concatenator((C) in Figure 2). 
The next behavior of the name-concatenator 
agent is to create CAs (namc-concatenator
subs) and to send the message solve with a 
TFS to each created CA running in parallel. 
The message contains one of the TFSs in I". 
Each name-concatenator-sub asks a CSA to con
catenate FIRST and LAST in a TFS. Then 
each CSA concatenates them using the defi
nite clause concatenate_name given in (A) of 
Figure 2. The result is returned to the name
concatenator-sub which had asked to do tile job. 
Note that the name-coneatenator-sub can ask 
any of the existing CSAs. All CSAs can basi
cally perform concatenation in parallel and in
dependent way. Then, the name-cones~chafer 
waits for the name-concatenator-sub to return 
concatenated names, and puts the return val
ues into the variable R. 
The ('A name-com'atc,ator controls the over
all process. It controls parallelism by creating 
('As and s(,nding messages to them. On the 
other hand, all the el)orations Oil TFSs are per
formed by CSAs when they are asked by CAs. 
Supt)ose thai one is trying to implement a 
i)arsing system based on PSTFS. The distinc
tion between CAs att(t CSAs roughly corre
sponds to the distinction between an al)stract 
parsing schema and application of phrase struc
ture rules. Itere, a parsing schema means a 
high-level description of a parsing algorithm in 
which the application of phrase structure rules 
is regarded as an atomic operation or a sub
routine. This distinction is a minor factor in 
writing a sequential parser, but it has a major 
impact on a parallel environment. 
For instance, suppose that several distinct 
agents evoke applications of phrase structure 
rules against the same data simultaneously, and 
the applications are accompanied with destruc
tive operations on the data. This can cause an 
anomaly, since the agents will modify the orig
inal data in unpredictable order and there is 
no way to keep consistency. In order to avoid 
this anomaly, one has to determine what is an 
atomic operation and provide a method to pre
vent the anomMy when atomic operations are 
evoked by several agents. In our fi'amework, 
any action taken by CSAs is viewed as such 
an atomic operation and it is guaranteed that 
no anomaly occurs even if CSAs concurrently 
1LiLFeS suppor|s definite clause programs, it TFS 
version of ttorn chutses. 
969 
Local Heap 
.... i{:i :: !i?i{:~ :::~:~!:~ ':{:{ {i :}: { {: 
Shared Heap Area 
PSTFS 
Figure 3: Inside of the PSTFS 
perform operations on the same data. This 
can be done by introducing copying of TFSs, 
which does not require any destructive opera
tions. The details are described in the next sec
tion. 
The other implication of the distinction be
tween CAs and CSAs is that this enables effi
cient communication between agents in a natu
ral way. During parsing in HPSG, it is possible 
that TFSs with hundreds of nodes can be gen
erated. Encoding such TFSs in a message and 
sending them in an efficient way are not triv
ial. PSTFS provides a communication scheme 
that enables efficient sending/receiving of such 
TFSs. This becomes possible because of the 
distinction of agents. In other words, since (?As 
cannot modify a TFS, CAs do not have to have 
a real image of TFSs. When CSAs return the 
results of computations to CAs, the CSAs send 
only an ID of a TFS. Only when the ID is passed 
to other CSAs and they try to modify a TFS 
with the ID, the actual transfer of the TFS's 
real image occurs. Since the transfer is car
ried out only between CSAs, it can be directly 
performed using a low level representation of 
TFSs used in CSAs in an efficient manner. Note 
that if CAs were to modify TFSs directly, this 
scheme could not have been used. 
3 Architecture

This section explains the inner structure of 
PSTFS focusing on the execution mechanism of 
CSAs (See (Taura, 1997) for further detail on 
CAs). A CSA is implemented by modifying the 
abstract machine for TFSs (i.e., LiAM), origi
nally designed for executing LiLFeS (Makino et 
al., 1998). 
The important constraint in designing the ex
ecution mechanism for CSAs is that TFSs gen
erated by CSAs must be kept unmodified. This 
is because the TFSs must be used with several 
agents in parallel. If the TFS had been modi
fied by a CSA and if other agents did not know 
the fact, the expected results could not have 
been obtained. Note that unification, which is 
(i) Copying from shared heap 
-L0cal Heap 
a Shared Heap 
 iiiiiiiiiiiiiiiiiiiii S iiiiiiiiiiiii!ii? .... 
(ii) Computation on local heap 
:~'o: :~iiii!iiiiii:iiiiiiiii? ~' ~:!:i:il;ii!i:iiiiiiiiiiiiiiiiii!iiiiiii;ii:!ii? ~: .~i::iiiiiiiiii:iiiiii:iiiii! ~r ...... ~x ~iiiiiiiiiii!iiiiiiiiii\[ililil i;~iiiiiii;iii~iiii~;i;iiii~ii\[i7iiiiii;ii~iiiiiii~ii~iiiiiiii~iiiiiiiii~iii~7ii iiiiiiiii~iiiii;: 
R ~ @ Local Heap 
Shared Heap 
(iii) Write resulting TFSs to shared heap 
S 
~'~R~,.~j ~ Local Heap 
Figure d: Operation steps on PSTFS 
a major operation on TFSs, is a destructive op
eralion, and modifications are likely to occur 
while executing CSAs. Our execution mecha
nism handles this problem by letting CSAs copy 
TFSs generated by other CSAs at each time. 
Though this may not look like an efficient way 
at first glance, it has been performed efficiently 
by shared memory mechanisms and our copying 
methods. 
A CSA uses two different types of memory 
areas as its heap: 
• shared heap 
• local heap 
A local heap is used for temporary operations 
during the computation inside a CSA. A CSA 
cannot read/write local heap of other CSAs. A 
shared heap is used as a medium of commu
nication between CSAs, and it is realized on 
a shared memory. When a CSA completes a 
computation on TFSs, it writes the result on 
a shared heap. Since the shared heap can be 
read by any CSAs, each CSA can read the re
sult performed by any other CSAs. However, 
the portion of a shared heap that the CSA can 
write to is limited. Any other CSA cannot write 
on that portion. 
Next:, we look at the steps performed by a 
CSA when it is asked by CAs with a message. 
970 
Note that the message only contains the IDs of 
the TFSs as described in the previous section. 
The IDs are realized a~s pointers on the shared 
heap. 
1. Copy TFSs pointed at by the IDs in the 
message fi'om the shared heap to the local 
heap of the CSA. ((i)in Figure 4.) 
2. Process a query using LiAM and the local 
heap. ((ii)in Figure 4.) 
3. If a query has an answer, the result is 
copied to the portion of the shared heap 
writable by the CSA. Keep IDs on the 
copied TFSs. If there is no answer for the 
query, go to Step 5. ((iii) in Figure 4.) 
4. Evoke backtracking in LiAM and go to Step 
2. 
5. Send the message, including the kept IDs, 
back to the CA that had asked the task. 
Note that:, in step 3, the results of the compu
tation becomes readable by other CSAs. This 
procedure has the following desirable features. 
Simultaneous Copying An identical TFS on 
a shared hea I) can be (:opied by several 
('SAs simultaneously. This is due to our 
shared memory medmnism and the prop
erty of LiAM that copying does not have 
ally side-ef\['ect on TFSs 2. 
Simultaneous/Safe Writing CSAs can 
write on their own shared heap without the 
danger of accidental modification by other 
CSAs. 
Demand Driven Copying As described in 
the previous section, the transfer of real 
images of TFSs is performed only after the 
IDs of the TFSs reach to the CSAs requir
ing the TFSs. Redundant copying/sending 
of the TFSs' real image is reduced, and the 
transfer is performed efficiently by mecha
nisms originally provided by LiAM. 
With efficient data transfer in shared-memory 
machines, these features reduce the overhead of 
parallelization. 
Note that copying in the procedures makes 
it possible to support non-determinism in NLP 
systems. For instance, during parsing, interme
diate parse trees must be kept. In a chart pars
ing for a unification-based grammar, generated 
2Actually, this is not trivial. Copying in Step 3 nor
realizes TFSs and stores the TFSs into a continuous re
gion on a shared heap. TI'Ss stored in such a way can 
be copied without any side-effect. 
edges are kept untouched, and destructive oper
ations on the results nmst be done after copying 
them. The copying of TFSs in the above steps 
realizes such nmchanisms in a natural way, as it 
is designed for efficient support for data sharing 
and destructive operations on shared heaps by 
parallel agents. 
4 Applieation
and Performance 
Evaluation 
This section describes two difl%rent types of 
IIPSG parsers implemented on PSTFS. One is 
designed for our Japanese grammar and the al
gorithm is a parallel version of the CKY algo
rithm (Kasami, 1965). The other is a parser for 
an ALE-style Grammar (Carpenter and Penn, 
1994). The algorithms of both parsers are based 
on parallel parsing algorithms for CFG (Ni
nomiya et al., 1997; Nijholt, 1994; Grishman 
and Chitrao, 1988; Thompson, 1994). Descrip
tions of both parsers are concise. Both of them 
are written in less than 1,000 lines. This shows 
thal our I'STFS can be easily used. With the 
high i)erfornaance of the parsers, this shows the 
feasibi\]ily and tlexibility of our PSTFS. 
For sin@icily of discussion, we assume that 
Ill)S(; consists of' lexical entries and rule 
schemala, l,exical entries can be regarded as 
'l'l.Ss assigned to each word. A rule schema is 
a r,l-in the fi)rm of z -abe:.., where z.,./,.c 
are T li'Ss. 
4.1 Parallel
CKY-style HPSG Parsing 
Algorit hm 
A sequential CKY parser for CI"(; uses a data 
structure called a trianyular table. Let 1'} 4 de
note a cell in the triang(flar table. Each cell \]'},j 
has a set of the non-terminal symbols in CFG 
that can generate the word sequence from tile 
i + 1-th word to the j-th word in an input sen
tence. The sequential CKY algorithm computes 
each Fi,j according to a certain order. 
Our algorithm for a parallel CKY-style parser 
for HPSG computes each Fi,j in parallel. Note 
that b\]-,j contains TFSs covering the word se
quence fi'om the i + 1-th word to the j-th 
word, not non-terminMs. We consider only the 
rule schemata with a form of z --. ab where 
z,a,b are TFSs. Parsing is started by a CA 
called 7)ATtSgT4. PAT48gT~ creates cell-agents 
Ci,j(O <_ i < j < n) and distributes them to pro
cessors on a parallel machine (Figure 5). Each Ci, j 
computes F},j in parallel. More precisely, 
Ci,j(j i = 1) looks up a dictionary and obtains 
lexical entries. Ci,j(j i > 1) waits for the mes
sages including I'},k and Fk,j for all k(i < k < j) 
from other cell-agents. When Ci,j receives /'},k 
and I"k j for an arbitrary k, Ci,j computes rI'FSs 
1)3' aI)plying rule schemata to each meint)ers of 
971 
Figure 5: Correspondence between CKY matrix 
and agents: C,i,j correspond to the element of a 
CKY triangular matrix 
Fi,k and Fk,j. The computed TFSs are consid
ered to be mothers of members of Fi,k and Fk, 
and they are added to Fi,j. Note that these ap J
plications of rule schemata are done in parallel 
in several CSAs 3. Finally. when computation of 
\[},j (using Fi,~. and \[k-,j for all k(i < k < j)) is 
completed, Ci,j distributes Fi,j to other agents 
waiting for/~'/j. Parsing is completed when the 
COml)utation of \[o.,~ is completed. 
We have done a series of experiments on a 
shared-memory parallel machine. SUN Ultra 
Enterprise 10000 consisting of 64 nodes (each 
node is a 2,50 MHz UltraSparc) and 6 GByte 
shared memory. The corpus consists of 879 
random sentences from the EDR Japanese cor
pus written in Japanese (average length of sen
tences is 20.8) 4 . The grammar we used is an 
underspecified Japanese HPSG grammar (Mit
suishi et al., 1998) consisting of 6 ID-schemata 
and 39 lexical entries (assigned to functional 
words) and 41 lexical-entry-templates (assigned 
to parts of speech). This grammar has wide cov
erage and high accuracy for real-world texts s. 
Table 1 shows the result and comparison with 
a parser written in LiLFeS. Figure 6 shows 
its speed-up. From the Figure 6, we observe 
that the maximum speedup reaches up to 12.4 
times. The average parsing time is 85 msec per 
aCSAs cannot be added dynamically in our imple
mentation. So, to gain the maximum parallelism, we 
assigned a CSA to each processor. Each Ci.j asks the 
CSA on the same processor to apply rule schemata. 
4We chose 1000 random sentences from the EDR 
Japanese corpus, and the used 897 sentences are all the 
parsable sentences by the grammar. 
5This grammar can generate parse trees for 82% of 
10000 sentences from the EDR Japanese corpus and the 
dependency accuracy is 78%. 
Number of , Av~ of Parsing Time( .... ) 
Processors J PSTFS | LiLFeS 
\] 10$7 991 10 248 
20 138 30 i06 
40 93 50 85 
60 135 
Table h Average parsing time per sentence 
S~e~up 
14 
12 
10 
8 
8 
4 
2 
0 
J ,/./
/ 
5 i I 1 J 
10 20 30 40 50 
# of t0¢oces~or6 
Figure 6: Speed-up of parsing time on parallel 
CKY-style HPSG parser 
SOIl t on ce 6 . 
4.2 Chart-based Parallel HPSG 
Parsing Algorithm for ALE 
Grammar 
Next, we developed a parallel chart-based 
HPSG parser for an ALE-style grammar. The 
algorithm is based on a chart schema on which 
each agent throws active edges and inactive 
edges containing a TFS. When we regard the 
rule schemata as a set of rewriting rules in 
CFG, this algorithm is exactly the same as 
the Thompson's algorithm (Thompson, 1994) 
and similar to PAX (Matsumoto, 1987). The 
main difference between the chart-based parser 
and our CKY-style parser is that the ALE-style 
parser supports a n-branching tree. 
A parsing process is started by a CA called 
:P.AT~S£7~. It .creates word-position agents 
Pk(0 _< k _< n), distributes them to parallel 
processors and waits for them to complete their 
tasks. The role of the word-position agent 79k 
6Using 60 processors is worse than with 50 processors. Ill general, when the number of processes increases 
to near or more than the number of existing processors, 
context switch between processes occurs frequently on 
shared-memory parallel machines (many people can use 
the machines simultaneously). We believe tile cause for 
the inefficiency when using 60 processors lies in such con
text switchcs. 
972 
ences 
\[ 4 he er~'uades her to walk 
elttences 
~~a.~ y W tOl~ e tries to ties to 
| (2) \[ ~ho sees kim whom \] " " \] he tries to see wMks 
\[ (3) \] ~andy who sees kim who 
\[~ believes her to tend to walk walks 
Table 2: Test corpus for parallel ALE-style 
llPSG parser 
Short Length ~entences 
Number of Avg. of Parsing Time(reset) Processors PSTFS \[ LiLFeS I ALE 
1 l O 625160 125 1590 
20 156 30 127 
40 205 50 142 
60 170 
Lon~ Length 5entence~ 
Number of Avg. ol P&r~ing Time(msec~ Processors PSTFS 
I LiLFeS I ALE 
110 193013208 30867 389370 
20 2139 30 1776 
40 1841 50 1902 
60 2052 
is to collect edges adjacent to the position k. 
A word-position agent has its own active edges 
and inactive edges. An active edge is in the form 
(i,z ~ AoxB}, where A is a set of TFSs which 
have already been unified with an existing con
stituents, B is a set of TFSs which have not 
been unified yet, and x is the TFS which can be 
unified with the constituent in an inactive edge 
whose left-side is in position k. Inactive edges 
are in the form (k, x, j}, where k is the left-side 
position of th(, constituent x and j is the right
side position of the constituent x. That is, the 
set of all inactive edges whose left-side position 
is k are collected by T'-k. 
hi our algorithm, Pk. is always waiting for eF 
ther an active e(l,,'e or an inactive edge. and t)er
torlns the following 1)rocedure when receiving an 
edge. 
• When Pk receives an active edge (i,z -
el o xB), ~k preserve tire edge and tries to 
find the unifiable constituent with x from 
the set of inactive edges that 7)/~ has ah'eady 
received. If the unification succeeds, a new 
active edge {i,z ~ Ax o B} is created. If 
the dot in the new active edge reaches to 
the end of RHS (i.e. B = 0), a new inactive 
edge is created and is sent to Pi. Otherwise 
the new active edge is sent to 7)j. 
• When Tak receives an inactive edge (k, x, j), 
7~k preserves the edge and tries to find the 
unifiable constituent on the right side of 
the dot from the set of active edges that 
T'k has already received. If the unification 
succeeds, a new active edge {i, z ~ Ax oB) 
is created. If the dot in the new active edge 
reaches to the end of RHS (i.e. B = 0), a 
new inactive edge is created and is sent to 
7)/. Otherwise the new active edge is sent 
to ~Oj. 
As long as word-position-agents follow these 
behavior, they can run in parallel without any 
other restriction. 
We have done a series of experiments in the 
same machine settings as the experiments with 
Table 3: Average parsing time per sentence 
Speed-up 
t2 
10 
0 10 20 30 40 50 60 
# of Processors 
Figure 7: Speed-up of t)arsing time on chart
based paralM Ill S(, parser 
the CKY-style IIPSG parser. We measured 
both its speed up and real parsing time, and 
we compared our parallel parser with the ALE 
system and a sequential parser on LiLFeS. The 
grammar we used is a sample ItPSG grammar 
attached to ALE system r, which has 7 schemata 
and 62 lexical entries. The test corpus we 
used in this experiment is shown in the Table 
2. Results and comparison with other sequen
tial parsing systems are given in Table 3. Its 
speedup is shown in Figure 7. From the figure, 
we observe that the maximum speedup reaches 
up to 10.9 times and its parsing time is 1.776 
msec per sentence. 
4.3 Discussion

In both parsers, parsing time reaches a level 
required by reM-time applications, though we 
used computationally expensive grammar for
mMisms, i.e. IIPSG with reasonable coverage 
and accuracy. This shows the feasibility of our 
rThis sample granlingtr is converted to LiLFeS style 
half automatically. 
973 
Processor ID Processor Status 
=F V 
40 ~ = I 
="-----" =
. -. -o I .---~ -------__-
0 2 
616,12 616.14 616.16 616.1g 
\]..K< \] ~ccv 
P 
Figure 8: Processors status 
framework for the goM to provide a parallel pro
gramming environment for real-time NLP. In 
addition, our parallel HPSG parsers are con
siderably more efficient than other sequential 
IIPSG parsers. 
However, the speed-up is not proportional to 
the number of processors. We think that this is 
because the parallelism extracted in our parsing 
algorithm is not enough. Figure 8 shows the log 
of parsing Japanese sentences by the CKY-style 
parser. The black lines indicate when a proces
sor is busy. One can see thal many l)r<)cessors 
are Dequently idle. 
We think that this idle time does not sug
gest that parallel NLP systems are useless. On 
the contrary, this suggest; that parallel NLP sys
tems have many possibilities. If we introduce 
semantic processing for instance, overall pro
cessing time may not change because the idle 
time is used for semantic processing. Another 
possibility is the use of parallel NLP systems as 
a server. Even if we feed several sentences at a 
time, throughput will not change, because the 
idle time is used for parsing different sentences. 
5 Conclusion
and Future Work 
We described PSTFS, a substrate for parallel 
processing of typed feature structures. PSTFS 
serves as an efficient programming environment 
for implementing parallel NLP systems. We 
have shown the feasibility and flexibility of 
our PSTFS through the implementation of two 
HPSG parsers. 
For the future, we are considering the use of 
our HPSG parser on PSTFS for a speech recog
nition system, a NaturM Language Interface or 
Speech Machine Translation applications. 

References 

Adriaens and Hahn, editors. 1994. Parallel 
Natural Language Plvcessing. Ablex Publish
ing Corporation, New Jersey. 

Bob Carpenter and Gerald Penn. 1994. ALE 
2.0 user's guide. Technical report, Carnegie 
Mellon University Laboratory for Computa
tional Linguistics, Pittsburgh, PA. 

Bob Carpenter. 1992. The Logic of Typed Fea
ture Structures. Cambridge University Press, 
Cambridge, England. 

K. Clark and S. Gregory. 1986. Parlog: Parallel 
programming in logic. Journal of the A CM 
Transaction on Programming Languages and 
Systems, 8(1):1 49. 

Ralph Grishman and Mehesh Chitrao. 1988. 
Evaluation of a parallel chart parser. In Pro
ceedings of the second Conference on Applied 
Natural Language Processing, pages 71-76. 
Association for Computational Linguistics. 

T. Kasami. 1965. An efficient recognition and 
syntax algorithm for context-free languages. 
Technical Report AFCRL-65-758, Air Force 
Cambrige Research Lab., Bedford, Mass. 

Takaki Makino, Minoru Yoshida, Kentaro Tori
sawa, and Jun'ichi Tsujii. 1998. LiLFeS 
towards a practical HPSG parser. In 
COLING-A CL '98 Proceedings, August. 

Yuji Matsumoto. 1987. A parallel parsing sys
tem for natural language analysis. In Proceed
in(Is of 3rd International Conference on Logic 
l'ro(tramming, pages 396 409. 

Yutaka Mitsuishi, Kentaro Torisawa, and 
Jun'ichi Tsujii. 19.(t8. HPSG-style underspec
ifie(I ,lai)anese grammar with wide coverage. 
In C'OLING-A CL'98 Proceedings, August. 

Anton Nijholt, 1994. Parallel Natural Language 
Processing, chapter Parallel Approaches to 
Context-Free Language Parsing, pages 135 
167. Ablex Publishing Corporation. 

Takashi Ninomiya, Kentaro Torisawa, Kenjiro 
Taura, and Jun'ichi Tsujii. 1997. A par
allel cky parsing algorithm on large-scale 
distributed-memory parallel machines. In 
PACLING '97, pages 223-231, September. 

Kenjiro Taura. 1997. Efficient and Reusable 
Implementation of Fine-Grain Multithread
lug and Garbage Collection on Distributed
Memory Parallel Computers. Ph.D. thesis, 
Department of Information Sciencethe, Uni
versity of Tokyo. 

Henry S. Thompson, 1994. Parallel Natural 
Language Processing, chapter Parallel Parsers 
for Context-Free Grammars-Two Actual Im
plementations Comparesd, pages 168-187. 
Ablex Publishing Corporation. 

Kazunori Ueda. 1985. Guarded horn clauses. 
Technical Report TR-103, ICOT. 

