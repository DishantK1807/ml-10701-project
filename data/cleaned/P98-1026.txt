Separating Surface Order and Syntactic Relations in a Dependency Grammar Norbert BrSker Universitiit Stuttgart Azenbergstr.
12 D-70174 Stuttgart NOBI~IMS.
UNI-STUTTGART. DE Abstract This paper proposes decoupling the dependency tree from word order, such that surface ordering is not determined by traversing the dependency tree.
We develop the notion of a word order domain structure, which is linked but structurally dissimilar to the syntactic dependency tree.
The proposal results in a lexicalized, declarative, and formally precise description of word order; features which lack previous proposals for dependency grammars.
Contrary to other lexicalized approaches to word order, our proposal does not require lexical ambiguities for ordering alternatives.
1 Introduction
Recently, the concept of valency has gained considerable attention.
Not only do all linguistic theories refer to some reformulation of the traditional notion of valency (in the form of 0grid, subcategorization list, argument list, or extended domain of locality); there is a growing number of parsers based on binary relations between words (Eisner, 1997; Maruyama, 1990).
Given this interest in the valency concept, and the fact that word order is one of the main difference between phrase-structure based approaches (henceforth PSG) and dependency grammar (DG), it is valid to ask whether DG can capture word order phenomena without recourse to phrasal nodes, traces, slashed categories, etc.
A very early result on the weak generative equivalence of context-free grammars and DGs suggested that DGs are incapable of describing surface word order (Gaifman, 1965).
This result has recently been critizised to apply only to impoverished DGs which do not properly represent formally the expressivity of contemporary DG variants (Neuhaus & Br6ker, 1997).
Our position will be that dependency relations are motivated semantically (Tesni~re, 1959), and need not be projective (i.e., may cross if projected onto the surface ordering).
We argue for so-called word order domains, consisting of partially ordered sets of words and associated with nodes in the dependency tree.
These order domains constitute a tree defined by set inclusion, and surface word order is determined by traversing this tree.
A syntactic analysis therefor consists of two linked, but dissimilar trees.
Sec. 2 will briefly review approaches to word order in DG.
In Sec.
3, word order domains will be defined, and Sec.
4 introduces a modal logic to describe dependency structures.
Sec. 5 applies our approach to the German clause and Sec.
6 relates it to some PSG approaches.
2 Word
Order in DG A very brief characterization of DG is that it recognizes only lexical, not phrasal nodes, which are linked by directed, typed, binary relations to form a dependency tree (Tesni~re, 1959; Hudson, 1993).
The following overview of DG flavors shows that various mechanisms (global rules, general graphs, procedural means) are generally employed to lift the limitation of projectivity and discusses some shortcomings of these proposals.
Functional Generative Description (Sgall et al., 1986) assumes a language-independent underlying order, which is represented as a projective dependency tree.
This abstract representation of the sentence is mapped via ordering rules to the concrete surface realization.
Recently, Kruijff (1997) has given a categorialstyle formulation of these ordering rules.
He assumes associative categorial operators, permuting the arguments to yield the surface ordering.
One difference to our proposal is that 174 we argue for a representational account of word order (based on valid structures representing word order), eschewing the non-determinism introduced by unary operators; the second difference is the avoidance of an underlying structure~ which stratifies the theory and makes incremental processing difficult.
Meaning-Text Theory (Melc'fik, 1988) assumes seven strata of representation.
The rules mapping fi'om the unordered dependency trees of surface-syntactic representations onto the annotated lexeme sequences of deep-morphological representations include global ordering rules which allow discontinuities.
These rules have not yet been formally specified (Melc'~tk & Pertsov, 1987p.1870.
Word Grammar (WG, Hudson (1990)) is based on general graphs instead of trees.
The ordering of two linked words is specified together with their dependency relation, as in the proposition "object of verb follows it".
Extraction of, e.g., objects is analyzed by establishing an additional dependency called visitor between the verb and the extractee, which requires the reverse order, as in "visitor of verb precedes it".
This results in inconsistencies, since an extracted object must follow the verb (being its object) and at the same time precede it (being its visitor).
The approach compromises the semantic motivation of dependencies by adding purely order-induced dependencies.
WG is similar to our proposal in that it also distinguishes a propositional meta language describing the graph-based analysis structures.
Dependency Unification Grammar (DUG, Hellwig (1986)) defines a tree-like data structure for the representation of syntactic analyses.
'Using morphosyntactic features with special interpretations, a word defines abstract positions into which modifiers are mapped.
Partial orderings and even discontinuities can thus be described by allowing a modifier to occupy a position defined by some transitive head.
The approach requires that the parser interpretes several features specially, and it cannot restrict the scope of discontinuities.
Slot Grammar (McCord, 1990) employs a number of rule types, some of which are exclusively concerned with precedence.
So-called head/slot and slot/slot ordering rules describe the precedence in projective trees, referring to arbitrary predicates over head and modifiers.
Extractions (i.e., discontinuities) are merely handled by a mechanism built into the parser.
3 Word
Order Domains Summarizing the previous discussion, we require the following of a word order description for DG: • not to compromise the semantic motivation of dependencies, • to be able to restrict discontinuities to certain constructions and delimit their scope, • to be lexicalized without requiring lexical ambiguities for the representation of ordering alternatives, • to be declarative (i.e., independent of an analysis procedure), and • to be formally precise and consistent.
The subsequent definition of an order domain structure and its linking to the dependency tree satisify these requirements.
3.1 The
Order Domain Structure A word order domain is a set of words, generalizing the notion of positions in DUG.
The cardinality of an order domain may be restricted to at most one element, at least one element, or by conjunction to exactly one element.
Each word is associated with a sequence of order domains, one of which must contain the word itself, and each of these domains may require that its elements have certain features.
Order domains can be partially ordered based on set inclusion: If an order domain d contains word w (which is not associated with d), every word w ~ contained in a domain d ~ associated with w is also contained in d; therefor, d ~ C d for each d ~ associated with w.
This partial ordering induces a tree on order domains, which we call the order domain structure.
Take the example of German "Den Mann hat der Junge gesehen" ("the manAGe has the boyNoM seen").
Its dependency tree is shown in Fig.l, with word order domains indicated by dashed circles.
The finite verb, "hat", defines a sequence of domains, <dl, d2, d3>, which roughly correspond to the topological fields in the German main clause.
The nouns "Mann" 175 ',, subj_~.~_~.
", :d3',,' 'C.
"derJunge; '.ge~ehen.,,,,:.den Mann-., "'.
". ' Figure 1: Dependency Tree and Order Domains for "Den Mann hat der Junge gesehen" dl d,4 hat d 5 d 6 Mann Junge gesehen Figure 2: Order Domain Structure for "Den Mann hat der Junge gesehen" aud "Junge" and the participle "gesehen" each define one order domain (d4,cl5,d6, resp.).
Set inclusion gives rise to the domain structure in Fig.2, where the individual words are attached by dashed lines to their including domains (dl and d4 collapse, being identical).
1 3.2 Surface Ordering How is the surface order derived from an order domain structure?
First of all, the ordering of domains is inherited by their respective elements, i.e., "Mann" precedes (any element of) d2, '!hat" follows (any element of) dl, etc.
Ordering within a domain, e.g., of "hat" and d6, or d5 and d6, is based on precedence predicates (adapting the precedence predicates of WG).
There are two different types, one ordering a word w.r.t, any other element of the domain it is associated with (e.g., "hat" w.r.t, d6), and another ordering two modifiers, referring to the dependency relations they occupy (d5 and d6, referring to subj and vpart).
A verb like "hat" introduces two precedence predicates, requiring other words to follow itself and the participle to follow subject and object, resp.: 2 "hat" ~ (<.
A (vpart) >{subj,obj}) ~Note that in this case, we have not a single rooted tree, but rather an ordered sequence of trees (by virtue of ordering dl, d2, and d3) as domain structure.
In general, we assume the sentence period to govern the finite verb and to introduce a single domain for the complete sentence.
2For details of the notation, please refer to Sec.
4. Informally, the first conjunct is satisfied by any domain in which no word precedes "hat", and the second conjunct is satisfied by any domain in which no subject or object follows a participle.
The domain structure in Fig.2 satisfies these restrictions since nothing follows the participle, and because "den Mann" is not an element of d2, which contains "hat".
This is an important interaction of order domains and precedence predicates: Order domains define scopes for precedence predicates.
In this way, we take into account that dependency trees are flatter than PS-based ones 3 and avoid the formal inconsistencies noted above for WG.
3.3 Linking
Domain Structure and Dependency Tree Order domains easily extend to discontinuous dependencies.
Consider the non-projective tree in Fig.1.
Assuming that the finite verb governs the participle, no projective dependency between the object "den Mann" and the participle "gesehen" can be established.
We allow nonprojectivity by loosening the linking between dependency tree and domain structure: A modifier (e.g., "Mann") may not only be inserted into a domain associated with its direct head ("gesehen"), but also into a domain of a transitive head ("hat"), which we will call the positional head.
The possibility of inserting a word into a domain of some trausitive head raises the questions of how to require contiguity (as needed in most cases), and how to limit the distance between the governor and the modifier in the case of discontinuity.
From a descriptive viewpoint, the syntactic construction is often cited to determine the possibility and scope of discontinuities (Bhatt, 1990; Matthews, 1981).
In PSbased accounts, the construction is represented by phrasal categories, and extraction is limited by bounding nodes (e.g., Haegeman (1994), Becker et al.(1991)). In dependency-based accounts, the construction is represented by the dependency relation, which is typed or labelled to indicate constructional distinctions which are configurationally defined in PSG.
Given this correspondence, it is natural to employ dependencies in the description of discontinuities as fol3Note that each phrasal level in PS-based trees defines a scope for linear precedence rules, which only apply to sister nodes.
176 lows: For each modifier of a certain head, a set of dependency types is defined which may link the direct head and the positional head of the modifier ("gesehen" and "hat", resp.).
If this set is empty, both heads are identical and a contiguous attachment results.
The impossibility of extraction from, e.g., a finite verb phrase may follow from the fact that the dependency embedding finite verbs, propo, may not appear on any path between a direct and a positional head.
4 4 The Description Language This section sketches a logical language describing the dependency structure.
It is based on modal logic and owes much to work of Blackburn (1994).
As he argues, standard Kripke models can be regarded as directed graphs with node annotations.
We will use this interpretation to represent dependency structures.
Dependencies and the mapping from dependency tree to order domain structure are described by modal operators, while simple properties such as word class, features, and cardinality of order domains are described by modal propositions.
4.1 Model
Structures In the following, we assume a set of words, l/Y, ordered by a precedence relation, -<, a set of dependency types, T), a set of atomic feature values.4, and a set of word classes, C.
We define a family of dependency relations Rd C W × ~42, d E :D and for convenience abbreviate the union UdET~ Rd as R:D.
Def: A dependency tree is a tuple (W, Wr, R~, VA, Vc), where R~ forms a tree over VP rooted in Wr, VA : V~ ~ 2 A maps words to sets of features, and V¢ : 1/~ ~ C maps words to word classes.
Def: An order domain (over W) m is a set of words from ~) where VWl,W2,W3 E VV : (wl -< w2-<w3Awl EmAw3 Era) ~ w2 E m.
Def: An order domain structure (over W) f14 is a set of order domains where Vm, m ~ E .£4 : mMm ~ = OVm C m'Vm ~ C m.
4One review pointed out that some verbs may allow extractions, i.e., that this restriction is lexical, not universal.
This fact can easily be accomodated because the possibility of discontinuity (and the dependency types across which the modifier may be extracted) is described in the lexical entry of the verb.
In fact, a universal restriction could not even be stated because the treatment is completely lexicalized.
Def: A dependency structure T is a tuple (VV, Wr, R~, VA, Vc, .A4, VM > where (I,V, wr, Rz~, VA, VC> is a dependency tree, A4 is an order domain structure over ~V, and VAa : V~ ~ .All n maps words to order domain sequences.
Additionally, we require for a dependency structure four more conditions: (1) Each word w is contained in exactly one of the domains from V~(w), (2) all domains in V~(w) are pairwise disjoint, (3) each word (except w~) is contained in at least two domains, one of which is associated with a (transitive) head, and (4) the (partial) ordering of domains (as described by VM) is consistent with the precedence of the words contained in the domains (see (Brhker, 1997) for more details).
4.2 The
Language £:~ Fig.3 defines the logical language /:~ used to describe dependency structures.
Although they have been presented differently, they can easily be rewritten as (multimodal) Kripke models: The dependency relation Rd is represented as modality (d> and the mapping from a word to its ith order domain as modality o~.5 All other formulae denote properties of nodes, and can be formulated as unary predicates most evident for word class and feature assignment.
For the precedence predicates <.
and <~, there are inverses >.
and >~.
For presentation, the relation places C 142 x 142 has been introduced, which holds between two words iff the first argument is the positional head of the second argument.
A more elaborate definition of dependency structures and £~ defines two more dimensions, a feature graph mapped off the dependency tree much like the proposal of Blackburn (1994), and a conceptual representation based on terminological logic, linking content words with reference objects and dependencies with conceptual roles.
5 The
German Clause Traditionally, the German main clause is described using three topological fields; the initial and middle fields are separated by the finite (auxiliary) verb, and the middle and the 5The modality O~ can be viewed as an abbreviation of o~ O~, composed of a mapping from a word to its ith order domain and from that domain to all its elements.
177 Syntax (valid formulae) Semantics (satisfaction relation) c • £v, Vc • C T,w a • £v, Va • A T,w <d) ¢ • £v, Vd • 79,¢ • £v T,w <.
6 £v, T,w <~ •£9, V6c_79 $~ •£v, VTcD oi single • ED, Vi • $V,%4 o~filled • £D, Vi • ~V D~a•£D, Vi•$V,a•A ¢^¢ • £~, V¢,¢ • £v -~¢ 6.£~, V¢ • £v T, w T, w T, w T, w T, iv T, w T, w ~c ka <d) ¢ :¢* c = Yc(w) :¢* a e Y (w) :¢~ 3w' 6 142 : wRdW' A T, w' ~ ¢ :~ 3m • M : (V.~(w) = (...m)...
^Vw' • m : (w = w' Vw -< w')) :¢~ ~3w',w",w ''' • W : places(w',w) Aplaces(w', w") A w'" R6w A w'" "< w $6 :¢~ 3w',w" • ~42 : wRvwA places(w", w) A w" R;w' o !{ w'• (,,,11^ \] o~single :¢t, w' ~Bw" : (w"RT)w'n,, < 1 w" e k obfilled I 1 t:3~a :¢* Vw' • Oi(V.M(w)) : T,w' k a ~¢A¢ :¢~,T,w~¢andT, w~¢ --,¢ :¢¢, not T, w ~ ¢ Figure 3: Syntax and Semantics of Ev Formulae Vfin ~ ol(single A filled) A OLinitial \[1\] A O L (middle A norel) \[2\] A 0 3 single A D L (final A norel) \[3\] A V2 ¢~ (middleA <, A\[3~norel) \[4\] A VEnd ¢~ (middleA >,) \[5\] A Vl ¢~ (initial A norel) \[6\] Figure 4: Domain Description of finite verbs "hat" A Vfin \[7\] A (subj)("Junge" A 1"0) \[8\] A(vpart) ("gesehen" A S0 \[9\] A ~final A >{subj,obj} \[i0\] A (obj)("Mann" A tvpart})) \[11\] Figure 5: Hierachical Structure final fields by infinite verb parts such as separable prefixes or participles.
We will generalize this field structure to verb-initial and verb-final clauses as well, without going into the linguistic motivation due to space limits.
The formula in Fig.4 states that all finite verbs (word class Vfin 6 C) define three order domains, of which the first requires exactly one element with the feature initial \[1\], the second allows an unspecified number of elements with features middle and norel \[2\], and the third allows at most one element with features final and norel \[3\].
The features initial, middle, and final 6 .4 serve to restrict placement of certain phrases in specific fields; e.g., no reflexive pronouns can appear in the final field.
The norel 6 .4 feature controls placement of a relative NP or PP, which may appear in the initial field only in verb-final clauses.
The order types are defined as follows: In a verb-second clause (feature V2), the verb is placed at the beginning (<).
of the middle field (middle), and the element of the initial field cannot be a relative phrase (o~norel in \[4\]).
In a verb-final clause (VEnd), the verb is placed at the end (>).
of the middle field, with no restrictions for the initial field (relative clauses and non-relative verb-final clauses are subordinated to the noun and conjunction, resp).
\[5\]. In a verb-initial clause (Vl), the verb occupies the initial field \[6\].
The formula in Fig.5 encodes the hierarchical structure from Fig.1 and contains lexical restrictions on placement and extraction (the surface is used to identify the word).
Given this, the order type of "hat" is determined as follows: The participle may not be extraposed (~final in \[10\]; a restriction from the lexical entry of "hat"), it must follow "hat" in d2.
Thus, the verb cannot be of order type VEnd, which would require it to be the last element in its domain (>.
in \[5\]).
"Mann" is not adjacent to "gesehen", but may be extracted across the dependency vpart (${vpart} in \[11\]), allowing its insertion into a domain defined by "hat".
It cannot precede "hat" in d2, because "hat" must either begin d2 (due to <.
in \[4\]) or itself go into dl.
But dl allows only one phrase (single), leaving only the domain structure from Fig.2, and thus the order type V2 for "hat".
178 6 Comparison to PSG Approaches One feature of word order domains is that they factor ordering alternatives from the syntactic tree, much like feature annotations do for morphological alternatives.
Other lexicalized grammars collapse syntactic and ordering information and are forced to represent ordering alternatives by lexical ambiguity, most notable L-TAG (Schabes et al., 1988) and some versions of CG (Hepple, 1994).
This is not necessary in our approach, which drastically reduces the search space for parsing.
This property is shared by the proposal of Reape (1993) to associate HPSG signs with sequences of constituents, also called word order domains.
Surface ordering is determined by the sequence of constituents associated with the root node.
The order domain of a mother node is the sequence union of the order domains of the daughter nodes, which means that the relative order of elements in an order domain is retained, but material from several domains may be interleaved, resulting in discontinuities.
Whether an order domain allows interleaving with other domains is a parameter of the constituent.
This approach is very similar to ours in that order domains separate word order from the syntactic tree, but there is one important difference: Word order domains in HPSG do not completely free the hierarchical structure from ordering considerations, because discontinuity is specified per phrase, not per modifier.
For example, two projections are required for an NP, the lower one for the continuous material (determiner, adjective, noun, genitival and prepositional attributes) and the higher one for the possibly discontinuous relative clause.
This dependence of hierarchical structure on ordering is absent from our proposal.
We may also compare our approach with the projection architecture of LFG (Kaplan & Bresnan, 1982; Kaplan, 1995).
There is a close similarity of the LFG projections (c-structure and f-structure) to the dimensions used here (order domain structure and dependency tree, respectively).
C-structure and order domains represent surface ordering, whereas f-structure and dependency tree show the subcategorization or valence requirements.
What is more, these projections or dimensions are linked in both accounts by'an element-wise mapping.
The difference between the two architectures lies in the linkage of the projections or dimensions: LFG maps f-structure off c-structure.
In contrast, the dependency relation is taken to be primitive here, and ordering restrictions are taken to be indicators or consequences of dependency relations (see also BrSker (1998b, 1998a)).
7 Conclusion
We have presented an approach to word order for DG which combines traditional notions (semantically motivated dependencies, topological fields) with contemporary techniques (logical description language, model-theoretic semantics).
Word order domains are sets of partially ordered words associated with words.
A word is contained in an order domain of its head, or may float into an order domain of a transitive head, resulting in a discontinuous dependency tree while retaining a projective order domain structure.
Restrictions on the floating are expressed in a lexicalized fashion in terms of dependency relations.
An important benefit is that the proposal is lexicalized without reverting to lexical ambiguity to represent order variation, thus profiting even more from the efficiency considerations discussed by Schabes et al.(1988). It is not yet clear what the generative capacity of such lexicalized discontinuous DGs is, but at least some index languages (such as anbnc n) can be characterized.
Neuhaus & BrSker (1997) have shown that recognition and parsing of such grammars is A/'7~-complete.
A parser operating on the model structures is described in (Hahn et al., 1997).
References Becket, T., A.
Joshi & O.
Rainbow (1991).
LongDistance scrambling and tree-adjoining grammar.
In Proc.
5th Conf.
of the European Chapter of the ACL, pp.
21-26. Bhatt, C.
(1990). Die syntaktische Struktur der Nominalphrase im Deutschen.
Studien zur deutschen Grammatik 38.
Tfibingen: Narr.
Blackburn, P.
(1994). Structures, Languages and Translations: The Structural Approach to Feature Logic.
In C.
Rupp, M.
Rosner & R.
Johnson (Eds.), Constraints, Language and Computation, pp.
1-27. London: Academic Press.
BrSker, N.
(1997). Eine Dependenzgrammatik zur Kopplung heterogener Wissenssysteme auf modallogischer Basis.
Dissertation, Deutsches Seminar, Universit~it Freiburg.
BrSker, N.
(1998a). How to define a context-free backbone for DGs: An experiment in grammar conversion.
In Proc.
o\] the COLINGA CL'98 workshop "Processing of Dependencybased Grammars".
Montreal/CAN, Aug 15, 1998.
BrSker, N.
(1998b). A Projection Architecture for Dependency Grammar and How it Compares to LFG.
In Proc.
1998 Int'l Lexical-Functional Grammar Conference.
(accepted as alternate paper) Brisbane/AUS: Jun 30-Jul 2, 1998.
Eisner, J.
(1997). Bilexical Grammars and a CubicTime Probabilistic Parser.
In Proc.
of Int'l Workshop on Parsing Technologies, pp.
54-65. Boston/MA: MIT.
Gaifman, H.
(1965). Dependency Systems and Phrase Structure Systems.
Information and Control, 8:304-337.
Haegeman, L.
(1994). Introduction to Government and Binding.
Oxford/UK: Basil Blackwell.
Hahn, U., P.
Neuhaus & N.
BrSker (1997).
MessagePassing Protocols for Real-World Parsing An Object-Oriented Model and its Preliminary Evaluation.
In Proc.
Int'l Workshop on Parsing Technology, pp.
101-112. Boston/MA: MIT, Sep 17-21, 1997.
Hellwig, P.
(1986). Dependency Unification Grammar.
In Proc.
I1th Int'l Conf.
on Computational Linguistics, pp.
195-198. Hepple, M.
(1994). Discontinuity and the Lambek Calculus.
In Proc.
15th Int'l Conf.
on Computational Linguistics, pp.
1235-1239. Kyoto/JP.
Hudson, R.
(1990). English Word Grammar.
Oxford/UK: Basil Blackwell.
Hudson, R.
(1993). Recent developments in dependency theory.
In J.
Jacobs, A.
v. Stechow, W.
Sternefeld & T.
Vennemann (Eds.), Syntax.
Ein internationales Handbuch zeitgenSssischer Forsehung, pp.
329-338. Berlin: Walter de Gruyter.
Kaplan, R.
(1995). The formal architecture of Lexical-Functional Grammar.
In M.
Dalrymple, R.
Kaplan, J.
I. Maxwell &: A.
Zaenen (Eds.), Formal Issues in Lexical-Functional Grammar, pp.
7-27. Stanford University.
Kaplan, R.
& J.
Bresnan (1982).
Lexical-Functional Grammar: A Formal System for Grammatical Representation.
In J.
Bresnan & R.
Kaplan (Eds.), The Mental Representation of Grammatical Relations, pp.
173-281. Cambridge, MA: MIT Press.
Kruijff, G.-J.
v. (1997).
A Basic Dependency-Based Logical Grammar.
Draft Manuscript.
Prague: Charles University.
Maruyama, H.
(1990). Structural Disambiguation with Constraint Propagation.
In Proc.
28th Annual Meeting of the ACL, pp.
31-38. Pittsburgh/PA.
Matthews, P.
(1981). Syntax.
Cambridge Textbooks in Linguistics, Cambridge/UK: Cambridge Univ.
Press. McCord, M.
(1990). Slot Grammar: A System for Simpler Construction of Practical Natural Language Grammars.
In R.
Studer (Ed.), Natural Language and Logic, pp.
118-145. Berlin, Heidelberg: Springer.
Melc'hk, I.
(1988). Dependency Syntax: Theory and Practice.
Albany/NY: State Univ.
Press of New York.
Melc'hk, I.
& N.
Pertsov (1987).
Surlace Syntax of English: A Formal Model within the MTT Framework.
Philadelphia/PA: John Benjamins.
Neuhaus, P.
&: N.
BrSker (1997).
The Complexity of Recognition of Linguistically Adequate Dependency Grammars.
In Proc.
35th Annual Meeting of the A CL and 8th Conf.
of the EA CL, pp.
337-343. Madrid, July 7-12, 1997.
Reape, M.
(1993). A Formal Theory of Word Order: A Case Study in West Germanic.
Doctoral Dissertation.
Univ. of Edinburg.
Schabes, Y., A.
Abeille & A.
Joshi (1988).
Parsing Strategies with 'Lexicalized' Grammars: Application to TAGs.
In Proc.
12th Int'l Con\].
on Computational Linguistics, pp.
578-583. Sgall, P., E.
Hajicova & J.
Panevova (1986).
The Meaning of the Sentence in its Semantic and Pragmatic Aspects.
Dordrecht/NL: D.Reidel.
Tesni&e, L.
(1959). Elemdnts de syntaxe structurale.
Paris: Klincksiek .

