<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Delphine Cl´ement</author>
<author>Brigitte Laboisse</author>
</authors>
<title>Cr´eation d’un r´ef´erentiel d’indicateurs de mesure de la qualit´e des donn´ees CRM</title>
<date>2007</date>
<booktitle>In Actes du 3e atelier Qualit´e des Donn´ees et des Connaissances</booktitle>
<pages>5--14</pages>
<location>Namur, Belgique</location>
<marker>Cl´ement, Laboisse, 2007</marker>
<rawString>Delphine Cl´ement and Brigitte Laboisse. 2007. Cr´eation d’un r´ef´erentiel d’indicateurs de mesure de la qualit´e des donn´ees CRM. In Actes du 3e atelier Qualit´e des Donn´ees et des Connaissances, pages 5–14, Namur, Belgique. En conjonction avec EGC 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William H Fletcher</author>
</authors>
<title>Toward cleaner Web corpora: recognizing and repairing problems with hybrid online documents</title>
<date>2007</date>
<journal>Corpus Linguistics</journal>
<volume>27</volume>
<location>Birmingham</location>
<contexts>
<context>good usage rules. This method presents the advantage of quickly obtaining good results of typographical rewriting but it remains strongly related to the way in which the text was written and encoded (Fletcher, 2007). We will reconsider the case of certain typographical ambiguities which cannot be raised. 4.2. Orthographical corrections 4.2.1. Basic modules We correct the orthographical errors by connecting seve</context>
</contexts>
<marker>Fletcher, 2007</marker>
<rawString>William H. Fletcher. 2007. Toward cleaner Web corpora: recognizing and repairing problems with hybrid online documents. Corpus Linguistics 2007, Birmingham, 27– 30 July. Benoˆıt Habert, Adeline Nazarenko, and Andr´e Salem.</rawString>
</citation>
<citation valid="true">
<title>Les linguistiques de corpus. Armand Colin/Masson</title>
<date>1997</date>
<location>Paris</location>
<marker>1997</marker>
<rawString>1997. Les linguistiques de corpus. Armand Colin/Masson, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoˆıt Habert</author>
<author>C Fabre</author>
<author>F Issac</author>
</authors>
<title>De l’´ecrit au num´erique : constituer, normaliser et exploiter les corpus ´electroniques. Masson ´Editeur/Inter´editions</title>
<date>1998</date>
<location>Paris</location>
<contexts>
<context> access to the data, the NLP community is provided with a set of powerful tools which make it possible to handle these corpora, whatever the linguistic field concerned and the objective of the study (Habert et al., 1998). All the corpora we can obtain over the Internet do not necessarily offer a basic quality sufficient for textual analysis. The corpora generally include a variable number of errors – these errors ca</context>
</contexts>
<marker>Habert, Fabre, Issac, 1998</marker>
<rawString>Benoˆıt Habert, C. Fabre, and F. Issac. 1998. De l’´ecrit au num´erique : constituer, normaliser et exploiter les corpus ´electroniques. Masson ´Editeur/Inter´editions, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
<author>David St-Onge</author>
</authors>
<title>Lexical chains as representations of context for the detection and correction of malapropisms</title>
<date>1998</date>
<booktitle>In Christiane Fellbaum, editor, WordNet: an electronic lexical database, Language, Speech and Communication, chapter 13</booktitle>
<pages>305--332</pages>
<publisher>The MIT Press</publisher>
<location>Cambridge, Massachusetts</location>
<contexts>
<context> to correct: the case of the words which are correct on the three previous levels but which do not make sense in the context of the analyzed sentence. This kind of errors is called “malapropisms” by (Hirst and St-Onge, 1998) who developed a method based upon the synset in WordNet for detecting these malapropisms (an ingenuous machine instead of an ingenious machine). This Figure 7: Interface validation of the automatic </context>
</contexts>
<marker>Hirst, St-Onge, 1998</marker>
<rawString>Graeme Hirst and David St-Onge. 1998. Lexical chains as representations of context for the detection and correction of malapropisms. In Christiane Fellbaum, editor, WordNet: an electronic lexical database, Language, Speech and Communication, chapter 13, pages 305–332. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Hofmann</author>
<author>Wouter Weerkamp</author>
</authors>
<title>Web Corpus Cleaning using Content and Structure. Cahiers du Cental</title>
<date>2007</date>
<contexts>
<context>ra generally include a variable number of errors – these errors can be of typographical, orthographical, or syntactic nature – which can generate noise when using automatic language processing tools (Hofmann and Weerkamp, 2007). The problem of the quality of the data rises up in the industrial world, in particular in marketing services where cleaning data bases is relevant in order to improve the client relationship and to</context>
</contexts>
<marker>Hofmann, Weerkamp, 2007</marker>
<rawString>Katja Hofmann and Wouter Weerkamp. 2007. Web Corpus Cleaning using Content and Structure. Cahiers du Cental, 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Imprimerie nationale</author>
<author>Paris</author>
</authors>
<title>Lexique des r`egles typographiques en usage `a l’Imprimerie nationale, 3`eme edition</title>
<date>2004</date>
<pages>mai.</pages>
<marker>nationale, Paris, 2004</marker>
<rawString>Imprimerie nationale, Paris, 2004. Lexique des r`egles typographiques en usage `a l’Imprimerie nationale, 3`eme edition, mai.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Levenshtein</author>
</authors>
<title>Binary codes capable of correction deletions, insertions and reversals</title>
<date>1965</date>
<booktitle>Doklady Akademii Nauk SSSR</booktitle>
<pages>163--4</pages>
<contexts>
<context>s (the error gourvernement is corrected in gouvernement); • Calculation of distance between two textual chains founded on the Levenshtein’s distance. 4.2.2. Levenshtein distance Levenshtein distance (Levenshtein, 1965) is a measure of similarity between two strings. This measure is based upon the number of operations (deletion, insertion, substitution) needed to transform a string into another string. OPERATION EX</context>
</contexts>
<marker>Levenshtein, 1965</marker>
<rawString>Vladimir Levenshtein. 1965. Binary codes capable of correction deletions, insertions and reversals. Doklady Akademii Nauk SSSR, 163(4):845–848.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Mitton</author>
</authors>
<title>English spelling and the computer. Birkbeck ePrints, London. Available at</title>
<date>1996</date>
<pages>00000469</pages>
<contexts>
<context> depends on two factors: on the one hand, the type of error we want to underscore, and on the other hand, the type of resources we can obtain for producing data reference. 2.1. Orthographical errors (Mitton, 1996) explained that the study of orthographical errors consists in checking, for each word from a text, that this word exists in the language. Two methods allow us to check the spelling of a word: the fi</context>
<context>ist for detecting syntactic errors: the first one is based upon the comparison of possible syntactic tag combinations, the second one is based upon the set of the syntactic rules used in a language. (Mitton, 1996) underlined that these two types of analysis tend to send back a much too high number of false errors. We will see that a third method exists, based upon n-grams of words, which, however could not be</context>
<context>ntactic errors. 2.2.2. Method based upon rules An other method to detect syntactic errors is based upon the language rules. This method consists in applying syntactic rules on the corpus to analyze. (Mitton, 1996) recommends, in case of analysis failure, to make another analysis, making the rule more permissive until the analysis be completed. The impossibility of an analysis implies a syntactic error. This m</context>
</contexts>
<marker>Mitton, 1996</marker>
<rawString>Roger Mitton. 1996. English spelling and the computer. Birkbeck ePrints, London. Available at: http://eprints.bbk.ac.uk/archive/00000469.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guillaume Pinot</author>
</authors>
<title>Correction orthographique en contexte</title>
<date>2005</date>
<tech>Technical report, LINA. Rapport</tech>
<note>de stage</note>
<contexts>
<context> agreement were more frequent (96,9%) than the errors of gender agreement (3,1%). We produced a syntactic module of correction based upon a contextual study of the vicinity of the words between them (Pinot, 2005). We thus test the treating of any word in the plural, when such words follow a determiner (in fact, a list of articles or prepositions contracted in the plural aux, les, des). However, let us specif</context>
</contexts>
<marker>Pinot, 2005</marker>
<rawString>Guillaume Pinot. 2005. Correction orthographique en contexte. Technical report, LINA. Rapport de stage.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Ringlstetter</author>
<author>Klaus U Schulz</author>
<author>Stoyan Mihov</author>
</authors>
<title>Orthographic Errors in Web Pages: Toward Cleaner Web Corpora</title>
<date>2006</date>
<journal>Computational Linguistics</journal>
<volume>32</volume>
<contexts>
<context>t include less errors; • To apply automatic correction processing to the corpus; • To eliminate this corpus to choose a cleaner corpus. Within the framework of corpus constitution from the Internet, (Ringlstetter et al., 2006) proposed a threshold of 5x19 of errors (that is to say a maximum of 5 errors for 1000 elements considered) as acceptable threshold of conservation of a corpus. We insist on the fact that it is impor</context>
<context>ifficult to automatically correct. A certification without taking into account the upper case begining words do not provide an error rate (5.18%) lower than the defined threshold. In accordance with (Ringlstetter et al., 2006), the threshold of 5x19 seems to be a good limit as long as we want to keep a corpus of quality. 4. Automatic correction The methods of certification of corpus, because they make it possible to highl</context>
</contexts>
<marker>Ringlstetter, Schulz, Mihov, 2006</marker>
<rawString>Christoph Ringlstetter, Klaus U. Schulz, and Stoyan Mihov. 2006. Orthographic Errors in Web Pages: Toward Cleaner Web Corpora. Computational Linguistics, 32(3):295–340, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurent Romary</author>
<author>Susanne Salmon-Alt</author>
<author>Gil Francopoulo</author>
</authors>
<title>Standards going concret: from LMF to Morphalou</title>
<date>2004</date>
<booktitle>In Workshop on Electronic Dictionaries, Coling 2004</booktitle>
<location>Geneva</location>
<contexts>
<context> Figure 1: Number of entries in some French language dictionaries. Some electronical dictionaries do not only present lemmas but also the inflected forms. The lexical called “Morphalou”, realized by (Romary et al., 2004), contains 95 810 lemmas and 524 725 inflected forms while the monolingual dictionaries realized within the project EuRADic1 contains such lemmas and inflected forms for 5 european languages. Each le</context>
</contexts>
<marker>Romary, Salmon-Alt, Francopoulo, 2004</marker>
<rawString>Laurent Romary, Susanne Salmon-Alt, and Gil Francopoulo. 2004. Standards going concret: from LMF to Morphalou. In Workshop on Electronic Dictionaries, Coling 2004, Geneva.</rawString>
</citation>
</citationList>
</algorithm>

