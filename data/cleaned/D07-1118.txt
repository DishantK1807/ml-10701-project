Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp.
1103–1111, Prague, June 2007.
c©2007 Association for Computational Linguistics Building Domain-Specific Tagers without Anotated (Domain) Data John E.
Miler 1 Manabu Tori 2 K.
Vijay-Shanker 1 1 Computer & Information Sciences University of Delaware Newark, DE 19716 {jmiller,vijay}@cis.udel.edu 2 Biostatistics, Bioinformatics and Biomathematics Georgetown University Medical Center Washington, DC 20057 mt352@georgetown.edu Abstract Part of spech taging is a fundamental component in many NLP systems.
When tagers developed in one domain are used in another domain, the perforance can degrade considerably.
We present a method for developing tagers for new doains without requiring POS anotated text in the ne domain.
Our method involves using raw doain text and identifying related ords to form a domain specific lexicon.
This lexicon provides the initial lexical probabilities for EM trainig of an HM model.
We evaluate the method by aplying it in the Biolgy doain and show that we achieve results that are comparable ith some tagers developed for this domain.
1 Introduction
As Natural Language Procesing (NLP) technology advances and more text becomes available, it is being aplied ore and often in specialized domains.
Part of Spech (POS) taging is often a fundamental component to these NLP aplications and hence its acuracy can have a significant impact on the aplication’s suces.
The suces that the tagers have atained is often not replicated when the domain is changed.
Degradation of acuracy in a new domain can be overcome by developing an anotated corpus for that specific domain, e.g., as in the Biolgy domain.
However, this solution is feasible only if there is suficient interest in the use of NLP technolgy in that domain, and there are suficient funding and resources.
In contrast, our aproach is to use xisting resources, and rapidly develop tagers for new domains without using the time and efort to develop anotated at.
In this work, we use the Wal Stret Journal (WSJ) corpus (Marcus et al, 193) and large amounts of domain-specific raw text to develop tagers.
We evaluate our methodolgy in the Biology domain and show the resulting performance is competitive with some tagers built with supervised learnig for that domain.
Also, e note that the acuracy of tagers trained on the WSJ corpus drops of considerably when aplied to this domain.
Smith et al.(205) report that the Bril tager (195) has an acuracy of 86.8% on 100 sentences taken from Medline, and that the Xerox tagger (Cuting et al.192) has an acuracy of 93.1% on the same sentences.
They atribute this drop of to the fact that only 57.8% of the 10,0 most frequent words can be found in WSJ corpus.
This observation provides further impetus to developing lexicon for tagers in the new domains.
In the next section, we discus our general approach.
The details of the EM trainig of the HM tager are given in Section 3.
Section 4 provides details of how a domain specific lexicon is created.
Next, we discus the evaluation of our models and analysis based on the results.
Section 6 discuses related work and those works from which we have taken some ideas.
Section 7 has soe concluding remarks.
2 Basic
Methodology Inadequate treatment of domain-specific vocabulary is often the primary cause in the degradation of performance when a tager trained in one genre of text is ported to a new domain.
The significance of out-of-vocabulary ords has ben noted in reduced acuracy of NLP components in the Biolgy 1103 domain (e.g., Lease and Charniak, 205; Smith et al.204). The handling of domain-specific vocabulary is the focus of our aproach.
It is quite comon to use sufix information in the prediction of POS tags for ocurences of new words.
However, its efectivenes may be limited in English, hich is not a highly inflected language.
However, even for English, we find that not only can sufix information be used online during tagging, but also the presence or absence of morphologicaly related words can provide considerable information to pre-build a lexicon that asociates posible tags with words.
Consider the example of the word “broaden”.
While the sufix “en” ay be utilized to predict the likelihod of verbal tags (VB and VBP) for the word uring taging, if we were to build a lexicon ofline, the existence of the ords “broadened”, “broadenig”, “broadens” and “broad” give further evidence to treat “broaden” as a verb.
This type of information has ben used before in (Cucerzan and Yarowsky, 200).
In the above example, the presence or absence of words with the sufix morphemes sugests POS tag information in two ways: 1) The presence of a sufix orpheme in a ord sugests a POS tag or a smal set of POS tags for the word.
This is the type of information most tagers use to predict tags for unknown words during the taging proces; 2) The presence of the morpheme can also indicate posible tags for the words it ataches to.
For example, the derivational morpheme “ment” indicates “government” is likely to be an N and also that the word it ataches to, “govern” is likely to be a verb.
Inflectional and derivational morphemes don’t attach to words of just any POS category; they are particular.
Thus, we can propose the posibility of J (adjective) to “broad” and VB or VBP to “govern” (based on the fact the derivational morphemes “en” and “ment” atach to them) even though by themselves they don’t have any sufix information that ight be indicative of J and VB or VBP.
Aditional sufixes (that may or may not be taken from a standard list of English inflectional and derivational morphemes) can also be used.
As an example, the sufix “ate” can be asociated with a smal set of tags: VB or VBP (“educate”, “create”), J (“adequate”, “apropriate”), and N (“candiate”, “climate”).
Note the posibility or imposibility of the adition of “tion” and “ly” can help distinguish betwen the verbal and adjectival situations.
In contrast, most tagers that use just sufix information during the taging proces wil ned strong contextual information (i.e., tags of nearby words) in making their prediction for each ocurence, as such sufixes can be asociated with multiple tags.
To utilize such information, we ned a dictionary of words in the doain for hich we are interested in building a tager.
Such a dictionary wil alow us to propose posible tags for a domain word such as “phosphorylate”.
If we can verify hether words like “phosphorylation”, “phosphorylates”, and “phosphorylately,” are available in the domain then we can obtain considerable information regarding the posible tags that can be asociated with “phosphorylate”.
But we canot asume the availability of a dictionary of ords in the doain.
However, it would sufice to have a large text corpus, hich e cal Text-Lex.
We use it as a proxy for a domain dictionary by obtainig a list of words and their relative frequency of appearance in the domain.
Rather than using anualy developed rules that asign posible tags for words based on the presence or absence of related ords, we wish to aply a more empirical methodolgy.
Since this sort of inforation is specific to a language rather than a domain, we can use an anotated corpus in another doain to provide exemplars.
We use the WSJ (Marcus et al.193) corpus, a POS anotated corpus, for this purpose.
For example, we can se that “phosphorylate” in the Biolgy domain and “create” in the WSJ corpus are similar in the sense both take on “tion”, “ed”, and “ing” sufixes but not “ly” for instance.
Since the WSJ corpus would provide POS tag information for “create”, e can use it to inform us for “phosphorylate”.
The above ethod forms the basis for our determination of the set of tags that are to be asociated with the domain words.
However, the actual tag to be asigned for an ocurence in text depends on the context of use.
We capture this information by using a first-order HM tager odel.
For the transitional probabilities, we begin by using WSJ-based probabilities as a starting point and then adjust to the new domain by using a domain specific text and using EM trainig.
EM also alows for adjusting lexical probabilities derived using WSJ words as exemplars.
We cal the domain specific text used for trainig of our HM tager as Text-EM.
While this could be the same 1104 as Text-Lex, we distinguish the two since Text-EM could be smaler than Text-Lex.
From Text-Lex, we only extract a list of words and their frequency of ocurences.
In contrast, we use Text-EM as a text and hence as a sequence of ords.
In this work, the set of sufixes that we use is adapted those found in a GRE preparation webpage (DeForest, 200).
A few aditional sufixes were obtained from the online English Dictionary AlWords.com (205).
In the futre we expect to consider autoatic minig of useful sufixes from a domain.
Furtherore, prefixes are also useful for our purposes.
However apart from a few prefixes used in hyphenated ords, we haven’t yet incorporated prefix information in a systematic way into our framework.
In this paper, our evaluation domain is molecular biolgy.
Large amounts of text are easily available in the form of Medline abstracts.
We use only about 1% of the edline text datbase for TextLex.
Another reason for selcting this evaluation domain is that we have a considerable amount POS-anotated text in this domain, and the ost recent techniques of supervised POS tag learnig have ben used in developing tagers for this domain.
This alows us to evaluate our tager using the anotated text for evaluation as wel as to compare our tager with others developed for this domain.
The POS-anotated text we use is the welknown GENIA (Tateisi et al, 203) corpus that was developed at University of Tokyo.
3 Expectation
Maximization Trainig Our tager is a first-order Hiden Markov Model (HM) tager that is trained using Expectation aximization (EM) since we do not asume existence of anotated dat in the new doain.
1 Although
we use the GENIA corpus, e take only the raw text and strip of the anotated information for obtainig the Text-EM.
Our HM is based on bigram odeling and hence our transitional probabilities corespond to P(t | t’) where t and t’ are POS tags.
The emisions that label the transition edges wil be discused in the next section and include domain words as wel as certain types of “coded words”.
1 We
considered a 2 nd order model as well, but early work showed negligible advantage predicting to the same trainig set.
Folowing Wang and Schurmans (205) we chose to focus on quality of estimation over model complexity.
The initial transitional probabilities are not randomly chosen but rather taken from the WSJ corpus.
If we take the transitional probabilities as a representation of syntactic preferences, then EM learnig using Text-EM may be taken as adjustment of the gramatical preferences in the WSJ corpus to those in the new domain.
In order to adjust the gramatical preference to the new domain, we start fro smothed WSJ bigram probabilities.
If e started fro unsmothed WSJ bigram probabilities, then EM would not alow us to acount for transitions that are not observed in the WSJ corpus.
For example, in scientific text, transition from RB (the right round bracket) to VBG may be posible, while it does not ocur in the WSJ corpus.
Hence, e smoth the WSJ bigram probabilities with WSJ unigram probabilities.
We compute smothed initial bigram probabilities as P(t | t’) = λ P WSJ (t | t’) + (1-λ) P WSJ (t), where λ=0.9.
We felt employing techniques suggested in (Brants, 200) gave to high a preference for unigram probabilities.
The initial emit probability is obtained from the domain text Text-Lex.
The proces is described in the next section.
This information is derived purely from sufix and sufix distribution, or from orthographic information and does not acount for the actual context of ocurences in the domain text.
We take this sufix-based (and orthographic-based) emit probabilities as reasonable initial lexical probabilities.
EM trainig wil adjust them as necesary.
We made one minor modification to the standard forward-backward EM algorithm.
We dampen the change in transitional and emit probabilities for each iteration.
Significant diferences in lexical probabilities betwen the new domain and WSJ can make undue changes in transitional probabilities and this in turn can further lead the lexical probabilities to head in the wrong direction.
By ading a damping factor, we can prevent the unsupervised trainig to spiral out of control.
Hence we let the new transitional probability be given by P(t | t’) = λ P NEW (t | t’) + (1-λ) P OLD (t | t’) where P OLD represents the transitional probability in the previous iteration and P NEW represents the probability by standard use of forward-backward algorithm.
We use a damping factor of 0.5 for both transitional and emit probabilities.
For the emit probabilities, this has the fect of moderating POS 1105 preferences derived from the trainig dat and preserving words and POSes from the lexicon for use in the test set.
Even with the damping factor, EM learnig followed the patern of ‘Early Maximu’ described by Elworthy (194), where with god initial estimates EM learnig only improves acuracy for a few iterations.
For our EM trainig, we fixed iteration 2 as our ‘best’ E trained model.
4 Development
of the Lexicon and Initial Probabilities As noted earlier, we use a domain text, Text-Lex, to develop the initial lexical probabilities for the HM.
The esential proces is as folows.
Let a word w apear a suficient number of times in Text-Lex (at least 5 times).
We lok in Text-Lex for related words in order to asign a feature vector with this ord.
Each feature is writen as –x+y, here x and y represent sufixes or the empty string (here represented as _).
Features: The feature –x+y represents the word formed by replacing some sufix x in w by some sufix y.
Consider the word “creation”.
“–ion+_” coresponds to the stem word “create” and “– ion+ion” coresponds to the ord “creation” itself.
The feature “–ion+ed” captures information about the word “created” whereas the feature “-_+s” corresponds to word “creations”.
Now consider a word like “history”.
While this might have non-zero values for “-y+ic” (historic) or “-_+s” (histories), we are likely to set zero value for “–ory+_” (unles “hist” or “histe” is found in Text-Lex).
This zero value represents the fact that although “history” has “ory” as a sufix, it has no stem.
Such a distinction (whether or not there is a ste) bears much information for sufixes like “ate” and “ory”.
We use sufix clases rather than actual sufixes as we believe this provides a more apropriate level of abstraction.
Given a word w ith a sufix x (for a word with no sufix from our list of suffixes, x is taken to be _.
i.e., epty string), we examine whether removal of x from w leads to another ord by using a few basic variations that can be found in any rudimentary exposition on English morpholgy.
For exaple, for the sufix “ed”, we atempt to replace “ied” with “y” which relates “purified” with “purify” and recognizes the speling alternation of i/y.
Thus for the word “purify” the feature “-+ed” represents the presence of “purified” since “+ed” represents the sufix clas rather than the actual sufix.
Similarly, we also consider removal of a sufix and, if necesary, ading an “e” to se if such a word exists.
This alows us to relate “creation” with “create” or “activate” with “active”.
Also doubling of a few consonants is atempted to relate “ocurence” and “ocur”.
Finaly, when a word could have two sufixes, the word is considered to always have the longer functional sufix.
Hence, e consider “government” to have “ment” sufix rather than “ent” sufix.
Feature Vectors: There are two diferent types of vectors we use for any word, one caled Bin (for binary count) and other caled RFreq (for relative frequency).
In the Bin vector asociated with “creation”, al these four features wil get the value one, asuming that the four coresponding words are found in Text-Lex.
On the other hand, asuming “creatory” is not found in Text-Lex, the feature “-ion+ory” would get a zero value.
For RFreq vector, instead of ones and zeros, we first start with the frequency of ocurences of each word and then ormalize so that the sum of al feature values is one.
Thus, for example, a word with 4 features having non-zero frequencies of 10, 20, 30 and 40 wil have the respective values et to 0.1, 0.2, 0.3 and 0.4.
A word with four features having non-zero frequency, hich are 1, 2, 3 and 4, wil also have same 4 relative frequency values.
Our intuition is that the Bin vector is helpful in detrminig the set of tags that can be asociated with a word and that the RFreq vector can augment this information regarding the likelihod of these tags.
For example, a one for the “-ing+_” feature in a Bin vector (thus disqualifying a word like “during”) may be suficient to predict VBG, J and N tags.
However, this may not sufice to provide the ordering of likelihod among these tags for this word.
On the other hand, it sems to be the case that when the “ing” form apears far more often than the “ed” form, then the N tag is ost likely.
But if the “ed” for is more frequent, then VBG is most likely.
Examples in the WSJ corpus include “smoking”, “arketing”, “indexing”, and “restructuring” for the first kind, and “caling”, “counting”, “advising”, and “noting” for the second kind.
Exemplars in WSJ: Given a word w from TextLex, we lok for similar words from the WSJ corpus.
Even though the set of ords used in this cor1106 pus may difer substantialy from the domain text, our hypothesis is that words with siilar sufix distribution wil have similar POS tag asignments regardles of the domain.
We folow Cucerzan and Yarowsky (200) in using the kN method for finding similar words, but we difer in details of the construction of the feature vectors and istance computation.
For the word w e create the Bin and RFreq vectors based on distribution of words in Text-Lex.
Folowing the same method, e create the Bin and RFreq vectors for a word v in the WSJ corpus by using the distributions in the WSJ corpus.
Then we compute BinDist(w,v) as the number of features in which the two Bin vectors difer.
A similar RFDist is defined as a weighted sum of two distances: the first distance is L1-nor distance based on values of features for which both words have non-zero values for and the second distance is based on values of features for which one word has a zero value and other does not.
Thus, if the two words’ RFreq vectors are ! <w 1,...,w n > and ! <v 1,...,v n > respectively then ! RFsame(w,v)= w i "v i w i #0$v i #0 % ! RFdiff(w,v)= i "v i w i =0#v i $0 % + w i "v i w i $0#v i =0 % and, ! RFDist(w,v)=RFsame(w,v)+"RFdiff(w,v) For RFDist(.), we used δ =2.
Given a word w, we find the 5 nearest neighbors from the WSJ corpus and use their average lexical probabilities to obtain the lexical probabilities for w.
We investigate the use of Bin vector information and RFreq vector information for computing the distances (i.e., BinDist().
and RFDist().
as wel as a hybrid measure that combines these two distances.
We also considered smothing the lexical probabilities obtained in the above fashion.
Let w be a word for which the above method sugests tags t 1,…,t n in order of likelihod (t 1 is most probable).
Then we consider sqrt-score(t i )= ! n+1"i . We then asign probabilities based on this score after normalizing them so that the probabilities for the n tags wil sum to 1.
Thus, for example, if a word w has thre posible tags, no mater what the original lexical probabilities were detrmined to be, if t 1 is detrmined to be most probable, then P(t 1 |w) wil be 0.418 by this ethod.
The second most probable tag wil be asigned 0.341.
The intuition behind this square rot smothing method is that this smothing may be apropriate for low frequency words, where empirical probabilities based purely on a kN basis ay not be entirely apropriate if the new domain is very different.
The drawback of course is that if there is suficient information, we lose useful information by such flatenig.
And hen a tag is significantly more probable for a word then we lose this vital information.
For example, the ord “high” is mostly anotated as J in WSJ corpus but RB and N are also posible.
Square rot smothing wil flaten this distribution considerably.
Neverthels, we wish to investigate whether this method of smothing the distribution is enough in conjunction with EM.
EM adjusts the probability from observing the number and context of ocurences in the domain text.
2 Coded
Words: No mater how large Text-Lex is, there wil be words that do not apear a suficient number of times (we take this number to be 5).
We agregate such ords acording to their sufixes, if they corespond to one of the predefined sufixes.
Then each word with sufix x is considered to be an instance of a “coded” word SFX-x.
If a word does not have any of these suffixes then they fal into the coded clas unknown.
For each such coded word, we asign the tags and probabilities based on similarly agregated words in the WSJ corpus.
e have two other broad clases of words that we treat diferently.
Coded words are formed based on orthographic characteristics, which include but are not limited to Grek leters, Roman numerals, digits, uper or lower case single leters, uper case leter sequences, cardinals, certain prefix words, and their combinations.
Since they are relatively easy to tag, we do not use the WSJ corpus for them but handle it programaticaly.
Finaly, if a word ocurs often in WSJ or is asigned tags such as CD, FW, MD, PRP, DT, WDT, etc.
(tags which can’t be predicted by means of sufix or suffix-related words), we ad this word together with the tags and probability into the domain lexicon that we are building.
2 We
also considered linear and square functions for smoothing while reporting only the sqrt results in section 5.
1107 5 Evaluation and Analysis As noted earlier, our evaluation is on molecular biolgy text.
For Text-Lex, we used 13,66 titles/abstracts of research papers, a smal fraction of the Medline datbase available fro the National Library of Medicine.
These abstracts were contained in just five of the 50 compresed at files in the 206 version of the Medline datbase.
These abstracts cover topics more broadly in Biomedicine and not just molecular biolgy.
On the other hand, we use for Text-EM, text which can be regarded to be in a subfield of molecular biolgy.
Text-EM is the text from the GENIA corpus (version 3.02) described in (Tateisi et al.203). This coresponds to about 200 abstracts, which are anotated with POS tag information (using the same tags used in the WSJ corpus).
We use a 5fold cros-validation, i.e., 5 partitions are formed and experiments conducted 5 times and results averaged.
For each test partition, the remainder partitions are used for “trainig”.
In our case, this is unsupervised since we use EM and hence we totaly disregard the POS tag information that is associated with the words.
We note that both the text for EM trainig as el as for testing come from the same domain.
We first evaluate the proces of building the lexicon.
This time we consider the entire GENIA corpus and not any partition.
We first considered al words in the GENIA corpus for which we can expect our kN method to asign a tag.
Hence al words that would be treated as coded words are ignored.
For each such word, we consider the tags asigned to them in the GENIA corpus and form pairs <w,t>.
We are interested in the word type and not token and hence we wil not have any multiple ocurences of a pair <w,t>.
Our kN ethod identifies 96.3% of these pairs; we can think of this as recal.
This makes our aproach efective, especialy given the fact that the kN method only assigns 1.92 tags on an average to these words in the GENIA corpus.
Next considering al ords apearing in the GEIA corpus, our lexicon includes a corect tag in 9.0% of the cases on a word-token basis.
These results are sumarized belo.
Characteristic Statistic kN Recal (word-type) 96.3% Average Number Tags/Word 1.92 tags Lexicon Recal (word-token) 9.0% We now turn to the valuation of the acuracy of our HM.
As mentioned earlier, these results are based on 5-fold cros-validation experiments.
The best results (95.7%) were obtained for the case where we tok the lexical probabilities directly from kN using only RFDist and by discarding al tags asigned with probability les than 0.2. 3 These results compare favorably to ther tagers developed for the Biolgy domain.
The MedPost tager (se Section 6) achieved an acuracy of 94.1% when we aplied it to the GENIA abstracts.
The PenBioIE tager (se Section 6) achieved an acuracy of 95.1%.
Note that output from the PenBioIE tager is not fuly compatible with GENIA anotation due to some diferences in its tokenization.
Even if the diferences in acuracies can be discounted ue to tokenization or even systematic diferences in anotation betwen the training and test corpora, our main point is that our results compare favorably (our tager competitive) with tagers that were developed for the Biomedicine domain using supervised trainig.
These results are sumarized in the table below.
POS Tager %Acuracy ur HM (5-fold) 95.7% MedPost 94.1 PenBioIE 95.1% GENIA supervised 98.26 MedPost sems intended to cover al of Biomedicine, since its lexicon is based on the 10,0 ost frequently ocuring words from Medline and for which the set of posible tags were manualy specified.
The PenBioIE tager as developed using 315 Medline abstracts using another subfield of molecular biolgy.
None of these acuracies however are as high as those of the GENIA tager (Tsuruoka et al.205) which was trained (supervised) using GENIA corpus and uses a machine learnig model more sophisticated than the simple first-order HM tager we use.
This model considers more features including words to the right.
The best results (98.26%) were obtained when lexicon from thre diferent sources were agregated.
3 Banko
and Moore (204) showed only slight improvement in tag accuracy between .01 and .1 cutoffs with a lexicon built from annotated data.
We opted for the .02 cutoff because of our ‘noisier’ lexicon.
1108 Returnig to the results for our tagers, we also tried BinDist in the kN method, with and ithout square rot smothing.
These results were typicaly les than the above-mentioned result.
We also compared using a square rot smoth on RFDist obtainig results aproximately 1% lower than without the square rot soth.
We next present some examples that ilustrate strengths and weakneses of the curent model.
An example that shows that EM trainig akes god adjustent to the domain is the improvement in taging of verbal categories.
We conducted a detailed eror analysis on one of the cros-validation partitions and noted that the acuracy on al verbal POS tags improved after EM trainig.
A noteworthy case is the improvement in taging of VBP originaly misclasified as VB.
Since most English words that are VB can also be VBP, and since they are anotated more frequently in WSJ as VB, the initial lexicon usualy has a higher probability assigned to VB for most words.
As EM trainig progreses, we noted that the frequency of VBP mistaged as VB decreases.
Similarly, misclasifications of BG as N also drops in the final model (by 40.3% on Text-EM) as compared to the initial model based on WSJ transitional probabilities and initial lexicon derived using WSJ words as exemplars.
Previously, in the context of parsing Biomedical text, Lease and Charniak (204) mention the occurences of sequences of multiple N is more frequent in the GENIA corpus than in the WSJ corpus and that it could lead to parsing erors.
e din’t observe this problem here, but rather the contrary situation where any Js were initialy mistaged as N.
About 2% of these misclasifications are corected after EM trainig.
While our model adjusts wel in these cases to the new doain, sometimes the drift leads to worse performance.
An exaple is in the misclasification of VBN as J.
The most frequent word for which this misclasification ocurs in the word “activated”.
These misclasifications ocur in the context such as “the activated cels”.
The use of VBN rather than J is hard to detrmine on basis of just surface features and perhaps has to do more with the meanig of the word.
In supervised setting, if suficient such cases were anotated then this would be learned.
But in an unsupervised setting this turns out to be a problem case.
Despite the fact that RFDist predicted VBN as most probable tag for “activated”, EM trainig makes this situation worse.
Analysis of words with most frequent erors revealed many cases from orthographic oded words.
Many ocurences of single lower case leters (which could have LS, SYM or N tags) were labeld as LS whereas the GENIA taging used N.
Our model taged “+/-” always as SYM whereas because of the context of use, GENIA anotations were C.
(In fact, GENIA does not apear to use the SYM tag).
Similarly, “<” and “>” were often mistaged as SYM by our model whereas based on context they are anotated as JR.
6 Related
Work The impact of out-of-vocabulary words on NLP aplications has ben noted before.
The degradation in performance of components, which were trained on the WSJ corpus, but used on biomedical text has ben noted (Lease and Charniak, 204, Smith et al, 205).
Smith et al.(205) use this observation in the design of their POS tager, MedPost, by building a Markov model with a lexicon containig the 10,0 most frequent ords from Medline, and using anotated text from the Biomedical text for supervised trainig.
There are many unsupervised aproaches to POS taging.
We focus now on those that are most closely related to our work and contain ideas that have influenced this ork.
There have ben many uses of EM trainig to build HM tagers (Kupiec, 192; Elworthy, 194; Banko and More, 204; Wang and Schurmans, 205).
Banko and More (204) achieved beter acuracy by restricting the set of posible tags that are asociated with words.
By eliminating posibilities that may apear rarely with a word, they reduce the chances of unsupervised trainig spiraling along an unlikely path.
We believe by using our aproach we considerably reduce the set of tags to what is apropriate for each word.
Further, we to remove any tag asociated ith low probability by kN method.
Usualy these tags are noise introduced by some inapropriate xemplar.
Wang and Schurman (205) sugest that EM algorithm be modified such that at any iteration the unigra tag probability be held constant to the true probability for each tag.
Again, this might serve to stop a drift in unsupervised methods towards making a tag’s probability becoe larger than it should 1109 be.
However, the true probability canot be known ahead of time and certainly not in a new domain.
While a WSJ bigram probability ned not reflect the coresponding preferences in the new domain, our use of starting from WSJ probabilities and then damping changes to transition probabilities was otivated by a similar concern of not leting a drift towards making soe (bigram) tags to frequent during EM iterations.
Using sufixation paterns for purposes of predicting POS tags has ben considered before.
Although as far as we know, we are the first to aply it for domain adaptation purposes.
Schone and Jurafsky (201) consider clusters of words (obtained by some “perfect” clustering algorithm) and then compute a measure of how “afixy” a cluster is.
For example, a cluster containig words “climb” and “jup” may be related by sufixing operation +s to a cluster that contains words “climbs” and “jumps”.
The percentage of ords in a cluster that are so related provides a measure of how “afixy” a cluster.
This together with five other atributes of clusters (such as whether words in a cluster precede those of another cluster, optionality) and language universals induce POS tags for these clusters from corpora.
This method does not use POS taged corpora (although in the reported experiment the initial “perfect” clusters were obtained fro the Brown corpus using the POS tag information).
In contrast, we use the POS taged WSJ corpus to asist in the induction of tag information for our lexicon.
In this respect, our method is closer to the aproach of Cucerzan and Yarowsky (200).
Our use of the kN method to identify tags and their probabilities for words was inspired by this work.
However, their use of kN method was in the context of supervised learnig.
The method was aplied for handling words unsen in the training dat.
The estimated probabilities were used during the taging proces.
Instead of just aplying the method for unknown words, i.e., words not present in the trainig dat, our aproach is to create the ntire lexicon in the new domain.
As Lease and Charniak (204), among others, have noted, the distribution of N tag sequences as wel as tag distributions in the Biomedical domain could ifer from WSJ text.
Since our aim is to adjust to the new domain, we employed unsupervised learnig in the for of EM trainig, unlike the supervised taging model development aproach of Cucerzan and Yarowsky.
Another significant diference is that their method etrmines nearest neighbors not only on the basis of sufix-related words but also on the basis of nearby words context.
Since our motivation, on the other hand, is to move to a new doain, we din’t consider detction of similarity on the basis of word contexts.
In contrast, we have shown that the aproach of identifying ords on the basis of sufixation paterns and using them as exemplars can be aplied efectively even when the doain of aplication is substantialy diferent from the text (the WSJ corpus) providing the exeplars.
7 Conclusions
As NLP technolgy continues to be aplied in ew domains, it becomes more important to consider the isue of portability to new domains.
To cope with domain-specific vocabulary and also diferent use of vocabulary in a new domain, we exploited sufix information of words.
While use of sufix information per se has ben employed in many existing POS tagers, its use is often liited to an online maner, where ach word is examined independently from the existence of its orpholgicaly related words.
As shown in (Cucerzan and Yarowsky, 200), such information can provide considerable information to build a lexicon that asociates posible tags with words.
However, we use this information only to provide the initial values.
We aply EM algorithm to adjust these initial probabilities to the new doain.
The results in Section 5 show that we achieve god performance in the valuation domain, which is comparable with two recently developed tagers for this domain.
We also show in section 5 examples of how EM unlearns some WSJ bias and adjusts to the new domain.
While we introduce a damping factor to slow down changes in iterations of EM trainig, we believe there is scope for further improvement to minimize drift.
Furthermore, there is scope to improve our kN method as discused at the end of Section 5.
In the futre, we also expect to consider methods that may automaticaly mine sufixes in a new doain and use these doain-specific sufixes.
We used the kN method to asociate words in the new domain with posible POS tags.
Despite the often-stated notion that English is not morpholgicaly rich, we find that sufix-based methods can stil help make significant inroads.
1110 Our method ofers the chance to develop god taggers for specialized domains.
For example, the GENIA corpus and PenBioIE corpus are specializations within molecular biolgy, but tagers developed on one corpus degrades in performance on the other.
Using our method, we could use diferent Text-EM for these specializations even if we retain Medline as Text-Lex.
In the same way, e could develop a tager for the medical domain, which as a distinct vocabulary fro biolgy.
References Banko, M.
and Moore, R.C. 2004.
Part of Speech Tagging in Context.
In Procedings, 20th International Conference on Computational Linguistics (Coling 2004), Geneva, Switzerland, pp.556-561.
Baum, L.
1972. An inequality and asociated maximization technique in statistical estimation for probabilistic functions of a Markov proces.
Inequalities 3:1-8.
Brants, T.
2000. TNT a statistical part-of-speech tagger.
In Procedings of the Sixth Applied Natural Language Procesing Conference ANLP-2000.
Bril, E.
1995. Transformation-based eror-driven learning and natural language procesing: A case study in part of speech tagging.
Computational Linguistics, 21(4):543-565.
Cucerzan, S.
and Yarowsky, D.
2000. Language independent minimaly supervised induction of lexical probabilities.
Procedings of ACL-2000, Hong Kong, pages 270-277.
Cutting, D., Kupiec, J., Pedersen, J., and Sibun, P.
1992. A practical part of sppech tagger.
Procedings of 3rd Conference on Applied Natural Language Procesing, 53-58.
DeForest, J.
2000. Graduate Record Exam Sufixed web page.
Michigan State University.
http:/ww.msu.edu/~defores1/gre/sufx/gre_suffx.htm Elworthy, D.
1994. Does Baum-Welch re-estimation help taggers.
In Procedings of the Fourth Conference on Applied Natural Language Procesing, ACL.
Kulick, S., Bies, A., Liberman, M., Mandel, M., McDonald, R., Palmer, M., Schein, A.
and Ungar, L.
Integrated Annotation for Biomedical Information Extraction.
HLT/NACL, 2004.
Kupiec, J.
1992. Robust Part-of-speech Tagging Using a Hidden Markov Model.
Computer Speech and Language, 6.
Lease, M.
and Charniak, E.
2005. Parsing Biomedical Literature.
IJCNLP-2005: 58-69.
Marcus, M., Santorini, B., Marcinkiewicz, M.A. 1993.
Building a large annotated corpus of English: The Penn Trebank.
Computational Linguistics, 19:313-330.
PennBioIE. 2005.
Mining The Bibliome Project.
http:/bioie.ldc.upenn.edu/. Schone, P.
and Jurafsky, D.
2001, Language Independent Induction of Part of Speech Clas Labels Using Language Universals.
IJCAI Workshop on Text Learning: Beyond Supervision.
Smith, L., Rindflesch, T., Wilbur, W.J. 2004.
MedPost: a part-of-speech tagger for biological text.
Bioinformatics 20 (14):2320-2321.
Smith, L., Rindflesch, T., Wilbur, W.J. 2005.
The importance of the lexicon in tagging biomedical text.
Natural Language Engineering 12(2) 1-17.
Tateisi, Y., Ohta, T., dong Kim, J., Hong, H., Jian, S., Tsuji, J.
2003. The GENIA corpus: Medline abstracts annotated with linguistic information.
In: Third meting of SIG on Text Mining, Inteligent Systes for Molecular Biology (ISB).
Tsuruoka, Y., Tateishi, Y., Kim, J.
D., Ohta, T., McNaught, J., Ananiadou, S., and Tsuji, J.
2005. Developing a Robust Part-of-Speech Tagger for Biomedical Text, Advances in Informatics 10th Panhelenic Conference on Informatics, LNCS 3746: 382-392.
Wang, Q.
and Schuurmans, D.
2005. Improved estimation for unsupervised part-of-speech tagging.
In IEE NLP-KE ww.AlWords.com. 2005.
English Dictionary and Language Guide.
AlSites LC. ww.AlSiteslc.om

