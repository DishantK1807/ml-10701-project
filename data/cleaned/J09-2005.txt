ApplyingComputationalModelsofSpatial
PrepositionstoVisuallySituatedDialog
JohnD.Kelleher
∗
DublinInstituteofTechnology
FintanJ.Costello
∗∗
UniversityCollegeDublin
This article describes the application of computational models of spatial prepositions to visually
situated dialog systems. In these dialogs, spatial prepositions are important because people
often use them to refer to entities in the visual context of a dialog. We ﬁrst describe a generic
architecture for a visually situated dialog system and highlight the interactions between the
spatial cognition module, which provides the interface to the models of prepositional semantics,
andtheothercomponentsinthearchitecture.Followingthis,wepresenttwonewcomputational
models of topological and projective spatial prepositions. The main novelty within these models
is the fact that they account for the contextual effect which other distractor objects in a visual
scene can have on the region described by a given preposition. We next present psycholinguistic
testsevaluatingourapproachtodistractorinterferenceonprepositionalsemantics,andillustrate
how thesemodels areusedfor bothinterpretationand generationof prepositional expressions.
1. Introduction
A growing number of computer applications share a visualized(virtual or real) space
withtheuser,forexamplegraphicdesignprograms,computergames,navigationaids,
robotsystems,andsoforth.Ifthesesystemsaretobeequippedwithdialoginterfaces,
they must be able to participate in visually situateddialog. Visually situateddialog is
spoken from a particular point of view within a physical or simulatedcontext. From
theoretical linguistic andcognitive perspectives, visually situateddialog systems are
interesting as they provide ideal testbeds for investigating the interaction between
language andvision. Fromahuman–computer interaction (HCI)perspective, visually
situated dialog systems promise many advantages to users interacting with these
systems. In this article we describe computational models for the interpretation and
generationofvisuallysituatedlocativeexpressionsinvolvingtopologicalandprojective
spatialprepositions.
ContributionsAninherentaspectofvisuallysituateddialogisreferencetoobjects
in the physical environment in which the dialog occurs. People often use locative
∗ SchoolofComputing,DublinInstituteofTechnology,KevinStreet,Dublin8,Ireland.E-mail:
john.kelleher@comp.dit.ie.
∗∗ SchoolofComputerScienceandInformatics,UniversityCollegeDublin,Belﬁeld,Dublin4,Ireland.
E-mail:ﬁntan.costello@ucd.ie.
Submissionreceived:31July2006;revisedsubmissionreceived:30March2007;acceptedforpublication:
4July2007.
©2008AssociationforComputationalLinguistics
ComputationalLinguistics Volume35,Number2
expressions, in particular spatial prepositions, to pick out objects in the visual envi-
ronment. In this article we present computational models of the semantics of spatial
prepositionsandillustratehowthesemodelscanbeusedinavisuallysituateddialog
system for reference resolution and generation. These models are designed to handle
referenceresolutionandgenerationincomplexvisualenvironmentscontainingmulti-
pleobjects,andtoaccountforthecontextualinﬂuencewhichthepresenceofmultiple
objects has on the semantics of spatial prepositions. In this our models move beyond
other accounts, which typically donot model thecontextual inﬂuence of other objects
on spatial semantics. Because most real-worldvisual scenes are complex andcontain
multipleobjects,ourmodelsforthesemanticsofspatialprepositionsareimportantfor
visuallysituateddialogsystemsintendedtooperateusefullyintherealworld.
Overview We begin in Section 2 by describing some terminology we use when
discussing locative expressions. In Section 3 we present an abstract architecture for
a visually situated dialog system and, using this architecture, illustrate how the spa-
tial reasoning component of the architecture interacts with the other components of
the system. In Section 4 we review psycholinguistic data on the semantics of spatial
prepositions.Section5reviewspreviouscomputationalmodelsofspatialprepositional
semantics.Section6presentsourcomputationalmodelsaccountingforthesemanticsof
spatialprepositionsandtheinﬂuenceofvisualcontextonthosesemantics,andSection7
presentspsycholinguisticevaluationofthesemodels.Section8presentsapplicationsof
themodelsinimplementedsystems.Section8.1presentsanapplicationofourmodels
to the interpretation of locative expressions, basedon Kelleher, Kruijff, andCostello
(2006),andSection8.2presentsalgorithmswhichusethesemodelstogeneratelocative
expressionstoidentifyobjectsinvisualscenesfromKelleherandKruijff(2006).
2. Terminology
Ourcomputationalmodelsaredesignedtointerpretandgeneratelocativeexpressions
involving spatial prepositions. The term locative expression describes “an expression
involving a locative prepositional phrase together with whatever the phrase modiﬁes
(noun, clause, etc.)” (Herskovits 1986, page 7). In this article we use the term target
(T) to refer to the object that is being locatedby a locative expression andthe term
landmark
1
(L)torefertotheobjectrelativetowhichthetarget’slocationisdescribed;
see Example (1). We will use the term distractor to describe any object in the visual
contextthatisneitherthelandmarknorthetarget.
Example 1
[Theman]
T
near[thetable]
L
.
TheEnglishlexiconofspatialprepositionsnumbersabove80members(notconsid-
eringcompoundssuchas right next to)(Landau1996).Withinthissetadistinctioncan
bemadebetweenstaticanddynamicprepositions:staticprepositionsprimarily
2
denote
1 Thereisawealthoftermsusedintheliteraturedescribinglocativeexpressions.Theterms local object,
ﬁgure object,andtrajectorareallequivalenttoourtermtargetwhilethetermsreference object,ground,
andrelatumareequivalenttoourtermlandmark.
2 Staticprepositionscanbeusedindynamiccontexts,forexample, theman ran behind thehouse,and
dynamicprepositionscanbeusedinstaticones,forexample,thetree lay across theroad.
272
KelleherandCostello ComputationalModelsofSpatialPrepositionsforVSD
Figure1
Architectureofavisuallysituateddialogsystem.
the location of an object, dynamic prepositions primarily denote the path of an object
(Jackendoff1983;Herskovits1986),seeExamples(2)and(3).
Example 2
Thetreeis[behind]
static
thehouse.
Example 3
Themanwalked[across]
dynamic
theroad.
In general, the set of static prepositions can be decomposed into two sets called
topological and projective. Topological prepositions are the category of prepositions
referringtoaregionthatisproximal tothelandmark; forexample, at, near.Often,the
distinctions between the semantics of the different topological prepositions is based
on pragmatic constraints, for example the use of at licenses the target to be in contact
with the landmark, while the use of near does not. Projective prepositions describe a
region projected from the landmark in a particular direction, with the speciﬁcation of
thedirectiondependentontheframeofreference
3
beingused;forexample,totheright
of,tothe leftof.
3. VisuallySituated DialogSystem Architecture
In this section we present an abstract implementation-independent architecture for a
visuallysituateddialogsystemandhighlighttheroleplayedbyspatialreasoninginthe
functioningofthesystem.Inparticular,wedescribehowmodelsofspatialprepositional
semanticsareimportantforreferenceresolutionandgeneration.
The distinguishing characteristic of a visually situated dialog system is that the
systemhastheabilitytovisuallyperceivetheenvironmentinwhichadialogissituated.
Consequently, these systems use both visual andlinguistic contextual information to
understandusercommandsandtogeneratelinguisticdescriptionsoftheenvironment.
Figure1illustratesthevisualdialogsystemarchitecturewewilldescribe.Thearrowsin
theﬁgurerepresentdataﬂowsthroughthesystem;theboxesarethemaininformation
processingcomponents.
3 Inthecontextofprojectiveprepositions,aframeofreferenceconsistsofsixhalf-lineaxeswithashared
origin;inEnglish,theseaxesareusuallylabelledfront,back,right,left,above,below.InEnglish,three
differentframesofreferencearedistinguished: absolute,intrinsic,andviewer-centered.Interestingly
however,althoughtheuseofatripartitesystemiscommoninEuropeanlanguages,thisisnotuniversal,
withmanylanguagestakingdifferentapproacheshere.WedirecttheinterestedreadertoLevinson(1996,
2003)andLevelt(1996)forfurtherdiscussiononframesofreference.
273
ComputationalLinguistics Volume35,Number2
Figure2
Exampleinputandoutputdatafromavisionsubsystem.
There are two information inputs into this system: the vision subsystem andthe
speech interpretation pipeline. The vision subsystem directly updates the system’s
representation of the visual context. The basic requirements for the vision subsystem
are that it is able to detect andcategorize the objects in the visual context andcan
provide geometric positioning information for each visible object. Figure 2 illustrates
theanalysisthatavisionsubsystemmaygenerateforagivenscene.
The speech interpretation pipeline begins with speech recognition. This module
takes a speech utterance from the user andcreates a string representation of it. The
parser uses this string to construct a structuredrepresentation of the input. Parsers
range in function from wide-coverage syntactic focused parsers, such as Cahill et al.’s
(2004) probabilistic Lexical-Functional Grammar (LFG) parser, to narrow coverage se-
manticbasedparsers,forexampletheCoSyparser(Kruijff,Kelleher,andHawes2006).
Figure3illustratesthetypesofanalysesproducedbythesedifferenttypesofparsersfor
theinputstring is the box near the ball?Theparsetreeontheleftwasgeneratedusinga
probabilisticwide-coverageLFGparser.
4
Theparsetreeprovidesasyntacticanalysisof
theinputstring.
Generally, parsers developed for interactive dialog systems integrate semantic, as
well as syntactic, information in their grammars. In these parsers the elements in the
lexicon andgrammar are basedon an analysis of the entities andrelations of the
speciﬁcdomainthesystemisdesignedfor.Theseparserssacriﬁcecoveragefordepthof
analysis.Foradialogsystem,theadvantageofthisdeeperanalysisisthatthesemantic
informationintheparser’soutputcanbeusedbythedialogmanagertorelatetheinput
totherestofthedialog.TheparsestructureontherightofFigure3illustratesthetypeof
semanticallyrichrepresentationthataninteractivedialogsystemparsermightproduce
(thisparticularrepresentationwasgeneratedbytheCoSyparser).
TheCoSyparserusesaCombinatoryCategorialGrammarthatrepresentslinguistic
meaning using an ontologically rich sorted relational structure (Baldridge and Kruijff
4 Ademooftheparserisavailableat:http://lfg-demo.computing.dcu.ie/lfgparser.html.Theparser
alsoprovidesdetailedLFGf-structuresforinputstrings.
274
KelleherandCostello ComputationalModelsofSpatialPrepositionsforVSD
Figure3
Exampleparsestructuresforthestringis thebox near theball?
2002,2003).Withinthisrepresentationthestatementb2:phys-objmeansthatthereferent
b2isoftypephys-obj(i.e.,aphysicalobjectasdeﬁnedbytheontologythegrammarin-
dexes).Thesemanticcontributionoftheprepositionalphraseneartheballisrepresented
bythe <Location> structureanditssubcomponents.Thisstructuredescribesalocative
prepositional phrase containing a static preposition that locates the referent b2 in the
regionrthatisproximaltothelandmarkdescribedbythe<Anchor>subcomponent.It
shouldbe notedthat the syntactic andsemantic representation of prepositions within
grammarsisanareaofongoingresearch(seeGawron1986;Tseng2000;Beermannand
Hellan 2004). The analysis presentedhere of the prepositional phrase near the ball is
intended to illustrate some of the semantic features that prepositions may introduce
into a grammar andis not intendedas a comprehensive account of how prepositions
shouldbegrammaticallyrepresented.
The ﬁnal stage in the interpretation pipeline is to categorize how the utterance
relatestothecurrentdialogcontext.Thiscategorizationisdrivenbythedialogmanager
andinvolves interpreting an utterance as a dialog act (Bunt 1994; Carletta et al. 1997;
Klein 1999). One of the important tasks in this process is resolving the references
in the input. Consequently, the dialog manager may invoke the reference resolution
component. Reference resolution is one of two functions in the architecture where
spatialreasoningplaysanimportantrole.Fromacomputationalperspective,reference
resolutioninvolvestwomaintasks:
1. Creatingandmaintainingamodelofwhatthesystemconsidersas
mutualknowledge(thismodelshouldcontainalltheobjectsthatare
availableforreferenceandtheirproperties)
2. Matchingtherepresentationintroducedbyagivenreferringexpression
toanelement(orelements)inthesetofpossiblereferents
In a visually situateddialog a referring expression may be exophoric (i.e., denote
an object in the visual context which has not yet been mentionedin the dialog) or it
maybeanaphoric(i.e.,accessarepresentationofapreviousreferringexpressioninthe
dialogcontext).Peopleoftenusethespatiallocationofanobject,describedusingspatial
prepositions,whenmakingexophoricreferences.Asaresult,inordertointerpretthese
referencesthesystemmusthaveaccesstomodelsofthesemanticsoftheprepositions
275
ComputationalLinguistics Volume35,Number2
Figure4
Themappingperformedbythespatialreasoningmodulefromqualitativetogeometric
representationsduringtheinterpretationofalocativeexpression.
used.Inthisarchitecturethisaccessisprovidedthroughthespatialreasoningcompo-
nent.Figure4illustratesthetranslationbetweenthequalitative,parser-generated,and
thegeometric,visionsubsystem–generatedrepresentationsthatmustbeperformedin
ordertointerpretaspatiallocativeexpression.
At different stages during a dialog the dialog manager may recognize that the
system needs to generate a response to the last input utterance. For example, the
utterance may have been a question, such as whereisx?or which x?. In such cases,
thedialogmanagerinformsthecontentplannerofthis.Theroleofthecontentplanner
is to determine the semantic content that should be included in the system’s output,
rather than the linguistic realization of this content. Indeed, the content planner may
generate a logical representation closer to the parse structure on the right of Figure 3
thantoanaturallanguagedescription.
Generating referring expressions (GRE) is a key stage in content planning. GRE is
thesecondfunctioninthearchitecturewherespatialreasoningplaysanimportantrole.
ThefunctionoftheGREcomponentistodeterminethesetofpropertiesthatdistinguish
a particular target object from the other objects in the scene. For example, in response
toaquestionsuchaswhichx?theGREcomponentmaydeterminethatacolorandtype
descriptionissufﬁcienttodistinguishthetargetobject,resultinginananswersuchasthe
blue x beinglinguisticallyrealized.However,itmaybethecasethatthelocationofthe
targetinthesceneistheonlywaytodistinguishit.Insuchcases,theGREcomponent
needs access to computational models of the spatial prepositions if it is to determine
which spatial relation is most suitable. Figure 5 illustrates the translation from a geo-
metric to qualitative representation that is performedduring the GRE process by the
spatialreasoningmodulewhenalocativedescriptionisbeinggeneratedbythesystem.
Once the content planning andGRE processes have been completed, the realizer
determinesasurfacelinguisticforminwhichthiscontentcanbeconveyed.Finally,the
speechsynthesissystemsgeneratethespeechoutputforthelinguisticstringcreatedby
therealizer.
4. Psycholinguistic Dataon Spatial Prepositions
Spatial reasoning is a complex activity that involves at least two levels of process-
ing: a geometric level where metric, topological, andprojective properties are handled
276
KelleherandCostello ComputationalModelsofSpatialPrepositionsforVSD
Figure5
Themappingperformedbythespatialreasoningmodulefromgeometrictoqualitativeto
representationsduringthegenerationofalocativedescription.
(Herskovits 1986), anda functional level where the normal function of an entity affects
thespatialrelationshipsattributedtoitinacontext(CoventryandGarrod2004).
Therehasbeenmuchexperimentalworkdoneonspatialreasoningandlanguage.
Some of this work has focusedon functional aspects of prepositional semantics (e.g.,
HaywardandTarr1995;Coventry1998;Garrod,Ferrier,andCampbell1999),andsome
ongeometricfactors(Gapp1995;LoganandSadler1996;RegierandCarlson2001).In
this article we are primarily concernedwith the geometric semantics of prepositions
and,consequently,ourreviewwillfocusontheexperimentaldatathataddressesgeo-
metricfactors.Wewillbeginbyreviewingtheexperimentaldatadescribingtopological
spatial prepositions. Following this, we will then review data relating to projective
prepositions.
Topological prepositions denote a region that is proximal to a landmark. Subse-
quently we discuss previous psycholinguistic experiments, focusing on how contex-
tual factors such as distance, size, and salience may affect proximity. We also present
examples showing that the location of other objects in a scene may interfere with the
acceptabilityofaproximaldescriptiontolocateatargetrelativetoalandmark.
LoganandSadler(1996)examinedthesemanticsofseveralspatialprepositions.In
theirexperiments,ahumansubjectwasshownsentencesoftheform the X is [relation]
the O,eachwithapictureofaspatialconﬁgurationofanOinthecenterofaninvisible
7 × 7 cell grid, and an X in one of the 48 surrounding positions. The subject then had
toratehowwellthesentencedescribedthepicture,onascalefrom1(bad)to9(good).
Figure6givesthemeangoodnessratingfortherelation“nearto”asafunctionofthe
position occupiedby X (Logan andSadler 1996). It is clear from Figure 6 that ratings
diminishasthedistancebetweenXandOincreases,butalsothatevenattheextremes
ofthegridtheratingswerestillabove1(minimumrating).
Besides distance there are also other factors that determine the applicability of a
proximalrelation.Forexample,givenprototypicalsize,theregiondenotedby near the
buildingislargerthanthatofneartheapple.Moreover,anobject’ssaliencecouldinﬂuence
thedeterminationoftheproximalregionassociatedwithit;aswithsize,themoresalient
anobjectisthelargertheproximalregionassociatedwithit(Gapp1994).
Finally,thetwoscenesinFigure7showinterferenceasacontextualfactor.Forthe
scene on the left we can use the blue box is near the black box to describe object (c). This
seems inappropriate in thescene on theright. Placing an object (d)beside (b) appears
277
ComputationalLinguistics Volume35,Number2
Figure6
A7×7cellgridwithmeangoodnessratingsfortherelationthe Xis nearOasafunctionofthe
positionoccupiedbyX.
tointerferewiththeappropriatenessofusingaproximalrelationtolocate(c)relativeto
(b),eventhoughtheabsolutedistancebetween(c)and(b)hasnotchanged.
Thereareseveralimportantfeaturesthatareevidentfromthesedata.First,givena
context, subjects have the ability to grade the applicability of a spatial relation. Logan
andSadler(1996)introducedthetermspatialtemplatetodescribetherepresentationof
theregionsofacceptabilityassociatedwithapreposition.Aspatialtemplateiscentered
on the landmark and identiﬁes for each point in its space the acceptability of the
spatialrelationshipbetweenthelandmarkandthetargetappearingatthatpointbeing
describedbythepreposition.Second,thereisempiricalevidencepointingtotheeffects
ofdistancebetweenthatlandmarkandthetarget,andlandmarksalienceandsizeonthe
applicabilityofaproximity-basedpreposition.Finally,theexamplespresentedpointto
thefactthatthelocationofotherdistractorobjectsincontextmayalsointerferewiththe
applicabilityofapreposition.(ThemodelofproximitywepresentinSection6captures
allthesefactors.)
Figure 8 is a representation of the spatial template for the projective preposition
above described in Logan and Sadler (1996). The main points of note relating to these
data are that there are three regions in the spatial template (good, acceptable, and
bad)andtheseregionsaresymmetricaroundthecanonicaldirectionofthepreposition
withacceptabilityapproaching0astheangulardeviationfromthecanonicaldirection
approaches90degrees.However,itshouldbenotedthatthesedataweregathereddur-
ing an interpretation task andthat the task may have affectedthe subjects’ responses.
Figure7
Proximityanddistractorinterference.
278
KelleherandCostello ComputationalModelsofSpatialPrepositionsforVSD
Figure8
Spatialtemplatefortheprepositionabove(LoganandSadler1996),whereLMrepresentsthe
landmarkandthearrowshowsthecanonicaldirectionassociatedwiththepreposition.
Although the subjects may have ratedsome of the areas on the far right andleft of
the landmark as acceptable with respect to interpreting an utterance such as above the
landmark, this does not mean that they would use the word above to describe a target
objectintheseregionsrelativetothelandmark.Thishighlightsthefactthatpeoplemay
bemoreaccommodatingwhentheyareinterpretingalocativedescription(forexample,
they may extend the allowable angular deviation to 90 degrees) but be more speciﬁc
whengeneratingalocativedescription.
5. Previous Models of Topological and Projective Spatial Prepositions
Therehasbeenmuchresearchontheformalpropertiesandinteractionsoftopological
relations, for example Cohn et al. (1997) andKuipers (2000). However, before these
higher-level frameworks can be applied to real-world data, a model of proximity that
is capable of segmenting a region at the metric or geometric level is required. At
this geometric level previous approaches to modeling topological prepositions have
adoptedoneoftwoapproachestodeﬁningtheregionofproximity.Theﬁrstistoadopta
Voronoisegmentationofspace.Underthisapproachtheregionconsideredasproximal
toanobjectistheareasurroundingitthatisclosertoitthantoanyotherobjectinthe
scene.Thesecondistodeﬁnetheproximalregionintermsofthesizeofthelandmark.
Forexample, Gapp(1995)deﬁnestheareaofproximityastheregionwithintentimes
thesizeofthelandmarkobjectineachdirection.However,neitheroftheseapproaches
considertheeffectthatthelocationsofotherobjectsinthescenehaveontheproximity.
Consequently, they cannot distinguish between the different context provided by the
twoimagesinFigure7.
Several models of projective prepositions have been proposed (Yamada 1993;
OlivierandTsujii1994;Gapp1995;Fuhretal.1998;RegierandCarlson2001;Kelleher
and van Genabith 2006). Yamada (1993) introduced the concept of a potential ﬁeld
function to capture the gradation of applicability across the region described by the
preposition. Later work (Olivier andTsujii 1994; Gapp 1995) highlightedthe issue of
deﬁning the intended frame of reference. Building on this work and the psycholin-
guistic results of Carlson-Radvansky andLogan (1997), Kelleher andvan Genabith
279
ComputationalLinguistics Volume35,Number2
(2006)developedacomputationalmodelthatconstructedamodiﬁedspatialtemplatein
situationswhereframeofreferenceambiguityoccurred.Fuhretal.(1998)usedmodels
ofprepositionalsemanticsinordertointerpretnaturallanguagecommandstoarobotic
arm. Fuhr et al. segmentedthe space aroundan object into different regions based
on the sides and vertices of the object’s bounding box. One of the drawbacks of this
system,however,wasthatitcouldnotdistinguishbetweenthepositionoftwoormore
objectsthatwerefullyenclosedwithinagivenregion.Finally,RegierandCarlson(2001)
developedavectorsumalgorithmtocomputetheapplicabilityofaprojectiverelation
betweenalandmarkandatarget.However,aswithprevioustopologicalmodels,none
of these models consider the inﬂuence of other objects in the context of the landmark
target relationship. For example, the introduction of the long black object into image
2 in Figure 9 affects the interpretability of a reference such as thebluesquareabovethe
white rectangle.Inthenextsectionwedescribenewmodelsdesignedtoaccountforthe
inﬂuenceofotherobjectsinthesemanticsofspatialprepositions.
6. Models of Visual Context inTopological andProjective Spatial Prepositions
Ifacomputationalmodelisgoingtoaccommodatethegradationofapplicabilityacross
a preposition’s spatial template it must deﬁne the semantics of the preposition as
some sort of continuum function. A potential ﬁeld model is one form of continuum
measurethatiswidelyused(Yamada1993;Gapp1994;OlivierandTsujii1994;Regier
andCarlson 2001). Using this approach, a model of a preposition’s spatial template
is constructedusing a set of normalizedequations that, for a given origin andpoint,
computesavaluethatrepresentsthecostofacceptingthatpointastheinterpretationof
thepreposition.
Eachequationusedtoconstructthepotentialﬁeldrepresentationofapreposition’s
spatial templatemodels adifferentgeometric constraint speciﬁed bythepreposition’s
semantics.Forexample,fortopologicalprepositionssuchasnear,anequationinversely
proportional to the distance between a point and a landmark would be used, while
forprojectiveprepositionssuchas to the right of,anequationmodelingtheangularde-
viationofapointfromtheidealizeddirectiondenotedbytheprepositionwouldbein-
cludedintheconstructionset;Gapp(1995)andLoganandSadler(1996)bothnotedthat
acceptability of a projective preposition being usedto describe a location approaches
0 as the angular deviation of that location approaches 90 degrees. The potential ﬁeld
is then constructedby assigning each point in the ﬁeldan overall potential by inte-
grating the results computedfor that point by each of the equations in the construc-
tionset.
Figure9
Projectiveprepositionsanddistractorinterference.
280
KelleherandCostello ComputationalModelsofSpatialPrepositionsforVSD
Thispotentialﬁeldapproachdoesnot,however,accountfortheinﬂuenceofother
objects in the visual scene on the semantics of a topological or projective preposition.
Thebasicideainourcomputationalmodelsistoextendthepotentialﬁeldapproachby
overlaying the potential ﬁelds for each object in a visual scene and combining those
ﬁelds to produce relative potential ﬁelds for topological or projective prepositions.
Theserelativepotentialﬁeldsrepresentthesemanticsofthoseprepositionsasmodiﬁed
bythepresenceofotherobjectsinthevisualscene.
6.1Computational Model of Topological Prepositions
In this section we describe a model of relative proximity that uses (1) the distance
between objects, (2) the size andsalience of the landmark object, and(3) the location
ofotherobjectsinthescene.Ourmodelisbasedonﬁrstcomputingabsoluteproximity
between each point andeach landmark in a scene, andthen combining or overlaying
theresultingabsoluteproximityﬁeldstocomputetherelativeproximityofeachpoint
toeachlandmark.
6.1.1 Computing
Absolute Proximity Fields. We ﬁrst compute for each landmark an ab-
solute proximity ﬁeld givingeachpoint’sproximitytothatlandmark,independentof
proximitytoanyotherlandmark.Wecomputeﬁeldsontheprojectionofthesceneonto
the 2D-plane, representedas a 2D-array of points. At each point P in that array,the
absoluteproximityforlandmarkLis
prox
abs
(L,P)=(1−dist
normalized
(L,P))∗salience(L)(1)
Inthisequationtheabsoluteproximityforapoint P andalandmark L isafunctionof
boththedistancebetweenthepointandthelocationofthelandmark,andthesalience
ofthelandmark.
To represent distance we use a normalized distance function dist
normalized
(L,P),
which returns a value between 0 and1.
5
The smaller the distance between L and P,
thehighertheabsoluteproximityvaluereturned,thatis,themoreacceptableitistosay
thatPisclosetoL.Inthisway,thiscomponentoftheabsoluteproximityﬁeldcaptures
thegradualgradationinapplicabilityevidentinLoganandSadler(1996).
We model the inﬂuence of visual and discourse salience on absolute proximity as
afunctionsalience(L), returning a value between 0 and1 that represents the relative
salience of the landmark L in the scene (Equation (2)). For the current purposes we
assume that the relative salience of an object is the average of its visual salience (S
vis
)
anddiscoursesalience( S
disc
).
6
salience(L)=(S
vis
(L)+S
disc
(L))/2(2)
5 Wenormalizebycomputingthedistancebetweenthetwopoints,andthendividingthisdistancebythe
maximumdistancebetweenpointLandanypointinthescene.
6 Thereare,ofcourse,manyotheroperatorsthatcouldbeusedtocombinevisualandlinguisticsalience,
suchasmaximum(MAX(S
vis
(L),S
disc
(L)))orprobabilisticOR(S
vis
(L)+S
disc
(L)−(S
vis
(L)×S
disc
(L))).
Wecurrentlyhavenowayofdecidingamongtheseoperators.Fortunately,however,themodularnature
ofourframeworkwouldallowustochangethecomputationofrelativesaliencewithoutimpactingother
aspectsofourmodel,shouldevidenceinfavorofoneorotheroperatorbecomeavailable.
281
ComputationalLinguistics Volume35,Number2
Visualsalience S
vis
iscomputedusingthealgorithmofKelleherandvanGenabith
(2004).Computingarelativesalienceforeachobjectinasceneisbasedonitsperceivable
sizeanditscentralityrelativetotheviewer’sfocusofattention.Thealgorithmreturns
scores in the range of 0 to 1. As the algorithm captures object size, we can model
the effect of landmark size on proximity through the salience component of absolute
proximity. The discourse salience (S
disc
) of an object is computedbasedon recency of
mention(Hajicov´a1993)exceptwerepresentthemaximumoverallsalienceinthescene
as1,anduse0toindicatethatthelandmarkisnotsalientinthecurrentcontext.
Figure 10 shows computedabsolute proximity with salience values of 1, 0.6, and
0.5,forpointsfromtheupper-lefttothelower-rightofa2Dplane,withthelandmark
atthecenterofthatplane.Thegraphshowshowsalienceinﬂuencesabsoluteproximity
inourmodel:Foralandmarkwithhighsalience,pointsfarfromthelandmarkcanstill
havehighabsoluteproximitytoit.
6.1.2 Computing
Relative Proximity Fields.Oncewehaveconstructedabsoluteproximity
ﬁeldsforthelandmarksinascene,ournextstepistooverlaytheseﬁeldstoproducea
measure of relative proximity to each landmark at each point. For this we ﬁrst select
a landmark, and then iterate over each point in the scene comparing the absolute
proximityoftheselectedlandmarkatthatpointwiththeabsoluteproximityofallother
landmarks at that point. The relative proximity of a selected landmark at a point is
equaltotheabsoluteproximityﬁeldforthatlandmarkatthatpoint,minusthehighest
absoluteproximityﬁeldforanyotherlandmarkatthatpoint:
prox
rel
(P,L)=prox
abs
(P,L)−
MAX
∀L
X
negationslash=L
prox
abs
(P,L
X
)(3)
Theidea hereisthat theotherlandmark withthehighest absolute proximity isacting
incompetitionwiththeselectedlandmark.Ifthatotherlandmark’sabsoluteproximity
Figure10
AbsoluteproximityratingsforlandmarkLcenteredina2Dplane,pointsrangingfromplane’s
upper-leftcorner(〈−3,−3〉)tolowerrightcorner(〈3,3〉).
282
KelleherandCostello ComputationalModelsofSpatialPrepositionsforVSD
ishigherthantheabsoluteproximityoftheselectedlandmark,theselectedlandmark’s
relative proximity for the point will be negative. If the competing landmark’s absolute
proximity is slightly lower than the absolute proximity of the selectedlandmark, the
selectedlandmark’s relativeproximityforthepointwillbepositive,butlow.Onlywhen
the competing landmark’s absolute proximity is signiﬁcantly lower than the absolute
proximity of the selected landmark will the selected landmark have a high relative
proximityforthepointinquestion.
In Equation (3) the proximity of a given point to a selectedlandmark rises as
that point’s distance from the landmark decreases (the closer the point is to the land-
mark, the higher its proximity score for the landmark will be), but falls as that point’s
distance from some other landmark decreases (the closer the point is to some other
landmark, the lower its proximity score for the selected landmark will be). Figure 11
shows the relative proximity ﬁelds of two landmarks, L1 and L2, computed using
Equation (3) in a 1-dimensional (linear) space. The two landmarks have different de-
greesofsalience:asalienceof0.5forL1andof0.6forL2(representedbythedifferent
sizes of the landmarks). In this ﬁgure, any point where the relative proximity for one
particular landmark is above the zero line represents a point which is proximal to
that landmark, rather than to the other landmark. The extent to which that point is
above zero represents its degree of proximity to that landmark. The overall proximal
area for a given landmark is the overall area for which its relative proximity ﬁeld is
abovezero.Theleftandrightbordersoftheﬁgurerepresenttheboundaries(walls)of
thearea.
Figure11illustratesthreemainpoints.First,theoverallsizeofalandmark’sprox-
imal area is a function of the landmark’s position relative to the other landmark and
to the boundaries. For example, landmark L2 has a large open space between it and
the right boundary: Most of this space falls into the proximal area for that landmark.
Landmark L1 falls into quite a narrow space between the left boundary and L2. L1
thus has a much smaller proximal area in the ﬁgure than L2. Second, the relative
proximity ﬁeld for a landmark is a function of that landmark’s salience. This can be
seen in Figure 11 by considering the space between the two landmarks. In that space
the width of the proximal area for L2 is greater than that of L1, because L2 is more
salient.
Figure11
GraphofrelativeproximityﬁeldsfortwolandmarksL1andL2.Relativeproximityﬁeldswere
computedwithsaliencescoresof0.5forL1and0.6forL2.
283
ComputationalLinguistics Volume35,Number2
Figure12
Examplescene.
ThethirdpointconcernsareasofambiguousproximityinFigure11:areasinwhich
neither of the landmarks have a signiﬁcantly higher relative proximity than the other.
There are two such areas in the Figure. The ﬁrst is between the two landmarks, in
the region where one relative proximity ﬁeldline crosses the other. These points are
ambiguous in terms of relative proximity because these points are equidistant from
those two landmarks. The second ambiguous area is at the extreme right of the space
shown in Figure 11. This area is ambiguous because this area is distant from both
landmarks: Points in this area would not be judged proximal to either landmark. The
question of ambiguity in relative proximity judgments is considered in more detail in
Section8.1.
We will illustrate the different stages of the proximity model using the situation
illustrated in Figure 12. The task is to decide whether the target object is proximal to
thelandmarkobject.Figure13illustratestheabsolutepotentialﬁeldforthelandmark
Figure13
Theabsoluteproximityﬁeldsforthelandmark.
284
KelleherandCostello ComputationalModelsofSpatialPrepositionsforVSD
Figure14
Theabsoluteproximityﬁeldsforthelandmarkandthedistractor.
object. Figure 14 illustrates the absolute potential ﬁelds for the landmark and the
distractor object. Figure 15 illustrates the relative proximity ﬁeld that results from the
interaction between the landmark and distractors absolute proximity ﬁelds. Figure 16
illustratestheapplicationofthethresholdtothelandmark’srelativeproximityﬁeld.If
the target object is locatedin the region where the landmark’s relative proximity ﬁeld
Figure15
Thelandmark’srelativeproximityﬁeld.
285
ComputationalLinguistics Volume35,Number2
Figure16
Applyingthethresholdtothelandmark’srelativeproximityﬁeld.
is above the thresholdthe target is deemedto be proximal to the landmark. Figure 16
demonstratesthecontextualinﬂuencewhichthedistractorobjecthasonthelandmark’s
relativeproximityﬁeld:Theﬁeldshrinksonthesideofthelandmarknearthedistractor,
butexpandsonthesideawayfromthelandmark.
6.2Computational Model ofProjective Prepositions
The two main factors that impact on the applicability of a projective preposition de-
scribingthespatialrelationshipbetweenatargetobjectandalandmarkaretheangular
deviation of the target object’s position from the canonical direction described by the
preposition relative to the landmark and the distance of the target object from the
landmark.
The vector originating from the center of the landmark to the viewer’s position
describes the canonical search axis for in front of. We can produce the search vectors
fortheotherprojectiveprepositions(behind,left,right)byrotatingthisfrontvectorona
horizontalplane.Oncethecanonicalvectorvectorcforagivenprojectiveprepositionhasbeen
selected,theangulardeviationofagivenpointPpositionrelativetothelandmarkLcan
becomputedusingEquation(4):
angle(
vector
LP,vectorc)=cos
−1


vector
LP•vectorc
vextendsingle
vextendsingle
vextendsingle
vector
LP
vextendsingle
vextendsingle
vextendsingle|vectorc|


(4)
where
vector
LPisthevectorfromlandmark Ltopoint Pandvectorcisthecanonicalvectorforthe
projective preposition in question. This equation gives the angle between
vector
LP andthat
canonicalvector.
286
KelleherandCostello ComputationalModelsofSpatialPrepositionsforVSD
Usingthisequation,andthenormalizeddistancemeasuredescribedinSection6.1,
we deﬁne an absolute potential ﬁeld for the acceptability of a projective preposition
withcanonicalvectorvectorcforlandmarkLasfollows:
proj
abs
(L,P,vectorc)=0if (angle(
vector
LP,vectorc) > 90)or(dist
normalized
(L,P)=0),
= (angle(
vector
LP,vectorc)/dist
normalized
(L,P))otherwise (5)
Inthisequation,iftheanglebetweenapointPandthecanonicalvector vectorcisgreaterthan
90degrees,orifthedistancebetweenthelandmarkandthepointis0,theacceptability
of that point for the projective preposition is 0. Otherwise, the acceptability of that
pointisequaltotheanglebetweenthatpointandthecanonicalvector,dividedbythe
normalizeddistancebetweenthatpointandthelandmark.
We use this absolute potential ﬁeldfor projective prepositions in the same way
thatweusedtheabsoluteﬁeldforproximityinourmodeloftopologicalprepositions.
Once we have computedthe absolute potential ﬁeldfor each point relative to the
landmark we then do the same process for each of the distractor landmarks. We
then overlay the landmark applicabilities with those of the distractors by subtracting
the maximum applicability of any of the distractors at a point from the landmark’s
applicabilityatthatpoint,producingarelativepotentialﬁeldfortheprojectiveprepo-
sitionasinEquation(6):
proj
rel
(P,L,vectorc)=proj
abs
(P,Lvectorc)−
MAX
∀L
X
negationslash=L
proj
abs
(P,L
X,vectorc)(6)
We then apply a threshold, and the region above this threshold is taken to deﬁne
the area described by the projective preposition. Note that we can use Equation (6) to
computerelativepotentialﬁeldsforvariousdifferentprojectiveprepositions(infrontof,
behind,left,right,above,below)byselectingthedifferentcanonicalvectorscorresponding
tothoseprepositions.
Figures17through20illustratethedifferentstagesinthisprocess.Intheseimages
the origin is at the front right corner, the x-axis runs from right to left, the y-axis
from front to back, andthe z-axis is the vertical. The higher the z-axis value the more
applicabilitythepreposition.Figure17deﬁnesthebaselineapplicabilityof z =0.1.We
usethisbaselinebecausedividinganangulardeviationbydistancewillneverresultin
azerovalue;ratherapplicabilitywillapproach0asymptotically.Thebaselineprovides
a cut-off point for applicability. Figure 18 illustrates the potential ﬁeldcomputedfor
right of a landmark positioned at x = 100, y = 200, z = 0 with a search axis of x =1,
y = 0.Figure 19illustrates thepotential ﬁeldscomputed for right of thelandmark and
adistractorobjectpositionedat x =150, y =400, z =0.Finally,Figure20illustratesthe
potentialﬁeldthatresultsfor rightof thelandmarkwhenthedistractorpotentialﬁeldis
subtractedfromit.Figure20demonstratesthecontextualinﬂuencewhichthedistractor
objecthasonthelandmark’srelativepotentialﬁeldforthepreposition:thesizeofthe
ﬁeldisreducedbythepresenceofthedistractorobject.
7. Psycholinguistic Evaluations of Our Models
We now describe an experiment which tests our approach to relative proximity by
examiningthechangesinpeople’sjudgmentsoftheappropriatenessoftheexpression
near being used to describe the relationship between a target and landmark object in
287
ComputationalLinguistics Volume35,Number2
Figure17
Abaselineapplicabilityissettoz=0.1.
animagewhereadistractorobjectispresent.Allobjectsintheseimageswerecolored
shapes:circles,triangles,orsquares.
7.1 Materialsand
Procedure
All images usedin this experiment containeda central landmark object anda target
object, usually with a third distractor object. The landmark was always placed in the
Figure18
Thepotentialﬁelddescribingtheabsoluteapplicabilitymodelforrightofthelandmark.
288
KelleherandCostello ComputationalModelsofSpatialPrepositionsforVSD
Figure19
Thepotentialﬁeldsdescribingtheabsoluteapplicabilitymodelforrightofthelandmarkand
rightofadistractorobject.
middleofa7 × 7grid.Imagesweredividedintoeightgroupsofsiximageseach.Each
image in a group containedthe target object placedin one of six different cells on the
grid, numbered from 1 to 6. Figure 21 shows how we number these target positions
accordingtotheirnearnesstothelandmark.
Groupsareorganizedaccordingtothepresenceandpositionofadistractorobject.
In group a the distractor is directly above the landmark, in group b the distractor is
Figure20
Theresultingpotentialﬁeldforrightofthelandmarkwiththebaselineappliedtoit.
289
ComputationalLinguistics Volume35,Number2
Figure21
Relativelocationsoflandmark(L)targetpositions(1...6)anddistractorlandmarkpositions
(a...g)inimagesusedintheexperiment.
rotated45 degrees clockwise from the vertical, in group c it is directly to the right of
thelandmark,in d itisrotated135degreesclockwisefromthevertical,andsoon.The
distractorobjectisalwaysthesamedistancefromthecentrallandmark.Inadditionto
the distractor groups a,b,c,d,e,f, and g, there is an eighth group, group x, in which no
distractorobjectoccurs.
In the experiment, each image was displayed with a sentence of the form The
is near the , with a description of the target and landmark, respectively. The sentence
was presentedunder the image. Twelve participants took part in this experiment. All
participantswerenativeEnglishspeakersandallvolunteeredtotakepart.Participants
were not linguists andwere naive to the formal interpretation of spatial prepositions
andto the hypotheses being testedin the experiment. Participants were askedto rate
the acceptability of the sentence as a description of the image using a 10-point scale,
withzerodenotingnotacceptableatall;4or5denotingmoderatelyacceptable;and9
perfectlyacceptable.Figure22illustratesatrialfromtheexperiment.Eachparticipant
ratedeveryimageintheexperiment.Imageswerepresentedinrandomordertocontrol
forlearningeffects.
7.2Results and Discussion
Therewassigniﬁcantagreementbetweenparticipantsacrossall48images.Theaverage
pair-wisecorrelationbetweenparticipants’responseswasr=0.68.Therewasasigniﬁ-
cantcorrelationofresponsesbetweeneverypairofparticipants(p < 0.01forallpairs).
We assess participants’ responses by comparing their average proximity judgments
with those predicted by the absolute proximity equation (Equation (1)), and by the
relativeproximityequation(Equation(3)).Forbothequationsweassumethatallobjects
have a salience score of 1. With salience equal to 1, the absolute proximity equation
relates proximity between target and landmark objects to the distance between those
twoobjects,sothatthecloserthetargetistothelandmarkthehigheritsproximitywill
be. With salience equal to 1, the relative proximity equation relates proximity to both
distance between target and landmark and distance between target and distractor, so
290
KelleherandCostello ComputationalModelsofSpatialPrepositionsforVSD
Figure22
Anexampletrialfromtheproximityexperiment.
that the proximity of a given target object to a landmark rises as that target’s distance
fromthelandmarkdecreasesbutfallsasthetarget’sdistancefromsomeotherdistractor
object decreases. It shouldbe notedthat proximity scores in both Equations (1) and
(3) are multiplied by a constant salience andthat the evaluations we describe below
(correlation,multipleregression)factoroutmultiplicationbyaconstant.Consequently,
choosingaparticularvalueforsaliencedoesnotaffectourevaluationresults.
Inanalyzingourresultswearecomparingourbasicequationforabsoluteproximity
(Equation (1), in which proximity falls with increasing distance between target and
landmark) with the “relative proximity” extension of this equation (Equation (3), in
whichproximityfallswithincreasingdistancebetweentargetandlandmark,butrises
withdistancetodistractor).Becausebothequationsarequitesimilar(botharebasedon
target–landmarkdistance,whichisobviouslytheprimefactorinproximityjudgments),
weexpectbothequationstoproducequitesimilarresponses.Weexpect,however,that
the relative proximity equations will produce responses which are reliably closer to
people’sproximityjudgmentsthanthoseproducedbytheabsoluteproximityequation.
We initially usedSpearman’s rank-order correlation to compare people’s average
proximityscoreswiththoseproducedbyEquation(1)(absoluteproximity)andEqua-
tion(3)(relativeproximity)foreachgroup.Foreachgroupthisanalysisreplaceseach
proximity score with its rank within that group, andthen compares the ranks. Where
theranksreturnedbyanequationandtheranksfromparticipants’averageproximity
scores are identical, the correlation will be 1.0; where the ranks differ, the correlation
will drop. For the absolute proximity equation, the correlation was 1.0 in six of the
groups, and.94 in the two remaining groups (group c andgroup g). For the relative
proximityequation,theSpearman’srank-ordercorrelationwithpeople’sresponseswas
1.0ineachoftheeightgroups.Thefactthattherelativeproximityequationhasarank-
ordercorrelationof1.0inallgroupswhiletheabsoluteproximityequationfailstoreach
1.0intwogroups(predictingproximity-ranksincorrectlyinthosetwogroups)suggests
291
ComputationalLinguistics Volume35,Number2
thattherelative-proximityequationisabettermodelofpeople’sproximityresponses.
However,thefactthattherearesomanycorrelationsof1.0meansthatSpearman’srank-
ordercorrelationisnotparticularlyusefulindistinguishingbetweenthetwoequations.
We therefore use Pearson’s product-moment correlation to compare people’s average
proximityscoreswiththoseproducedbytheabsoluteandrelativeproximityequations.
Ratherthancomparingranks,thisanalysiscomparesactualproximityvalues.
Figure23showstheproduct-momentcorrelationsbetweenpeople’saverageprox-
imity ratings andthose producedby Equation (1) (absolute proximity) andby Equa-
tion (3) (relative proximity) for the eight groups in the experiment. In analyzing these
correlations we had two concerns: ﬁrst, to see whether, for each individual group, the
correlation produced by Equation (3) was reliably different from that produced by
Equation(1);andsecond,toseewhetheracrossallthegroups,thecorrelationproduced
by Equation (3) was reliably higher than that produced by Equation (1). In regard
to the ﬁrst question, we did not expect there to be particularly large differences in
correlation between the two equations, because both are basedon target–landmark
distance.Becauseweknowtarget–landmarkdistancetobeagoodpredictorofpeople’s
proximityjudgmentsweexpectedEquation(1)tohaveahighcorrelationwithpeople’s
proximity judgments, andwe expectedEquation (3) to improve on that correlation.
However,becausethecorrelationfromEquation(1)wasalreadyhigh,anyimprovement
in correlation from Equation (3) wouldbe relatively small. Indeedthis is what is seen
across the seven groups of interest: The average correlation from Equation (1) is high
(average 0.93), the average correlation from Equation (3) is higher (average 0.99), but
thedifferencebetweenthetwocorrelationsisrelativelysmall.UsingFisher’stechnique
forcomparingcorrelationcoefﬁcientsweﬁndnoreliabledifferencebetweencorrelation
coefﬁcientsinanygroup.
Given that the correlations for both Equations (1) and(3) are high we examined
whethertheresultsreturnedbyEquation(3)werereliablyclosertohumanjudgments
thanthosefromEquation(1).Forthe42imageswhereadistractorobjectwaspresentwe
recorded which equation gave a result that was closer to the participants’ normalized
averageforthatimage.In28casesEquation(3)wascloser,andin14Equation(1)was
closer(a2:1advantageforEquation(3),signiﬁcantinasigntest:n+=28,n− =14,Z=
2.2,p < 0.05). We conclude that proximity judgments for objects in our experiment
are best representedby relative proximity as computedin Equation (3). These results
supportour“relative”modelofproximity.
7
Inadditiontotheseanalyses,wealsocarriedoutamultipleregressionanalysisof
participants’ responses in the experiment, with target–landmark distance and target–
distractor distance as the predictor variables, and participant response as the depen-
dent variable. Because our experiment involved repeated-measures data, wefollowed
the procedure for regression analysis of repeated-measures data described by Lorch
and Myers (1990). This involves computing individual multiple regression for each
participant in our experiment, andthen using a t-test to analyze the regression coef-
ﬁcients produced for target–distractor distance and target–landmark distance in those
equations,acrossallparticipants.Recallthatinourrelativeproximityequation(Equa-
tion (3)) target–landmark distance had a negative coefﬁcient (as target–landmark dis-
tanceincreased,judgmentsoftarget–landmarkproximityfell)whereastarget–distractor
7 Notethat,inordertodisplaytherelationshipbetweenproximityvaluesgivenbyparticipants,computed
inEquation(1),andcomputedinEquation(3),thevaluesdisplayedinFigure23arenormalizedsothat
proximityvalueshaveameanof0andastandarddeviationof1.Thisnormalizationsimplymeansthat
allvaluesfallinthesameregionofthescale,andcanbeeasilycomparedvisually.
292
KelleherandCostello ComputationalModelsofSpatialPrepositionsforVSD
Figure23
Comparisonbetweennormalizedproximityscoresobservedandcomputedforeachgroup.
293
ComputationalLinguistics Volume35,Number2
distance had a positive coefﬁcient (as target–distractor distance increased, judgments
oftarget–landmarkproximityincreased).Ourprediction,therefore,isthatacrossthese
multiple regression analyses of participants’ responses, the target–landmark distance
variablewillreliablyhaveanegativecoefﬁcient,whereasthetarget–distractorvariable
willreliablyhaveapositivecoefﬁcient.
Table1showstheregressioncoefﬁcientsobtainedforthetarget–landmarkdistance
variable and the target–distractor distance variable, across the 12 participants in our
experiment.Asthistableshows,theregressioncoefﬁcientfortarget–landmarkdistance
was signiﬁcantly more likely to be negative (as predicted) whereas the regression
coefﬁcient for target–distractor distance was signiﬁcantly more likely to be positive
(again, as predicted). A single-group t-test showed that both target–landmark regres-
sioncoefﬁcientsandtarget–distractorregressioncoefﬁcientsreliablydifferedfromzero
(t(11)= −8.64, p < 0.01; t(11)=2.23, p < 0.05) indicating that both of these predictor
variables hada signiﬁcant andreliable effect on participants’ responses in the exper-
iment. There was no concern about collinearity between predictor variables in these
regression analyses, as the correlation between those variables (r=0.38, %var =0.14)
wasmuchlowerthanthatbetweenthepredictorvariablesandthedependentvariable
(r=0.93 or higher). Together these regression results, the sign-test results, andthe
comparative correlations described earlier all support the model of relative proximity
asdescribedinEquation(3).
8. Applications oftheModels
The model of proximity presentedhere has been implementedandusedas a compo-
nentinahuman–robotdialogsystem(KelleherandKruijff2006;Kelleher,Kruijff,and
Costello2006).Theproximityandprojectivemodelshavealsobeenintegratedintothe
LIVE virtual environment (Kelleher, Costello, andvan Genabith 2005). In this section
we will describe how the models are used in these systems to interpret and generate
locativeexpressions.
Table1
Regressioncoefﬁcientsfromindividualanalysesofsubjectsdatainproximityexperiment.
participant target–landmarkdistance target–distractordistance
1 −2.02 0.27
2 −1.53 0.09
3 −3.06 0.03
4 −2.45 −0.02
5 −2.23 0.06
6 −0.97 0.32
7 −3.09 0.42
8 −1.78 0.02
9 −0.80 0.16
10 −1.48 −0.24
11 −3.29 0.01
12 −3.29 0.44
M −2.17 0.13
SE 0.88 0.20
t −8.64* 2.23**
*p < 0.01,**p < 0.05.
294
KelleherandCostello ComputationalModelsofSpatialPrepositionsforVSD
8.1InterpretingSpatial References
WeusethecomputationalmodelsofSection6tointerpretspatialreferencestoobjects.
Inthissectionweillustratehowweuseourmodelofrelativeproximitytogroundthe
interpretation of a locative expression containing a topological preposition. Returning
to the architecture described in Section 3, the basic steps triggering the interpretation
ofalocativeare:(1)theuseruttersacommand,suchas pick up the ball near the red box,
(2)thespeechrecognitionmoduleprocessesthespeechsignalandpassestheresulting
stringtotheparser,(3)theparserconstructsaformalrepresentationofthemeaningof
the utterance, (4) the dialog manager categorizes the utterance to be a command and,
also,recognizestheneedtoresolvethereferringexpression the ball near the red box.At
thispointthereferenceresolutionmoduleistriggered.
The ﬁrst stage in reference resolution is to retrieve the context against which the
reference is to be resolved. This involves accessing the context model and retrieving
thesetofcurrentlyaccessibleobjects.Thissetisthensubdividedintothesetofobjects
fulﬁlling the landmark description, the set of objects fulﬁlling the description of the
targetobject,andthesetofobjectsfulﬁllingneitherdescription.
Foreachcandidatelandmarkandeachobjectthatisneitheracandidatelandmark
noracandidatetargetwecomputeanabsoluteproximityﬁeld.Foreachlandmarkwe
convert its absolute proximity ﬁeldinto a relative proximity ﬁeldby overlaying the
absolute proximity ﬁelds of the other landmarks and the other objects in the context
that are neither candidate landmarks nor target objects. For this we iterate over each
pointinthescene,andcomparethecompetingabsoluteproximityscoresateachpoint.
Iftheprimarylandmark’s(i.e.,thelandmarkwiththehighestrelativeproximityatthe
point) relative proximity exceeds the next highest relative proximity score at a given
pointbymorethanapredeﬁnedconﬁdenceinterval,thepointisintheproximityregion
anchoredaroundtheprimarylandmark.Otherwise,wetakeitasambiguousandnotin
theproximalregionthatisbeinginterpreted.Themotivationfortheconﬁdenceinterval
is to capture situations where the difference in relative proximity scores between the
primary landmark and one or more landmarks at a given point is relatively small.
Figure24illustratestheparsingofasceneintotheregions“near”twolandmarks.The
relativeproximityﬁeldsofthetwolandmarksareidenticaltothoseinFigure11,usinga
conﬁdenceintervalof0.1.Ambiguouspointsarewheretheproximityambiguityseries
isplottedat0.5.Theregions“near”eachlandmarkarethoseareasofthegraphwhere
eachlandmark’srelativeproximityseriesisthehighestplotonthegraph.
Figure 24 illustrates an important aspect of our model: the comparison of relative
proximity ﬁelds naturally deﬁnes the extent of vague proximal regions. For example,
see the region right of L2 in Figure 24. The extent of L2’s proximal region in this
directionisboundedbytheinterferenceeffectofL1’srelativeproximityﬁeld.Because
thelandmarks’relativeproximityscoresconverge,theareaonthefarrightoftheimage
is ambiguous with respect to which landmark it is proximal to. In effect, the model
capturesthefactthattheareaisrelativelydistantfrombothlandmarks. InSection8.2
wedescribeacognitiveloadhierarchyofprepositionsandhowweusethistogenerate
locativeexpressions.Followingthishierarchy,objectslocatedintheareaonthefarright
of the image shouldbe describedwith a projective relation such as to the right of L2
ratherthanaproximalrelationlikenear L2.
8.1.1AnExample.Toillustratethemodelfurtherwewillapplythemodeltoarealscene.
Figure25showsarealsceneontheleft-handside,andarenderingofthesceneanalysis
on the right-handside. For the shown scene analysis we have assumedall objects to
295
ComputationalLinguistics Volume35,Number2
Figure24
GraphofambiguousregionsoverlaidonrelativeproximityﬁeldsforlandmarksL1andL2,
withconﬁdenceinterval=0.1anddifferentsaliencescoresforL1(0.5)andL2(0.6).Locations
oflandmarksaremarkedonthex-axis.
have an equal salience: on the left, the blue ball; in the middle, the red ball; and on
the right, the green ball. As the analysis correctly shows, each object has a proximity
potential ﬁeld(shown in its own color) but, due to interference between potential
ﬁelds, we see that proximity is usually ambiguous between at least two landmarks.
Theregionsthatareambiguousbetweentwolandmarksarecoloredusingamixtureof
thecolors.Thewhiteareadenotestheregionsdeﬁnedasbeingambiguousbetweenthe
threeobjects.
Imagine we now place a secondblue ball in the scene andthe user inputs the
commandpickuptheblueballneartheredball.Asexplainedpreviously,whenthesystem
starts interpreting this reference it will split the context into a set of candidate target
objects, consisting of the two blue balls in the scene, the set of candidate landmarks,
consisting of the one redball, andthe set of remaining objects, the green ball. It will
thencomputeproximityﬁeldsforeachofthecandidatelandmarksandtheotherobjects
in the scene that are not candidate targets. It will then overlay these proximity ﬁelds
Figure25
Sceneanalysis.
296
KelleherandCostello ComputationalModelsofSpatialPrepositionsforVSD
to compute the relative proximity ﬁelds around each landmark. Figure 26 illustrates
the resulting proximity ﬁelds. As can be seen from the image the original blue ball is
insidetheredball’sproximityﬁeldandconsequentlyitwillbeselectedastheballtobe
pickedup.
Thisanalysishighlightstwoimportantaspectsofthemodel.First,wecanobserve
aninterferenceeffectbetweentheredballandthegreenball:Thepotentialﬁeldrepre-
sentingproximitytotheredballformsanellipsoid,beinginhibitedtotherightthrough
interference with the potential ﬁeldof the green ball. Second, the proximity ﬁeldof
theredballismuchlargerthenthatofthegreenball;thisisduetotherelativelyhigh
linguisticsalienceoftheredballcomparedtothegreenballduetoitbeingmentioned
inthereference.
8.2Generating References
Inthissectionweillustratehowweuseourmodelsofthesemanticsofspatialpreposi-
tionstoguidethegenerationofalocativeexpressioninvisualsituatedcontexts.
Inthearchitecturedescribedearlier,theGREcomponentistriggeredbythecontent
manager. Similar to reference resolution, GRE will ﬁrst retrieve the context from the
contextmodelandgeneratethereferencerelativetothiscontext.Ifalocativeexpression
isnecessarytheGREcomponent hasthreethingstodecide:(1)whatpropertiesofthe
targetobjecttoinclude,(2)whichobjectinthesceneshouldbeusedasalandmarkand
how shouldthat be described, and(3) which spatial relation to use (andhence which
prepositiontouse).
SeveralGREalgorithmshaveaddressedtheissueofgeneratinglocativeexpressions
(Dale and Haddock 1991; Horacek 1997; Gardent 2002; Krahmer and Theune 2002;
Figure26
Interpretingtheblue ballnear theredball.
297
ComputationalLinguistics Volume35,Number2
Varges2004).However,allthesealgorithmsassumetheGREcomponenthasaccessto
a predeﬁned scene model that deﬁnes all the spatial relations between all the entities
in the scene. For many visually situated dialog systems, in particular robotic dialog
systems,thisassumptionisaseriousdrawbackforthesealgorithms.Ifanagentwishes
to generate a contextually appropriate reference it cannot assume the availability of
a domain model, rather it must dynamically construct one. Moreover, constructing a
model containing all the spatial relationships between all the entities in the domain
is prone to combinatorial explosion, both in terms of the number of objects in the
context (the location of each object in the scene must be checkedagainst all the other
objects in the scene) andnumber of inter-object spatial relations (as a greater number
of spatial relations will require a greater number of comparisons between each pair
of objects). Furthermore, the context-free aprioriconstruction of such an exhaustive
scene model is cognitively implausible. Psychological research indicates that spatial
relations are not preattentively perceptually available (Treisman andGormican 1988).
Rather, their perception requires attention (Logan 1994, 1995). These ﬁndings point to
subjects constructing contextually dependent reduced relational scene models, rather
thananexhaustivecontext-freemodel.
The approach we adopt to generating locative expressions addresses the issue of
combinatorial explosion inherent in relational scene model construction by incremen-
tally creating a series of reduced scene models. Within each scene model only one
spatial relation is considered and only a subset of objects are considered as candidate
landmarks.Thisreducesboththenumberofrelationsthatmustbecomputedovereach
objectpairandthenumberofobjectpairs.Thedecisionastowhichrelationsshouldbe
includedineachscenemodelisguidedbyacognitively-motivatedhierarchyofspatial
relations. The set of candidate landmarks in a given scene is dependent on the set of
objectsinthescenethatfulﬁllthedescriptionofthetargetobjectandthesemanticrelation
that isbeingconsidered.
We use Dale andReiter’s (1995) incremental GRE algorithm as the starting point
for the generation framework. The incremental algorithm iterates through the proper-
tiesofthetargetobjectandforeachpropertycomputesthesetofdistractorobjectsfor
whichtheconjunctionofthepropertiesselectedsofar,andthecurrentproperty,hold.A
propertyisaddedtothelistofselectedpropertiesifitreducesthesizeofthedistractor
objectset.Thealgorithmsucceedswhenallthedistractorshavebeenruledout;itfails
if all the properties have been processedandthere are still some distractor objects.
Thealgorithmcanbereﬁnedbyorderingthecheckingofpropertiesaccordingtoﬁxed
preferences;forexample,ﬁrstataxonomicdescriptionofthetarget,secondanabsolute
property such as color, thirda relative property such as size. Dale andReiter also
stipulate that the type description of the target should be included in the description
even if its inclusion does not distinguish the target from any of the distractors; see
Algorithm 1. Dale andReiter argue that this algorithm has a polynomial complexity
andthatthetheoreticalruntimecanbecharacterizedas n
d
×n
l
:theruntimedepends
solelyonthenumberofdistractorobjects n
d
andthenumberofpropertiesconsidered
initerationsn
l
.Ifweassumethatn
d
andn
l
arebothproportionalton,thenumberofob-
jectsbeingconsidered,thenthecomplexityoftheincrementalalgorithmisofordern
2
.
The incremental algorithm generates a description (in terms of type, color, and
size) which distinguishes a given target object from a set of distractor objects (if such
a description exists). However, we wish to generate locative expressions which iden-
tify objects, rather than simple descriptions. These locative expressions may contain
a description of a landmark object (in terms of type, color, or size), of a target object
(type, color, or size), anda topological or projective preposition relating those two
298
KelleherandCostello ComputationalModelsofSpatialPrepositionsforVSD
Algorithm1TheBasicIncrementalAlgorithm
Require:T=targetobject;D=setofdistractorobjects.
Initialize:P= {type,color,size};DESC= {}
fori=0to|P| do
if |D| negationslash=0then
Dprime = {x : x ∈ D,P
i
(x)=P
i
(T)}
if |Dprime| < |D| then
DESC=DESC∪P
i
(T)
D= {x :x ∈ D,P
i
(x)=P
i
(T)}
end if
else
Distinguishingdescriptiongenerated
iftype(x) negationslash∈ DESCthen
DESC=DESC∪type(x)
end if
returnDESC
end if
endfor
Failedto generatedistinguishingdescription
returnDESC
objects.Togeneratesuchlocativeexpressionswerepeatedlycallthebasicincremental
algorithmforasequenceofdifferentpossiblespatialrelations.Thefactthateachcallto
the algorithm uses a different spatial relation results in a different set of objects from
the context being deﬁned as candidate landmarks for each function call. If a given
spatialrelationallowsthebasicincrementalalgorithmtogenerateadescriptionwhich
distinguishes the target object from the set of distractor objects, that spatial relation is
usedtogenerateanexpressionidentifyingthatobject.Otherwisewemoveonandcall
thebasicincrementalalgorithmforthenextspatialrelationinoursequence.
When generating a referring expression, we use a sequence of possible forms of
reference orderedby assumedcognitive load(see Figure 27), with simpler forms of
reference (those identifying object type, for example) coming early in the sequence
andmorecomplexforms(thoseinvolvingprojectiveprepositions,forexample)coming
later.Thismeansthatourapproachwillpreferentiallyproducesimplerexpressionsto
identify an object, andonly if no such simple expressions can be foundwhich distin-
guishthatobjectsuccessfullywillmorecomplextopologicalorprojectiveprepositions
Figure27
Cognitiveloadofreferenceforms.
299
ComputationalLinguistics Volume35,Number2
beproduced.Oursequenceofrelationscanbeextendedtoincluderelationsofternary
andhigheraritysuchas the ball between the box and the triangleorthe ball near the box and
the triangle.
We use the models of topological and projective prepositions described in Sec-
tions6.1and6.2todeﬁnetheregionsaroundalandmarktowhichagiventopologicalor
projectivedescriptionapplies.Ifthetargetoroneofthedistractorobjectsistheonlyob-
jectwithinthatregionaroundagivenlandmark,thisistakentorepresentacontrastive
use of a preposition relative to that landmark. If that region contains more than one
objectfromthetargetanddistractorobjectset,thenitisarelativeuseofthepreposition.
8.2.1 Landmarks
and Distinguishing Descriptions. In order to use a locative expression,
an object in the context must be selectedto function as the landmark. An implicit
assumptioninselectinganobjecttofunctionasalandmarkisthatthehearercaneasily
identifyandlocatetheobjectwithinthecontext.AsshowninExample(4),alandmark
canbethespeaker,thehearer,thescene,anobjectinthescene,oragroupofobjectsin
thescene.
8
Example 4
• theballonmyright[speaker]
• theballtoyourleft[hearer]
• theballontheright[scene]
• theballtotheleftofthebox[anobjectinthescene]
• theballinthemiddle[groupofobjects]
Clearly,decidingwhichobjectsinagivenvisualcontextcanfunctionaslandmarks
is a complex process. Some of the factors effecting this decision are object salience
andthe functional relationships between objects. However, one basic constraint on
landmarkselectionisthatthelandmarkshouldbedistinguishablefromthetarget.For
example, in the context provided by Figure 28 the ball has a relatively high salience,
becauseitisasingleton,despitethefactthatitissmallerandgeometricallylesscomplex
thantheotherﬁgures.Moreover,inthiscontext,theballistheonlyobjectinthescene
thatcanfunctionasalandmarkwithoutrecoursetousingthesceneitselforagrouping
ofobjectsinthescene.GiventhecontextinFigure28andallotherfactorsbeingequal,
using a locative such as the man to the left of the man wouldbe much less helpful than
usingtheman totheright of theball.
Following this observation, we treat an object as a candidate landmark if the
followingconditionsaremet:
1. Theobjectisnotthetarget.
2. Theobjectisnotamemberofthedistractorset.
Furthermore,atarget landmarkisamemberofthecandidatelandmarksetthatstands
inrelationtothetargetundertherelationbeingconsideredandadistractor landmark
8 SeeGorniakandRoy(2004)forfurtherdiscussionontheuseofspatialextremaofthesceneandgroups
ofobjectsinthesceneaslandmarks.
300
KelleherandCostello ComputationalModelsofSpatialPrepositionsforVSD
Figure28
Visualcontextusedtoillustratetherelativesemanticsoftopologicalandprojectiveprepositions.
isamemberofthecandidatelandmarksetthatstandsinrelationtoadistractorobject
undertherelationbeingconsidered.
Using these categories of landmark we can deﬁne a distinguishing locative de-
scription as a locative description where there is a target landmark that can be dis-
tinguishedusing the basic incremental algorithm from all the members of the set of
distractorlandmarkswhichstandundertherelationusedinthelocativeexpression.
Giventhis,ourapproachistotrytogenerateadistinguishingdescriptionusingthe
standard incremental algorithm. If this fails, we divide the context into three compo-
nents: the target, the distractor objects, and the set of candidate landmarks. We then
begin to iterate through the hierarchy of relations andfor each relation we create a
context model that deﬁnes the set of target and distractor landmarks. Once a context
modelhasbeencreatedweiteratethroughthetargetlandmarks(usingasalienceorder-
ingifthereismorethanone)andtrytocreateadistinguishinglocativedescription.A
distinguishinglocativedescriptioniscreatedbyusingthebasicincrementalalgorithm
to distinguish the target landmark from the distractor landmarks. If we succeed in
generating a distinguishing locative description we return the description and stop
processing.
Algorithm2TheLocativeIncrementalAlgorithm
Require:T=targetobject;D=setofdistractorobjects;R=hierarchyofrelations.
DESC=Basic-Incremental-Algorithm(T,D)
ifDESC negationslash=Distinguishingthen
createCLthe setofcandidate landmarks
CL= {x : x negationslash=T,DESC(x)=false}
fori=0to|R| do
create acontext modelfor relationR
i
consistingof TLthesetoftargetlandmarks andDLthe set
of distractorlandmarks
TL= {y : y ∈ CL,R
i
(T,y)=true}
DL= {z: z ∈ CL,R
i
(D,z)=true}
forj=0to|TL|bysalience(TL)do
LANDDESC=Basic-Incremental-Algorithm(TL
j,DL)
ifLANDDESC=Distinguishingthen
Distinguishinglocative generated
return{DESC,R
i,LANDDESC}
end if
end for
end for
endif
FAIL
301
ComputationalLinguistics Volume35,Number2
If we cannot create a distinguishing locative description we move on to the next,
morecomplexspatialrelationinthesequenceofspatialrelations,andattempttogener-
ateadistinguishinglocativedescriptionusingthatrelation.Thisprocesscontinuesuntil
eitheradistinguishingexpressionisproducedornopossiblespatialrelationsremain.
This algorithm runs the basic incremental algorithm a number of times for each
candidate relation in the list of possible relations. The length of this list will be a
constant; call it R. For each candidate relation, the number of times the incremental
algorithm runs is equal to the number of TL objects (the number of objects which
don’t fulﬁll the description of the target created by the current run of the incremental
algorithm,andwhichthetargetobjectstandsunderthecurrentlyselectedrelationto).
CallthenumberofTLobjectsn
TL
andnotethat n
TL
mustbelessthan,andproportional
to,n(thetotalnumberofobjects).Thenumberoftimesthebasicincrementalalgorithm
canrun,inoursystem,isthenproportionaltoN
TL
×R;replacingwithn
TL
withngives
n×R runs of the basic incremental algorithm. Inserting the complexity of the basic
incremental algorithm into this, we get an overall complexity of n
2
×n×R=n
3
×R,
which although worse than the basic incremental algorithm’s n
2
complexity, is still
polynomial.
Thisalgorithmcannotgenerateembeddedlocativedescriptions,suchas the bag on
the chair near the window, because it does not use spatial relations as properties to
describe the landmark. However, these descriptions can be generated if needed by
replacing the call to the basic incremental algorithm for the landmark object with a
calltothewholelocativeexpressionalgorithm,usingthetargetlandmarkasthetarget
object and the set of distractor landmarks as the distractors. A nice consequence of
thisapproachtogeneratingembeddedlocativedescriptionsisthatinﬁnitedescriptions
(e.g., the bag on the chair supporting the bag on the chair ...) will not be generatedas the
targetobjectisexcludedfromthecontextthatthelandmark’sdescriptionisgenerated
in.However,thecostofbeingabletogeneratetheseembeddeddescriptionsisahigher
exponentialcomplexity.
8.2.2 An
Example. We can illustrate the framework using the visual context provided
by the scene on the left of Figure 29. This context consists of two redboxes R1 and
Figure29
AvisualsceneandthetopologicalanalysisofR1andR2.
302
KelleherandCostello ComputationalModelsofSpatialPrepositionsforVSD
R2 andtwo blue balls B1 andB2. Imagine that we want to refer to B1. We begin by
calling the locative incremental algorithm, Algorithm 2. This in turn calls the basic
incrementalalgorithm,Algorithm1,whichwillreturntheproperty ball.However,this
isnotsufﬁcienttocreateadistinguishingdescriptionasB2isalsoaball.Inthiscontext
thesetofcandidatelandmarksequals {R1,R2} andtheﬁrstrelationinthehierarchyis
topological proximity, which we model as described in Section 6.1. The image on the
rightofFigure29illustratestheanalysisofthesceneusingthisframework:Thegreen
regionontheleftdeﬁnestheareadeemedtobeproximaltoR1,andtheyellowregion
on the right deﬁnes the area deemed to be proximal to R2. It is evident that B1 is in
theareaproximaltoR1;consequentlyR1isclassiﬁedasatargetlandmark.Asnoneof
thedistractors(i.e.,B2)arelocatedinaregionthatisproximaltoacandidatelandmark
therearenodistractorlandmarks.Asaresultwhenthebasicincrementalalgorithmis
called to create a distinguishing description for the target landmark R1 it will return
box and this will be deemed to be a distinguishing locative description. The overall
algorithm will then return the vector {ball, proximal, box} which wouldresult in the
realizergeneratingareferenceoftheform:theballnear the box.
9. Conclusions and Future Work
In this article we have described the application of computational models of spatial
prepositionstovisuallysituateddialogsystems.Thesecomputationalmodelsallowsys-
temstobothinterpretandgenerateexpressionswhichrefertotopologicalandprojective
relationsbetweenobjectsinthevisualenvironment.Thecomputationalmodelsofspa-
tialprepositionswepresentaredesignedtohandlereferenceresolutionandgeneration
incomplexvisualenvironmentscontainingmultipleobjects.Inparticular,thesemodels
are designed to account for the contextual inﬂuence which the presence of multiple
objects has on the semantics of topological andprojective prepositions. In this respect
our computational models move beyond other accounts of the semantics of spatial
prepositions, which typically do not model the contextual inﬂuence of other objects
on spatial semantics. Because most real-worldvisual scenes are complex andcontain
multipleobjects,ourcomputationalmodelsforthesemanticsofspatialprepositionsare
important for visually situated dialog systems intended to operate successfully in the
realworld.
Clearly there are many interesting areas for future work. To date our research has
focusedon a small number of static topological andprojective prepositions. We feel,
however,thatourframeworkwillapplyusefullytoarangeofothermorecomplexstatic
anddynamic prepositions, for example: between, among, within, along, beside, around.
These prepositions either involve several objects or multiple areas and, consequently,
ouraccountoftheeffectofdistractorobjectsonthetarget–landmarkrelationshipcould
provideaworthwhileperspectiveontheirsemantics.
Thisleadstoanotherpromisingareaforfuturework.Althoughourcurrentmodel
was designed to accommodate multiple distractor objects, our empirical studies have
focusedoncaseswherethereisonlyonedistractor.Animportantaimforfutureresearch
istoextendthesestudiesandtestthemodelinsituationswithmultipledistractors.
Fromatheoreticalpointofview,wefeelthatourapproachtothesemanticsofspa-
tialprepositionsillustratesanimportantpointforresearchersworkingonthesemantics
ofnaturallanguageingeneral:thatitispossibletoinvestigateandmodelsemanticsnot
solely as a linguistic phenomenon, but also in terms of non-linguistic factors such as
the visual environment in which language is used. For example, in the psychological
evaluationsdescribedinSection7wefoundthatthesemanticapplicabilityof“near”to
303
ComputationalLinguistics Volume35,Number2
therelationshipbetweenatargetandalandmarkobjectwasreliablyinﬂuencedbythe
presenceandlocationofathird,distractor,object,whichwasnotpartofthelinguistic
context.Thatthesemanticsoflanguageisinﬂuencedbynon-linguisticfactorsisanold
point andan obvious one: however, we think that our research on visually-situated
dialog systems makes a useful contribution by showing that these systems provide
ideal testbeds for investigating the interaction between language and vision, and for
developingdetailedandusefulcomputationalmodelsofhowthoseinteractionswork.
References
Baldridge,J.andG.J.M.Kruijff.2002.
CouplingCCGandhybridlogic
dependencysemantics.InProceedings
of the40th AnnualMeetingof theAssociation
for Computational Linguistics(ACL-02),
pages319–326,Philadelphia,PA.
Baldridge,J.andG.J.M.Kruijff.2003.
Multi-modalcombinatorycategorial
grammar.InProceedingsofthe 10th
Conferenceofthe EuropeanChapterof the
Association for ComputationalLinguistics
(EACL-03),volume1,pages211–218,
Budapest.
Beermann,D.andL.Hellan.2004.Semantic
decompositioninacomputationalHPSG
grammar:Atreatmentofaspectand
context-dependentdirectionals.In
Proceedingsof theHPSG04 Conference,
pages357–377,Leuven.
Bunt,H.1994.Contextanddialoguecontrol.
Think,3:19–31.
Cahill,A.,M.Burke,R.O’Donovan,J.van
Genabith,andA.Way.2004.Long-distance
dependencyresolutioninautomatically
acquiredwide-coveragePCFG-based
LFGapproximations.InProceedingsofthe
42nd AnnualMeetingoftheAssociation for
Computational Linguistics(ACL-04),
pages320–327,Barcelona.
Carletta,J.,A.Isard,S.Isard,J.C.Kowtko,
G.Doherty-Sneddon,andA.H.Anderson.
1997.Thereliabilityofadialoguestructure
codingscheme.ComputationalLinguistics,
23(1):13–32.
Carlson-Radvansky,L.A.andG.D.Logan.
1997.Theinﬂuenceofreferenceframe
selectiononspatialtemplateconstruction.
Journalof MemoryandLanguage,37:411–437.
Cohn,A.G.,B.Bennett,J.M.Gooday,and
N.Gotts.1997.RCC:Acalculusforregion
basedqualitativespatialreasoning.
GeoInformatica,1:275–316.
Coventry,K.R.1998.Spatialprepositions,
functionalrelations,andlexical
speciﬁcation.InP.Olivierand
K.P.Gapp,editors,Representationand
Processingof SpatialExpressions.Lawrence
ErlbaumAssociates,Hillsdale,NJ,
pages247–262.
Coventry,K.R.andS.Garrod.2004. Saying,
Seeingand Acting. ThePsychological
Semantics ofSpatial Prepositions.Essaysin
CognitivePsychologySeries.Lawrence
ErlbaumAssociates,Hillsdale,NJ.
Dale,R.andN.Haddock.1991.Generating
referringexpressionsinvolvingrelations.
InProceedingsofthe 5thConferenceof the
EuropeanChapterofthe Associationfor
Computational Linguistics(EACL-91),
pages161–166,Berlin.
Dale,R.andE.Reiter.1995.Computatinal
interpretationsofthegriceanmaximsin
thegenerationofreferringexpressions.
Cognitive Science,18:233–263.
Fuhr,T.,G.Socher,C.Scheering,and
G.Sagerer.1998.Athree-dimensional
spatialmodelfortheinterpretation
ofimagedata.InP.Olivierand
K.P.Gapp,editors,Representationand
Processingof SpatialExpressions.Lawrence
ErlbaumAssociates,Hillsdale,NJ,
pages103–118.
Gapp,K.P.1994.Basicmeaningsof
spatialrelations:Computationand
evaluationin3Dspace.InProceedings
of the12thNational ConferenceonArtiﬁcial
Intelligence(AAAI-94),Seattle,WA,
pages1393–1398.
Gapp,K.P.1995.Anempiricallyvalidated
modelforcomputingspatialrelations.In
Proceedingsof the19thGermanConferenceon
Artiﬁcial Intelligence(KI-95),Bielefeld,
Germany,pages245–256.
Gardent,C.2002.Generatingminimal
deﬁnitedescriptions.InProceedingsof
the 40thAnnual Meetingofthe Association
of ComputationalLinguistics(ACL-02),
pages96–103,Philadelphia,PA.
Garrod,S.,G.Ferrier,andS.Campbell.1999.
Inandon:Investigatingthefunctional
geometryofspatialprepositions.
Cognition,72:167–189.
Gawron,J.M.1986.Situationsand
prepositions.Linguisticsand Philosophy,
9(3):327–382.
Gorniak,P.andD.Roy.2004.Grounded
semanticcompositionforvisualscenes.
Journalof ArtiﬁcialIntelligenceResearch,
21:429–470.
304
KelleherandCostello ComputationalModelsofSpatialPrepositionsforVSD
Hajicov´a,E.1993.Issuesof SentenceStructure
andDiscoursePatterns,volume2of
Theoreticaland ComputationalLinguistics.
CharlesUniversityPress,Prague.
Hayward,W.G.andM.J.Tarr.1995.Spatial
languageandspatialrepresentation.
Cognition,55:39–84.
Herskovits,A.1986.Language andSpatial
Cognition:AnInterdisciplinaryStudyof
PrepositionsinEnglish.StudiesinNatural
LanguageProcessing.Cambridge
UniversityPress,Cambridge,UK.
Horacek,H.1997.Analgorithmfor
generatingreferentialdescriptionswith
ﬂexible interfaces.InProceedingsofthe
35thAnnual Meetingofthe Associationfor
ComputationalLinguistics,pages206–213,
Madrid.
Jackendoff,R.1983.Semantics andCognition.
CurrentStudiesinLinguistics.TheMIT
Press,Cambridge,MA.
Kelleher,J.,F.Costello,andJ.vanGenabith.
2005.Dynamicallystructuring,updating
andinterrelatingrepresentationsofvisual
andlingusiticdiscoursecontext. Artiﬁcial
Intelligence,167(1–2):62–102.
Kelleher,J.andJ.vanGenabith.2004.Visual
salienceandreferenceresolutionin
simulated3Denvironments. AI Review,
21(3-4):253–267.
Kelleher,J.andJ.vanGenabith.2006.A
computationalmodelofthereferential
semanticsofprojectiveprepositions.
InP.Saint-Dizier,editor,Syntaxand
Semanticsof Prepositions,Speechand
LanguageProcessing.KluwerAcademic
Publishers,Dordrecht,TheNetherlands,
pages199–216.
Kelleher,J.D.andG.J.Kruijff.2006.
Incrementalgenerationofspatial
referringexpressionsinsituateddialog.
InProceedingsof the3rdJoint Conference
ofthe InternationalCommitteeon
ComputationalLinguisticsand the
Associationfor Computational Linguistics
(COLING/ACL-06),pages1041–1048,
Sydney.
Kelleher,J.D.,G.J.Kruijff,andF.Costello.
2006.Proximityincontext:anempirically
groundedcomputationmodelof
proximityforprocessingtopological
spatialexpressions.InProceedingsof the
3rdJoint Conferenceofthe International
Committeeon ComputationalLinguistics
andthe Associationfor Computational
Linguistics(COLING/ACL-06),
pages745–752,Sydney.
Klein,M.1999.Anoverviewofthestateof
theartofcodingschemesfordialogueact
annotation.InV.Matousek,P.Mautner,
J.Ocelikov´a,andP.Sojka,editors, Text,
SpeechandDialogue (TSD’99),Lecture
NotesinComputerScience.Springer,
Berlin/Heidelberg,pages274–297.
Krahmer,E.andM.Theune.2002.Efﬁcient
context-sensitivegenerationofreferring
expressions.InK.vanDeemterand
R.Kibble,editors,InformationSharing:
ReferenceandPresuppositioninLanguage
GenerationandInterpretation.CLSI
Publications,Stanford,CA,pages223–263.
Kruijff,Geert-Jan,JohnKelleher,andNick
Hawes.2006.Informationfusionfor
visualreferenceresolutionindynamic
situateddialogue.InElisabethAndre,
LailaDybkjaer,WolfgangMinker,
HeikoNeumann,andMichaelWeber,
editors,InProceedingsofPerceptionand
InteractiveTechnologies (PIT06),
volume4021ofLectureNotes inComputer
Science.SpringerBerlin/Heidelberg,
pages117–128.
Kuipers,Benjamin.2000.Thespatial
semantichierarchy.ArtiﬁcialIntelligence,
19:191–233.
Landau,B.1996.Multiplegeometric
representationsofobjectsinlanguage
andlanguagelearners.InP.Bloom,
M.Peterson,L.Nadel,andM.Garrett,
editors,Language andSpace.MITPress,
Cambridge,MA,pages317–363.
Levelt,W.J.M.1996.Perspectivetakingand
ellipsisinspatialdescriptions.In
M.Bloom,P.Peterson,L.Nadell,and
M.Garrett,editors,Language andSpace.
MITPress,Cambridge,MA,pages77–108.
Levinson,S.1996.Frameofreferenceand
Molyneux’squestion:Crosslinguistic
evidence.InM.Bloom,P.Peterson,
L.Nadell,andM.Garrett,editors,
Language and Space.MITPress,
Cambridge,MA,pages109–170.
Levinson,S.2003.Space in Language and
Cognition:ExplorationsinCognitive
Diversity.CambridgeUniversityPress,
Cambridge,UK.
Logan,G.D.1994.Spatialattentionandthe
apprehensionofspatialrelations.Journal
ofExperimentalPsychology: Human
PerceptionandPerformance,20:1015–1036.
Logan,G.D.1995.Linguisticandconceptual
controlofvisualspatialattention.
CognitivePsychology,12:523–533.
Logan,G.D.andD.D.Sadler.1996.
Acomputationalanalysisofthe
apprehensionofspatialrelations.
InM.Bloom,P.Peterson,L.Nadell,
andM.Garrett,editors, Language and
305
ComputationalLinguistics Volume35,Number2
Space.MITPress,Cambridge,MA,
pages493–529.
Lorch,R.F.andJ.L.Myers.1990.
Regressionanalysesofrepeated
measuresdataincognitiveresearch.
Journalof ExperimentalPsychology:
Learning,Memory,and Cognition,
16(1):149–157.
Olivier,P.andJ.Tsujii.1994.Quantitative
perceptualrepresentationofprepositional
semantics.Artiﬁcial IntelligenceReview,
8:147–158.
Regier,TandL.Carlson.2001.Grounding
spatiallanguageinperception:
Anempiricalandcomputational
investigation.JournalofExperimental
Psychology: General,130(2):273–298.
Treisman,A.andS.Gormican.1988.Feature
analysisinearlyvision:Evidencefrom
searchassymetries.Psychological Review,
95:15–48.
Tseng,J.L.2000.The Representationand
SelectionofPrepositions.Ph.D.thesis,
UniversityofEdinburgh.
Varges,S.2004.Overgeneratingreferring
expressionsinvolvingrelationsand
booleans.InProceedingsof the3rd
InternationalConferenceonNatural
Language Generation(INLG-04),
pages171–181,Brighton.
Yamada,A.1993.StudiesinSpatial
DescriptionsUnderstandingBasedon
GeometricConstraintsSatisfaction.
Ph.D.thesis,UniversityofKyoto.
306
This article has been cited by:
1.Timothy Baldwin, Valia Kordoni, Aline Villavicencio. 2009. Prepositions in Applications:
A Survey and Introduction to the Special IssuePrepositions in Applications: A Survey and
Introduction to the Special Issue. Computational Linguistics 35:2, 119-149. [Citation] [PDF]
[PDF Plus]

