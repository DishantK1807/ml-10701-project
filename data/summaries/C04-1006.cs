Additional linguistic knowledge sources such as dependency trees or parse trees were used in (Cherry and Lin, 2003) and (Gildea, 2003).
P03-1011 P03-1012
In (Cherry and Lin, 2003)aprobabilitymodelPr(aJ1jfJ1 ;eI1)is used, which is symmetric per definition.
P03-1012
French English Train Sentences 128K Words 2.12M 1.93M Vocabulary 37542 29414 Singletons 12986 9572 Test Sentences 500 Words 8749 7946 (Och and Ney, 2003), the first 100 sentences of the test corpus are used as a development corpus to optimize model parameters that are not trained via the EM algorithm, e.g. the discounting parameter for lexicon smoothing.
J03-1002
5.1 Evaluation Criteria We use the same evaluation criterion as described in (Och and Ney, 2000).
P00-1056
We will show statistically significant improvements compared to state-of-the-art results in (Och and Ney, 2003).
J03-1002
Bilingual bracketing methods were used to produce a word alignment in (Wu, 1997).
J97-3002
In (Och and Ney, 2003), the alignment quality of statistical models is compared to alternative approaches, e.g. using the Dice coefficient or the competitive linking algorithm.
J03-1002
As we use the same training and testing conditions as (Och and Ney, 2003), we will refer to the results presented in that article as the baseline results.
J03-1002
Alignments In (Och and Ney, 2003) generalized alignments are used, thus the final Viterbi alignments of both translation directions are combined using some heuristic.
J03-1002
95.2 95.3 4.7 93.6 90.8 7.5 (Toutanova et al., 2002), extensions to the HMM-based alignment model are presented.
W02-1012
The HMM-based alignment model was introduced in (Vogel et al., 1996).
C96-2141
For the Verbmobil task, the refined method of (Och and Ney, 2003) is used.
J03-1002
Work The popular IBM models for statistical machine translation are described in (Brown et al., 1993).
J93-2003
Symmetrization In Table 3 and Table 4, we present the following experiments performed for both the Verbmobil and the Canadian Hansards task: â€  Base: the system taken from (Och and Ney, 2003) that we use as baseline system.
J03-1002
We have evaluated these methods on the Verbmobil task and the Canadian Hansards task and compared our results to the state-of-the-art system of (Och and Ney, 2003).
J03-1002
These applications depend heavily on the quality of the word alignment (Och and Ney, 2000).
P00-1056
In Table 3, we compare both interpolation variants for the Verbmobil task to (Och and Ney, 2003).
J03-1002
These alignment models stem from the source-channel approach to statistical machine translation (Brown et al., 1993).
J93-2003
In (Och and Ney, 2003), it is shown that the statistical approach performs very well compared to alternative approaches, e.g. based on the Dice coefficient or the competitive linking algorithm (Melamed, 2000).
J00-2004 J03-1002
(Melamed, 2000) uses an alignment model that enforces one-to-one alignments for nonempty words.
J00-2004
Using the IBM translation models IBM-1 to IBM-5 (Brown et al., 1993), as well as the Hidden-Markov alignment model (Vogel et al., 1996), we can produce alignments of good quality.
C96-2141 J93-2003
A detailed description of the popular translation models IBM-1 to IBM-5 (Brown et al., 1993), aswellastheHidden-Markovalignmentmodel (HMM) (Vogel et al., 1996) can be found in (Och and Ney, 2003).
C96-2141 J03-1002 J93-2003
Word alignment models were first introduced in statistical machine translation (Brown et al., 1993).
J93-2003
We use the same training schemes (model sequences) as presented in (Och and Ney, 2003).
J03-1002
Obvious applications are the extraction of bilingual word or phrase lexica (Melamed, 2000; Och and Ney, 2000).
J00-2004 P00-1056
A good overview of these models is given in (Och and Ney, 2003).
J03-1002
