Proceedings of the Workshop on BioNLP: Shared Task, pages 50–58,
Boulder, Colorado, June 2009. c 2009 Association for Computational Linguistics
High-precisionbiologicalevent extractionwith a conceptrecognizer
K. BretonnelCohen∗, KarinVerspoor∗, HelenL. Johnson,ChrisRoeder,
PhilipV. Ogren, WilliamA. BaumgartnerJr., ElizabethWhite,HannahTipney, and Lawrence Hunter
Centerfor ComputationalPharmacology
University of ColoradoDenver Schoolof Medicine
PO Box 6511, MS 8303, Aurora,CO 80045USA
kevin.cohen@gmail.com, karin.verspoor@ucdenver.edu, helen.linguist@gmail.com,
chris.roeder@ucdenver.edu, philip@ogren.info, william.baumgartner@ucdenver.edu,
elizabeth.white@colorado.edu, hannah.tipney@ucdenver.edu, larry.hunter@ucdenver.edu
Abstract
We approachedthe problems of event detec-
tion,argumentidentification,andnegationand
speculationdetectionas one of conceptrecog-
nition and analysis. Our methodology in-
volved using the OpenDMAPsemanticparser
with manually-written rules. We achieved
state-of-the-artprecision for two of the three
tasks, scoring the highest of 24 teams at pre-
cision of 71.81 on Task 1 and the highestof 6
teams at precisionof 70.97 on Task 2.
The OpenDMAPsystem and the rule set are
availableat bionlp.sourceforge.net.
*Thesetwo authorscontributed equallyto the
paper.
1 Introduction
We approached the problem of biomedical event
recognitionas one of conceptrecognitionand anal-
ysis. Concept analysis is the process of taking a
textual input and building from it an abstract rep-
resentation of the concepts that are reflected in it.
Conceptrecognitioncan be equivalent to the named
entity recognition task when it is limited to locat-
ing mentionsof particularsemantictypes in text, or
it can be more abstractwhen it is focusedon recog-
nizingpredicative relationships,e.g. events and their
participants.
2 BioNLP’09Shared Task
Our system was entered into all three of the
BioNLP’09(Kim et al., 2009) sharedtasks:
• Event detection and characterization This
task requires recognitionof 9 basic biological
events: gene expression, transcription,protein
catabolism,proteinlocalization,binding,phos-
phorylation,regulation,positive regulationand
negative regulation. It requires identification
of the core THEME and/or CAUSE participants
in the event, i.e. the protein(s)being produced,
broken down, bound,regulated,etc.
• Event argument recognitionThis task builds
on the previous task, addingin additionalargu-
ments of the events, such as the site (proteinor
DNA region)of a bindingevent, or the location
of a proteinin a localizationevent.
• Recognition of negations and speculations
This task requiresidentificationof negationsof
events (e.g. event X did not occur),and specu-
lation about events (e.g. We claim that event X
should occur).
3 Our
approach
We used the OpenDMAPsystem developed at the
University of ColoradoSchoolof Medicine(Hunter
et al., 2008) for our submission to the BioNLP
’09 Shared Task on Event Extraction. OpenDMAP
is an ontology-driven, integrated concept analysis
system that supports information extraction from
text through the use of patterns represented in a
classic form of “semanticgrammar,” freely mixing
text literals, semanticallytyped basal syntacticcon-
stituents, and semanticallydefined classes of enti-
ties. Our approachis to take advantage of the high
50
quality ontologies available in the biomedical do-
main to formally define entities, events, and con-
straints on slots within events and to develop pat-
terns for how conceptscan be expressedin text that
take advantageof both semanticand linguisticchar-
acteristicsof the text. We manuallybuilt patternsfor
each event type by examiningthe training data and
by using native speaker intuitionsabout likely ways
of expressingrelationships,similar to the technique
describedin (Cohenet al., 2004). The patternschar-
acterize the linguistic expression of that event and
identify the arguments (participants) of the events
according to (a) occurrencein a relevant linguistic
context and (b) satisfaction of appropriatesemantic
constraints,as definedby our ontology. Our solution
resultsin very highprecisioninformationextraction,
althoughthe currentrule set has limitedrecall.
3.1 The
reference ontology
The central organizing structure of an OpenDMAP
project is an ontology. We built the ontology
for this project by combining elements of several
community-consensusontologies—theGene Ontol-
ogy (GO), Cell Type Ontology (CTO), BRENDA
Tissue Ontology (BTO), Foundational Model of
Anatomy(FMA), Cell Cycle Ontology(CCO), and
Sequence Ontology (SO)—and a small number of
additionalconceptsto representtask-specificaspects
of the system,such as event triggerwords. Combin-
ing the ontologieswas donewith the Promptplug-in
for Prot´eg´e.
Theontologyincludedconceptsrepresentingeach
event type. These were representedas frames, with
slots for the various things that needed to be re-
turnedby the system—thetrigger word and the var-
ious slot fillers. All slot fillers were constrainedto
be concepts in some community-consensusontol-
ogy. The core event argumentswere constrainedin
the ontologyto be of typeproteinfromthe Sequence
Ontology (except in the case of regulation events,
wherebiologicalevents themselves couldsatisfythe
THEME role), whilethe type of the other event argu-
ments varied. For instance, the ATLOC argument
of a gene expression event was constrained to be
one of tissue (from BTO), cell type (from CTO), or
cellularcomponent(fromGO-CellularComponent),
whilethe BINDING argumentof a bindingevent was
constrainedto be one of binding site, DNA, domain,
or chromosome(all from the SO and all tagged by
LingPipe).Table 1 lists the varioustypes.
3.2 Namedentityrecognition
For proteins, we used the gold standard annota-
tions provided by the organizers. For other seman-
tic classes, we constructed a compoundnamed en-
tity recognitionsystemwhichconsistsof a LingPipe
GENIAtaggingmodule(LingPipe,(Alias-i,2008)),
and several dictionarylook-upmodules. The dictio-
nary lookup was done using a componentfrom the
UIMA(IBM, 2009; Ferrucciand Lally, 2004) sand-
box calledthe ConceptMapper.
We loaded the ConceptMapper with dictionar-
ies derived from several ontologies, including the
Gene Ontology Cellular Component branch, Cell
Type Ontology, BRENDA Tissue Ontology, and
the SequenceOntology. The dictionaries contained
the names and name variants for each concept in
each ontology, and matches in the input documents
were annotatedwith the relevant conceptID for the
match. The only modifications that we made to
these community-consensusontologies were to re-
move the singleconceptcell from the Cell Type On-
tology and to add the synonym nuclear to the Gene
OntologyCell Componentconceptnucleus.
Theproteinannotationswereusedto constrainthe
text entitiesthat could satisfythe THEME role in the
events of interest. The other named entities were
addedfor the identificationof non-coreevent partic-
ipantsfor Task 2.
3.3 Pattern
developmentstrategies
3.3.1 Corpus
analysis
Usinga tool that we developedfor visualizingthe
trainingdata (describedbelow), a subsetof the gold-
standard annotations were grouped by event type
and by trigger word type (nominalization,passive
verb, active verb, or multiword phrase). This orga-
nizationhelpedto suggestthe argumentstructuresof
the event predicatesand also highlightedthe varia-
tion within argument structures. It also showed the
natureof more extensive interveningtext that would
need to be handledfor the patternsto achieve higher
recall.
Based on this corpus analysis, patterns were de-
velopedmanuallyusingan iterative processin which
individualpatternsor groupsof patternswere tested
51
Table 1: Semanticrestrictionson Task 2 event arguments. CCO = Cell Cycle Ontology, FMA = FoundationalModel
of Anatomy, other ontologiesidentifiedin the text.
Event Type Site AtLoc ToLoc
binding protein domain (SO),
binding site (SO), DNA
(SO), chromosome(SO)
gene expression gene (SO), biological
entity (CCO)
tissue (BTO), cell type
(CTO), cellular compo-
nent (GO)
localization cellular component
(GO)
cellular component
(GO)
phosphorylation amino acid (FMA),
polypeptideregion (SO)
proteincatabolism cellular component
(GO)
transcription gene (SO), biological
entity (CCO)
on the trainingdatato determinetheirimpacton per-
formance. Pattern writers started with the most fre-
quent triggerwords and argumentstructures.
3.3.2 Triggerwords
In the trainingdata,we wereprovidedannotations
of all relevant event types occurringin the training
documents. These annotations included a trigger
word specifyingthe specific word in the input text
which indicated the occurrenceof each event. We
utilized the trigger words in the training set as an-
chors for our linguistic patterns. We built patterns
around the generic concept of, e.g. an expression
trigger word and then varied the actual strings that
wereallowedto satisfythatconcept.We thenran ex-
perimentswithour patternsand thesevaryingsets of
trigger words for each event type, discardingthose
that degraded system performancewhen evaluated
with respectto the gold standardannotations.
Most often a trigger word was removed from an
event type trigger list because it was also a trig-
ger word for another event type and therefore re-
duced performanceby increasingthe false positive
rate. For example, the trigger words “level” and
“levels” appearin the trainingdata triggerword lists
of gene expression,transcription,and all three regu-
lation event types.
The selection of trigger words was guided by a
frequency analysis of the trigger words provided in
the task trainingdata. In a post-hocanalysis,we find
that a differentproportionof the set of triggerwords
was finallychosenfor each differentevent type. Be-
tween 10-20% of the top frequency-ranked trigger
words were used for simple event types, with the
exception that phosphorylationtrigger words were
chosenfrom the top 30%. For instance,for gene ex-
pressionall of the top 15 mostfrequenttriggerwords
wereused(correspondingto the top 16%). For com-
plex event types(theregulations)betterperformance
was achieved by limiting the list to between5-10%
of the most frequenttriggerwords.
In addition, variants of frequent trigger words
wereincluded.For instance,the nominalization“ex-
pression”is the most frequentgene expressiontrig-
ger word and the verbal inflections“expressed”and
“express”are also in the top 20%. The verbalinflec-
tion “expresses”is ranked lower than the top 30%,
but was nonethelessincludedas a triggerword in the
gene expressionpatterns.
3.3.3 Patterns
As in our previous publicationson OpenDMAP,
we refer to our semantic rules as patterns. For
this task, each pattern has at a minimum an event
argument THEME and an event-specific trigger
word. For example, {phosphorylation} :=
52
[phosphorylation nominalization][Theme],
where [phosphorylization nominalization]
representsa triggerword. Both elements are defined
semantically. Event THEMEs are constrained by
restrictions placed on them in the ontology, as
describedabove.
The methodologyfor creatingcomplex event pat-
terns such as regulation was the same as for sim-
ple events, with the exception that the THEMEs
were defined in the ontology to also include bio-
logical processes. Iterative pattern writing and test-
ing was a little more arduous because these pat-
terns relied on the success of the simple event pat-
terns, and hence more in-depth analysis was re-
quired to perform performance-increasingpattern
adjustments. For further details on the pattern lan-
guage,the readeris referredto (Hunteret al., 2008).
3.3.4 Nominalizations
Nominalizationswere very frequent in the train-
ing data; for seven out of nine event types, the most
commontriggerword was a nominalization.In writ-
ing our grammars,we focusedon these nominaliza-
tions. To write grammars for nominalizations,we
capitalizedon some of the insights from (Cohen et
al., 2008). Non-ellided(or otherwiseabsent) argu-
ments of nominalizationscan occur in three basic
positions:
• Within the noun phrase, after the nominaliza-
tion, typicallyin a prepositionalphrase
• Withinthe nounphrase,immediatelypreceding
the nominalization
• Externalto the noun phrase
The first of these is the most straightforward to
handle in a rule-based approach. This is particu-
larly true in the case of a task definition like that
of BioNLP’09, which focused on themes, since an
examinationof the training data showed that when
themeswere post-nominalin a prepositionalphrase,
then that phrasewas most commonlyheadedby of.
The second of these is somewhat more challeng-
ing. This is because both agents and themes can
occur immediately before the nominalization,e.g.
phenobarbitalinduction (induction by phenobarbi-
tal) and trkA expression(expressionof trkA).To de-
cidehow to handlepre-nominalarguments,we made
use of the data on semanticroles and syntacticposi-
tion found in (Cohenet al., 2008). That study found
that themes outnumberedagents in the prenominal
positionby a ratio of 2.5 to 1. Based on this obser-
vation, we assigned pre-nominal arguments to the
themerole.
Noun-phrase-external arguments are the most
challenging,both for automatic processingand for
human interpreters; one of the major problems is
to differentiate between situations where they are
presentbut outsideof the nounphrase,andsituations
wherethey are entirelyabsent. Sincethe currentim-
plementationof OpenDMAPdoes not have robust
access to syntactic structure, our only recourse for
handling these arguments was through wildcards,
and since they mostlydecreasedprecisionwithouta
correspondingincreasein recall, we did not attempt
to capturethem.
3.3.5 Negationand
speculation
Corpus analysis of the training set revealed two
broad categories each for negation and speculation
modifications,all of whichcan be describedin terms
of the scope of modification.
Negation
Broadly speaking, an event itself can be negated
or some aspect of an event can be negated. In other
words, the scope of a negation modificationcan be
over the existenceof an event (first examplebelow),
or over an argumentof an existingevent (secondex-
ample).
• This failure to degrade IkappaBalpha ...
(PMID10087185)
• AP-1 but not NF-IL-6 DNA binding activity ...
(PMID10233875)
Patterns were written to handle both types of
negation. The negation phrases “but not” and “but
neither” were appended to event patterns to catch
those events that were negated as a result of a
negated argument. For event negation, a more ex-
tensive list of trigger words was used that included
verbal phrasessuch as “failure to” and “absenceof.”
The search for negated events was conducted in
two passes. Events for whichnegation cues fall out-
side the span of text that stretchesfrom argumentto
53
event trigger word were handled concurrentlywith
the search for events. A second search was con-
ductedon extractedevents for negationcues that fell
withinthe argumentto event triggerword span,such
as
. . . IL-2 does not induceI kappaB alphadegrada-
tion (PMID10092783)
This second pass allowed us to capture one addi-
tional negation (6 rather than 5) on the test data.
Speculation
The two types of speculationin the training data
can be describedby the distinctionbetween“de re”
and “de dicto” assertions. The “de dicto” assertions
of speculationin the trainingdata are modifications
that call into question the degree of known truth of
an event, as in
. . . CTLA-4 ligation did not appear to affect the
CD28mediatedstabilization (PMID10029815)
The “de re” speculationaddress the potentialex-
istence of an event rather that its degree of truth. In
these cases, the event is often being introduced in
text by a statementof intentionto studythe event, as
in
. . . we investigated CTCF expression
. . . [10037138]
To addressthesedistincttypesof speculation,two
sets of trigger words were developed. One set con-
sisted largely of verbs denoting research activities,
e.g. research, study, examine investigate, etc. The
other set consistedof verbs and adverbs that denote
uncertainty, and includedtriggerwords such as sug-
gests, unknown,and seems.
3.4 Handlingof
coordination
Coordinationwas handledusing the OpenNLPcon-
stituent parser along with the UIMA wrappers that
they provide via their code repository. We chose
OpenNLPbecause it is easy to train a model, it in-
tegrates easily into a UIMA pipeline, and because
of competitive parsingresults as reportedby Buyko
(Buyko et al., 2006). The parser was trained using
500 abstracts from the beta version of the GENIA
treebank and 10 full-text articles from the CRAFT
corpus (Verspoor et al., In press). From the con-
stituent parse we extracted coordination structures
into a simplified data structure that captures each
conjunction along with its conjuncts. These were
provided to downstream components. The coordi-
nation componentachieves an F-score of 74.6% at
the token level and an F-score of 57.5% at the con-
junctlevel whenevaluatedagainst GENIA.For both
measuresthe recallwas higherthan the precisionby
4% and 8%, respectively.
We utilized the coordinationanalysis to identify
events in whichthe THEME argumentwas expressed
as a conjoinednoun phrase. These were assumedto
have a distributed reading and were post-processed
to create an individual event involving each con-
junct, and furtherfilteredto only includegiven (A1)
protein references. So, for instance, analysis of the
sentence in the example below should result in the
detection of three separate gene expression events,
involving the proteins HLA-DR,CD86, and CD40,
respectively.
NAC was shown to down-regulate the
production of cytokines by DC as well
as their surface expression of HLA-
DR, CD86 (B7-2), and CD40 molecules
. . . (PMID10072497)
3.5 Software
infrastructure
We took advantage of our existing infrastructure
based on UIMA (The Unstructured Information
Management Architecture) (IBM, 2009; Ferrucci
and Lally, 2004) to supporttext processingand data
analysis.
3.5.1 Developmenttools
We developed a visualizationtool to enable the
linguisticpattern writers to better analyze the train-
ing data. This tool shows the source text one sen-
tenceat a timewiththe annotatedwordshighlighted.
A list following each sentence shows details of the
annotations.
3.6 Errors
in the trainingdata
In somecases,there were discrepanciesbetweenthe
training data and the official problem definitions.
This was a source of problemsin the pattern devel-
opmentphase. For example,phosphorylationevents
are defined in the task definition as having only a
THEME and a SITE. However, there were instances
in the trainingdata that includedboth a THEME and
a CAUSE argument. When those events were identi-
fied by our systemand the CAUSE was labelled,they
54
were rejected during a syntactic error check by the
test server.
4 Results
4.1 OfficialResults
We are listed as Team 13. Table 2 shows our re-
sults on the official metrics. Our precision was the
highest achieved by any group for Task 1 and Task
2, at 71.81 for Task 1 and 70.97 for task 2. Our re-
calls were much lower and adversely impacted our
F-measure; ranked by F-measure, we ranked 19th
out of 24 groups.
We notedthat our resultsfor the exact matchmet-
ric and for the approximatematch metric were very
close, suggestingthat our techniquesfor named en-
tity recognition and for recognizing trigger words
are doing a good job of capturing the appropriate
spans.
4.2 Otheranalysis:Bug fixes and coordination
handling
In addition to our official results, we also report in
Table 3 (see last page) the results of a run in which
we fixed a numberof bugs. This representsour cur-
rent best estimateof our performance.The precision
dropsfrom71.81for Task1 to 67.19,andfrom70.97
for Task 2 to 65.74, but these precisions are still
well above the second-highestprecisions of 62.21
for Task 1 and 56.87 for Task 2. As the table shows,
we had correspondingsmall increases in our recall
to 17.38 and in our F-measureto 27.62 for Task 1,
and in our recallto 17.07and F-measureto 27.10for
Task 2.
We evaluatedthe effects of coordinationhandling
by doing separate runs with and without this ele-
ment of the processingpipeline. Compared to our
unofficial results, which had an overall F-measure
for Task 1 of 27.62 and for Task 2 of 27.10, a ver-
sion of the systemwithouthandlingof coordination
had an overall F-measurefor Task 1 of 24.72and for
Task 2 of 24.21.
4.3 Error
Analysis
4.3.1 False
negatives
To better understandthe causes of our low recall,
we performeda detailederror analysisof false neg-
atives using the devtest data. (Note that this section
includesa very small numberof examplesfrom the
devtest data.) We found five major causes of false
negatives:
• Interveningmaterialbetweentriggerwordsand
arguments
• Coordinationthat was not handledby our coor-
dinationcomponent
• Low coverage of triggerwords
• Anaphoraand coreference
• Appositive gene namesand symbols
Intervening material For reasons that we detail
in the Discussion section, we avoided the use of
wildcards. This, and the lack of syntactic analy-
sis in the version of the system that we used (note
that syntactic analyses can be incorporatedinto an
OpenDMAPworkflow), meantthat if there was text
interveningbetweena triggerwordandan argument,
e.g. in to efficiently [express] in developing thymo-
cytes a mutant form of the [NF-kappaB inhibitor]
(PMID 10092801), where the bracketed text is the
trigger word and the argument, our pattern would
not match.
UnhandledcoordinationOur coordinationsystem
only handled coordinatedprotein names. Thus, in
cases where other important elements of the utter-
ance, such as the trigger word transcriptionin tran-
scription and subsequent synthesis and secretion
of galectin-3(PMID 8623933)were in coordinated
structures,we missedthe relevant event arguments.
Low coverage of trigger words As we discuss in
the Methods section, we did not attempt to cover
all triggerwords, in part becausesome less-frequent
triggerwords were involved in multipleevent types,
in part because some of them were extremely low-
frequency and we did not want to overfit to the train-
ing data,andin partdueto the timeconstraintsof the
sharedtask.
Anaphora and coreference Recognition of some
events in the data would require the ability to do
anaphoraand coreferenceresolution. For example,
in Although 2 early lytic transcripts, [BZLF1] and
[BHRF1], were also detected in 13 and 10 cases,
respectively, the lack of ZEBRAstainingin any case
indicatesthat these lytic transcriptsare most likely
55
Tasks 1 and 3 Task 2
Event class GS answer R P F R P F
Localization 174 (18) 18 (18) 10.34 100.00 18.75 9.77 94.44 17.71
Binding 347 (44) 110 (44) 12.68 40.00 19.26 12.32 39.09 18.74
Gene expression 722 (263) 306 (263) 36.43 85.95 51.17 36.43 85.95 51.17
Transcription 137 (18) 20 (18) 13.14 90.00 22.93 13.14 90.00 22.93
Proteincatabolism 14 (4) 6 (4) 28.57 66.67 40.00 28.57 66.67 40.00
Phosphorylation 135 (30) 30 (30) 22.22 100.00 36.36 20.14 93.33 33.14
EVENTTOTAL 1529 (377) 490 (377) 24.66 76.94 37.35 24.30 76.12 36.84
Regulation 291 (9) 19 (9) 3.09 47.37 5.81 3.08 47.37 5.79
Positive regulation 983 (32) 65 (32) 3.26 49.23 6.11 3.24 49.23 6.08
Negative regulation 379 (10) 22 (10) 2.64 45.45 4.99 2.37 40.91 4.49
REGULATIONTOTAL 1653 (51) 106 (51) 3.09 48.11 5.80 3.02 47.17 5.67
Negation 227 (4) 76 (4) 1.76 5.26 2.64
Speculation 208 (14) 105 (14) 6.73 13.33 8.95
MODIFICATIONTOTAL 435 (18) 181 (18) 4.14 9.94 5.84
ALL TOTAL 3182 (428) 596 (428) 13.45 71.81 22.66 13.25 70.97 22.33
Table 2: Official scores for Tasks 1 and 2, and modification scores only for Task 3, from the approximate span
matching/approximaterecursive matchingtable. GS = goldstandard(truepositives) (given for Tasks1/3 only),answer
= all responses(true positives) (given for tasks 1/3 only), R = recall, P = precision, F = F-measure. All results are as
calculatedby the official scoringapplication.
[expressed] by rare cells in the biopsies entering
lytic cycle (PMID 8903467), where the bracketed
text is the argumentsand the trigger word, the syn-
tacticobjectof the verb is the anaphoricnounphrase
these lytic transcripts, so even with the addition of
a syntacticcomponentto our system,we still would
not have recognizedthe appropriateargumentswith-
out the abilityto do anaphoraresolution.
Appositives The annotationguidelinesfor proteins
apparently specified that when a gene name was
presentin an appositive with its symbol,the symbol
was selectedas the gold-standardargument. For this
reason, in examples like [expression] of Fas ligand
[FasL] (PMID10092076),where the bracketed text
is the trigger word and the argument,the gene name
constituted intervening material from the perspec-
tive of our patterns,whichthereforedid not match.
We returnto a discussionof recalland its implica-
tions for systemslike ours in the Discussionsection.
4.3.2 False
positives
Although our overall rate of false positives was
low, we sampled45 false positive events distributed
across the nine event types and reviewed them with
a biologist.
We noted two main causes of error. The most
common was that we misidentified a slot filler or
were missing a slot filler completely for an actual
event. The other main reasonfor false positives was
when we erroneously identified a (non)event. For
example, in coexpression of NF-kappa B/Rel and
Sp1 transcriptionfactors (PMID7479915),we mis-
takenly identifiedSp1 transcriptionas an event.
5 Discussion
Our resultsdemonstratethat it is possibleto achieve
state-of-theart precisionover a broad range of tasks
and event types using our approach of manually
constructed, ontologically typed rules—our preci-
sion of 71.81 on Task 1 was ten points higher than
the second-highestprecision(62.21),and our preci-
sion of 70.97 on Task 2 was 14 points higher than
the second-highestprecision(56.87). It remainsthe
case that our recall was low enough to drop our F-
measureconsiderably. Will it be the case that a sys-
tem like ours can scale to practicalperformancelev-
els nonetheless?Four factorssuggestthat it can.
The first is that there is considerableredundancy
in the data; although we have not quantified it for
this data set, we note that the same event is often
56
Tasks 1 and 3 Task 2
Event class GS answer R P F R P F
Localization 174 (33) 41 (33) 18.97 80.49 30.70 16.67 69.05 26.85
Binding 347 (62) 152 (62) 17.87 40.79 24.85 17.48 40.13 24.35
Gene expression 722 (290) 344 (290) 40.17 84.30 54.41 40.17 84.30 54.41
Transcription 137 (28) 31 (28) 20.44 90.32 33.33 20.44 90.32 33.33
Proteincatabolism 14 (4) 6 (4) 28.57 66.67 40.00 28.57 66.67 40.00
Phosphorylation 135 (47) 48 (47) 34.81 97.92 51.37 32.37 84.91 46.88
EVENTTOTAL 1529 (464) 622 (464) 30.35 74.60 43.14 29.77 72.77 42.26
Regulation 291 (11) 31 (11) 3.78 35.48 6.83 3.77 35.48 6.81
Positive regulation 983 (60) 129 (60) 6.10 46.51 10.79 6.08 46.51 10.75
Negative regulation 379 (18) 41 (18) 4.75 43.90 8.57 4.49 41.46 8.10
REGULATIONTOTAL 1653 (89) 201 (89) 5.38 44.28 9.60 5.31 43.78 9.47
Negation 227 (6) 129 (6) 2.64 4.65 3.37
Speculation 208 (25) 165 (25) 12.02 15.15 13.40
MODIFICATIONTOTAL 435 (31) 294 (31) 7.13 10.54 8.50
ALL TOTAL 3182 (553) 823 (553) 17.38 67.19 27.62 17.07 65.74 27.10
Table 3: Updatedresultson test data for Tasks 1-3, with importantbug fixes in the code base. See key above.
mentionedrepeatedly, but for knowledgebasebuild-
ing and other uses of the extractedinformation,it is
only strictly necessary to recognize an event once
(althoughmultiplerecognitionof the same assertion
may increaseour confidencein its correctness).
The second is that there is often redundancy
across the literature; the best-supportedassertions
will be reportedas initial findingsand then repeated
as backgroundinformation.
The third is that these recall results reflect an ap-
proach that made no use of syntactic analysis be-
yond handling coordination. There is often text
presentin the input that cannotbe disregarded with-
out either using wildcards, which generally de-
creased precision in our experiments and which
we generally eschewed, or making use of syntac-
tic informationto isolate phrasal heads. Syntactic
analysis, particularlywhen combinedwith analysis
of predicate-argument structure, has recently been
shown to be an effective tool in biomedical infor-
mation extraction (Miyao et al., 2009). There is
broad need for this—for example, of the thirty lo-
calization events in the training data whose trigger
word was translocation, a full eighteen had inter-
vening textual material that made it impossible for
simple patterns like translocationof[Theme] or
[ToLoc]translocationto match.
Finally, our recallnumbersreflecta very shortde-
velopmentcycle, with as few as four patterns writ-
ten for many event types. A less time-constrained
pattern-writingeffort would almost certainly result
in increasedrecall.
Acknowledgments
We gratefully acknowledge Mike Bada’s help in
loadingthe SequenceOntologyinto Prot´eg´e.
This work was supported by NIH
grants R01LM009254, R01GM083649, and
R01LM008111 to Lawrence Hunter and
T15LM009451to PhilipOgren.
References
Alias-i. 2008. LingPipe3.1.2.
EkaterinaBuyko, JoachimWermter, MichaelPoprat,and
Udo Hahn. 2006. Automaticallymapping an NLP
core engine to the biology domain. In Proceedings
of the ISMB2006joint BioLINK/Bio-Ontologies meet-
ing.
K. B. Cohen, L. Tanabe, S. Kinoshita, and L. Hunter.
2004. A resource for constructing customized test
suites for molecularbiology entity identificationsys-
tems. BioLINK2004, pages 1–8.
K. Bretonnel Cohen, Martha Palmer, and Lawrence
Hunter. 2008. Nominalizationand alternations in
biomedicallanguage. PLoS ONE, 3(9).
57
D. Ferrucci and A. Lally. 2004. Building an example
applicationwiththe unstructuredinformationmanage-
ment architecture. IBM Systems Journal, 43(3):455–
475, July.
Lawrence Hunter, Zhiyong Lu, James Firby, William
A. BaumgartnerJr., HelenL. Johnson,PhilipV. Ogren,
and K. Bretonnel Cohen. 2008. OpenDMAP: An
open-source,ontology-driven conceptanalysisengine,
with applications to capturing knowledge regarding
proteintransport,proteininteractionsand cell-specific
gene expression. BMC Bioinformatics, 9(78).
IBM. 2009. UIMA Java framework. http://uima-
framework.sourceforge.net/.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun’ichi Tsujii. 2009. Overview
of BioNLP’09 shared task on event extraction. In
Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop. To
appear.
Yusuke Miyao, Kenji Sagae, Rune Saetre, Takuya Mat-
suzaki, and Jun’ichi Tsujii. 2009. Evaluating contri-
butions of natural language parsers to protein-protein
interactionextraction. Bioinformatics, 25(3):394–400.
Karin Verspoor, K. Bretonnel Cohen, and Lawrence
Hunter. In press. The textual characteristicsof tradi-
tional and Open Access scientificjournalsare similar.
BMC Bioinformatics.
58

