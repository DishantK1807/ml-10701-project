1:151	Learning Nonstructural Distance Metric by Minimum Cluster Distortions Daichi Mochihashi, Genichiro Kikui ATR Spoken Language Translation research laboratories Hikaridai 2-2-2, Keihanna Science City Kyoto 619-0288, Japan daichi.mochihashi@atr.jp genichiro.kikui@atr.jp Kenji Kita Center for Advanced Information Technology, Tokushima University Minamijosanjima 2-1 Tokushima 770-8506, Japan kita@is.tokushima-u.ac.jp Abstract Much natural language processing still depends on the Euclidean (cosine) distance function between two feature vectors, but this has severe problems with regard to feature weightings and feature correlations.
2:151	To answer these problems, we propose an optimal metric distance that can be used as an alternative to the cosine distance, thus accommodating the two problems at the same time.
3:151	This metric is optimal in the sense of global quadratic minimization, and can be obtained from the clusters in the training data in a supervised fashion.
4:151	We confirmed the effect of the proposed metric distance by a synonymous sentence retrieval task, document retrieval task and the K-means clustering of general vectorial data.
5:151	The results showed constant improvement over the baseline method of Euclid and tf.idf, and were especially prominent for the sentence retrieval task, showing a 33% increase in the 11-point average precision.
6:151	1 Introduction Natural language processing involves many kinds of linguistic expressions, such as sentences, phrases, documents and the collection of documents.
7:151	Comparing these expressions based on semantic proximity is a fundamental task and has many applications.
8:151	Generally, two basic approaches exist to compare two expressions: (a) structural and (b) nonstructural.
9:151	Structural approaches make use of syntactic parsing or dependency analysis to make a rigorous comparison; nonstructural approaches use vector representation and provide a rough but fast comparison that is required for search/retrieval from a vast amount of corpora.
10:151	While structural approaches have recently become available in a kernel-based sophisticated treatment (Collins and Duffy, 2001; Suzuki et al. , 2003), here we concentrate on nonstructural comparison.
11:151	This is not only because nonstructural comparison constitutes an integral part in structural methods (that is, even in hierarchical methods the leaf comparison is still atomic), but because it is frequently embedded in many applications where structural parsings are not available or computationally too expensive.
12:151	For example, information retrieval has long used the bag of words approach (Baeza-Yates and Ribeiro-Neto, 1999; Schutze, 1992) mainly due to a lack of scalable segmentation algorithms and the huge amount of data involved.
13:151	While segmentation algorithms, such as TEXTTILING (Hearst, 1994) and its recent successors using the inter-paragraph similarity matrix (Choi, 2000), all themselves use nonstructural cosine similarity as a measure of semantic proximity between paragraphs.
14:151	However, the distance function so far has been largely defined and used ad hoc, usually by a tf.idf weighting scheme (Salton and Yang, 1973) and a simple cosine similarity, equivalently, an Euclidean dot product.
15:151	In this paper, we propose an optimal distance function that is parameterized by a global metric matrix.
16:151	This metric is optimal in the sense of global quadratic minimization, and can be learned from the given clusters in the training data.
17:151	These clusters are often attributable with many forms, such as paragraphs, documents or document collections, as long as the items in the training data are not completely independent.
18:151	This paper is organized as follows.
19:151	In section 2 we describe the issue of traditional Euclidean distances, and section 3 places it into general perspective with related works in machine learning.
20:151	Section 4 introduces the proposed metric, and section 5 validates its effect on the task of sentence retrieval, document retrieval and the K-means clustering.
21:151	Sections 6 and 7 present discussions and the conclusion.
22:151	2 Issues with Euclidean distances When we address nonstructural matching, linguistic expressions are often modeled by a feature vector ~x 2 Rn, with its elements x1 ::: xn corresponding to the number of occurrences of ith feature.
23:151	If features are simply words, this is called a bag of words; but in general, features are not restricted to this kind, and we will use the general term feature in the rest of the paper.
24:151	To measure the distance between two vectors ~u;~v, a dot product or Euclidean distance d(~u;~v)2 = (~u ~v)T (~u ~v) (1) = Pni=1(ui vi)2 (where T denotes a transposition) has been employed so far 1, with a heuristic feature weighting such as tf.idf in a preprocessing stage.
25:151	However, there are two main problems with this distance: (1) The correlation between features is ignored.
26:151	(2) Feature weighting is inevitably arbitrary.
27:151	Problem (1) is especially important in languages, because linguistic features (e.g. , words) generally have strong correlations between them, such as collocations or typical constructions.
28:151	But this correlation cannot be considered in a simple dot product.
29:151	While it is possible to address this with a specific kernel function, such as polynomials (Muller et al. , 2001), this is not available for many problems, such as information retrieval or question answering, that do not fit classifications or cannot be easily kernelized.
30:151	Problem (2) is a more subtle but inherent one: while tf.idf often works properly in practice, there are several options, especially in tf such as logs or square roots, but we have no principle with which to choose from.
31:151	Further, it has no theoretical basis that gives any optimality as a distance function.
32:151	3 Related Works The issues above of feature correlations and feature weightings can be summarized as a problem of defining an appropriate metric in the feature space, based on the distribution of data.
33:151	This problem has recently been highlighted in the field of machine learning research.
34:151	(Xing et al. , 2002) has an objective that is quite similar to that of this paper, and gives a metric matrix that resembles ours based on sample pairs of similar points as training data.
35:151	(Bach and Jordan, 2004) and (Schultz and Joachims, 2004) seek to answer the same problem with an additional scenario of spectral clustering and relative comparisons in Support Vector Machines, respectively.
36:151	In this aspect, our work is a straight successor of (Xing et al. , 2002) where its general usage in vector space is preserved.
37:151	We offer a discussion on the similarity to our method and our advantages 1When we normalize the length of the vectors j~uj = j~vj = 1 as commonly adopted, (~u ~v)T (~u ~v) = j~uj2 + j~vj2 2~u ~v / ~u ~v = cos(~u;~v) ; therefore, this includes a cosine similarity (Manning and Schutze, 1999).
38:151	in section 6.
39:151	Finally, we note that the Fisher kernel of (Jaakkola and Haussler, 1999) has the same concept that gives an appropriate similarity of two data through the Fisher information matrix obtained from the empirical distribution of data.
40:151	However, it is often approximated by a unit matrix because of its heavy computational demand.
41:151	In the field of information retrieval, (Jiang and Berry, 1998) proposes a Riemannian SVD (R-SVD) from the viewpoint of relevance feedback.
42:151	This work is close in spirit to our work, but is not aimed at defining a permanent distance function and does not utilize cluster structures existent in the training data.
43:151	4 Defining an Optimal Metric To solve the problems in section 2, we note the function that synonymous clusters play.
44:151	There are many levels of (more or less) synonymous clusters in linguistic data: phrases, sentences, paragraphs, documents, and, in a web environment, the site that contains the document.
45:151	These kinds of clusters can often be attributed to linguistic expressions because they nest in general so that each expression has a parent cluster.
46:151	Since these clusters are synonymous, we can expect the vectors in each cluster to concentrate in the ideal feature space.
47:151	Based on this property, we can introduce an optimal weighting and correlation in a supervised fashion.
48:151	We will describe this method below.
49:151	4.1 The Basic Idea As stated above, vectors in the same cluster must have a small distance between each other in the ideal geometry.
50:151	When we measure an L2-distance between ~u and ~v by a Mahalanobis distance parameterized by M: dM(~u;~v)2 = (~u ~v)T M(~u ~v) (2) = Pni=1 Pnj=1 mij(ui vi)(uj vj); where symmetric metric matrix M gives both corresponding feature weights and feature correlations.
51:151	When we take M = I (unit matrix), we recover the original Euclidean distance (1).
52:151	Equation (2) can be rewritten as (3) because M is symmetric: dM(~u;~v)2 = (M1=2(~u ~v))T (M1=2(~u ~v)): (3) Therefore, this distance amounts to a Euclidean distance in M1=2-mapped space (Xing et al. , 2002).
53:151	Note that this distance is global, and different from the ordinary Mahalanobis distance in pattern recognition (for example, (Duda et al. , 2000)) that is defined for each cluster one by one, using a clusterspecific covariance matrix.
54:151	That type of distance cannot be generalized to new kinds of data; therefore, it has been used for local classifications.
55:151	What we want is a global distance metric that is generally useful, not a measure for classification to predefined clusters.
56:151	In this respect, (Xing et al. , 2002) shares the same objective as ours.
57:151	Therefore, we require an optimization over all the clusters in the training data.
58:151	Generally, data in the clusters are distributed as in figure 1(a), comprising ellipsoidal forms that have high (co)variances for some dimensions and low (co)variances for other dimensions.
59:151	Further, the cluster is not usually aligned to the axes of coordinates.
60:151	When we find a global metric matrix M that minimizes the cluster distortions, namely, one that reduces high variances and expands low variances for the data to make a spherical form as good as possible in the M1=2-mapped space (figure 1(b)), we can expect it to capture necessary and unnecessary variations and correlations on the features, combining information from many clusters to produce a more reliable metric that is not locally optimal.
61:151	We will find this optimal M below.
62:151	xn x1 x2 High variance High covariance Low variance (a) Original space x1 x2 xn (b) Mapped space Figure 1: Geometry of feature space.
63:151	4.2 Global optimization over clusters Suppose that each data (for example, sentences or documents) is a vector ~s 2 Rn, and the whole corpus can be divided into N clusters, X1 ::: XN.
64:151	That is, each vector has a dimension n, and the number of clusters is N. For each cluster Xi, cluster centroid ci is calculated as ~ci = 1=jXijP~s2Xi ~s, where jXj denotes the number of data in X. When necessary, each element in ~sj or ~ci is referenced as sjk or cik (k = 1::: n).
65:151	The basic idea above is formulated as follows.
66:151	We seek the metric matrix M that minimizes the distance between each data ~sj and the cluster centroid ~ci, dM(~sj;~ci) for all clusters X1 ::: XN . Mathematically, this is formulated as a quadratic minimization problem M = arg min M NX i=1 X ~sj2Xi dM(~sj;~ci)2 = arg min M NX i=1 X ~sj2Xi (~sj ~ci)T M(~sj ~ci) (4) under a scale constraint (j j means determinant) jMj = 1: (5) Scale constraint (5) is necessary for excluding a degenerate solution M = O. 1 is an arbitrary constant: when we replace 1 by c, c2M becomes a new solution.
67:151	This minimization problem is an extension to the method of MindReader (Ishikawa et al. , 1998) to multiple clusters, and has a unique solution below.
68:151	Theorem The matrix that solves the minimization problem (4,5) is M = jAj1=nA 1; (6) where A = [akl] is defined by akl = NX i=1 X sj2Xi (sjl cil)(sjk cik) : (7) Proof: See Appendix A. When A is singular, we can use as A 1 a MoorePenrose matrix pseudoinverse A+.
69:151	Generally, A consists of linguistic features and is very sparse, and often singular.
70:151	Therefore, A+ is nearly always necessary for the above computation.
71:151	For details, see Appendix B. 4.3 Generalization While we assumed through the above construction that each cluster is equally important, this is not the case in general.
72:151	For example, clusters with a small number of data may be considered weak, and in the hierarchical clustering situation, a grandmother cluster may be weaker.
73:151	If we have confidences 1 ::: N for the strength of clustering for each cluster X1 ::: XN, this information can be incorporated into (4) by a set of normalized cluster weights i : M = arg min M NX i=1 i X ~sj2Xi (~sj ~ci)T M(~sj ~ci); where i = i= PNj=1 j, and we obtain a respectively weighted solution in (7).
74:151	Further, we note that when N = 1, this metric recovers the ordinary Mahalanobis distance in pattern recognition.
75:151	However, we used equal weights for the experiments below because the number of data in each cluster was approximately equal.
76:151	5 Experiments We evaluated our metric distance on the three tasks of synonymous sentence retrieval, document retrieval, and the K-means clustering of general vectorial data.
77:151	After calculating M on the training data of clusters, we applied it to the test data to see how well its clusters could be recovered.
78:151	As a measure of cluster recovery, we use 11-point average precision and R-precision for the distribution of items of the same cluster in each retrieval result.
79:151	Here, R equals the cardinality of the cluster; therefore, R-precision shows the precision of cluster recovery.
80:151	5.1 Synonymous sentence retrieval 5.1.1 Sentence cluster corpus We used a paraphrasing corpus of travel conversations (Sugaya et al. , 2002) for sentence retrieval.
81:151	This corpus consists of 33,723,164 Japanese translations, each of which corresponds to one of the original English sentences.
82:151	By way of this correspondence, Japanese sentences are divided into 10,610 clusters.
83:151	Therefore, each cluster consists of Japanese sentences that are possible translations from the same English seed sentence that the cluster has.
84:151	From this corpus, we constructed 10 sets of data.
85:151	Each set contains random selection of 200 training clusters and 50 test clusters, and each cluster contains a maximum of 100 sentences 2.
86:151	Experiments were conducted on these 10 datasets for each level of dimensionality reduction (see below) to produce average statistics.
87:151	5.1.2 Features and dimensionality reduction As a feature of a sentence, we adopted unigrams of all words and bigrams of functional words from the part-of-speech tags, because the sequence of functional words is important in the conversational corpus.
88:151	While the lexicon is limited for travel conversations, the number of features exceeds several thousand or more.
89:151	This may be prohibitive for the calculation of the metric matrix, therefore, we additionally compressed the features with SVD, the same method used in Latent Semantic Indexing (Deerwester et al. , 1990).
90:151	5.1.3 Sentence retrieval results Qualitative result Figure 5 (last page) shows a sample retrieval result.
91:151	A sentence with (*) mark at the end is the correct answer, that is, a sentence from the same original cluster as the query.
92:151	We can see that the results with the metric distance contain 2When the number of data in the cluster exceeds this limit, 100 sentences are randomly sampled.
93:151	All sampling are made without replacement.
94:151	less noise than a standard Euclid baseline with tf.idf weighting, achieving a high-precision retrieval.
95:151	Although the high rate of dimensionality reduction in figure 6 shows degradation due to the dimension contamination, the effect of metric distance is still apparent despite bad conditions.
96:151	Quantitative result Figure 2 shows the averaged precision-recall curves of retrieval and figure 3 shows 11-point average precisions, for each rate of dimensionality reduction.
97:151	Clearly, our method achieves higher precision than the standard method, and does not degrade much with feature compressions unless we reduce the dimension too much, i.e., to < 5%.
98:151	0 0.2 0.4 0.6 0.8 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall 1% 5% 10% 20% 50% (a) Metric distance + idf 0 0.2 0.4 0.6 0.8 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall 1% 5% 10% 20% 50% (b) Euclidean + idf Figure 2: Precision-recall of sentence retrieval.
99:151	0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0 5 10 15 20 25 30 35 40 45 50 Precision Dimension Reduction(%) Metric distance +idf Euclidean distance +idf Figure 3: 11-point average precision.
100:151	5.2 Document retrieval As a method of tackling clusters of texts, the text classification task has recently made great advances with a Nave Bayes or SVM classifiers (for example, (Joachims, 1998)).
101:151	However, they all aim at classifying texts into a few predefined clusters, and cannot deal with a document that fits neither of the clusters.
102:151	For example, when we regard a website as a cluster of documents, the possible clusters are numerous and constantly increasing, which precludes classificatory approaches.
103:151	For these circumstances, document clustering or retrieval will benefit from a global distance metric that exploits the multitude of cluster structures themselves.
104:151	5.2.1 Newsgroup text dataset For this purpose, we used the 20-Newsgroup dataset (Lang, 1995).
105:151	This is a standard text classification dataset that has a relatively large number of classes, 20.
106:151	Among the 20 newsgroups, we selected 16 clusters of training data and 4 clusters of test data, and performed 5-fold cross validation.
107:151	The maximum number of documents per cluster is 100, and when it exceeds this limit, we made a random sampling of 100 documents as the sentence retrieval experiment.
108:151	Because our proposed metric is calculated from the distribution of vectors in high-dimensional feature space, it becomes inappropriate if the norm of the vectors (largely proportional to document length) differs much from document to document.
109:151	3 Therefore, we used subsampling/oversampling to form a median length (130 words) on training documents.
110:151	Further, we preprocessed them with tf.idf as a baseline method.
111:151	5.2.2 Results Table 1 shows R-precision and 11-point average precision.
112:151	Since the test data contains 4 clusters, the baselines of precision are 0.25.
113:151	We can see from both results that metric distance produces a better retrieval over the tf.idf and dot product.
114:151	However, refinements in precision are certain (average p = 0.0243) but subtle.
115:151	This can be thought of as the effect of the dimensionality reduction performed.
116:151	We first decompose data matrix X by SVD: X = USV 1 and build a k-dimensional compressed representation Xk = VkX; where Vk denotes a k-largest submatrix of V . From the equation (3), this means a Euclidean distance of M1=2Xk = M1=2VkX.
117:151	Therefore, Vk may subsume the effect of M in a preprocessing stage.
118:151	Close inspection of table 1 shows this effect as a tradeoff between M and Vk.
119:151	To make the most of metric distance, we should consider metric induction and dimensionality reduction simultaneously, or reconsider the problem in kernel Hilbert space.
120:151	Dim.
121:151	R-precision 11-pt Avr.
122:151	Prec.
123:151	Red.
124:151	Metric Euclid Metric Euclid 0.5% 0.421 0.399 0.476 0.455 1% 0.388 0.368 0.450 0.430 2% 0.359 0.343 0.425 0.409 3% 0.344 0.330 0.411 0.399 4% 0.335 0.323 0.402 0.392 5% 0.329 0.318 0.397 0.388 10% 0.316 0.307 0.379 0.376 20% 0.343 0.297 0.397 0.365 Table 1: Newsgroup text retrieval results.
125:151	3Normalizing documents to unit length effectively maps them to a high-dimensional hypersphere; this proved to produce an unsatisfactory result.
126:151	Defining metrics that work on a hypersphere like spherical K-means (Dhillon and Modha, 2001) requires further research.
127:151	5.3 K-means clustering and general vectorial data Metric distance can also be used for clustering or general vectorial data.
128:151	Figure 4 shows the K-means clustering result of applying our metric distance to some of the UCI Machine Learning datasets (Blake and Merz, 1998).
129:151	K-means clustering was conducted 100 times with a random start, where K equals the known number of classes in the data 4.
130:151	Clustering precision was measured as an average probability that a randomly picked pair of data will conform to the true clustering (Xing et al. , 2002).
131:151	We also conducted the same clustering for documents of the 20-Newsgroup dataset to get a small increase in precision like the document retrieval experiment in section 5.2.
132:151	(a) wine dataset (b) protein dataset (c) iris dataset (d) soybean dataset Figure 4: K-means clustering of UCI Machine Learning dataset results.
133:151	The horizontal axis shows compressed dimensions (rightmost is original).
134:151	The right bar shows clustering precision using Metric distance, and the left bar shows that using Euclidean distance.
135:151	6 Discussion In this paper, we proposed an optimal distance metric based on the idea of minimum cluster distortion in training data.
136:151	Although vector distances have frequently been used in natural language processing, this is a rather neglected but recently highlighted problem.
137:151	Unlike recently proposed methods with spectral methods or SVMs, our method assumes no such additional scenarios and can be considered as 4Because of the small size of the dataset, we did not apply cross-validation as in other experiments.
138:151	a straight successor to (Xing et al. , 2002)s work.
139:151	Their work has the same perspective as ours, and they calculate a metric matrix A that is similar to ours based on a set S of vector pairs (~xi;~xj) that can be regarded as similar.
140:151	They report that the effectiveness of A increases as the number of the training pairs S increases; this requires O(n2) sample points from n training data, and must be optimized by a computationally expensive Newton-Raphson iteration.
141:151	On the other hand, our method uses only linear algebra, and can induce an ideal metric using all the training data at the same time.
142:151	We believe this metric can be useful for many vector-based language processing methods that have used cosine similarity.
143:151	There remains some future directions for research.
144:151	First, as we stated in section 4.3, the effect of a cluster weighted generalized metric must be investigated and optimal weighting must be induced.
145:151	Second, as noted in section 5.2.1, the dimensionality reduction required for linguistic data may constrain the performance of the metric distance.
146:151	To alleviate this problem, simultaneous dimensionality reduction and metric induction may be necessary, or the same idea in a kernel-based approach is worth considering.
147:151	The latter obviates the problem of dimensionality, while it restricts the usage to a situation where the kernel-based approach is available.
148:151	7 Conclusion We proposed a global metric distance that is useful for clustering or retrieval where Euclidean distance has been used.
149:151	This distance is optimal in the sense of quadratic minimization over all the clusters in the training data.
150:151	Experiments on sentence retrieval, document retrieval and K-means clustering all showed improvements over Euclidean distance, with a significant refinement with tight training clusters in sentence retrieval.
151:151	Acknowledgement The research reported here was supported in part by a contract with the National Institute of Information and Communications Technology entitled A study of speech dialogue translation technology based on a large corpus.

