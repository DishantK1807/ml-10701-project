<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Brandon Beamer</author>
<author>Suma Bhat</author>
<author>Brant Chee</author>
<author>Andrew Fister</author>
<author>Alla Rozovskaya</author>
<author>Roxana Girju</author>
</authors>
<title>Uiuc: A knowledge-rich approach to identifying semantic relations between nominals</title>
<date>2007</date>
<booktitle>In Fourth International Workshop on Semantic Evaluations (SemEval-2007</booktitle>
<contexts>
<context>otated for the presence or absence of relations like Cause-Effect. A system based on support vector machines was able to distinguish Cause-Effect noun pairs from other noun pairs with 77.5% accuracy (Beamer et al., 2007). Thus, the prior work on both temporal and causal relations point to a similar conclusion: finding temporal and causal relations in arbitrary text is difficult, but in carefully selected subsets of </context>
</contexts>
<marker>Beamer, Bhat, Chee, Fister, Rozovskaya, Girju, 2007</marker>
<rawString>Brandon Beamer, Suma Bhat, Brant Chee, Andrew Fister, Alla Rozovskaya, and Roxana Girju. 2007. Uiuc: A knowledge-rich approach to identifying semantic relations between nominals. In Fourth International Workshop on Semantic Evaluations (SemEval-2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bethard</author>
<author>James H Martin</author>
</authors>
<title>Identification of event mentions and their semantic class</title>
<date>2006</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<contexts>
<context>ined event pairs from the Penn TreeBank. Because gold standard events were not available for the entire TreeBank, events were first identified automatically, using the event identification system of (Bethard and Martin, 2006). Conjoined event pairs were identified using a simple set of tree-walking rules, resulting in 5,013 event pairs1. These conjoined event pairs then served as the basis for the annotation. For both te</context>
</contexts>
<marker>Bethard, Martin, 2006</marker>
<rawString>Steven Bethard and James H. Martin. 2006. Identification of event mentions and their semantic class. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bethard</author>
<author>James H Martin</author>
</authors>
<title>CU-TMP: Temporal relation classification using syntactic and semantic features</title>
<date>2007</date>
<booktitle>In Fourth International Workshop on Semantic Evaluations (SemEval-2007</booktitle>
<publisher>Steven</publisher>
<contexts>
<context>ied set of temporal relations. Systems performed well on its tense identification task, but poorly on the other tasks which often required multiple stages of implicit temporal logic (Pus¸cas¸u, 2007; Bethard and Martin, 2007). Building on the lessons of TimeBank and TempEval, Bethard and colleagues (Bethard et al., 2007) annotated some verb-clause constructions in the TimeBank, and showed that with a small amount of data</context>
</contexts>
<marker>Bethard, Martin, 2007</marker>
<rawString>Steven Bethard and James H. Martin. 2007. CU-TMP: Temporal relation classification using syntactic and semantic features. In Fourth International Workshop on Semantic Evaluations (SemEval-2007). Steven Bethard, James H. Martin, and Sara Klingenstein.</rawString>
</citation>
<citation valid="true">
<title>Timelines from text: Identification of syntactic temporal relations</title>
<date>2007</date>
<booktitle>In International Conference on Semantic Computing (ICSC</booktitle>
<marker>2007</marker>
<rawString>2007. Timelines from text: Identification of syntactic temporal relations. In International Conference on Semantic Computing (ICSC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Branimir Boguraev</author>
<author>Rie Kubota Ando</author>
</authors>
<title>Timebank-driven timeml analysis</title>
<date>2005</date>
<booktitle>Annotating, Extracting and Reasoning about Time and Events, Dagstuhl Seminars. German Research Foundation</booktitle>
<editor>In Graham Katz, James Pustejovsky, and Frank Schilder, editors</editor>
<contexts>
<context>al., 2003), a small set of newswire documents annotated for events, times and the temporal relations between them. A variety of systems for identifying temporal relations were trained on this corpus (Boguraev and Ando, 2005; Mani et al., 2006) but systems had poor performance, in part due to the the low interannotator agreement and fine granularity of the TimeBank temporal relations. In an attempt to improve on the Time</context>
</contexts>
<marker>Boguraev, Ando, 2005</marker>
<rawString>Branimir Boguraev and Rie Kubota Ando. 2005. Timebank-driven timeml analysis. In Graham Katz, James Pustejovsky, and Frank Schilder, editors, Annotating, Extracting and Reasoning about Time and Events, Dagstuhl Seminars. German Research Foundation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Preslav Nakov</author>
<author>Vivi Nastase</author>
<author>Stan Szpakowicz</author>
<author>Peter Turney</author>
<author>Deniz Yuret</author>
</authors>
<title>Semeval-2007 task 04: Classification of semantic relations between nominals</title>
<date>2007</date>
<booktitle>In Fourth International Workshop on Semantic Evaluations (SemEval-2007</booktitle>
<contexts>
<context>erbs with CAUSAL and NON-CAUSAL relations, they were able to train decision tree models that achieved 73.9% precision and 88.7% recall. Inspired by the success of this approach, Girju and colleagues (Girju et al., 2007) organized a SemEval 2007 task in which pairs of nouns were selected by carefully constructed web search queries, and annotated for the presence or absence of relations like Cause-Effect. A system ba</context>
<context>nnotated as (strengthening NO-REL saying). 4. Causal Annotation Scheme Many earlier efforts at annotating causality relied on only intuitive notions of the term cause (Khoo et al., 2000; Girju, 2003; Girju et al., 2007). In an attempt to make these notions more explicit, a couple different causal annotation schemes were explored in the current work. Event Pairs Agreement Kappa 50 78 0.67 100 64 0.46 200 80 0.70 200</context>
</contexts>
<marker>Girju, Nakov, Nastase, Szpakowicz, Turney, Yuret, 2007</marker>
<rawString>Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Szpakowicz, Peter Turney, and Deniz Yuret. 2007. Semeval-2007 task 04: Classification of semantic relations between nominals. In Fourth International Workshop on Semantic Evaluations (SemEval-2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
</authors>
<title>Automatic detection of causal relations for question answering</title>
<date>2003</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL) Workshop on Multilingual Summarization and Question Answering Machine Learning and Beyond</booktitle>
<contexts>
<context>nstance was annotated as (strengthening NO-REL saying). 4. Causal Annotation Scheme Many earlier efforts at annotating causality relied on only intuitive notions of the term cause (Khoo et al., 2000; Girju, 2003; Girju et al., 2007). In an attempt to make these notions more explicit, a couple different causal annotation schemes were explored in the current work. Event Pairs Agreement Kappa 50 78 0.67 100 64 </context>
</contexts>
<marker>Girju, 2003</marker>
<rawString>Roxana Girju. 2003. Automatic detection of causal relations for question answering. In Annual Meeting of the Association for Computational Linguistics (ACL) Workshop on Multilingual Summarization and Question Answering Machine Learning and Beyond.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher S G Khoo</author>
<author>Jaklin Kornfilt</author>
<author>Robert N Oddy</author>
<author>Sung Hyon Myaeng</author>
</authors>
<title>Automatic extraction of cause-effect information from newspaper text without knowledge-based inferencing. Literary and Linguistic Computing</title>
<date>1998</date>
<contexts>
<context>ral relations with accuracies of nearly 90%. Like work on temporal relations, early work in causal relations aimed to identify the relations in arbitrary text. Khoo and colleagues (Khoo et al., 2000; Khoo et al., 1998) tried to identify all causal relations in a section of the Wall Street Journal using hand-crafted patterns, but had inter-annotator agreement problems, and achieved only 24.9% precision and 67.7% re</context>
</contexts>
<marker>Khoo, Kornfilt, Oddy, Myaeng, 1998</marker>
<rawString>Christopher S. G. Khoo, Jaklin Kornfilt, Robert N. Oddy, and Sung Hyon Myaeng. 1998. Automatic extraction of cause-effect information from newspaper text without knowledge-based inferencing. Literary and Linguistic Computing, 13(4):177–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher S G Khoo</author>
<author>Syin Chan</author>
<author>Yun Niu</author>
</authors>
<title>Extracting causal knowledge from a medical database using graphical patterns</title>
<date>2000</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context>to find these temporal relations with accuracies of nearly 90%. Like work on temporal relations, early work in causal relations aimed to identify the relations in arbitrary text. Khoo and colleagues (Khoo et al., 2000; Khoo et al., 1998) tried to identify all causal relations in a section of the Wall Street Journal using hand-crafted patterns, but had inter-annotator agreement problems, and achieved only 24.9% pre</context>
<context>rengthening, this instance was annotated as (strengthening NO-REL saying). 4. Causal Annotation Scheme Many earlier efforts at annotating causality relied on only intuitive notions of the term cause (Khoo et al., 2000; Girju, 2003; Girju et al., 2007). In an attempt to make these notions more explicit, a couple different causal annotation schemes were explored in the current work. Event Pairs Agreement Kappa 50 78</context>
</contexts>
<marker>Khoo, Chan, Niu, 2000</marker>
<rawString>Christopher S. G. Khoo, Syin Chan, and Yun Niu. 2000. Extracting causal knowledge from a medical database using graphical patterns. In Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kingsbury</author>
<author>Martha Palmer</author>
</authors>
<title>From treebank to propbank. In Language Resources and Evaluation</title>
<date>2002</date>
<contexts>
<context>r identifying the more difficult temporal and causal relations. 1. Introduction Recent corpus annotation efforts have made the semantic structure of text much more accessible. Projects like PropBank (Kingsbury and Palmer, 2002), TimeBank (Pustejovsky et al., 2003) and the Penn Discourse TreeBank (Miltsakaki et al., 2004) have linked words together with a wide variety of semantic relations. Still, many gaps exist. Consider </context>
</contexts>
<marker>Kingsbury, Palmer, 2002</marker>
<rawString>Paul Kingsbury and Martha Palmer. 2002. From treebank to propbank. In Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Marc Verhagen</author>
<author>Ben Wellner</author>
<author>Chong Min Lee</author>
<author>James Pustejovsky</author>
</authors>
<title>Machine learning of temporal relations</title>
<date>2006</date>
<booktitle>In International Conference on Computational Linguistics and Annual Meeting of the Association for Computational Linguistics (COLING-ACL</booktitle>
<contexts>
<context>f newswire documents annotated for events, times and the temporal relations between them. A variety of systems for identifying temporal relations were trained on this corpus (Boguraev and Ando, 2005; Mani et al., 2006) but systems had poor performance, in part due to the the low interannotator agreement and fine granularity of the TimeBank temporal relations. In an attempt to improve on the TimeBank annotation sch</context>
</contexts>
<marker>Mani, Verhagen, Wellner, Lee, Pustejovsky, 2006</marker>
<rawString>Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min Lee, and James Pustejovsky. 2006. Machine learning of temporal relations. In International Conference on Computational Linguistics and Annual Meeting of the Association for Computational Linguistics (COLING-ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank</title>
<date>1994</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<contexts>
<context>the Penn Discourse TreeBank (Miltsakaki et al., 2004) have linked words together with a wide variety of semantic relations. Still, many gaps exist. Consider the following text from the Penn TreeBank (Marcus et al., 1994): (1) “I ate a bad tuna sandwich, got food poisoning and had to have a shot in my shoulder,” he says. wsj 0409 It is clear to readers of this sentence that the food poisoning occurred BEFORE the shot</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1994. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleni Miltsakaki</author>
<author>Rashmi Prasad</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>Annotating discourse connectives and their arguments</title>
<date>2004</date>
<booktitle>In Human Language Technologies and North American Chapter of the Assocation of Computational Linguistics (HLT-NAACL) Workshop on Frontiers in Corpus Annotation</booktitle>
<contexts>
<context>tation efforts have made the semantic structure of text much more accessible. Projects like PropBank (Kingsbury and Palmer, 2002), TimeBank (Pustejovsky et al., 2003) and the Penn Discourse TreeBank (Miltsakaki et al., 2004) have linked words together with a wide variety of semantic relations. Still, many gaps exist. Consider the following text from the Penn TreeBank (Marcus et al., 1994): (1) “I ate a bad tuna sandwich</context>
</contexts>
<marker>Miltsakaki, Prasad, Joshi, Webber, 2004</marker>
<rawString>Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and Bonnie Webber. 2004. Annotating discourse connectives and their arguments. In Human Language Technologies and North American Chapter of the Assocation of Computational Linguistics (HLT-NAACL) Workshop on Frontiers in Corpus Annotation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Kadri Hacioglu</author>
<author>Valerie Krugler</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Support vector learning for semantic argument classification</title>
<date>2005</date>
<booktitle>Machine Learning</booktitle>
<volume>60</volume>
<contexts>
<context>k between CAUSAL and NO-REL. We chose support vector machine (SVM) classifiers for our machine learning experiments because they have been successful in a variety of related NLP tasks (Reitter, 2003; Pradhan et al., 2005; Bethard et al., 2007). In particular, we used the SVMperf implementation because it has dramatically reduced training times and can optimize against the F1-measure and other loss functions directly.</context>
</contexts>
<marker>Pradhan, Hacioglu, Krugler, Ward, Martin, Jurafsky, 2005</marker>
<rawString>Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne Ward, James H. Martin, and Daniel Jurafsky. 2005. Support vector learning for semantic argument classification. Machine Learning, 60(1):11–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Pus¸cas¸u</author>
</authors>
<title>Wvali: Temporal relation identification by syntactico-semantic analysis</title>
<date>2007</date>
<booktitle>In Fourth International Workshop on Semantic Evaluations (SemEval-2007</booktitle>
<marker>Pus¸cas¸u, 2007</marker>
<rawString>Georgiana Pus¸cas¸u. 2007. Wvali: Temporal relation identification by syntactico-semantic analysis. In Fourth International Workshop on Semantic Evaluations (SemEval-2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Patrick Hanks</author>
<author>Roser Saur´ı</author>
<author>Andrew See</author>
</authors>
<title>Robert Gaizauskas, Andrea Setzer, Dragomir Radev, Beth Sundheim</title>
<date>2003</date>
<booktitle>In Corpus Linguistics</booktitle>
<pages>647--656</pages>
<location>David Day, Lisa Ferro, and</location>
<marker>Pustejovsky, Hanks, Saur´ı, See, 2003</marker>
<rawString>James Pustejovsky, Patrick Hanks, Roser Saur´ı, Andrew See, Robert Gaizauskas, Andrea Setzer, Dragomir Radev, Beth Sundheim, David Day, Lisa Ferro, and Marcia Lazo. 2003. The timebank corpus. In Corpus Linguistics, pages 647–656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Reitter</author>
</authors>
<title>Simple signals for complex rhetorics: On rhetorical analysis with rich-feature support vector models. LDV-Forum, GLDV-Journal for Computational Linguistics and Language Technology</title>
<date>2003</date>
<pages>18--1</pages>
<contexts>
<context>tions in a section of the Wall Street Journal using hand-crafted patterns, but had inter-annotator agreement problems, and achieved only 24.9% precision and 67.7% recall with their patterns. Reitter (Reitter, 2003) trained support vector machine models on discourse relations like Attribution, Cause and Elaboration annotated on top of the Wall Street Journal, but while his system performed well for relations li</context>
<context>ssification task between CAUSAL and NO-REL. We chose support vector machine (SVM) classifiers for our machine learning experiments because they have been successful in a variety of related NLP tasks (Reitter, 2003; Pradhan et al., 2005; Bethard et al., 2007). In particular, we used the SVMperf implementation because it has dramatically reduced training times and can optimize against the F1-measure and other lo</context>
</contexts>
<marker>Reitter, 2003</marker>
<rawString>David Reitter. 2003. Simple signals for complex rhetorics: On rhetorical analysis with rich-feature support vector models. LDV-Forum, GLDV-Journal for Computational Linguistics and Language Technology, 18(1/2):38–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Verhagen</author>
<author>Robert Gaizauskas</author>
<author>Frank Schilder</author>
<author>Mark Hepple</author>
<author>Graham Katz</author>
<author>James Pustejovsky</author>
</authors>
<date>2007</date>
<contexts>
<context>terannotator agreement and fine granularity of the TimeBank temporal relations. In an attempt to improve on the TimeBank annotation scheme, Verhagen and colleagues organized the TempEval competition (Verhagen et al., 2007) which used a stricter annotation interface and a simplified set of temporal relations. Systems performed well on its tense identification task, but poorly on the other tasks which often required mul</context>
<context> word and. In preparation for the annotation of such a corpus, we designed two annotation schemes: one for temporal relations and one for causal relations. 3. Temporal Annotation Scheme The TempEval (Verhagen et al., 2007) guidelines served as a starting point for the temporal annotation work here. TempEval tried to simplify the TimeBank annotation scheme, using the labels BEFORE, OVERLAP, AFTER, BEFORE-OR-OVERLAP, OV</context>
</contexts>
<marker>Verhagen, Gaizauskas, Schilder, Hepple, Katz, Pustejovsky, 2007</marker>
<rawString>Marc Verhagen, Robert Gaizauskas, Frank Schilder, Mark Hepple, Graham Katz, and James Pustejovsky. 2007.</rawString>
</citation>
</citationList>
</algorithm>

