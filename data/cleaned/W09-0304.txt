Proceedings of the EACL 2009 Workshop on Language Technology and Resources for Cultural Heritage,
Social Sciences, Humanities, and Education –LaTeCH – SHELT&R 2009, pages 26–34,
Athens, Greece, 30 March 2009. c©2009 Association for Computational Linguistics
Evaluatingthepairwisestringalignmentof pronunciations
MartijnWieling
Universityof Groningen
TheNetherlands
m.b.wieling@rug.nl
JelenaProki´c
Universityof Groningen
TheNetherlands
j.prokic@rug.nl
JohnNerbonne
Universityof Groningen
TheNetherlands
j.nerbonne@rug.nl
Abstract
Pairwisestring alignment(PSA)is an im-
portant general technique for obtaining a
measureof similaritybetweentwo strings,
used e.g., in dialectology, historical lin-
guistics, transliteration,and in evaluating
name distinctiveness. The current study
focuseson evaluatingdifferentPSAmeth-
ods at the alignment level instead of via
thedistancesit induces. About3.5million
pairwisealignmentsofBulgarianphonetic
dialect data are used to compare four al-
gorithms with a manually corrected gold
standard. The algorithms evaluated in-
clude three variantsof the Levenshteinal-
gorithmaswellasthePairHiddenMarkov
Model. Our results show that while all
algorithms perform very well and align
around 95% of all alignments correctly,
thereare specificqualitative differencesin
the (mis)alignmentsof the different algo-
rithms.
1 Introduction
Our cultural heritage is not only accessible
through museums, libraries, archives and their
digital portals, it is alive and well in the varied
culturalhabitspracticedtodayby the variouspeo-
ples of the world. To researchand understandthis
culturalheritagewerequireinstrumentswhichare
sensitive to its signals,and, in particularsensitive
to signals of common provenance. The present
paper focuses on speech habits which even today
bear signals of common provenance in the vari-
ous dialects of the world’s languages, and which
have also been recorded and preserved in major
archivesoffolkcultureinternationally. Wepresent
work in a research line which seeks to develop
digital instrumentscapable of detecting common
provenanceamongpronunciation habits,focusing
in this paperon the issue of evaluatingthe quality
of theseinstruments.
Pairwise string alignment (PSA) methods, like
the popular Levenshtein algorithm (Levenshtein,
1965) which uses insertions(alignmentsof a seg-
mentagainstagap),deletions(alignmentsofagap
against a segment) and substitutions (alignments
of two segments) often form the basis of deter-
mining the distance between two strings. Since
there are many alignmentalgorithmsand specific
settings for each algorithm influencing the dis-
tance between two strings (Nerbonne and Klei-
weg, 2007), evaluationis very importantin deter-
miningthe effectivenessof the distancemethods.
Determining the distance (or similarity) be-
tween two phoneticstrings is an importantaspect
of dialectometry, and alignmentquality is impor-
tant in applications in which string alignment is
a goal in itself, for example, determining if two
words are likely to be cognate (Kondrak, 2003),
detecting confusable drug names (Kondrak and
Dorr, 2003), or determining whether a string is
the transliterationof the same name from another
writingsystem(Pouliquen,2008).
In this paper we evaluate string distance mea-
sures on the basis of data from dialectology. We
thereforeexplaina bit moreof the intendeduse of
the pronunciationdistancemeasure.
Dialect atlases normally contain a large num-
ber of pronunciationsof the sameword in various
places throughout a language area. All pairs of
pronunciationsof correspondingwords are com-
pared in order to obtain a measure of the aggre-
gate linguisticdistancebetweendialectalvarieties
(Heeringa, 2004). It is clearthat the qualityof the
measurementis of crucialimportance.
Almostall evaluationmethodsin dialectometry
focus on the aggregate results and ignore the in-
dividualword-pairdistancesand individualalign-
ments on which the distances are based. The fo-
cus on the aggregate distance of 100 or so word
26
pairs effectively hides many differences between
methods.For example,Heeringa et al. (2006)find
no significantdifferences in the degrees to which
severalpairwisestringdistancemeasurescorrelate
withperceptualdistanceswhenexaminedatanag-
gregate level. Wieling et al. (2007) and Wieling
and Nerbonne(2007)also reportalmostno differ-
ence betweendifferent PSA algorithmsat the ag-
gregate level. It is importantto be ableto evaluate
the differenttechniquesmoresensitively, whichis
why this paper examinesalignmentquality at the
segmentlevel.
Kondrak (2003) applies a PSA algorithm to
align words in different languagesin order to de-
tect cognates automatically. Exceptionally, he
doesprovideanevaluationofthestringalignments
generatedby differentalgorithms.But he restricts
his examinationto a set of only 82 gold standard
pairwisealignmentsandheonlydistinguishescor-
rect and incorrectalignmentsand doesnot lookat
misalignedphones.
In the current study we introduce and evaluate
several alignment algorithmsmore extensively at
the alignment level. The algorithms we evaluate
include the Levenshtein algorithm(with syllabic-
ity constraint), which is one of the most popular
alignmentmethodsandhassuccessfullybeenused
in determining pronunciationdifferences in pho-
netic strings (Kessler, 1995; Heeringa, 2004). In
addition we look at two adaptations of the Lev-
enshtein algorithm. The first adaptation includes
theswap-operation(WagnerandLowrance,1975),
whilethesecondadaptationincludesphoneticseg-
ment distances, which are generated by applying
an iterative pointwise mutual information (PMI)
procedure(Churchand Hanks, 1990). Finallywe
include alignments generated with the Pair Hid-
denMarkov Model(PHMM)as introducedto lan-
guage studies by Mackay and Kondrak (2005).
They reportedthatthePairHiddenMarkov Model
outperformed ALINE, the best performing algo-
rithmat the alignmentlevel in the aforementioned
study of Kondrak (2003). The PHMM has also
successfullybeenusedin dialectologyby Wieling
et al. (2007).
2 Dataset
The dataset used in this study consists of 152
words collectedfrom197 sitesequallydistributed
over Bulgaria. The transcribed word pronuncia-
tions includediacriticsand suprasegmentals(e.g.,
intonation).Thetotalnumberofdifferentphonetic
types(or segments)is 98.1
The gold standard pairwise alignment was au-
tomatically generated from a manually corrected
gold standard set of N multiple alignments (see
Proki´c et al., 2009) in the followingway:
• Every individual string (including gaps) in
the multiple alignment is aligned with ev-
ery other string of the same word. With 152
words and 197 sites and in some cases more
than one pronunciations per site for a cer-
tainword,thetotalnumberofpairwisealign-
mentsis about3.5 million.
• If a resulting pairwise alignment contains a
gap in both strings at the same position (a
gap-gap alignment),these gaps are removed
fromthe pairwisealignment. We justifythis,
reasoning that no alignment algorithm may
be expected to detect parallel deletions in a
singlepairofwords. Thereisnoevidencefor
this in the singlepair.
To make this clear, considerthe multiplealign-
ment of three Bulgarian dialectal variants of the
word ‘I’ (as in ‘I am’):
j "A s
"A z i
j "A
Using the procedure above, the three generated
pairwisealignmentsare:
j "A s j "A s "A z i
"A z i j "A j "A
3 Algorithms
Four algorithmsare evaluated with respect to the
quality of their alignments, including three vari-
ants of the Levenshtein algorithm and the Pair
HiddenMarkov Model.
3.1 TheVC-sensitive Levenshteinalgorithm
The Levenshtein algorithmis a very efficient dy-
namicprogrammingalgorithm,whichwasfirstin-
troducedby Kessler(1995)as a tool for computa-
tionallycomparingdialects. The Levenshteindis-
tancebetweentwo stringsis determinedby count-
ing the minimum number of edit operations (i.e.
insertions, deletions and substitutions)needed to
transformone stringintothe other.
1The dataset is available online at the website
http://www.bultreebank.org/BulDialects/
27
For example,the Levenshteindistancebetween
[j"As] and ["Azi], two Bulgarian dialectal variants
of the word ‘I’ (as in ‘I am’),is 3:
j"As deletej 1
"As subst. s/z 1
"Az inserti 1
"Azi
3
Thecorrespondingalignmentis:
j "A s
"A z i
1 1 1
The Levenshtein distance has been used fre-
quently and successfully in measuring linguis-
tic distancesin several languages,includingIrish
(Kessler, 1995),Dutch(Heeringa, 2004)and Nor-
wegian (Heeringa, 2004). Additionally, the Lev-
enshtein distance has been shown to yield aggre-
gate results that are consistent (Cronbach’s α =
0.99) and valid when compared to dialect speak-
ers judgementsof similarity(r ≈ 0.7; Heeringa et
al., 2006).
Following Heeringa (2004), we have adapted
the Levenshteinalgorithmslightly, so that it does
not allow alignments of vowels with consonants.
We refer to this adapted algorithm as the VC-
sensitive Levenshteinalgorithm.
3.2 TheLevenshteinalgorithmwiththeswap
operation
Because metathesis (i.e. transpositionof sounds)
occurs relatively frequently in the Bulgarian di-
alect data (in 21 of 152 words), we extend the
VC-sensitive Levenshtein algorithm as described
insection3.1toincludetheswap-operation(Wag-
ner and Lowrance, 1975), which allows two ad-
jacent characters to be interchanged. The swap-
operationis also known as a transposition,which
was introducedwith respect to detecting spelling
errors by Damerau(1964). As a consequencethe
Dameraudistance refers to the minimumnumber
of insertions,deletions,substitutionsand transpo-
sitions required to transform one string into the
other. In contrastto Wagnerand Lowrance(1975)
and in line with Damerau (1964) we restrict the
swap operation to be only allowed for string X
and Y when xi = yi+1 and yi = xi+1 (with xi
beingthe token at positioni in stringX):
xi xi+1
yi yi+1
>< 1
Note that a swap-operationin the alignmentis in-
dicatedbythesymbol‘><’. Thefirstnumberfol-
lowingthis symbolindicatesthe cost of the swap-
operation.
Consider the alignment of [vr"7] and [v"7r],2
twoBulgariandialectalvariantsoftheword‘peak’
(mountain). The alignment involves a swap and
resultsin a totalLevenshteindistanceof 1:
v r "7
v "7 r
>< 1
However, the alignmentof the transcription[vr"7]
withanotherdialectaltranscription[v"ar] doesnot
allow a swap and yields a total Levenshtein dis-
tanceof 2:
v r "7
v "a r
1 1
Including just the option of swapping identical
segments in the implementation of the Leven-
shtein algorithm is relatively easy. We set the
cost of the swap operationto one3 plus twice the
cost of substituting xi with yi+1 plus twice the
cost of substitutingyi with xi+1. In this way the
swap operationwill be preferredwhen xi = yi+1
and yi = xi+1, but not when xi negationslash= yi+1 and/or
yi negationslash= xi+1. In the first case the cost of the swap
operation is 1, which is less than the cost of the
alternative oftwo substitutions.Inthesecondcase
the cost is either 3 (if xi negationslash= yi+1 or yi negationslash= xi+1) or
5 (if xi negationslash= yi+1 and yi negationslash= xi+1), which is higher
than the cost of using insertions,deletionsand/or
substitutions.
Just as in the previous section,we do not allow
vowelstoalignwithconsonants(exceptinthecase
of a swap).
3.3 TheLevenshteinalgorithmwith
generatedsegmentdistances
The VC-sensitive Levenshtein algorithm as de-
scribed in section 3.1 only distinguishesbetween
vowels and consonants. However, more sensi-
tive segmentdistancesare alsopossible.Heeringa
(2004) experimented with specifying phonetic
segment distancesbased on phoneticfeaturesand
2We use transcriptions in which stress is marked on
stressedvowels instead of before stressedsyllables. We fol-
low in this the Bulgarian convention insteadof the IPA con-
vention.
3Actually the cost is set to 0.999 to prefer an alignment
involvingaswapoveranalternativealignmentinvolvingonly
regulareditoperations.
28
also based on acoustic differences derived from
spectrograms,but he did not obtain improved re-
sultsat the aggregate level.
Insteadof using segment distancesas these are
(incompletely) suggested by phonetic or phono-
logical theory, we tried to determine the sound
distances automatically based on the available
data. We used pointwise mutual information
(PMI; Church and Hanks, 1990) to obtain these
distances. It generates segment distances by as-
sessing the degree of statistical dependence be-
tweenthe segmentsx andy:
PMI(x,y) = log2
parenleftbigg p(x,y)
p(x)p(y)
parenrightbigg
(1)
Where:
• p(x,y): the number of times x and y occur
at the same position in two aligned strings
X and Y, divided by the total number of
alignedsegments(i.e.the relative occurrence
of the alignedsegmentsx and y in the whole
dataset). Note that eitherx or y can be a gap
in the caseof insertionor deletion.
• p(x) and p(y): the numberof times x (or y)
occurs, divided by the total number of seg-
mentoccurrences(i.e.therelative occurrence
of x or y in the whole dataset). Dividing by
this term normalizesthe empiricalfrequency
with respect to the frequency expected if x
andy are statisticallyindependent.
ThegreaterthePMIvalue,themoresegmentstend
to cooccurin correspondences.Negative PMIval-
ues indicatethat segments do not tend to cooccur
in correspondences,whilepositive PMIvaluesin-
dicatethatsegmentstendto cooccurin correspon-
dences. The segment distances can therefore be
generatedbysubtractingthePMIvaluefrom0and
adding the maximum PMI value (i.e. lowest dis-
tance is 0). In that way correspondingsegments
obtainthe lowestdistance.
Based on the PMI value and its conversion to
segmentdistances,we developedan iterative pro-
cedure to automatically obtain the segment dis-
tances:
1. Thestringalignmentsaregeneratedusingthe
VC-sensitiveLevenshteinalgorithm(seesec-
tion3.1).4
4We also used the Levenshtein algorithm without the
vowel-consonantrestrictionto generate the PMI values, but
this had a negative effecton the performance.
2. The PMI value for every segmentpair is cal-
culated according to (1) and subsequently
transformed to a segment distance by sub-
tracting it from zero and adding the maxi-
mumPMIvalue.
3. The Levenshtein algorithm using these seg-
ment distances is applied to generate a new
set of alignments.
4. Step2and3arerepeateduntilthealignments
of two consecutive iterations do not differ
(i.e.convergenceis reached).
The potential merit of using PMI-generated seg-
mentdistancescanbemadeclearbythefollowing
example. Considerthestrings[v"7n] and[v"7ïk@],
Bulgarian dialectalvariantsof the word ‘outside’.
The VC-sensitive Levenshtein algorithm yields
the following(correct)alignment:
v "7 n
v "7 ï k @
1 1 1
But alsothe alternative (incorrect)alignment:
v "7 n
v "7 ï k @
1 1 1
The VC-sensitive Levenshtein algorithm gener-
atestheerroneousalignmentbecauseithasnoway
to identify that the consonant [n] is nearer to the
consonant [ï] than to the consonant [k]. In con-
trast, the Levenshtein algorithm which uses the
PMI-generatedsegment distances only generates
the correct first alignment,becausethe [n] occurs
relatively more often aligned with [ï] than with
[k] so that the distance between [n] and [ï] will
be lower than the distance between [n] and [k].
The idea behind this procedure is similar to Ris-
tad’ssuggestiontolearnsegmentdistancesforedit
distanceusing an expectationmaximizationalgo-
rithm (Ristad and Yianilos, 1998). Our approach
differs from their approach in that we only learn
segmentdistancesbasedon the alignmentsgener-
ated by the VC-sensitive Levenshtein algorithm,
while Ristad and Yianilos (1998) learn segment
distancesbyconsideringallpossiblealignmentsof
two strings.
3.4 ThePairHiddenMarkov
Model
The Pair Hidden Markov Model (PHMM) also
generatesalignmentsbasedon automaticallygen-
erated segment distances and has been used suc-
29
Figure 1: Pair Hidden Markov Model. Image
courtesyof Mackayand Kondrak(2005).
cessfully in language studies (Mackay and Kon-
drak,2005;Wielinget al., 2007).
A Hidden Markov Model (HMM) is a proba-
bilisticfinite-statetransducerthatgeneratesanob-
servation sequence by starting in an initial state,
goingfrom state to state basedon transitionprob-
abilities and emitting an output symbol in each
state based on the emission probabilities in that
state for that output symbol(Rabiner, 1989). The
PHMM was originally proposed by Durbin et al.(1998) for aligning biologicalsequencesand was
first used in linguistics by Mackay and Kondrak
(2005) to identify cognates. The PHMM differs
from the regular HMM in that it outputs two ob-
servation streams (i.e. a series of alignments of
pairsof individualsegments)insteadof only a se-
ries of single symbols. The PHMM displayed in
Figure1 has threeemittingstates: the substitution
(‘match’)state(M) whichemitstwo alignedsym-
bols, the insertionstate (Y) whichemitsa symbol
and a gap, and the deletionstate (X) which emits
a gap and a symbol.
The following example shows the state se-
quenceforthepronunciations[j"As] and["Azi] (En-
glish‘I’):
j "A s
"A z i
X M M Y
Before generatingthe alignments,all probabil-
ities of the PHMM have to be estimated. These
probabilitiesconsist of the 5 transition probabili-
ties shown in Figure 1: epsilon1, λ, δ, τXY and τM. In
additionthereare98emissionprobabilitiesforthe
insertion state and the deletion state (one for ev-
ery segment) and 9604 emission probabilitiesfor
thesubstitutionstate. Theprobabilityofstartingin
oneofthethreestatesissetequaltotheprobability
of goingfromthesubstitutionstateto thatparticu-
larstate. TheBaum-Welchexpectationmaximiza-
tion algorithm(Baum et al., 1970) can be used to
iterativelyreestimatetheseprobabilitiesuntila lo-
cal optimumis found.
To prevent order effects in training,every word
pair is consideredtwice (e.g., wa −wb and wb −
wa). Theresultinginsertionanddeletionprobabil-
itiesarethereforethesame(foreachsegment),and
the probabilityof substitutingx for y is equal to
the probabilityof substitutingy for x, effectively
yielding4802distinctsubstitutionprobabilities.
Wieling et al. (2007) showed that using Dutch
dialect data for training, sensible segment dis-
tances were obtained; acoustic vowel distances
on the basis of spectrograms correlated signifi-
cantly (r = −0.72) with the vowel substitution
probabilitiesof the PHMM. Additionally, proba-
bilities of substituting a symbol with itself were
much higher than the probabilities of substitut-
ing an arbitrary vowel with another non-identical
vowel (mutatis mutandis for consonants), which
were in turn muchhigherthan the probabilitiesof
substitutinga vowel for a consonant.
Aftertraining,thewellknownViterbialgorithm
canbeusedtoobtainthebestalignments(Rabiner,
1989).
4 Evaluation
As described in section 2, we use the generated
pairwisealignmentsfromagoldstandardofmulti-
plealignmentsforevaluation.Inaddition,welook
at theperformanceof a baselineof pairwisealign-
ments,whichisconstructedbyaligningthestrings
according to the Hamming distance (i.e. only al-
lowingsubstitutionsandnoinsertionsordeletions;
Hamming,1950).
Theevaluationprocedureconsistsofcomparing
the alignments of the previously discussed algo-
rithms includingthe baselinewith the alignments
of the gold standard. For the comparisonwe use
the standard Levenshtein algorithm without any
restrictions.Theevaluationproceedsas follows:
1. The pairwise alignments of the four algo-
rithms,thebaselineandthegoldstandardare
generatedand standardized(see section4.1).
When multiple equal-scoringalignmentsare
30
generatedby an algorithm,only one (i.e. the
final)alignmentis selected.
2. In each alignment, we convert each pair of
alignedsegmentstoasingletoken,sothatev-
ery alignmentof two stringsis convertedto a
singlestringof segmentpairs.
3. Foreveryalgorithmthesetransformedstrings
are aligned with the transformed strings of
the gold standard using the standard Leven-
shteinalgorithm.
4. The Levenshtein distances for all these
strings are summed up resulting in the total
distance between every alignment algorithm
and the gold standard. Only if individual
segmentsmatchcompletelythe segmentdis-
tanceis 0, otherwiseit is 1.
Toillustratethisprocedure,considerthefollowing
goldstandardalignmentof[vl"7k] and[v"7lk], two
Bulgariandialectalvariantsof the word ‘wolf’:
v l "7 k
v "7 l k
Everyalignedsegmentpairisconvertedtoasingle
token by adding the symbol ‘/’ between the seg-
ments and using the symbol ‘-’ to indicate a gap.
Thisyieldsthe followingtransformedstring:
v/v l/"7 "7/l k/k
Suppose another algorithm generates the follow-
ing alignment(notdetectingthe swap):
v l "7 k
v "7 l k
Thetransformedstringfor this alignmentis:
v/v l/"7/"7 -/l k/k
To evaluate this alignment,we align this string to
thetransformedstringofthegoldstandardandob-
taina Levenshteindistanceof 3:
v/v l/"7 "7/l k/k
v/v l/"7/"7 -/l k/k
1 1 1
By repeatingthisprocedurefor all alignmentsand
summing up all distances, we obtain total dis-
tancesbetweenthe gold standardand every align-
mentalgorithm. Algorithmswhichgeneratehigh-
quality alignmentswill have a low distance from
thegoldstandard,whilethedistancewillbehigher
for algorithms which generate low-quality align-
ments.
4.1 Standardization
The gold standard contains a number of align-
ments which have alternative equivalent align-
ments, most notably an alignment containing an
insertion followed by a deletion (which is equal
to the deletion followed by the insertion), or an
alignmentcontaininga syllabicconsonantsuchas
["ô"], which in fact matches both a vowel and a
neighboringr-like consonantand can thereforebe
alignedwith eitherthe vowel or the consonant. In
order to prevent punishing the algorithms which
do not match the exact gold standard in these
cases, the alignmentsof the gold standardand all
alignmentalgorithmsare transformedto one stan-
dardformin all relevant cases.
For example, considerthe correct alignmentof
[v"iA] and [v"ij], two Bulgariandialectalvariations
of the Englishpluralpronoun‘you’:
v "i A
v "i j
Of course,this alignmentis as reasonableas:
v "i A
v "i j
To avoid punishing the first, we transform all in-
sertions followed by deletions to deletions fol-
lowed by insertions, effectively scoring the two
alignmentsthe same.
For the syllabic consonants we transform all
alignments to a form in which the syllabic con-
sonant is followed by a gap and not vice versa.
For instance,aligning[v"ô"x] with[v"Arx] (English:
‘peak’)yields:
v "ô" x
v "A r x
Whichis transformedto the equivalentalignment:
v "ô" x
v "A r x
5 Results
We will report both quantitative results using the
evaluation method discussed in the previous sec-
tion, as well as the qualitative results, where we
focusoncharacteristicerrorsofthedifferentalign-
mentalgorithms.
5.1 Quantitative
results
Becausethere are two algorithmswhich use gen-
eratedsegmentdistances(or probabilities)in their
alignments,we first check if these values are sen-
sibleand comparableto eachother.
31
5.1.1 Comparisonof
segmentdistances
With respect to the PMI results (convergence
was reached after 7 iterations, taking less than
5 CPU
minutes), we indeed found sensible re-
sults: the averagedistancebetweenidenticalsym-
bols was significantlylower than the distancebe-
tween pairs of different vowels and consonants
(t < −13,p < .001). Because we did not allow
vowel-consonants alignments in the Levenshtein
algorithm,noPMIvaluesweregeneratedforthose
segmentpairs.
Just as Wieling et al. (2007), we found sen-
sible PHMM substitution probabilities (conver-
gence was reached after 1675 iterations, taking
about 7 CPU hours): the probabilityof matching
a symbol with itself was significantlyhigher than
the probability of substituting one vowel for an-
other(similarlyforconsonants),whichinturnwas
higherthanthe probabilityof substitutinga vowel
witha consonant(allt’s > 9,p < .001).
To allow a fair comparisonbetweenthePHMM
probabilities and the PMI distances, we trans-
formed the PHMM probabilities to log-odds
scores (i.e. dividing the probability by the rela-
tive frequency of the segments and subsequently
taking the log). Because the residues after the
linear regression between the PHMM similarities
and PMI distanceswere not normallydistributed,
we used Spearman’s rank correlation coefficient
to assess the relationship between the two vari-
ables. We found a highly significant Spearman’s
ρ = −.965(p < .001), which means that the re-
lationshipbetweenthePHMMsimilaritiesandthe
PMIdistancesis verystrong. Whenlookingat the
insertionsanddeletionswealsofoundasignificant
relationship:Spearman’s ρ = −.736(p < .001).
5.1.2 Evaluationagainstthegoldstandard
Usingtheproceduredescribedinsection4,wecal-
culated the distances between the gold standard
and the alignment algorithms. Besides reporting
the totalnumberof misalignedtokens,we alsodi-
vided this number by the total number of aligned
segments in the gold standard (about 16 million)
to get an idea of the error rate. Note that the error
rateis0intheperfectcase,butmightrisetonearly
2 in theworstcase,whichis an alignmentconsist-
ing of only insertionsand deletionsand therefore
up to twice as long as the alignmentsin the gold
standard. Finally, we also report the total number
of alignments(word pairs) which are not exactly
equalto the alignmentsof the goldstandard.
The results are shown in Table 1. We can
clearly see that all algorithms beat the baseline
and align about 95% of all string pairs correctly.
Whilethe LevenshteinPMIalgorithmalignsmost
strings perfectly, it misaligns slightly more indi-
vidual segments than the PHMM and the Leven-
shtein algorithm with the swap operation (i.e. it
makes more segment alignment errors per word
pair). The VC-sensitive Levenshtein algorithm
in general performs slightly worse than the other
threealgorithms.
5.2 Qualitative
results
Let us first note that it is almost impossible for
anyalgorithmtoachieveaperfectoverlapwiththe
goldstandard,becausethe goldstandardwas gen-
erated from multiplealignmentsand thereforein-
corporatesotherconstraints.For example,while a
certainpairwisealignmentcouldappearcorrectin
aligning two consonants, the multiple alignment
could show contextual support (from pronuncia-
tions in other varieties) for separating the conso-
nants. Consequently, all algorithmsdiscussedbe-
low make errorsof this kind.
In general, the specific errors of the VC-
sensitive Levenshtein algorithm can be separated
into three cases. First, as we illustratedin section
3.3, the VC-sensitive Levenshtein algorithm has
no way to distinguish between aligning a conso-
nant with one of two neighboringconsonantsand
sometimeschoosesthe wrongone (thisalso holds
for vowels). Second,it does not allow alignments
of vowels with consonants and therefore cannot
detectcorrectvowel-consonantalignmentssuchas
correspondencesof [u] with [v] initially. Third,
for the same reason the VC-sensitive Levenshtein
algorithm is also not able to detect metathesis of
vowelswithconsonants.
The misalignments of the Levenshtein algo-
rithm with the swap-operationcan also be split in
three cases. It suffers from the same two prob-
lemsastheVC-sensitive Levenshteinalgorithmin
choosingtoaligna consonantincorrectlywithone
of two neighboringconsonantsand not beingable
to align a vowel with a consonant. Third, even
thoughit alignssome of the metathesiscases cor-
rectly, it alsomakessomeerrorsbyincorrectlyap-
plyingthe swap-operation.For example,consider
the alignment of [s"irjIni] and [s"irjnI], two Bul-
gariandialectalvariationsof the word ‘cheese’,in
whichthe swap-operationis applied:
32
Algorithm Misalignedsegments(error rate) Incorrectalignments(%)
Baseline(Hammingalgorithm) 2510094 (0.1579) 726844 (20.92%)
VC-sens.Levenshteinalgorithm 490703 (0.0309) 191674 (5.52%)
LevenshteinPMIalgorithm 399216 (0.0251) 156440 (4.50%)
Levenshteinswap algorithm 392345 (0.0247) 161834 (4.66%)
Pair HiddenMarkov Model 362423 (0.0228) 160896 (4.63%)
Table1: Comparisonto goldstandardalignments.All differencesare significant(p < 0.01).
s "i rj I n i
s "i rj n I
0 0 0 >< 1 1
However, thetwo I’sarenotrelatedandshouldnot
beswapped,whichisreflectedinthegoldstandard
alignment:
s "i rj I n i
s "i rj n I
0 0 0 1 0 1
The incorrect alignments of the Levenshtein
algorithm with the PMI-generated segment dis-
tances are mainly caused by its inability to align
vowels with consonantsand therefore, just as the
VC-sensitive Levenshteinalgorithm,it fails to de-
tect metathesis. On the other hand, using seg-
mentdistancesoftensolves the problemof select-
ing whichof two plausibleneighborsa consonant
shouldbe alignedwith.
Because the PHMM employs segment substi-
tution probabilities,it also often solves the prob-
lem of aligning a consonantto one of two neigh-
bors. In addition, the PHMM often correctly
aligns metathesisinvolving equal as well as sim-
ilar symbols,even realizingan improvement over
the Levenshtein swap algorithm. Unfortunately,
many wrong alignments of the PHMM are also
caused by allowing vowel-consonantalignments.
Since the PHMM does not take context into ac-
count,it also alignsvowels and consonantswhich
oftenplaya rolein metathesiswhennometathesis
is involved.
6 Discussion
This study provides an alternative evaluation of
stringdistancealgorithmsby focusingon theiref-
fectiveness in aligning segments. We proposed,
implemented,and tested the new procedure on a
substantialbodyof data. Thisprovidesa new per-
spective on the quality of distance and alignment
algorithmsas they have beenusedin dialectology,
where aggregate comparisons had been at times
frustratinglyinconclusive.
In addition, we introduced the PMI weight-
ing within the Levenshtein algorithm as a sim-
ple means of obtaining segment distances, and
showed that it improves on the popular Leven-
shtein algorithm with respect to alignment accu-
racy.
WhiletheresultsindicatedthatthePHMMmis-
aligned the fewest segments, training the PHMM
is a lengthy process lasting several hours. Con-
sidering that the Levenshtein algorithm with the
swap operation and the Levenshtein algorithm
with the PMI-generated segment distances are
much quicker to (train and) apply, and that they
have onlyslightlylowerperformancewithrespect
to the segment alignments,we actuallyprefer us-
ing those methods. Anotherargument in favor of
using one of these Levenshtein algorithmsis that
it is a prioriclearerwhat type of alignmenterrors
to expect from them, while the PHMMalgorithm
is lesspredictableand harderto comprehend.
While our results are an indicationof the good
qualityoftheevaluatedalgorithms,weonlyevalu-
atedthe algorithmson a singledatasetfor whicha
goldstandardwasavailable. Ideallywewouldlike
to verify theseresultson otherdatasets,for which
gold standards consisting of multiple or pairwise
alignmentsare available.
Acknowledgements
We aregratefulto PeterKleiweg forextendingthe
LevenshteinalgorithmintheL04packagewiththe
swap-operation. We also thank Greg Kondrakfor
providingtheoriginalsourcecodeofthePairHid-
den Markov Models. Finally, we thank Therese
Leinonenand SebastianK¨urschnerof the Univer-
sityof GroningenandEsteve Valls i Alechaof the
University of Barcelonafor their useful feedback
on our ideas.
33
References
LeonardE. Baum,TedPetrie,GeorgeSoules,andNor-
manWeiss. 1970. Amaximizationtechniqueoccur-
ring in the statistical analysis of probabilisticfunc-
tions of Markov Chains. The Annalsof Mathemati-
cal Statistics, 41(1):164–171.
Kenneth W. Church and Patrick Hanks. 1990. Word
associationnorms,mutualinformation,andlexicog-
raphy. ComputationalLinguistics, 16(1):22–29.
Fred J. Damerau. 1964. A techniquefor computerde-
tectionand correction of spellingerrors. Communi-
cationsof the ACM, 7:171–176.
Richard Durbin, Sean R. Eddy, Anders Krogh, and
Graeme Mitchison. 1998. Biological Sequence
Analysis: ProbabilisticModelsof Proteins and Nu-
cleic Acids. Cambridge University Press, United
Kingdom,July.
Richard Hamming. 1950. Error detecting and error
correcting codes. Bell System Technical Journal,
29:147–160.
Wilbert Heeringa, Peter Kleiweg, CharlotteGooskens,
and John Nerbonne. 2006. Evaluationof stringdis-
tancealgorithmsfordialectology. InJohnNerbonne
and Erhard Hinrichs, editors, Linguistic Distances,
pages51–62,Shroudsburg, PA. ACL.
WilbertHeeringa. 2004. MeasuringDialectPronunci-
ationDifferencesusingLevenshteinDistance. Ph.D.
thesis,RijksuniversiteitGroningen.
Brett Kessler. 1995. Computational dialectology in
Irish Gaelic. In Proceedings of the seventh con-
ference on European chapter of the Associationfor
ComputationalLinguistics, pages 60–66, San Fran-
cisco,CA,USA.Morgan Kaufmann PublishersInc.
Grzegorz Kondrakand BonnieDorr. 2003. Identifica-
tion of ConfusableDrug Names: A New Approach
and EvaluationMethodology. ArtificialIntelligence
in Medicine, 36:273–291.
Grzegorz Kondrak. 2003. Phonetic Alignment and
Similarity. Computers and the Humanities, 37:273–
291.
VladimirLevenshtein. 1965. Binarycodes capableof
correctingdeletions, insertionsand reversals. Dok-
ladyAkademiiNaukSSSR, 163:845–848.
Wesley Mackay and Grzegorz Kondrak. 2005. Com-
putingwordsimilarityandidentifyingcognateswith
Pair Hidden Markov Models. In Proceedings of
the 9th Conference on ComputationalNatural Lan-
guage Learning (CoNLL), pages 40–47, Morris-
town,NJ,USA.AssociationforComputationalLin-
guistics.
JohnNerbonneandPeterKleiweg. 2007. Towarda di-
alectologicalyardstick. JournalofQuantitativeLin-
guistics, 14:148–167.
Bruno Pouliquen. 2008. Similarity of names across
scripts: Edit distance using learned costs of N-
Grams. In Bent Nordstr¨om and Aarne Ranta, ed-
itors, Proceedings of the 6th international Con-
ference on Natural Language Processing (Go-
Tal’2008), volume5221,pages405–416.
Jelena Proki´c, Martijn Wieling, and John Nerbonne.
2009. Multiple sequence alignmentsin linguistics.
InPiroskaLendvaiandLarsBorin,editors,Proceed-
ingsoftheEACL2009WorkshoponLanguageTech-
nology and Resources for Cultural Heritage, Social
Sciences,Humanities,and Education.
Lawrence R. Rabiner. 1989. A tutorial on Hidden
Markov Modelsand selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257–
286.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learn-
ing string-editdistance. IEEE Transactionson Pat-
tern Analysis and Machine Intelligence, 20:522–
532.
Robert Wagner and Roy Lowrance. 1975. An exten-
sionofthestring-to-stringcorrectionproblem. Jour-
nal of the ACM, 22(2):177–183.
Martijn Wieling and John Nerbonne. 2007. Dialect
pronunciationcomparisonand spoken word recog-
nition. In Petya Osenova, editor, Proceedings of
theRANLPWorkshoponComputationalPhonology,
pages71–78.
Martijn Wieling, Therese Leinonen, and John Ner-
bonne. 2007. Inducing sound segment differences
usingPairHiddenMarkov Models. In MarkEllison
JohnNerbonneandGreg Kondrak,editors,Comput-
ing and Historical Phonology: 9th Meeting of the
ACLSpecialInterestGroupforComputationalMor-
phology and Phonology, pages48–56.
34

